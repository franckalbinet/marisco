# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/handlers/helcom.ipynb.

# %% auto 0
__all__ = ['fname_in', 'fname_out_nc', 'fname_out_csv', 'zotero_key', 'ref_id', 'varnames_lut_updates', 'coi_val',
           'coi_units_unc', 'unmatched_fixes_biota_species', 'get_maris_species', 'unmatched_fixes_biota_tissues',
           'get_maris_bodypart', 'unmatched_fixes_sediments', 'get_maris_sediments', 'renaming_unit_rules', 'coi_dl',
           'coi_coordinates', 'kw', 'load_data', 'CompareDfsAndTfmCB', 'GetSampleTypeCB', 'LowerStripRdnNameCB',
           'get_unique_nuclides', 'get_varnames_lut', 'get_nuc_id_lut', 'RemapRdnNameCB', 'ParseTimeCB',
           'SanitizeValue', 'unc_rel2stan', 'LookupBiotaBodyPartCB', 'get_biogroup_lut', 'LookupBiogroupCB',
           'get_taxon_info_lut', 'LookupTaxonInformationCB', 'preprocess_sedi', 'LookupSedimentCB', 'LookupUnitCB',
           'get_detectionlimit_lut', 'LookupDetectionLimitCB', 'RemapDataProviderSampleIdCB', 'get_filtered_lut',
           'LookupFiltCB', 'get_helcom_method_desc', 'RecordMeasurementNoteCB', 'RemapStationIdCB',
           'RemapSedSliceTopBottomCB', 'LookupDryWetRatio', 'ddmmmm2dddddd', 'FormatCoordinates', 'get_renaming_rules',
           'SelectAndRenameColumnCB', 'ReshapeLongToWide', 'get_attrs', 'enums_xtra', 'encode', 'encode_or']

# %% ../../nbs/handlers/helcom.ipynb 12
import pandas as pd # Python package that provides fast, flexible, and expressive data structures.
import numpy as np
from tqdm import tqdm # Python Progress Bar Library
from functools import partial # Function which Return a new partial object which when called will behave like func called with the positional arguments args and keyword arguments keywords
import fastcore.all as fc # package that brings fastcore functionality, see https://fastcore.fast.ai/.
from pathlib import Path # This module offers classes representing filesystem paths
from dataclasses import asdict
from typing import List, Dict, Callable,  Tuple
from math import modf
from cftime import num2pydate 
from collections import OrderedDict


from ..utils import (has_valid_varname, match_worms, match_maris_lut, Match)
from ..callbacks import (Callback, Transformer, EncodeTimeCB, SanitizeLonLatCB)
from ..metadata import (GlobAttrsFeeder, BboxCB, DepthRangeCB, TimeRangeCB, ZoteroCB, KeyValuePairCB)
from ..configs import (nuc_lut_path, nc_tpl_path, cfg, cache_path, cdl_cfg, Enums, lut_path,
                             species_lut_path, sediments_lut_path, bodyparts_lut_path, 
                             detection_limit_lut_path, filtered_lut_path, area_lut_path)
from ..serializers import NetCDFEncoder,  OpenRefineCsvEncoder
import warnings

# %% ../../nbs/handlers/helcom.ipynb 17
fname_in = '../../_data/accdb/mors/csv'
fname_out_nc = '../../_data/output/100-HELCOM-MORS-2024.nc'
fname_out_csv = '../../_data/output/100-HELCOM-MORS-2024.csv'
zotero_key ='26VMZZ2Q'
ref_id = 100

# %% ../../nbs/handlers/helcom.ipynb 21
def load_data(src_dir: str, smp_types: List[str] = ['SEA', 'SED', 'BIO']) -> Dict[str, pd.DataFrame]:
    """
    Load HELCOM data and return the data in a dictionary of dataframes with the dictionary key as the sample type.
    
    Args:
    src_dir (str): The directory where the source CSV files are located.
    smp_types (List[str]): A list of sample types to load. Defaults to ['SEA', 'SED', 'BIO'].
    
    Returns:
    Dict[str, pd.DataFrame]: A dictionary with sample types as keys and their corresponding dataframes as values.
    """
    dfs = {}
    lut_smp_type = {'SEA': 'seawater', 'SED': 'sediment', 'BIO': 'biota'}
    
    for smp_type in smp_types:
        fname_meas = smp_type + '02.csv'  # Measurement (i.e., radioactivity) information
        fname_smp = smp_type + '01.csv'  # Sample information
        
        df_meas = pd.read_csv(Path(src_dir) / fname_meas)
        df_smp = pd.read_csv(Path(src_dir) / fname_smp)
        
        df = pd.merge(df_meas, df_smp, on='KEY', how='left')
        dfs[lut_smp_type[smp_type]] = df
    
    return dfs

# %% ../../nbs/handlers/helcom.ipynb 36
import pandas as pd
import numpy as np
from typing import List, Dict
from ..callbacks import Callback, Transformer

class CompareDfsAndTfmCB(Callback):
    "Create a dataframe of dropped data. Data included in the `dfs` not in the `tfm`."
    
    def __init__(self, dfs: Dict[str, pd.DataFrame]):
        fc.store_attr()
    
    def __call__(self, tfm: Transformer) -> None:
        self._initialize_tfm_attributes(tfm)
        for grp in tfm.dfs.keys():
            dropped_df = self._get_dropped_data(grp, tfm)
            tfm.dfs_dropped[grp] = dropped_df
            tfm.compare_stats[grp] = self._compute_stats(grp, tfm)

    def _initialize_tfm_attributes(self, tfm: Transformer) -> None:
        """Initialize attributes in `tfm`."""
        tfm.dfs_dropped = {}
        tfm.compare_stats = {}

    def _get_dropped_data(self, grp: str, tfm: Transformer) -> pd.DataFrame:
        """
        Get the data that is present in `dfs` but not in `tfm.dfs`.
        
        Args:
        grp (str): The group key.
        tfm (Transformer): The transformation object containing `dfs`.
        
        Returns:
        pd.DataFrame: Dataframe with dropped rows.
        """
        index_diff = self.dfs[grp].index.difference(tfm.dfs[grp].index)
        return self.dfs[grp].loc[index_diff]
    
    def _compute_stats(self, grp: str, tfm: Transformer) -> Dict[str, int]:
        """
        Compute comparison statistics between `dfs` and `tfm.dfs`.
        
        Args:
        grp (str): The group key.
        tfm (Transformer): The transformation object containing `dfs`.
        
        Returns:
        Dict[str, int]: Dictionary with comparison statistics.
        """
        return {
            'Number of rows in dfs': len(self.dfs[grp].index),
            'Number of rows in tfm.dfs': len(tfm.dfs[grp].index),
            'Number of dropped rows': len(tfm.dfs_dropped[grp].index),
            'Number of rows in tfm.dfs + Number of dropped rows': len(tfm.dfs[grp].index) + len(tfm.dfs_dropped[grp].index)
        }


# %% ../../nbs/handlers/helcom.ipynb 42
class GetSampleTypeCB(Callback):
    """Set the 'Sample type' column in the DataFrames based on a lookup table."""
    
    def __init__(self, type_lut=None):
        """
        Initialize the GetSampleTypeCB callback.

        Args:
            type_lut (dict, optional): A lookup table to map group names to sample types.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Apply the sample type lookup to DataFrames in the transformer.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for key in tfm.dfs.keys():
            df = tfm.dfs[key]
            
            # Determine the sample type
            sample_type = self._get_sample_type(key)
            
            # Set the 'Sample type' column
            df['samptype_id'] = sample_type

    def _get_sample_type(self, group_name):
        """
        Determine the sample type for a given group name using the lookup table.

        Args:
            group_name (str): The name of the group.

        Returns:
            str: The sample type.
        """
        
        # Return the sample type from the lookup table
        return self.type_lut[group_name.upper()]


# %% ../../nbs/handlers/helcom.ipynb 51
class LowerStripRdnNameCB(Callback):
    """Convert nuclide names to lowercase and strip any trailing spaces."""

    def __call__(self, tfm):
        for key in tfm.dfs.keys():
            self._process_nuclide_column(tfm.dfs[key])

    def _process_nuclide_column(self, df):
        """Apply transformation to the 'NUCLIDE' column of the given DataFrame."""
        df['NUCLIDE'] = df['NUCLIDE'].apply(self._transform_nuclide)

    def _transform_nuclide(self, nuclide):
        """Convert nuclide name to lowercase and strip trailing spaces."""
        return nuclide.lower().strip()


# %% ../../nbs/handlers/helcom.ipynb 56
def get_unique_nuclides(dfs: Dict[str, pd.DataFrame]) -> List[str]:
    """
    Get a list of unique radionuclide types measured across samples.

    Args:
        dfs (Dict[str, pd.DataFrame]): A dictionary where keys are sample names and values are DataFrames.

    Returns:
        List[str]: A list of unique radionuclide types.
    """
    # Collect unique nuclide names from all DataFrames
    nuclides = set()
    for df in dfs.values():
        nuclides.update(df['NUCLIDE'].unique())

    return list(nuclides)

# %% ../../nbs/handlers/helcom.ipynb 59
varnames_lut_updates = {
    'k-40': 'k40',
    'cm243244': 'cm243_244_tot',
    'cs134137': 'cs134_137_tot',
    'pu239240': 'pu239_240_tot',
    'pu238240': 'pu238_240_tot',
    'cs138': 'cs137',
    'cs139': 'cs137',
    'cs140': 'cs137',
    'cs141': 'cs137',
    'cs142': 'cs137',
    'cs143': 'cs137',
    'cs144': 'cs137',
    'cs145': 'cs137',
    'cs146': 'cs137'}

# %% ../../nbs/handlers/helcom.ipynb 61
def get_varnames_lut(
    dfs: Dict[str, pd.DataFrame], 
    lut: Dict[str, str] = varnames_lut_updates
) -> Dict[str, str]:
    """
    Generate a lookup table for radionuclide names, updating with provided mappings.

    Args:
        dfs (Dict[str, pd.DataFrame]): A dictionary where keys are sample names and values are DataFrames.
        lut (Dict[str, str], optional): A dictionary with additional mappings to update the lookup table.

    Returns:
        Dict[str, str]: A dictionary mapping radionuclide names to their corresponding names.
    """
    # Generate a base lookup table from unique nuclide names
    unique_nuclides = get_unique_nuclides(dfs)
    base_lut = {name: name for name in unique_nuclides}

    # Update the base lookup table with additional mappings
    base_lut.update(lut)
    
    return base_lut

# %% ../../nbs/handlers/helcom.ipynb 63
def get_nuc_id_lut():
    df = pd.read_excel(nuc_lut_path(), usecols=['nc_name','nuclide_id'])
    return df.set_index('nc_name').to_dict()['nuclide_id']

# %% ../../nbs/handlers/helcom.ipynb 65
class RemapRdnNameCB(Callback):
    """Remap and standardize radionuclide names to MARIS radionuclide names and define nuclide ids."""
    
    def __init__(self, 
                 fn_lut: Callable[[Dict[str, pd.DataFrame]], Dict[str, str]] = partial(get_varnames_lut, lut=varnames_lut_updates),
                 nuc_id_lut: Callable[[], Dict[str, str]] = get_nuc_id_lut):
        """
        Initialize the RemapRdnNameCB with functions to generate lookup tables for radionuclide names 
        and nuclide IDs.

        Args:
            fn_lut (Callable, optional): A function that takes a dictionary of DataFrames and returns a lookup table 
                                         for remapping radionuclide names.
            nuc_id_lut (Callable, optional): A function that returns a lookup table for nuclide IDs.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """Apply lookup tables to remap radionuclide names and obtain nuclide IDs in DataFrames.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut(tfm.dfs)
        nuc_id_lut = self.nuc_id_lut()
        
        for grp in tfm.dfs:
            df = tfm.dfs[grp]
            self._remap_nuclide_names(df, lut)
            self._apply_nuclide_ids(df, nuc_id_lut)

    def _remap_nuclide_names(self, df: pd.DataFrame, lut: Dict[str, str]):
        """
        Remap radionuclide names in the 'NUCLIDE' column of the DataFrame using the provided lookup table.

        Args:
            df (pd.DataFrame): DataFrame containing the 'NUCLIDE' column.
            lut (Dict[str, str]): Lookup table for remapping radionuclide names.
        """
        if 'NUCLIDE' in df.columns:
            df['NUCLIDE'] = df['NUCLIDE'].replace(lut)
        else:
            print(f"No 'NUCLIDE' column found in DataFrame of group {df.name}")

    def _apply_nuclide_ids(self, df: pd.DataFrame, nuc_id_lut: Dict[str, str]):
        """
        Apply nuclide IDs to the 'NUCLIDE' column using the provided nuclide ID lookup table.

        Args:
            df (pd.DataFrame): DataFrame containing the 'NUCLIDE' column.
            nuc_id_lut (Dict[str, str]): Lookup table for nuclide IDs.
        """
        if 'NUCLIDE' in df.columns:
            df['nuclide_id'] = df['NUCLIDE'].map(nuc_id_lut)
        else:
            print(f"No 'NUCLIDE' column found in DataFrame of group {df.name}")


# %% ../../nbs/handlers/helcom.ipynb 76
class ParseTimeCB(Callback):
    def __init__(self):
        fc.store_attr()
            
        
    def __call__(self, tfm):
        for grp in tfm.dfs.keys():
            df = tfm.dfs[grp]
            self._process_dates(df)
            self._define_beg_period(df)

    def _process_dates(self, df: pd.DataFrame):
        """
        Process and correct date and time information in the DataFrame.

        Args:
            df (pd.DataFrame): DataFrame containing the 'DATE', 'YEAR', 'MONTH', and 'DAY' columns.
        """
        # get 'time' from 'DATE' column
        df['time'] = pd.to_datetime(df['DATE'], format='%m/%d/%y %H:%M:%S')
        # if 'DATE' column is nan, get 'time' from 'YEAR','MONTH' and 'DAY' column. 
        # if 'DAY' or 'MONTH' is 0 then set it to 1. 
        df.loc[df["DAY"] == 0, "DAY"] = 1
        df.loc[df["MONTH"] == 0, "MONTH"] = 1
        
        # if 'DAY' and 'MONTH' is nan but YEAR is not nan then set 'DAY' and 'MONTH' both to 1. 
        condition = (df["DAY"].isna()) & (df["MONTH"].isna()) & (df["YEAR"].notna())
        df.loc[condition, "DAY"] = 1
        df.loc[condition, "MONTH"] = 1
        
        condition = df['DATE'].isna() # if 'DATE' is nan. 
        df['time']  = np.where(condition,
                                            # 'coerce', then invalid parsing will be set as NaT. NaT will result if the number of days are not valid for the month.
                                        pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], format='%y%m%d', errors='coerce'),  
                                        pd.to_datetime(df['DATE'], format='%m/%d/%y %H:%M:%S'))
        
    def _define_beg_period(self, df: pd.DataFrame):
        """
        Create a standardized date representation for Open Refine.
        
        Args:
            df (pd.DataFrame): DataFrame containing the 'time' column.
        """
        df['begperiod'] = df['time']

# %% ../../nbs/handlers/helcom.ipynb 90
# Columns of interest
coi_val = {'seawater' : { 'val' : 'VALUE_Bq/m³'},
                 'biota':  {'val' : 'VALUE_Bq/kg'},
                 'sediment': { 'val' : 'VALUE_Bq/kg'}}

# %% ../../nbs/handlers/helcom.ipynb 91
class SanitizeValue(Callback):
    "Sanitize value by removing blank entries and ensuring the 'value' column is retained."

    def __init__(self, coi: dict):
        """
        Initialize the SanitizeValue callback.

        Args:
            coi (dict): Dictionary containing column names for values based on group.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Sanitize the DataFrames in the transformer by removing rows with blank values in specified columns.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            self._sanitize_dataframe(tfm.dfs[grp], grp)

    def _sanitize_dataframe(self, df: pd.DataFrame, grp: str):
        """
        Remove rows where specified value columns are blank and ensure the 'value' column is included.

        Args:
            df (pd.DataFrame): DataFrame to sanitize.
            grp (str): Group name to determine column names.
        """
        value_col = self.coi.get(grp, {}).get('val')
        if value_col and value_col in df.columns:
            df.dropna(subset=[value_col], inplace=True)
            # Ensure 'value' column is retained
            if 'value' not in df.columns:
                df['value'] = df[value_col]

# %% ../../nbs/handlers/helcom.ipynb 98
# Make measurement and uncertainty units consistent
def unc_rel2stan(df: pd.DataFrame, meas_col: str, unc_col: str) -> pd.Series:
    """
    Convert relative uncertainty to absolute uncertainty.

    Args:
        df (pd.DataFrame): DataFrame containing measurement and uncertainty columns.
        meas_col (str): Name of the column with measurement values.
        unc_col (str): Name of the column with relative uncertainty values (percentages).

    Returns:
        pd.Series: Series with calculated absolute uncertainties.
    """
    return df.apply(lambda row: row[unc_col] * row[meas_col] / 100, axis=1)


# %% ../../nbs/handlers/helcom.ipynb 100
# Columns of interest
coi_units_unc = [('seawater', 'VALUE_Bq/m³', 'ERROR%_m³'),
                 ('biota', 'VALUE_Bq/kg', 'ERROR%'),
                 ('sediment', 'VALUE_Bq/kg', 'ERROR%_kg')]

# %% ../../nbs/handlers/helcom.ipynb 116
unmatched_fixes_biota_species = {
    'CARD EDU': 'Cerastoderma edule',
    'LAMI SAC': 'Saccharina latissima',
    'PSET MAX': 'Scophthalmus maximus',
    'STIZ LUC': 'Sander luciopercas'}

# %% ../../nbs/handlers/helcom.ipynb 125
get_maris_species = partial(get_maris_lut,
                            fname_in, fname_cache='species_helcom.pkl', 
                            data_provider_lut='RUBIN_NAME.csv',
                            data_provider_id_col='RUBIN',
                            data_provider_name_col='SCIENTIFIC NAME',
                            maris_lut=species_lut_path,
                            maris_id='species_id',
                            maris_name='species',
                            unmatched_fixes=unmatched_fixes_biota_species,
                            as_dataframe=False,
                            overwrite=False)

# %% ../../nbs/handlers/helcom.ipynb 135
unmatched_fixes_biota_tissues = {
    3: 'Whole animal eviscerated without head',
    12: 'Viscera',
    8: 'Skin'}

# %% ../../nbs/handlers/helcom.ipynb 139
class LookupBiotaBodyPartCB(Callback):
    """Update bodypart id based on MARIS body part LUT (dbo_bodypar.xlsx)"""
    def __init__(self, fn_lut: Callable[[], dict]):
        """
        Initialize the LookupBiotaBodyPartCB with a function to generate the lookup table.

        Args:
            fn_lut (Callable[[], dict]): Function that returns the lookup table dictionary.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Remap biota body parts in the DataFrame using the lookup table and print unmatched TISSUE values.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        tfm.dfs['biota']['body_part'] = tfm.dfs['biota']['TISSUE'].apply(lambda x: self._get_body_part(x, lut))

    def _get_body_part(self, tissue_value: str, lut: dict):
        """
        Get the matched_id from the lookup table and print TISSUE if the matched_id is -1.

        Args:
            tissue_value (str): The TISSUE value from the DataFrame.
            lut (dict): The lookup table dictionary.

        Returns:
            The matched_id from the lookup table.
        """
        match = lut.get(tissue_value, Match(-1, None, None, None))
        if match.matched_id == -1:
            self.print_unmatched_tissue(tissue_value)
        return match.matched_id

    def print_unmatched_tissue(self, tissue_value: str):
        """
        Print the TISSUE value if the matched_id is -1.

        Args:
            tissue_value (str): The TISSUE value from the DataFrame.
        """
        print(f"Unmatched TISSUE: {tissue_value}")


# %% ../../nbs/handlers/helcom.ipynb 141
get_maris_bodypart = partial(get_maris_lut,
                             fname_in,
                             fname_cache='tissues_helcom.pkl', 
                             data_provider_lut='TISSUE.csv',
                             data_provider_id_col='TISSUE',
                             data_provider_name_col='TISSUE_DESCRIPTION',
                             maris_lut=bodyparts_lut_path,
                             maris_id='bodypar_id',
                             maris_name='bodypar',
                             unmatched_fixes=unmatched_fixes_biota_tissues)

# %% ../../nbs/handlers/helcom.ipynb 149
def get_biogroup_lut(maris_lut: str) -> dict:
    """
    Retrieve a lookup table for biogroup ids from a MARIS lookup table.

    Args:
        maris_lut (str): Path to the MARIS lookup table (Excel file).

    Returns:
        dict: A dictionary mapping species_id to biogroup_id.
    """
    species = pd.read_excel(maris_lut)
    return species[['species_id', 'biogroup_id']].set_index('species_id').to_dict()['biogroup_id']


# %% ../../nbs/handlers/helcom.ipynb 151
class LookupBiogroupCB(Callback):
    """Update biogroup id based on MARIS species LUT (dbo_species.xlsx)"""
    def __init__(self, fn_lut: Callable[[], dict]):
        """
        Initialize the LookupBiogroupCB with a function to generate the lookup table.

        Args:
            fn_lut (Callable[[], dict]): Function that returns the lookup table dictionary.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Update the 'bio_group' column in the DataFrame using the lookup table and print unmatched species values.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        tfm.dfs['biota']['bio_group'] = tfm.dfs['biota']['species'].apply(lambda x: self._get_biogroup(x, lut))

    def _get_biogroup(self, species_value: str, lut: dict) -> int:
        """
        Get the biogroup id from the lookup table and print species if the biogroup id is not found.

        Args:
            species_value (str): The species value from the DataFrame.
            lut (dict): The lookup table dictionary.

        Returns:
            int: The biogroup id from the lookup table.
        """
        biogroup_id = lut.get(species_value, -1)
        if biogroup_id == -1:
            self.print_unmatched_species(species_value)
        return biogroup_id

    def print_unmatched_species(self, species_value: str):
        """
        Print the species value if the biogroup id is not found.

        Args:
            species_value (str): The species value from the DataFrame.
        """
        print(f"Unmatched species: {species_value}")


# %% ../../nbs/handlers/helcom.ipynb 159
def get_taxon_info_lut(maris_lut: str) -> dict:
    """
    Retrieve a lookup table for Taxonname from a MARIS lookup table.

    Args:
        maris_lut (str): Path to the MARIS lookup table (Excel file).

    Returns:
        dict: A dictionary mapping species_id to biogroup_id.
    """
    species = pd.read_excel(maris_lut)
    return species[['species_id', 'Taxonname', 'Taxonrank','TaxonDB','TaxonDBID','TaxonDBURL']].set_index('species_id').to_dict()

# TODO include Commonname field after next MARIS data reconciling process.

# %% ../../nbs/handlers/helcom.ipynb 160
class LookupTaxonInformationCB(Callback):
    """Update taxon names based on MARIS species LUT (dbo_species.xlsx)."""
    def __init__(self, fn_lut: Callable[[], dict]):
        """
        Initialize the LookupTaxonNameCB with a function to generate the lookup table.

        Args:
            fn_lut (Callable[[], dict]): Function that returns the lookup table dictionary.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Update the 'taxon_name' column in the DataFrame using the lookup table and print unmatched species IDs.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        
        
        self._set_taxon_rep_name(tfm.dfs['biota'])
        tfm.dfs['biota']['Taxonname'] =  tfm.dfs['biota']['species'].apply(lambda x: self._get_name_by_species_id(x, lut['Taxonname']))
        #df['Commonname'] = df['species'].apply(lambda x: self._get_name_by_species_id(x, lut['Commonname']))
        tfm.dfs['biota']['Taxonrank'] =  tfm.dfs['biota']['species'].apply(lambda x: self._get_name_by_species_id(x, lut['Taxonrank']))
        tfm.dfs['biota']['TaxonDB'] =  tfm.dfs['biota']['species'].apply(lambda x: self._get_name_by_species_id(x, lut['TaxonDB']))
        tfm.dfs['biota']['TaxonDBID'] =  tfm.dfs['biota']['species'].apply(lambda x: self._get_name_by_species_id(x, lut['TaxonDBID']))
        tfm.dfs['biota']['TaxonDBURL'] =  tfm.dfs['biota']['species'].apply(lambda x: self._get_name_by_species_id(x, lut['TaxonDBURL']))


    def _set_taxon_rep_name(self, df: pd.DataFrame):
        """
        Remap the 'TaxonRepName' column to the 'RUBIN' column values.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        # Ensure both columns exist before attempting to remap
        if 'RUBIN' in df.columns:
            df['TaxonRepName'] = df['RUBIN']
        else:
            print("Warning: 'RUBIN' column not found in DataFrame.")
            
            

    def _get_name_by_species_id(self, species_id: str, lut: dict) -> str:
        """
        Get the  name from the lookup table and print species ID if the taxon name is not found.

        Args:
            species_id (str): The species ID from the DataFrame.
            lut (dict): The lookup table dictionary.

        Returns:
            str: The name from the lookup table.
        """
        name = lut.get(species_id, 'Unknown')  # Default to 'Unknown' if not found
        if name == 'Unknown':
            print(f"Unmatched species ID: {species_id} for {lut.keys()[0]}")
        return name


# %% ../../nbs/handlers/helcom.ipynb 169
unmatched_fixes_sediments = {
    #np.nan: 'Not applicable',
    -99: '(Not available)'
}

# %% ../../nbs/handlers/helcom.ipynb 172
get_maris_sediments = partial(
    get_maris_lut,
    fname_in, 
    fname_cache='sediments_helcom.pkl', 
    data_provider_lut='SEDIMENT_TYPE.csv',
    data_provider_id_col='SEDI',
    data_provider_name_col='SEDIMENT TYPE',
    maris_lut=sediments_lut_path,
    maris_id='sedtype_id',
    maris_name='sedtype',
    unmatched_fixes=unmatched_fixes_sediments)

# %% ../../nbs/handlers/helcom.ipynb 174
def preprocess_sedi(df, column_name='SEDI'):
    """
    Preprocess the 'SEDI' column in the DataFrame by handling missing values and specific replacements.

    Args:
        df (pd.DataFrame): The DataFrame containing the 'SEDI' column.
        column_name (str): The name of the column to preprocess. Default is 'SEDI'.
    
    Returns:
        pd.DataFrame: The DataFrame with preprocessed 'SEDI' column.
    """
    if column_name in df.columns:
        df[column_name] = df[column_name].fillna(-99).astype('int')
        df[column_name].replace([56, 73], -99, inplace=True)
    return df


# %% ../../nbs/handlers/helcom.ipynb 175
class LookupSedimentCB(Callback):
    """Update sediment id based on MARIS species LUT (dbo_sedtype.xlsx)."""
    def __init__(self, fn_lut: Callable[[], dict], preprocess_fn: Callable[[pd.DataFrame, str], pd.DataFrame] = preprocess_sedi):
        """
        Initialize the LookupSedimentCB with a function to generate the lookup table and a preprocessing function.

        Args:
            fn_lut (Callable[[], dict]): Function that returns the lookup table dictionary.
            preprocess_fn (Callable[[pd.DataFrame, str], pd.DataFrame]): Function to preprocess the sediment DataFrame. Default is preprocess_sedi.
        """
        fc.store_attr()
        self.preprocess_fn = preprocess_fn

    def __call__(self, tfm: 'Transformer'):
        """
        Remap sediment types in the DataFrame using the lookup table and handle specific replacements.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        
        # Set SedRepName
        tfm.dfs['sediment']['SedRepName']  = tfm.dfs['sediment']['SEDI'] 

        # Apply preprocessing to the 'SEDI' column
        tfm.dfs['sediment'] = self.preprocess_fn(tfm.dfs['sediment'])
        
        # Apply the lookup function
        tfm.dfs['sediment']['sed_type'] = tfm.dfs['sediment']['SEDI'].apply(lambda x: self._get_sediment_type(x, lut))

    def _get_sediment_type(self, sedi_value: int, lut: dict):
        """
        Get the matched_id from the lookup table and print SEDI if the matched_id is -1.

        Args:
            sedi_value (int): The SEDI value from the DataFrame.
            lut (dict): The lookup table dictionary.

        Returns:
            The matched_id from the lookup table.
        """
        match = lut.get(sedi_value, Match(-1, None, None, None))
        if match.matched_id == -1:
            self._print_unmatched_sedi(sedi_value)
        return match.matched_id

    def _print_unmatched_sedi(self, sedi_value: int):
        """
        Print the SEDI value if the matched_id is -1.

        Args:
            sedi_value (int): The SEDI value from the DataFrame.
        """
        print(f"Unmatched SEDI: {sedi_value}")


# %% ../../nbs/handlers/helcom.ipynb 183
# Define unit names renaming rules
renaming_unit_rules = {
    'seawater': 1,  # 'Bq/m3'
    'sediment': 4,  # 'Bq/kgd' for sediment
    'biota': {
        'D': 4,  # 'Bq/kgd'
        'W': 5,  # 'Bq/kgw'
        'F': 5   # 'Bq/kgw' (assumed to be 'Fresh', so set to wet)
    }
}


# %% ../../nbs/handlers/helcom.ipynb 185
class LookupUnitCB(Callback):
    """Set the 'unit' id column in the DataFrames based on a lookup table."""
    def __init__(self, renaming_unit_rules=renaming_unit_rules):
        """
        Initialize the LookupUnitCB with unit renaming rules.

        Args:
            renaming_unit_rules (dict): Dictionary containing renaming rules for different unit categories.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply unit renaming rules to DataFrames within the transformer.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs:
            rules = renaming_unit_rules.get(grp)
            if rules is not None:
                # if group tules include a dictionary, apply the dictionay. 
                if isinstance(rules, dict):
                    # Apply rules based on the 'BASIS' column
                    tfm.dfs[grp]['unit'] = tfm.dfs[grp]['BASIS'].apply(lambda x: rules.get(x, 0))
                else:
                    # Apply a single rule to the entire DataFrame
                    tfm.dfs[grp]['unit'] = rules


# %% ../../nbs/handlers/helcom.ipynb 193
# Columns of interest
coi_dl = {'seawater' : { 'val' : 'VALUE_Bq/m³',
                        'unc' : 'ERROR%_m³',
                        'dl' : '< VALUE_Bq/m³'},
                 'biota':  {'val' : 'VALUE_Bq/kg',
                            'unc' : 'ERROR%',
                            'dl' : '< VALUE_Bq/kg'},
                 'sediment': { 'val' : 'VALUE_Bq/kg',
                              'unc' : 'ERROR%_kg',
                              'dl' : '< VALUE_Bq/kg'}}

# %% ../../nbs/handlers/helcom.ipynb 195
def get_detectionlimit_lut():
    df = pd.read_excel(detection_limit_lut_path(), usecols=['name','id'])
    return df.set_index('name').to_dict()['id']

# %% ../../nbs/handlers/helcom.ipynb 197
class LookupDetectionLimitCB(Callback):
    """Remap value type to MARIS format."""

    def __init__(self, 
                 coi=coi_dl,
                 fn_lut=get_detectionlimit_lut):
        """
        Initialize the LookupDetectionLimitCB with configuration options and lookup function.

        Args:
            coi (dict): Configuration options for column names.
            fn_lut (Callable): Function that returns a lookup table.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Remap detection limits in the DataFrames using the lookup table.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        
        for grp in tfm.dfs:
            df = tfm.dfs[grp]
            self._update_detection_limit(df, grp, lut)
    
    def _update_detection_limit(self, df: pd.DataFrame, grp: str, lut: dict):
        """
        Update detection limit column in the DataFrame based on lookup table and rules.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
            grp (str): The group name to get the column configuration.
            lut (dict): The lookup table dictionary.
        """
        detection_col = self.coi[grp]['dl']
        value_col = self.coi[grp]['val']
        uncertainty_col = self.coi[grp]['unc']
        
        # Copy detection limit column
        df['detection_limit'] = df[detection_col]
        
        # Fill values with '=' or 'Not Available'
        condition = ((df[value_col].notna()) & (df[uncertainty_col].notna()) &
                     (~df['detection_limit'].isin(lut.keys())))
        df.loc[condition, 'detection_limit'] = '='
        df.loc[~df['detection_limit'].isin(lut.keys()), 'detection_limit'] = 'Not Available'
        
        # Perform lookup
        df['detection_limit'] = df['detection_limit'].map(lut)


# %% ../../nbs/handlers/helcom.ipynb 205
class RemapDataProviderSampleIdCB(Callback):
    """Remap 'KEY' column to 'samplabcode' in each DataFrame."""

    def __init__(self):
        """
        Initialize the RemapDataProviderSampleIdCB.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Remap 'KEY' column to 'samplabcode' in the DataFrames.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs:
            self._remap_sample_id(tfm.dfs[grp])
    
    def _remap_sample_id(self, df: pd.DataFrame):
        """
        Remap the 'KEY' column to 'samplabcode' in the DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['samplabcode'] = df['KEY']


# %% ../../nbs/handlers/helcom.ipynb 212
def get_filtered_lut() -> dict:
    """
    Retrieve a filtered lookup table from an Excel file.

    Returns:
        dict: A dictionary mapping names to IDs.
    """
    df = pd.read_excel(filtered_lut_path(), usecols=['name', 'id'])
    return df.set_index('name').to_dict()['id']


# %% ../../nbs/handlers/helcom.ipynb 216
class LookupFiltCB(Callback):
    """Lookup FILT value."""
    
    def __init__(self,
                 rules=renaming_rules,
                 fn_lut=get_filtered_lut):
        """
        Initialize the LookupFiltCB with renaming rules and a function for generating the lookup table.

        Args:
            rules (dict): Dictionary mapping FILT codes to their corresponding names.
            fn_lut (Callable[[], dict]): Function that returns the lookup table dictionary.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Update the FILT column in the DataFrames using the renaming rules and lookup table.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        rules = self.rules
        
        for grp in tfm.dfs.keys():
            if "FILT" in tfm.dfs[grp].columns:
                self._update_filt_column(tfm.dfs[grp], rules, lut)

    def _update_filt_column(self, df: pd.DataFrame, rules: dict, lut: dict):
        """
        Update the FILT column based on renaming rules and lookup table.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
            rules (dict): Dictionary mapping FILT codes to their corresponding names.
            lut (dict): Dictionary for lookup values.
        """
        # Fill values that are not in the renaming rules with 'Not available'.
        df['FILT'] = df['FILT'].apply(lambda x: rules.get(x, 'Not available'))
        
        # Perform lookup
        df['FILT'] = df['FILT'].map(lambda x: lut.get(x, 0))


# %% ../../nbs/handlers/helcom.ipynb 224
def get_helcom_method_desc():
    df = pd.read_csv(Path(fname_in) / 'ANALYSIS_METHOD.csv')
    return df.set_index('METHOD').to_dict()['DESCRIPTION']

# %% ../../nbs/handlers/helcom.ipynb 225
class RecordMeasurementNoteCB(Callback):
    """Record measurement notes by adding a 'measurenote' column to DataFrames."""
    
    def __init__(self, fn_lut: Callable[[], Dict[str, str]]):
        """
        Initialize the RecordMeasurementNoteCB with a function to generate the lookup table.

        Args:
            fn_lut (Callable[[], Dict[str, str]]): Function that returns the lookup table dictionary with METHOD as key and DESCRIPTION as value.
        """
        self.fn_lut = fn_lut
        fc.store_attr()

    def __call__(self, tfm: Transformer):
        """
        Apply the lookup table to add 'measurenote' to DataFrames in the transformer.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        for grp, df in tfm.dfs.items():
            if 'METHOD' in df.columns:
                self._add_measurementnote(df, lut)
            else:
                print(f"Warning: 'METHOD' column not found in DataFrame for group '{grp}'")

    def _add_measurementnote(self, df: pd.DataFrame, lut: Dict[str, str]):
        """
        Map 'METHOD' values to 'measurenote' using the provided lookup table.

        Args:
            df (pd.DataFrame): DataFrame containing the 'METHOD' column.
            lut (Dict[str, str]): Lookup table dictionary mapping METHOD to DESCRIPTION.
        """
        df['measurenote'] = df['METHOD'].map(lut)
        

# %% ../../nbs/handlers/helcom.ipynb 233
class RemapStationIdCB(Callback):
    """Remap Station ID to MARIS format."""

    def __init__(self):
        """
        Initialize the RemapStationIdCB with no specific parameters.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Iterate through all DataFrames in the transformer object and remap 'STATION' to 'station_id'.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            self._remap_station_id(tfm.dfs[grp])

    def _remap_station_id(self, df: pd.DataFrame):
        """
        Remap 'STATION' column to 'station_id' in the given DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['station'] = df['STATION']

# %% ../../nbs/handlers/helcom.ipynb 240
class RemapSedSliceTopBottomCB(Callback):
    """Remap Sediment slice top and bottom to MARIS format."""

    def __init__(self):
        """
        Initialize the RemapSedSliceTopBottomCB with no specific parameters.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Iterate through all DataFrames in the transformer object and remap sediment slice top and bottom.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        if 'sediment' in tfm.dfs:
            self._remap_sediment_slice(tfm.dfs['sediment'])

    def _remap_sediment_slice(self, df: pd.DataFrame):
        """
        Remap 'LOWSLI' column to 'bottom' and 'UPPSLI' column to 'top' in the given DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['bottom'] = df['LOWSLI']
        df['top'] = df['UPPSLI']


# %% ../../nbs/handlers/helcom.ipynb 247
class LookupDryWetRatio(Callback):
    """Lookup dry-wet ratio and format for MARIS."""

    def __init__(self):
        """
        Initialize the LookupDryWetRatio callback with no specific parameters.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Iterate through all DataFrames in the transformer object and apply the dry-wet ratio lookup.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            if 'DW%' in tfm.dfs[grp].columns:
                self._apply_dry_wet_ratio(tfm.dfs[grp])

    def _apply_dry_wet_ratio(self, df: pd.DataFrame):
        """
        Apply dry-wet ratio conversion and formatting to the given DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['dry_wet_ratio'] = df['DW%']
        # Convert 'DW%' = 0% to NaN.
        df.loc[df['dry_wet_ratio'] == 0, 'dry_wet_ratio'] = np.NaN


# %% ../../nbs/handlers/helcom.ipynb 255
# Columns of interest coordinates
coi_coordinates = {
    'seawater': {
        'lon_d': 'LONGITUDE (dddddd)',
        'lat_d': 'LATITUDE (dddddd)',
        'lon_m': 'LONGITUDE (ddmmmm)',
        'lat_m': 'LATITUDE (ddmmmm)'
    },
    'biota': {
        'lon_d': 'LONGITUDE dddddd',
        'lat_d': 'LATITUDE dddddd',
        'lon_m': 'LONGITUDE ddmmmm',
        'lat_m': 'LATITUDE ddmmmm'
    },
    'sediment': {
        'lon_d': 'LONGITUDE (dddddd)',
        'lat_d': 'LATITUDE (dddddd)',
        'lon_m': 'LONGITUDE (ddmmmm)',
        'lat_m': 'LATITUDE (ddmmmm)'
    }
}

# %% ../../nbs/handlers/helcom.ipynb 256
def ddmmmm2dddddd(ddmmmm):
    """
    Convert coordinates from 'ddmmmm' format to 'dddddd' format.
    
    Args:
        ddmmmm (float): Coordinates in 'ddmmmm' format where 'dd' are degrees and 'mmmm' are minutes.
    
    Returns:
        float: Coordinates in 'dddddd' format.
    """
    # Split into degrees and minutes
    mins, degs = modf(ddmmmm)
    # Convert minutes to decimal
    mins = mins * 100
    # Convert to 'dddddd' format
    return round(int(degs) + (mins / 60), 6)


# %% ../../nbs/handlers/helcom.ipynb 257
class FormatCoordinates(Callback):
    """Format coordinates for MARIS. Converts coordinates from 'ddmmmm' to 'dddddd' format if needed.

    Args:
        coi (dict): Dictionary containing column names for longitude and latitude in various formats.
        fn_convert_cor (Callable): Function to convert coordinates from 'ddmmmm' to 'dddddd' format."""
    def __init__(self, coi: dict, fn_convert_cor: Callable[[float], float]):
        """
        Initialize the FormatCoordinates callback.

        Args:
            coi (dict): Column names mapping for coordinates.
            fn_convert_cor (Callable): Function to convert coordinates.
        """
        self.coi = coi
        self.fn_convert_cor = fn_convert_cor
        fc.store_attr()

    def __call__(self, tfm):
        """
        Apply formatting to coordinates in the DataFrame.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            self._format_coordinates(tfm.dfs[grp], grp)

    def _format_coordinates(self, df: pd.DataFrame, grp: str):
        """
        Format coordinates in the DataFrame for a specific group.

        Args:
            df (pd.DataFrame): DataFrame to modify.
            grp (str): Group name to determine column names.
        """
        lon_col_d = self.coi[grp]['lon_d']
        lat_col_d = self.coi[grp]['lat_d']
        lon_col_m = self.coi[grp]['lon_m']
        lat_col_m = self.coi[grp]['lat_m']
        
        # Define condition where 'dddddd' format is not available or is zero
        condition = (
            (df[lon_col_d].isna() | (df[lon_col_d] == 0)) |
            (df[lat_col_d].isna() | (df[lat_col_d] == 0))
        )
        
        # Apply conversion function only to non-null and non-zero values
        df['lon'] = np.where(
            condition,
            df[lon_col_m].apply(lambda x: self._safe_convert(x)),
            df[lon_col_d]
        )
        
        df['lat'] = np.where(
            condition,
            df[lat_col_m].apply(lambda x: self._safe_convert(x)),
            df[lat_col_d]
        )
        
        # Drop rows where coordinate columns contain NaN values
        df.dropna(subset=['lat', 'lon'], inplace=True)

    def _safe_convert(self, value):
        """
        Convert coordinate value safely, handling NaN values.

        Args:
            value (float): Coordinate value to convert.

        Returns:
            float: Converted coordinate value.
        """
        if pd.isna(value):
            return value  # Return NaN if value is NaN
        try:
            return self.fn_convert_cor(value)
        except Exception as e:
            print(f"Error converting value {value}: {e}")
            return value  # Return original value if an error occurs


# %% ../../nbs/handlers/helcom.ipynb 273
# Define columns of interest (keys) and renaming rules (values).
def get_renaming_rules(encoding_type='netcdf'):
    vars = cdl_cfg()['vars']
    if encoding_type == 'netcdf':
        return OrderedDict({
            ('seawater', 'biota', 'sediment'): {
                # DEFAULT
                'lat': vars['defaults']['lat']['name'],
                'lon': vars['defaults']['lon']['name'],
                'time': vars['defaults']['time']['name'],
                'NUCLIDE': 'nuclide',
                'detection_limit': vars['suffixes']['detection_limit']['name'],
                'unit': vars['suffixes']['unit']['name'],
                'value': 'value',
                'uncertainty': vars['suffixes']['uncertainty']['name'],
                'counting_method': vars['suffixes']['counting_method']['name'],
                'sampling_method': vars['suffixes']['sampling_method']['name'],
                'preparation_method': vars['suffixes']['preparation_method']['name']
            },
            ('seawater',): {
                # SEAWATER
                'SALIN': vars['suffixes']['salinity']['name'],
                'SDEPTH': vars['defaults']['smp_depth']['name'],
                #'FILT': vars['suffixes']['filtered']['name'], Need to fix
                'TTEMP': vars['suffixes']['temperature']['name'],
                'TDEPTH': vars['defaults']['tot_depth']['name'],

            },
            ('biota',): {
                # BIOTA
                'SDEPTH': vars['defaults']['smp_depth']['name'],
                'species': vars['bio']['species']['name'],
                'body_part': vars['bio']['body_part']['name'],
                'bio_group': vars['bio']['bio_group']['name']
            },
            ('sediment',): {
                # SEDIMENT
                'sed_type': vars['sed']['sed_type']['name'],
                'TDEPTH': vars['defaults']['tot_depth']['name'],
            }
        })
    
    elif encoding_type == 'openrefine':
        return OrderedDict({
            ('seawater', 'biota', 'sediment'): {
                # DEFAULT
                'samptype_id': 'samptype_id',
                'lat': 'latitude',
                'lon': 'longitude',
                'station': 'station',
                'begperiod': 'begperiod',
                'samplabcode': 'samplabcode',
                #'endperiod': 'endperiod',
                'nuclide_id': 'nuclide_id',
                'detection_limit': 'detection',
                'unit': 'unit_id',
                'value': 'activity',
                'uncertainty': 'uncertaint',
                #'vartype': 'vartype',
                #'rangelow': 'rangelow',
                #'rangeupp': 'rangeupp',
                #'rl_detection': 'rl_detection',
                #'ru_detection': 'ru_detection',
                #'freq': 'freq',
                'SDEPTH': 'sampdepth',
                #'samparea': 'samparea',
                'SALIN': 'salinity',
                'TTEMP': 'temperatur',
                'FILT': 'filtered',
                #'oxygen': 'oxygen',
                #'sampquality': 'sampquality',
                #'station': 'station',
                #'samplabcode': 'samplabcode',
                #'profile': 'profile',
                #'transect': 'transect',
                #'IODE_QualityFlag': 'IODE_QualityFlag',
                'TDEPTH': 'totdepth',
                #'counmet_id': 'counting_method',
                #'sampmet_id': 'sampling_method',
                #'prepmet_id': 'preparation_method',
                'sampnote': 'sampnote',
                'measurenote': 'measurenote'
            },
            ('seawater',) : {
                # SEAWATER
                #'volume': 'volume',
                #'filtpore': 'filtpore',
                #'acid': 'acid'
            },
            ('biota',) : {
                # BIOTA
                'species': 'species_id',
                'Taxonname': 'Taxonname',
                'TaxonRepName': 'TaxonRepName',
                #'Commonname': 'Commonname',
                'Taxonrank': 'Taxonrank',
                'TaxonDB': 'TaxonDB',
                'TaxonDBID': 'TaxonDBID',
                'TaxonDBURL': 'TaxonDBURL',
                'body_part': 'bodypar_id',
                #'drywt': 'drywt',
                #'wetwt': 'wetwt',
                'dry_wet_ratio': 'percentwt',
                #'drymet_id': 'drymet_id'
            },
            ('sediment',): {
                # SEDIMENT
                'sed_type': 'sedtype_id',
                #'sedtrap': 'sedtrap',
                'top': 'sliceup',
                'bottom': 'slicedown',
                'SedRepName': 'SedRepName',
                #'drywt': 'drywt',
                #'wetwt': 'wetwt',
                'dry_wet_ratio': 'percentwt',
                #'drymet_id': 'drymet_id'
                
            }
        })
    else:
        print("Invalid encoding_type provided. Please use 'netcdf' or 'openrefine'.")
        return None

# %% ../../nbs/handlers/helcom.ipynb 274
class SelectAndRenameColumnCB(Callback):
    """A callback to select and rename columns in a DataFrame based on provided renaming rules
    for a specified encoding type. It also prints renaming rules that were not applied
    because their keys were not found in the DataFrame."""
    
    def __init__(self, fn_renaming_rules, encoding_type='netcdf', verbose=False):
        """
        Initialize the SelectAndRenameColumnCB callback.

        Args:
            fn_renaming_rules (function): A function that returns an OrderedDict of renaming rules.
            encoding_type (str): The encoding type ('netcdf' or 'openrefine') to determine which renaming rules to use.
            verbose (bool): Whether to print out renaming rules that were not applied.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Apply column selection and renaming to DataFrames in the transformer, and identify unused rules.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        try:
            renaming_rules = self.fn_renaming_rules(self.encoding_type)
        except ValueError as e:
            print(f"Error fetching renaming rules: {e}")
            return

        for group in tfm.dfs.keys():
            # Get relevant renaming rules for the current group
            group_rules = self._get_group_rules(renaming_rules, group)

            if not group_rules:
                continue

            # Apply renaming rules and track keys not found in the DataFrame
            df = tfm.dfs[group]
            df, not_found_keys = self._apply_renaming(df, group_rules)
            tfm.dfs[group] = df
            
            # Print any renaming rules that were not used
            if not_found_keys and self.verbose:
                print(f"\nGroup '{group}' has the following renaming rules not applied:")
                for old_col in not_found_keys:
                    print(f"Key '{old_col}' from renaming rules was not found in the DataFrame.")

    def _get_group_rules(self, renaming_rules, group):
        """
        Retrieve and merge renaming rules for the specified group based on the encoding type.

        Args:
            renaming_rules (OrderedDict): OrderedDict of all renaming rules.
            group (str): Group name to filter rules.

        Returns:
            OrderedDict: An OrderedDict of renaming rules applicable to the specified group.
        """
        relevant_rules = [rules for key, rules in renaming_rules.items() if group in key]
        merged_rules = OrderedDict()
        for rules in relevant_rules:
            merged_rules.update(rules)
        return merged_rules

    def _apply_renaming(self, df, rename_rules):
        """
        Select columns based on renaming rules and apply renaming, only for existing columns,
        while maintaining the order of the dictionary columns.

        Args:
            df (pd.DataFrame): DataFrame to modify.
            rename_rules (OrderedDict): OrderedDict of column renaming rules.

        Returns:
            tuple: A tuple containing:
                - The DataFrame with columns renamed and filtered.
                - A set of column names from renaming rules that were not found in the DataFrame.
        """
        existing_columns = set(df.columns)
        valid_rules = OrderedDict((old_col, new_col) for old_col, new_col in rename_rules.items() if old_col in existing_columns)

        # Create a list to maintain the order of columns
        columns_to_keep = [col for col in rename_rules.keys() if col in existing_columns]
        columns_to_keep += [new_col for old_col, new_col in valid_rules.items() if new_col in df.columns]

        df = df[list(OrderedDict.fromkeys(columns_to_keep))]

        # Apply renaming
        df.rename(columns=valid_rules, inplace=True)

        # Determine which keys were not found
        not_found_keys = set(rename_rules.keys()) - existing_columns
        return df, not_found_keys


# %% ../../nbs/handlers/helcom.ipynb 279
class ReshapeLongToWide(Callback):
    "Convert data from long to wide with renamed columns."
    def __init__(self, columns=['nuclide'], values=['value']):
        fc.store_attr()
        # Retrieve all possible derived vars (e.g 'unc', 'dl', ...) from configs
        self.derived_cols = [value['name'] for value in cdl_cfg()['vars']['suffixes'].values()]
    
    def renamed_cols(self, cols):
        "Flatten columns name"
        return [inner if outer == "value" else f'{inner}{outer}'
                if inner else outer
                for outer, inner in cols]

    def pivot(self, df):
        # Among all possible 'derived cols' select the ones present in df
        derived_coi = [col for col in self.derived_cols if col in df.columns]
        df.index.name = 'org_index'
        df=df.reset_index()
        idx = list(set(df.columns) - set(self.columns + derived_coi + self.values))
        
        # Create a fill_value to replace NaN values in the columns used as the index in the pivot table.
        # Check if num_fill_value is already in the dataframe index values. If num_fill_value already exists
        # then increase num_fill_value by 1 until a value is found for num_fill_value that is not in the dataframe. 
        num_fill_value = -999
        while (df[idx] == num_fill_value).any().any():
            num_fill_value += 1
        # Fill in nan values for each col found in idx. 
        for col in idx:   
            if pd.api.types.is_numeric_dtype(df[col]):
                fill_value = num_fill_value
            if pd.api.types.is_string_dtype(df[col]):
                fill_value = 'NOT AVAILABLE'
                
            df[col]=df[col].fillna(fill_value)

        pivot_df=df.pivot_table(index=idx,
                              columns=self.columns,
                              values=self.values + derived_coi,
                              fill_value=np.nan,
                              aggfunc=lambda x: x
                              ).reset_index()
        

        # Replace fill_value  with  np.nan
        pivot_df[idx]=pivot_df[idx].replace({'NOT AVAILABLE': np.nan,
                                             num_fill_value : np.nan})
        # Set the index to be the org_index
        pivot_df = pivot_df.set_index('org_index')
                
        return (pivot_df)

    def __call__(self, tfm):
        for grp in tfm.dfs.keys():
            tfm.dfs[grp] = self.pivot(tfm.dfs[grp])
            tfm.dfs[grp].columns = self.renamed_cols(tfm.dfs[grp].columns)

# %% ../../nbs/handlers/helcom.ipynb 288
kw = ['oceanography', 'Earth Science > Oceans > Ocean Chemistry> Radionuclides',
      'Earth Science > Human Dimensions > Environmental Impacts > Nuclear Radiation Exposure',
      'Earth Science > Oceans > Ocean Chemistry > Ocean Tracers, Earth Science > Oceans > Marine Sediments',
      'Earth Science > Oceans > Ocean Chemistry, Earth Science > Oceans > Sea Ice > Isotopes',
      'Earth Science > Oceans > Water Quality > Ocean Contaminants',
      'Earth Science > Biological Classification > Animals/Vertebrates > Fish',
      'Earth Science > Biosphere > Ecosystems > Marine Ecosystems',
      'Earth Science > Biological Classification > Animals/Invertebrates > Mollusks',
      'Earth Science > Biological Classification > Animals/Invertebrates > Arthropods > Crustaceans',
      'Earth Science > Biological Classification > Plants > Macroalgae (Seaweeds)']


# %% ../../nbs/handlers/helcom.ipynb 289
def get_attrs(tfm, zotero_key, kw=kw):
    return GlobAttrsFeeder(tfm.dfs, cbs=[
        BboxCB(),
        DepthRangeCB(),
        TimeRangeCB(cfg()),
        ZoteroCB(zotero_key, cfg=cfg()),
        KeyValuePairCB('keywords', ', '.join(kw)),
        KeyValuePairCB('publisher_postprocess_logs', ', '.join(tfm.logs))
        ])()

# %% ../../nbs/handlers/helcom.ipynb 291
def enums_xtra(tfm, vars):
    "Retrieve a subset of the lengthy enum as 'species_t' for instance"
    enums = Enums(lut_src_dir=lut_path(), cdl_enums=cdl_cfg()['enums'])
    xtras = {}
    for var in vars:
        unique_vals = tfm.unique(var)
        if unique_vals.any():
            xtras[f'{var}_t'] = enums.filter(f'{var}_t', unique_vals)
    return xtras

# %% ../../nbs/handlers/helcom.ipynb 293
def encode(fname_in, fname_out_nc, nc_tpl_path, **kwargs):
    dfs = load_data(fname_in)
    tfm = Transformer(dfs, cbs=[
                                GetSampleTypeCB(type_lut),
                                LowerStripRdnNameCB(),
                                RemapRdnNameCB(),
                                ParseTimeCB(),
                                EncodeTimeCB(cfg()),        
                                SanitizeValue(coi_val),                       
                                NormalizeUncCB(),
                                LookupBiotaSpeciesCB(get_maris_species),
                                LookupBiotaBodyPartCB(get_maris_bodypart),                          
                                LookupBiogroupCB(partial(get_biogroup_lut, species_lut_path())),
                                LookupTaxonInformationCB(partial(get_taxon_info_lut, species_lut_path())),
                                LookupSedimentCB(get_maris_sediments),
                                LookupUnitCB(),
                                LookupDetectionLimitCB(),    
                                RemapDataProviderSampleIdCB(),
                                RecordMeasurementNoteCB(get_helcom_method_desc),
                                LookupFiltCB(),
                                RemapStationIdCB(),
                                RemapSedSliceTopBottomCB(),
                                LookupDryWetRatio(),
                                FormatCoordinates(coi_coordinates, ddmmmm2dddddd),
                                SanitizeLonLatCB(),
                                SelectAndRenameColumnCB(get_renaming_rules, encoding_type='netcdf'),
                                ReshapeLongToWide()
                                ])
    tfm()
    encoder = NetCDFEncoder(tfm.dfs, 
                            src_fname=nc_tpl_path,
                            dest_fname=fname_out_nc, 
                            global_attrs=get_attrs(tfm, zotero_key=zotero_key, kw=kw),
                            verbose=kwargs.get('verbose', False),
                            enums_xtra=enums_xtra(tfm, vars=['species', 'body_part'])
                           )
    encoder.encode()

# %% ../../nbs/handlers/helcom.ipynb 302
def encode_or(fname_in, fname_out_csv, ref_id, **kwargs):
    dfs = load_data(fname_in)
    tfm = Transformer(dfs, cbs=[
                                GetSampleTypeCB(type_lut),
                                LowerStripRdnNameCB(),
                                RemapRdnNameCB(),
                                ParseTimeCB(),
                                EncodeTimeCB(cfg()),        
                                SanitizeValue(coi_val),                       
                                NormalizeUncCB(),
                                LookupBiotaSpeciesCB(get_maris_species),
                                LookupBiotaBodyPartCB(get_maris_bodypart),                          
                                LookupBiogroupCB(partial(get_biogroup_lut, species_lut_path())),
                                LookupTaxonInformationCB(partial(get_taxon_info_lut, species_lut_path())),
                                LookupSedimentCB(get_maris_sediments),
                                LookupUnitCB(),
                                LookupDetectionLimitCB(),    
                                RemapDataProviderSampleIdCB(),
                                RecordMeasurementNoteCB(get_helcom_method_desc),
                                LookupFiltCB(),
                                RemapStationIdCB(),
                                RemapSedSliceTopBottomCB(),
                                LookupDryWetRatio(),
                                FormatCoordinates(coi_coordinates, ddmmmm2dddddd),
                                SanitizeLonLatCB(),
                                SelectAndRenameColumnCB(get_renaming_rules, encoding_type='openrefine'),
                                CompareDfsAndTfmCB(dfs)
                                ])
    tfm()

    encoder = OpenRefineCsvEncoder(tfm.dfs, 
                                    dest_fname=fname_out_csv, 
                                    ref_id = ref_id,
                                    verbose = True
                                )
    encoder.encode()

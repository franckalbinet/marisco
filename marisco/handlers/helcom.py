# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/handlers/helcom.ipynb.

# %% auto 0
__all__ = ['fname_in', 'fname_out_nc', 'fname_out_csv', 'zotero_key', 'ref_id', 'varnames_lut_updates', 'coi_val',
           'coi_units_unc', 'unmatched_fixes_biota_species', 'get_maris_species', 'unmatched_fixes_biota_tissues',
           'get_maris_bodypart', 'unmatched_fixes_sediments', 'get_maris_sediments', 'renaming_unit_rules', 'coi_dl',
           'coi_coordinates', 'kw', 'load_data', 'CompareDfsAndTfm', 'LowerStripRdnNameCB', 'get_unique_nuclides',
           'get_varnames_lut', 'RemapRdnNameCB', 'ParseTimeCB', 'SanitizeValue', 'unc_rel2stan',
           'LookupBiotaBodyPartCB', 'get_biogroup_lut', 'LookupBiogroupCB', 'preprocess_sedi', 'LookupSedimentCB',
           'LookupUnitCB', 'get_detectionlimit_lut', 'LookupDetectionLimitCB', 'RemapDataProviderSampleIdCB',
           'get_filtered_lut', 'LookupFiltCB', 'RemapStationIdCB', 'RemapProfileIdCB', 'RemapSedSliceTopBottomCB',
           'RemapTaxonRepNameCB', 'LookupDryWetRatio', 'ddmmmm2dddddd', 'FormatCoordinates', 'get_renaming_rules',
           'SelectAndRenameColumnCB', 'ReshapeLongToWide', 'get_attrs', 'enums_xtra', 'encode',
           'LookupTimeFromEncodedTimeCB', 'GetSampleTypeCB', 'get_nucnames_lut', 'LookupNuclideByIdCB']

# %% ../../nbs/handlers/helcom.ipynb 7
import pandas as pd # Python package that provides fast, flexible, and expressive data structures.
import numpy as np
from tqdm import tqdm # Python Progress Bar Library
from functools import partial # Function which Return a new partial object which when called will behave like func called with the positional arguments args and keyword arguments keywords
import fastcore.all as fc # package that brings fastcore functionality, see https://fastcore.fast.ai/.
from pathlib import Path # This module offers classes representing filesystem paths
from dataclasses import asdict
from typing import List, Dict, Callable, Optional, Tuple

from ..utils import (has_valid_varname, match_worms, match_maris_lut, Match)
from ..callbacks import (Callback, Transformer, EncodeTimeCB, SanitizeLonLatCB)
from ..metadata import (GlobAttrsFeeder, BboxCB, DepthRangeCB, TimeRangeCB, ZoteroCB, KeyValuePairCB)
from ..configs import (base_path,nuc_lut_path, nc_tpl_path, cfg, cache_path, cdl_cfg, Enums, lut_path,
                             species_lut_path, sediments_lut_path, bodyparts_lut_path, 
                             detection_limit_lut_path, filtered_lut_path, area_lut_path)
from ..serializers import NetCDFEncoder
from collections.abc import Callable
from math import modf
import warnings
from ..netcdf_to_csv import (LookupTimeFromEncodedTime, GetSampleTypeCB,
                                   LookupNuclideByIdCB, ConvertLonLatCB, LookupUnitByIdCB,
                                   LookupValueTypeByIdCB, LookupSpeciesByIdCB, 
                                   LookupBodypartByIdCB, LookupSedimentTypeByIdCB)                                  
from ..serializers import OpenRefineCsvEncoder

# %% ../../nbs/handlers/helcom.ipynb 26
fname_in = '../../_data/accdb/mors/csv'
fname_out_nc = '../../_data/output/100-HELCOM-MORS-2024.nc'
fname_out_csv = '../../_data/output/100-HELCOM-MORS-2024.csv'
zotero_key ='26VMZZ2Q'
ref_id = 100

# %% ../../nbs/handlers/helcom.ipynb 29
def load_data(src_dir: str, smp_types: List[str] = ['SEA', 'SED', 'BIO']) -> Dict[str, pd.DataFrame]:
    """
    Load HELCOM data and return the data in a dictionary of dataframes with the dictionary key as the sample type.
    
    Args:
    src_dir (str): The directory where the source CSV files are located.
    smp_types (List[str]): A list of sample types to load. Defaults to ['SEA', 'SED', 'BIO'].
    
    Returns:
    Dict[str, pd.DataFrame]: A dictionary with sample types as keys and their corresponding dataframes as values.
    """
    dfs = {}
    lut_smp_type = {'SEA': 'seawater', 'SED': 'sediment', 'BIO': 'biota'}
    
    for smp_type in smp_types:
        fname_meas = smp_type + '02.csv'  # Measurement (i.e., radioactivity) information
        fname_smp = smp_type + '01.csv'  # Sample information
        
        df_meas = pd.read_csv(Path(src_dir) / fname_meas)
        df_smp = pd.read_csv(Path(src_dir) / fname_smp)
        
        df = pd.merge(df_meas, df_smp, on='KEY', how='left')
        dfs[lut_smp_type[smp_type]] = df
    
    return dfs


# %% ../../nbs/handlers/helcom.ipynb 44
import pandas as pd
import numpy as np
from typing import List, Dict
from ..callbacks import Callback, Transformer

class CompareDfsAndTfm(Callback):
    "Create a dataframe of dropped data. Data included in the `dfs` not in the `tfm`."
    
    def __init__(self, dfs: Dict[str, pd.DataFrame]):
        fc.store_attr()
    
    def __call__(self, tfm: Transformer) -> None:
        self._initialize_tfm_attributes(tfm)
        for grp in tfm.dfs.keys():
            dropped_df = self._get_dropped_data(grp, tfm)
            tfm.dfs_dropped[grp] = dropped_df
            tfm.compare_stats[grp] = self._compute_stats(grp, tfm)

    def _initialize_tfm_attributes(self, tfm: Transformer) -> None:
        """Initialize attributes in `tfm`."""
        tfm.dfs_dropped = {}
        tfm.compare_stats = {}

    def _get_dropped_data(self, grp: str, tfm: Transformer) -> pd.DataFrame:
        """
        Get the data that is present in `dfs` but not in `tfm.dfs`.
        
        Args:
        grp (str): The group key.
        tfm (Transformer): The transformation object containing `dfs`.
        
        Returns:
        pd.DataFrame: Dataframe with dropped rows.
        """
        index_diff = self.dfs[grp].index.difference(tfm.dfs[grp].index)
        return self.dfs[grp].loc[index_diff]
    
    def _compute_stats(self, grp: str, tfm: Transformer) -> Dict[str, int]:
        """
        Compute comparison statistics between `dfs` and `tfm.dfs`.
        
        Args:
        grp (str): The group key.
        tfm (Transformer): The transformation object containing `dfs`.
        
        Returns:
        Dict[str, int]: Dictionary with comparison statistics.
        """
        return {
            'Number of rows in dfs': len(self.dfs[grp].index),
            'Number of rows in tfm.dfs': len(tfm.dfs[grp].index),
            'Number of dropped rows': len(tfm.dfs_dropped[grp].index),
            'Number of rows in tfm.dfs + Number of dropped rows': len(tfm.dfs[grp].index) + len(tfm.dfs_dropped[grp].index)
        }


# %% ../../nbs/handlers/helcom.ipynb 49
class LowerStripRdnNameCB(Callback):
    """Convert nuclide names to lowercase and strip any trailing spaces."""

    def __call__(self, tfm):
        for key in tfm.dfs.keys():
            self._process_nuclide_column(tfm.dfs[key])

    def _process_nuclide_column(self, df):
        """Apply transformation to the 'NUCLIDE' column of the given DataFrame."""
        df['NUCLIDE'] = df['NUCLIDE'].apply(self._transform_nuclide)

    def _transform_nuclide(self, nuclide):
        """Convert nuclide name to lowercase and strip trailing spaces."""
        return nuclide.lower().strip()


# %% ../../nbs/handlers/helcom.ipynb 54
def get_unique_nuclides(dfs: Dict[str, pd.DataFrame]) -> List[str]:
    """
    Get a list of unique radionuclide types measured across samples.

    Args:
        dfs (Dict[str, pd.DataFrame]): A dictionary where keys are sample names and values are DataFrames.

    Returns:
        List[str]: A list of unique radionuclide types.
    """
    # Collect unique nuclide names from all DataFrames
    nuclides = set()
    for df in dfs.values():
        nuclides.update(df['NUCLIDE'].unique())

    return list(nuclides)

# %% ../../nbs/handlers/helcom.ipynb 57
varnames_lut_updates = {
    'k-40': 'k40',
    'cm243244': 'cm243_244_tot',
    'cs134137': 'cs134_137_tot',
    'pu239240': 'pu239_240_tot',
    'pu238240': 'pu238_240_tot',
    'cs138': 'cs137',
    'cs139': 'cs137',
    'cs140': 'cs137',
    'cs141': 'cs137',
    'cs142': 'cs137',
    'cs143': 'cs137',
    'cs144': 'cs137',
    'cs145': 'cs137',
    'cs146': 'cs137'}

# %% ../../nbs/handlers/helcom.ipynb 59
def get_varnames_lut(
    dfs: Dict[str, pd.DataFrame], 
    lut: Dict[str, str] = varnames_lut_updates
) -> Dict[str, str]:
    """
    Generate a lookup table for radionuclide names, updating with provided mappings.

    Args:
        dfs (Dict[str, pd.DataFrame]): A dictionary where keys are sample names and values are DataFrames.
        lut (Dict[str, str], optional): A dictionary with additional mappings to update the lookup table.

    Returns:
        Dict[str, str]: A dictionary mapping radionuclide names to their corresponding names.
    """
    # Generate a base lookup table from unique nuclide names
    unique_nuclides = get_unique_nuclides(dfs)
    base_lut = {name: name for name in unique_nuclides}

    # Update the base lookup table with additional mappings
    base_lut.update(lut)
    
    return base_lut

# %% ../../nbs/handlers/helcom.ipynb 61
class RemapRdnNameCB(Callback):
    """Remap radionuclide names to MARIS radionuclide names."""

    def __init__(self, fn_lut: Callable[[Dict[str, pd.DataFrame]], Dict[str, str]] =  partial(get_varnames_lut, lut=varnames_lut_updates)):
        """
        Initialize the RemapRdnNameCB with a function to generate the lookup table.

        Args:
            fn_lut (Callable, optional): A function that takes a dictionary of DataFrames and returns a lookup table.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply the lookup table to remap radionuclide names in DataFrames.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut(tfm.dfs)
        for grp in tfm.dfs:
            self._remap_nuclide_names(tfm.dfs[grp], lut)
    
    def _remap_nuclide_names(self, df: pd.DataFrame, lut: Dict[str, str]):
        """
        Remap radionuclide names in the 'NUCLIDE' column of the DataFrame.

        Args:
            df (pd.DataFrame): DataFrame containing the 'NUCLIDE' column.
            lut (Dict[str, str]): Lookup table for remapping radionuclide names.
        """
        if 'NUCLIDE' in df.columns:
            df['NUCLIDE'].replace(lut, inplace=True)
        else:
            raise ValueError("DataFrame must contain a 'NUCLIDE' column.")


# %% ../../nbs/handlers/helcom.ipynb 71
class ParseTimeCB(Callback):
    def __init__(self):
        fc.store_attr()
            
        
    def __call__(self, tfm):
        for grp in tfm.dfs.keys():
            df = tfm.dfs[grp]
            self._process_dates(df)

    def _process_dates(self, df: pd.DataFrame):
        """
        Process and correct date and time information in the DataFrame.

        Args:
            df (pd.DataFrame): DataFrame containing the 'DATE', 'YEAR', 'MONTH', and 'DAY' columns.
        """
        # get 'time' from 'DATE' column
        df['time'] = pd.to_datetime(df['DATE'], format='%m/%d/%y %H:%M:%S')
        # if 'DATE' column is nan, get 'time' from 'YEAR','MONTH' and 'DAY' column. 
        # if 'DAY' or 'MONTH' is 0 then set it to 1. 
        df.loc[df["DAY"] == 0, "DAY"] = 1
        df.loc[df["MONTH"] == 0, "MONTH"] = 1
        
        # if 'DAY' and 'MONTH' is nan but YEAR is not nan then set 'DAY' and 'MONTH' both to 1. 
        condition = (df["DAY"].isna()) & (df["MONTH"].isna()) & (df["YEAR"].notna())
        df.loc[condition, "DAY"] = 1
        df.loc[condition, "MONTH"] = 1
        
        condition = df['DATE'].isna() # if 'DATE' is nan. 
        df['time']  = np.where(condition,
                                            # 'coerce', then invalid parsing will be set as NaT. NaT will result if the number of days are not valid for the month.
                                        pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], format='%y%m%d', errors='coerce'),  
                                        pd.to_datetime(df['DATE'], format='%m/%d/%y %H:%M:%S'))

# %% ../../nbs/handlers/helcom.ipynb 85
# Columns of interest
coi_val = {'seawater' : { 'val' : 'VALUE_Bq/m³'},
                 'biota':  {'val' : 'VALUE_Bq/kg'},
                 'sediment': { 'val' : 'VALUE_Bq/kg'}}

# %% ../../nbs/handlers/helcom.ipynb 86
class SanitizeValue(Callback):
    "Sanitize value by removing blank entries and ensuring the 'value' column is retained."

    def __init__(self, coi: dict):
        """
        Initialize the SanitizeValue callback.

        Args:
            coi (dict): Dictionary containing column names for values based on group.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Sanitize the DataFrames in the transformer by removing rows with blank values in specified columns.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            self._sanitize_dataframe(tfm.dfs[grp], grp)

    def _sanitize_dataframe(self, df: pd.DataFrame, grp: str):
        """
        Remove rows where specified value columns are blank and ensure the 'value' column is included.

        Args:
            df (pd.DataFrame): DataFrame to sanitize.
            grp (str): Group name to determine column names.
        """
        value_col = self.coi.get(grp, {}).get('val')
        if value_col and value_col in df.columns:
            df.dropna(subset=[value_col], inplace=True)
            # Ensure 'value' column is retained
            if 'value' not in df.columns:
                df['value'] = df[value_col]

# %% ../../nbs/handlers/helcom.ipynb 93
# Make measurement and uncertainty units consistent
def unc_rel2stan(df: pd.DataFrame, meas_col: str, unc_col: str) -> pd.Series:
    """
    Convert relative uncertainty to absolute uncertainty.

    Args:
        df (pd.DataFrame): DataFrame containing measurement and uncertainty columns.
        meas_col (str): Name of the column with measurement values.
        unc_col (str): Name of the column with relative uncertainty values (percentages).

    Returns:
        pd.Series: Series with calculated absolute uncertainties.
    """
    return df.apply(lambda row: row[unc_col] * row[meas_col] / 100, axis=1)


# %% ../../nbs/handlers/helcom.ipynb 95
# Columns of interest
coi_units_unc = [('seawater', 'VALUE_Bq/m³', 'ERROR%_m³'),
                 ('biota', 'VALUE_Bq/kg', 'ERROR%'),
                 ('sediment', 'VALUE_Bq/kg', 'ERROR%_kg')]

# %% ../../nbs/handlers/helcom.ipynb 111
unmatched_fixes_biota_species = {
    'CARD EDU': 'Cerastoderma edule',
    'LAMI SAC': 'Saccharina latissima',
    'PSET MAX': 'Scophthalmus maximus',
    'STIZ LUC': 'Sander luciopercas'}

# %% ../../nbs/handlers/helcom.ipynb 120
get_maris_species = partial(get_maris_lut,
                            fname_in, fname_cache='species_helcom.pkl', 
                            data_provider_lut='RUBIN_NAME.csv',
                            data_provider_id_col='RUBIN',
                            data_provider_name_col='SCIENTIFIC NAME',
                            maris_lut=species_lut_path,
                            maris_id='species_id',
                            maris_name='species',
                            unmatched_fixes=unmatched_fixes_biota_species,
                            as_dataframe=False,
                            overwrite=False)

# %% ../../nbs/handlers/helcom.ipynb 130
unmatched_fixes_biota_tissues = {
    3: 'Whole animal eviscerated without head',
    12: 'Viscera',
    8: 'Skin'}

# %% ../../nbs/handlers/helcom.ipynb 134
class LookupBiotaBodyPartCB(Callback):
    """
    Update bodypart id based on MARIS dbo_bodypar.xlsx:
        - 3: 'Whole animal eviscerated without head',
        - 12: 'Viscera',
        - 8: 'Skin'
    """
    def __init__(self, fn_lut: Callable[[], dict]):
        """
        Initialize the LookupBiotaBodyPartCB with a function to generate the lookup table.

        Args:
            fn_lut (Callable[[], dict]): Function that returns the lookup table dictionary.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Remap biota body parts in the DataFrame using the lookup table and print unmatched TISSUE values.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        tfm.dfs['biota']['body_part'] = tfm.dfs['biota']['TISSUE'].apply(lambda x: self._get_body_part(x, lut))

    def _get_body_part(self, tissue_value: str, lut: dict):
        """
        Get the matched_id from the lookup table and print TISSUE if the matched_id is -1.

        Args:
            tissue_value (str): The TISSUE value from the DataFrame.
            lut (dict): The lookup table dictionary.

        Returns:
            The matched_id from the lookup table.
        """
        match = lut.get(tissue_value, Match(-1, None, None, None))
        if match.matched_id == -1:
            self.print_unmatched_tissue(tissue_value)
        return match.matched_id

    def print_unmatched_tissue(self, tissue_value: str):
        """
        Print the TISSUE value if the matched_id is -1.

        Args:
            tissue_value (str): The TISSUE value from the DataFrame.
        """
        print(f"Unmatched TISSUE: {tissue_value}")


# %% ../../nbs/handlers/helcom.ipynb 136
get_maris_bodypart = partial(get_maris_lut,
                             fname_in,
                             fname_cache='tissues_helcom.pkl', 
                             data_provider_lut='TISSUE.csv',
                             data_provider_id_col='TISSUE',
                             data_provider_name_col='TISSUE_DESCRIPTION',
                             maris_lut=bodyparts_lut_path,
                             maris_id='bodypar_id',
                             maris_name='bodypar',
                             unmatched_fixes=unmatched_fixes_biota_tissues)

# %% ../../nbs/handlers/helcom.ipynb 144
def get_biogroup_lut(maris_lut: str) -> dict:
    """
    Retrieve a lookup table for biogroup ids from a MARIS lookup table.

    Args:
        maris_lut (str): Path to the MARIS lookup table (Excel file).

    Returns:
        dict: A dictionary mapping species_id to biogroup_id.
    """
    species = pd.read_excel(maris_lut)
    return species[['species_id', 'biogroup_id']].set_index('species_id').to_dict()['biogroup_id']


# %% ../../nbs/handlers/helcom.ipynb 146
class LookupBiogroupCB(Callback):
    """
    Update biogroup id based on MARIS dbo_species.xlsx
    """
    def __init__(self, fn_lut: Callable[[], dict]):
        """
        Initialize the LookupBiogroupCB with a function to generate the lookup table.

        Args:
            fn_lut (Callable[[], dict]): Function that returns the lookup table dictionary.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Update the 'bio_group' column in the DataFrame using the lookup table and print unmatched species values.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        tfm.dfs['biota']['bio_group'] = tfm.dfs['biota']['species'].apply(lambda x: self._get_biogroup(x, lut))

    def _get_biogroup(self, species_value: str, lut: dict) -> int:
        """
        Get the biogroup id from the lookup table and print species if the biogroup id is not found.

        Args:
            species_value (str): The species value from the DataFrame.
            lut (dict): The lookup table dictionary.

        Returns:
            int: The biogroup id from the lookup table.
        """
        biogroup_id = lut.get(species_value, -1)
        if biogroup_id == -1:
            self.print_unmatched_species(species_value)
        return biogroup_id

    def print_unmatched_species(self, species_value: str):
        """
        Print the species value if the biogroup id is not found.

        Args:
            species_value (str): The species value from the DataFrame.
        """
        print(f"Unmatched species: {species_value}")


# %% ../../nbs/handlers/helcom.ipynb 156
unmatched_fixes_sediments = {
    #np.nan: 'Not applicable',
    -99: '(Not available)'
}

# %% ../../nbs/handlers/helcom.ipynb 159
get_maris_sediments = partial(
    get_maris_lut,
    fname_in, 
    fname_cache='sediments_helcom.pkl', 
    data_provider_lut='SEDIMENT_TYPE.csv',
    data_provider_id_col='SEDI',
    data_provider_name_col='SEDIMENT TYPE',
    maris_lut=sediments_lut_path,
    maris_id='sedtype_id',
    maris_name='sedtype',
    unmatched_fixes=unmatched_fixes_sediments)

# %% ../../nbs/handlers/helcom.ipynb 161
def preprocess_sedi(df, column_name='SEDI'):
    """
    Preprocess the 'SEDI' column in the DataFrame by handling missing values and specific replacements.

    Args:
        df (pd.DataFrame): The DataFrame containing the 'SEDI' column.
        column_name (str): The name of the column to preprocess. Default is 'SEDI'.
    
    Returns:
        pd.DataFrame: The DataFrame with preprocessed 'SEDI' column.
    """
    if column_name in df.columns:
        df[column_name] = df[column_name].fillna(-99).astype('int')
        df[column_name].replace([56, 73], -99, inplace=True)
    return df


# %% ../../nbs/handlers/helcom.ipynb 162
class LookupSedimentCB(Callback):
    """
    Update sediment id based on MARIS dbo_sedtype.xlsx.
    """
    def __init__(self, fn_lut: Callable[[], dict], preprocess_fn: Callable[[pd.DataFrame, str], pd.DataFrame] = preprocess_sedi):
        """
        Initialize the LookupSedimentCB with a function to generate the lookup table and a preprocessing function.

        Args:
            fn_lut (Callable[[], dict]): Function that returns the lookup table dictionary.
            preprocess_fn (Callable[[pd.DataFrame, str], pd.DataFrame]): Function to preprocess the sediment DataFrame. Default is preprocess_sedi.
        """
        fc.store_attr()
        self.preprocess_fn = preprocess_fn

    def __call__(self, tfm: 'Transformer'):
        """
        Remap sediment types in the DataFrame using the lookup table and handle specific replacements.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()

        # Apply preprocessing to the 'SEDI' column
        tfm.dfs['sediment'] = self.preprocess_fn(tfm.dfs['sediment'])
        
        # Apply the lookup function
        tfm.dfs['sediment']['sed_type'] = tfm.dfs['sediment']['SEDI'].apply(lambda x: self._get_sediment_type(x, lut))

    def _get_sediment_type(self, sedi_value: int, lut: dict):
        """
        Get the matched_id from the lookup table and print SEDI if the matched_id is -1.

        Args:
            sedi_value (int): The SEDI value from the DataFrame.
            lut (dict): The lookup table dictionary.

        Returns:
            The matched_id from the lookup table.
        """
        match = lut.get(sedi_value, Match(-1, None, None, None))
        if match.matched_id == -1:
            self._print_unmatched_sedi(sedi_value)
        return match.matched_id

    def _print_unmatched_sedi(self, sedi_value: int):
        """
        Print the SEDI value if the matched_id is -1.

        Args:
            sedi_value (int): The SEDI value from the DataFrame.
        """
        print(f"Unmatched SEDI: {sedi_value}")


# %% ../../nbs/handlers/helcom.ipynb 170
# Define unit names renaming rules
renaming_unit_rules = {
    'seawater': 1,  # 'Bq/m3'
    'sediment': 4,  # 'Bq/kgd' for sediment
    'biota': {
        'D': 4,  # 'Bq/kgd'
        'W': 5,  # 'Bq/kgw'
        'F': 5   # 'Bq/kgw' (assumed to be 'Fresh', so set to wet)
    }
}


# %% ../../nbs/handlers/helcom.ipynb 172
class LookupUnitCB(Callback):
    def __init__(self, renaming_unit_rules=renaming_unit_rules):
        """
        Initialize the LookupUnitCB with unit renaming rules.

        Args:
            renaming_unit_rules (dict): Dictionary containing renaming rules for different unit categories.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply unit renaming rules to DataFrames within the transformer.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs:
            rules = renaming_unit_rules.get(grp)
            if rules is not None:
                # if group tules include a dictionary, apply the dictionay. 
                if isinstance(rules, dict):
                    # Apply rules based on the 'BASIS' column
                    tfm.dfs[grp]['unit'] = tfm.dfs[grp]['BASIS'].apply(lambda x: rules.get(x, 0))
                else:
                    # Apply a single rule to the entire DataFrame
                    tfm.dfs[grp]['unit'] = rules


# %% ../../nbs/handlers/helcom.ipynb 180
# Columns of interest
coi_dl = {'seawater' : { 'val' : 'VALUE_Bq/m³',
                        'unc' : 'ERROR%_m³',
                        'dl' : '< VALUE_Bq/m³'},
                 'biota':  {'val' : 'VALUE_Bq/kg',
                            'unc' : 'ERROR%',
                            'dl' : '< VALUE_Bq/kg'},
                 'sediment': { 'val' : 'VALUE_Bq/kg',
                              'unc' : 'ERROR%_kg',
                              'dl' : '< VALUE_Bq/kg'}}

# %% ../../nbs/handlers/helcom.ipynb 182
def get_detectionlimit_lut():
    df = pd.read_excel(detection_limit_lut_path(), usecols=['name','id'])
    return df.set_index('name').to_dict()['id']

# %% ../../nbs/handlers/helcom.ipynb 184
class LookupDetectionLimitCB(Callback):
    "Remap value type to MARIS format."

    def __init__(self, 
                 coi=coi_dl,
                 fn_lut=get_detectionlimit_lut):
        """
        Initialize the LookupDetectionLimitCB with configuration options and lookup function.

        Args:
            coi (dict): Configuration options for column names.
            fn_lut (Callable): Function that returns a lookup table.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Remap detection limits in the DataFrames using the lookup table.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        
        for grp in tfm.dfs:
            df = tfm.dfs[grp]
            self._update_detection_limit(df, grp, lut)
    
    def _update_detection_limit(self, df: pd.DataFrame, grp: str, lut: dict):
        """
        Update detection limit column in the DataFrame based on lookup table and rules.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
            grp (str): The group name to get the column configuration.
            lut (dict): The lookup table dictionary.
        """
        detection_col = self.coi[grp]['dl']
        value_col = self.coi[grp]['val']
        uncertainty_col = self.coi[grp]['unc']
        
        # Copy detection limit column
        df['detection_limit'] = df[detection_col]
        
        # Fill values with '=' or 'Not Available'
        condition = ((df[value_col].notna()) & (df[uncertainty_col].notna()) &
                     (~df['detection_limit'].isin(lut.keys())))
        df.loc[condition, 'detection_limit'] = '='
        df.loc[~df['detection_limit'].isin(lut.keys()), 'detection_limit'] = 'Not Available'
        
        # Perform lookup
        df['detection_limit'] = df['detection_limit'].map(lut)


# %% ../../nbs/handlers/helcom.ipynb 197
class RemapDataProviderSampleIdCB(Callback):
    """
    Remap 'KEY' column to 'data_provider_sample_id' in each DataFrame.
    """

    def __init__(self):
        """
        Initialize the RemapDataProviderSampleIdCB.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Remap 'KEY' column to 'data_provider_sample_id' in the DataFrames.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs:
            self._remap_sample_id(tfm.dfs[grp])
    
    def _remap_sample_id(self, df: pd.DataFrame):
        """
        Remap the 'KEY' column to 'data_provider_sample_id' in the DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['data_provider_sample_id'] = df['KEY']


# %% ../../nbs/handlers/helcom.ipynb 204
def get_filtered_lut() -> dict:
    """
    Retrieve a filtered lookup table from an Excel file.

    Returns:
        dict: A dictionary mapping names to IDs.
    """
    df = pd.read_excel(filtered_lut_path(), usecols=['name', 'id'])
    return df.set_index('name').to_dict()['id']


# %% ../../nbs/handlers/helcom.ipynb 209
class LookupFiltCB(Callback):
    "Lookup FILT value."
    
    def __init__(self,
                 rules=renaming_rules,
                 fn_lut=get_filtered_lut):
        """
        Initialize the LookupFiltCB with renaming rules and a function for generating the lookup table.

        Args:
            rules (dict): Dictionary mapping FILT codes to their corresponding names.
            fn_lut (Callable[[], dict]): Function that returns the lookup table dictionary.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Update the FILT column in the DataFrames using the renaming rules and lookup table.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        rules = self.rules
        
        for grp in tfm.dfs.keys():
            if "FILT" in tfm.dfs[grp].columns:
                self._update_filt_column(tfm.dfs[grp], rules, lut)

    def _update_filt_column(self, df: pd.DataFrame, rules: dict, lut: dict):
        """
        Update the FILT column based on renaming rules and lookup table.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
            rules (dict): Dictionary mapping FILT codes to their corresponding names.
            lut (dict): Dictionary for lookup values.
        """
        # Fill values that are not in the renaming rules with 'Not available'.
        df['FILT'] = df['FILT'].apply(lambda x: rules.get(x, 'Not available'))
        
        # Perform lookup
        df['FILT'] = df['FILT'].map(lambda x: lut.get(x, 0))


# %% ../../nbs/handlers/helcom.ipynb 233
class RemapStationIdCB(Callback):
    "Remap Station ID to MARIS format."

    def __init__(self):
        """
        Initialize the RemapStationIdCB with no specific parameters.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Iterate through all DataFrames in the transformer object and remap 'STATION' to 'station_id'.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            self._remap_station_id(tfm.dfs[grp])

    def _remap_station_id(self, df: pd.DataFrame):
        """
        Remap 'STATION' column to 'station_id' in the given DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['station_id'] = df['STATION']

# %% ../../nbs/handlers/helcom.ipynb 242
class RemapProfileIdCB(Callback):
    "Remap Profile ID to MARIS format."

    def __init__(self):
        """
        Initialize the RemapProfileIdCB with no specific parameters.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Iterate through all DataFrames in the transformer object and remap 'SEQUENCE' to 'profile_or_transect_id'.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            self._remap_profile_id(tfm.dfs[grp])

    def _remap_profile_id(self, df: pd.DataFrame):
        """
        Remap 'SEQUENCE' column to 'profile_or_transect_id' in the given DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['profile_or_transect_id'] = df['SEQUENCE']


# %% ../../nbs/handlers/helcom.ipynb 249
class RemapSedSliceTopBottomCB(Callback):
    "Remap Sediment slice top and bottom to MARIS format."

    def __init__(self):
        """
        Initialize the RemapSedSliceTopBottomCB with no specific parameters.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Iterate through all DataFrames in the transformer object and remap sediment slice top and bottom.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        if 'sediment' in tfm.dfs:
            self._remap_sediment_slice(tfm.dfs['sediment'])

    def _remap_sediment_slice(self, df: pd.DataFrame):
        """
        Remap 'LOWSLI' column to 'bottom' and 'UPPSLI' column to 'top' in the given DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['bottom'] = df['LOWSLI']
        df['top'] = df['UPPSLI']


# %% ../../nbs/handlers/helcom.ipynb 258
class RemapTaxonRepNameCB(Callback):
    """
    A callback to remap the 'TaxonRepName' column to the 'RUBIN' column in the 'biota' DataFrame.
    This is done to conform to the MARIS format.
    """
    
    def __init__(self):
        """
        Initialize the RemapTaxonRepNameCB callback.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply the taxon representation name remapping to the DataFrame in the transformer.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        # Check if 'biota' DataFrame exists in the transformer
        if 'biota' in tfm.dfs:
            self._remap_taxon_rep_name(tfm.dfs['biota'])

    def _remap_taxon_rep_name(self, df: pd.DataFrame):
        """
        Remap the 'TaxonRepName' column to the 'RUBIN' column values.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        # Ensure both columns exist before attempting to remap
        if 'RUBIN' in df.columns:
            df['TaxonRepName'] = df['RUBIN']
        else:
            print("Warning: 'RUBIN' column not found in DataFrame.")


# %% ../../nbs/handlers/helcom.ipynb 264
class LookupDryWetRatio(Callback):
    "Lookup dry-wet ratio and format for MARIS."

    def __init__(self):
        """
        Initialize the LookupDryWetRatio callback with no specific parameters.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Iterate through all DataFrames in the transformer object and apply the dry-wet ratio lookup.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            if 'DW%' in tfm.dfs[grp].columns:
                self._apply_dry_wet_ratio(tfm.dfs[grp])

    def _apply_dry_wet_ratio(self, df: pd.DataFrame):
        """
        Apply dry-wet ratio conversion and formatting to the given DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['dry_wet_ratio'] = df['DW%']
        # Convert 'DW%' = 0% to NaN.
        df.loc[df['dry_wet_ratio'] == 0, 'dry_wet_ratio'] = np.NaN


# %% ../../nbs/handlers/helcom.ipynb 271
# Columns of interest coordinates
coi_coordinates = {
    'seawater': {
        'lon_d': 'LONGITUDE (dddddd)',
        'lat_d': 'LATITUDE (dddddd)',
        'lon_m': 'LONGITUDE (ddmmmm)',
        'lat_m': 'LATITUDE (ddmmmm)'
    },
    'biota': {
        'lon_d': 'LONGITUDE dddddd',
        'lat_d': 'LATITUDE dddddd',
        'lon_m': 'LONGITUDE ddmmmm',
        'lat_m': 'LATITUDE ddmmmm'
    },
    'sediment': {
        'lon_d': 'LONGITUDE (dddddd)',
        'lat_d': 'LATITUDE (dddddd)',
        'lon_m': 'LONGITUDE (ddmmmm)',
        'lat_m': 'LATITUDE (ddmmmm)'
    }
}

# %% ../../nbs/handlers/helcom.ipynb 272
def ddmmmm2dddddd(ddmmmm):
    """
    Convert coordinates from 'ddmmmm' format to 'dddddd' format.
    
    Args:
        ddmmmm (float): Coordinates in 'ddmmmm' format where 'dd' are degrees and 'mmmm' are minutes.
    
    Returns:
        float: Coordinates in 'dddddd' format.
    """
    # Split into degrees and minutes
    mins, degs = modf(ddmmmm)
    # Convert minutes to decimal
    mins = mins * 100
    # Convert to 'dddddd' format
    return round(int(degs) + (mins / 60), 6)


# %% ../../nbs/handlers/helcom.ipynb 273
class FormatCoordinates(Callback):
    """
    Format coordinates for MARIS. Converts coordinates from 'ddmmmm' to 'dddddd' format if needed.

    Args:
        coi (dict): Dictionary containing column names for longitude and latitude in various formats.
        fn_convert_cor (Callable): Function to convert coordinates from 'ddmmmm' to 'dddddd' format.
    """
    def __init__(self, coi: dict, fn_convert_cor: Callable[[float], float]):
        """
        Initialize the FormatCoordinates callback.

        Args:
            coi (dict): Column names mapping for coordinates.
            fn_convert_cor (Callable): Function to convert coordinates.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Apply formatting to coordinates in the DataFrame.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            self._format_coordinates(tfm.dfs[grp], grp)

    def _format_coordinates(self, df: pd.DataFrame, grp: str):
        """
        Format coordinates in the DataFrame for a specific group.

        Args:
            df (pd.DataFrame): DataFrame to modify.
            grp (str): Group name to determine column names.
        """
        lon_col_d = self.coi[grp]['lon_d']
        lat_col_d = self.coi[grp]['lat_d']
        lon_col_m = self.coi[grp]['lon_m']
        lat_col_m = self.coi[grp]['lat_m']
        
        # Define condition where 'dddddd' format is not available or is zero
        condition = (
            (df[lon_col_d].isna() | (df[lon_col_d] == 0)) |
            (df[lat_col_d].isna() | (df[lat_col_d] == 0))
        )
        
        # Convert coordinates using the provided conversion function where needed
        df['lon'] = np.where(
            condition,
            df[lon_col_m].apply(self.fn_convert_cor),
            df[lon_col_d]
        )
        
        df['lat'] = np.where(
            condition,
            df[lat_col_m].apply(self.fn_convert_cor),
            df[lat_col_d]
        )


# %% ../../nbs/handlers/helcom.ipynb 289
# Define columns of interest (keys) and renaming rules (values).
def get_renaming_rules(encoding_type='netcdf'):
    vars = cdl_cfg()['vars']
    
    if encoding_type == 'netcdf':
        return {
            ('seawater', 'biota', 'sediment'): {
                # DEFAULT
                'lat': vars['defaults']['lat']['name'],
                'lon': vars['defaults']['lon']['name'],
                'time': vars['defaults']['time']['name'],
                'NUCLIDE': 'nuclide',
                'detection_limit': vars['suffixes']['detection_limit']['name'],
                'unit': vars['suffixes']['unit']['name'],
                'value': 'value',
                'uncertainty': vars['suffixes']['uncertainty']['name'],
                'counting_method': vars['suffixes']['counting_method']['name'],
                'sampling_method': vars['suffixes']['sampling_method']['name'],
                'preparation_method': vars['suffixes']['preparation_method']['name']
            },
            ('seawater',): {
                # SEAWATER
                'SALIN': vars['suffixes']['salinity']['name'],
                'SDEPTH': vars['defaults']['smp_depth']['name'],
                #'FILT': vars['suffixes']['filtered']['name'], Need to fix
                'TTEMP': vars['suffixes']['temperature']['name'],
                'TDEPTH': vars['defaults']['tot_depth']['name'],

            },
            ('biota',): {
                # BIOTA
                'SDEPTH': vars['defaults']['smp_depth']['name'],
                'species': vars['bio']['species']['name'],
                'body_part': vars['bio']['body_part']['name'],
                'bio_group': vars['bio']['bio_group']['name']
            },
            ('sediment',): {
                # SEDIMENT
                'sed_type': vars['sed']['sed_type']['name'],
                'TDEPTH': vars['defaults']['tot_depth']['name'],
            }
        }
    
    elif encoding_type == 'openrefine':
        return {
            ('seawater', 'biota', 'sediment'): {
                # DEFAULT
                'lat': 'latitude',
                'lon': 'longitude',
                'time': 'begperiod',
                #'endperiod': 'endperiod',
                'NUCLIDE': 'nuclide_id',
                'detection_limit': 'detection',
                'unit': 'unit_id',
                'value': 'activity',
                'uncertainty': 'uncertaint',
                #'vartype': 'vartype',
                #'rangelow': 'rangelow',
                #'rangeupp': 'rangeupp',
                #'rl_detection': 'rl_detection',
                #'ru_detection': 'ru_detection',
                #'freq': 'freq',
                'SDEPTH': 'sampdepth',
                #'samparea': 'samparea',
                'SALIN': 'salinity',
                'TTEMP': 'temperatur',
                'FILT': 'filtered',
                #'oxygen': 'oxygen',
                #'sampquality': 'sampquality',
                #'station': 'station',
                #'samplabcode': 'samplabcode',
                #'profile': 'profile',
                #'transect': 'transect',
                #'IODE_QualityFlag': 'IODE_QualityFlag',
                'TDEPTH': 'totdepth',
                #'counmet_id': 'counting_method',
                #'sampmet_id': 'sampling_method',
                #'prepmet_id': 'preparation_method',
                'sampnote': 'sampnote',
                'measurenote': 'measurenote'
            },
            ('seawater',): {
                # SEAWATER
                #'volume': 'volume',
                #'filtpore': 'filtpore',
                #'acid': 'acid'
            },
            ('biota',): {
                # BIOTA
                #'TaxonRepName': 'TaxonRepName',
                'species': 'species_id',
                'body_part': 'bodypar_id',
                #'drywt': 'drywt',
                #'wetwt': 'wetwt',
                #'percentwt': 'percentwt',
                #'drymet_id': 'drymet_id'
            },
            ('sediment',): {
                # SEDIMENT
                'sed_type': 'sedtype_id',
                #'sedtrap': 'sedtrap',
                'top': 'sliceup',
                'bottom': 'slicedown',
                #'SedRepName': 'SedRepName',
                #'drywt': 'drywt',
                #'wetwt': 'wetwt',
                #'percentwt': 'percentwt',
                #'drymet_id': 'drymet_id'
                
            }
        }
    
    else:
        print("Invalid encoding_type provided. Please use 'netcdf' or 'openrefine'.")
        return None


# %% ../../nbs/handlers/helcom.ipynb 291
class SelectAndRenameColumnCB(Callback):
    """
    A callback to select and rename columns in a DataFrame based on provided renaming rules
    for a specified encoding type. It also prints renaming rules that were not applied
    because their keys were not found in the DataFrame.
    """
    
    def __init__(self, fn_renaming_rules, encoding_type='netcdf', verbose=False):
        """
        Initialize the SelectAndRenameColumnCB callback.

        Args:
            fn_renaming_rules (function): A function that returns a dictionary of renaming rules.
            encoding_type (str): The encoding type ('netcdf' or 'openrefine') to determine which renaming rules to use.
            verbose (bool): Whether to print out renaming rules that were not applied.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Apply column selection and renaming to DataFrames in the transformer, and identify unused rules.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        try:
            renaming_rules = self.fn_renaming_rules(self.encoding_type)
        except ValueError as e:
            print(f"Error fetching renaming rules: {e}")
            return

        for group in tfm.dfs.keys():
            # Get relevant renaming rules for the current group
            group_rules = self._get_group_rules(renaming_rules, group)

            if not group_rules:
                continue

            # Apply renaming rules and track keys not found in the DataFrame
            df = tfm.dfs[group]
            df, not_found_keys = self._apply_renaming(df, group_rules)
            tfm.dfs[group] = df
            
            # Print any renaming rules that were not used
            if not_found_keys and self.verbose:
                print(f"\nGroup '{group}' has the following renaming rules not applied:")
                for old_col in not_found_keys:
                    print(f"Key '{old_col}' from renaming rules was not found in the DataFrame.")

    def _get_group_rules(self, renaming_rules, group):
        """
        Retrieve and merge renaming rules for the specified group based on the encoding type.

        Args:
            renaming_rules (dict): Dictionary of all renaming rules.
            group (str): Group name to filter rules.

        Returns:
            dict: A dictionary of renaming rules applicable to the specified group.
        """
        relevant_rules = [rules for key, rules in renaming_rules.items() if group in key]
        merged_rules = {}
        for rules in relevant_rules:
            merged_rules.update(rules)
        return merged_rules

    def _apply_renaming(self, df, rename_rules):
        """
        Select columns based on renaming rules and apply renaming, only for existing columns.

        Args:
            df (pd.DataFrame): DataFrame to modify.
            rename_rules (dict): Dictionary of column renaming rules.

        Returns:
            tuple: A tuple containing:
                - The DataFrame with columns renamed and filtered.
                - A set of column names from renaming rules that were not found in the DataFrame.
        """
        existing_columns = set(df.columns)
        valid_rules = {old_col: new_col for old_col, new_col in rename_rules.items() if old_col in existing_columns}

        # Ensure that columns to keep includes both existing and valid new columns
        columns_to_keep = set(valid_rules.keys()).union(valid_rules.values())
        columns_to_keep = columns_to_keep.intersection(existing_columns)
        df = df[list(columns_to_keep)]
        
        # Apply renaming
        df.rename(columns=valid_rules, inplace=True)

        # Determine which keys were not found
        not_found_keys = set(rename_rules.keys()) - existing_columns
        return df, not_found_keys


# %% ../../nbs/handlers/helcom.ipynb 296
class ReshapeLongToWide(Callback):
    "Convert data from long to wide with renamed columns."
    def __init__(self, columns=['nuclide'], values=['value']):
        fc.store_attr()
        # Retrieve all possible derived vars (e.g 'unc', 'dl', ...) from configs
        self.derived_cols = [value['name'] for value in cdl_cfg()['vars']['suffixes'].values()]
    
    def renamed_cols(self, cols):
        "Flatten columns name"
        return [inner if outer == "value" else f'{inner}{outer}'
                if inner else outer
                for outer, inner in cols]

    def pivot(self, df):
        # Among all possible 'derived cols' select the ones present in df
        derived_coi = [col for col in self.derived_cols if col in df.columns]
        df.index.name = 'org_index'
        df=df.reset_index()
        idx = list(set(df.columns) - set(self.columns + derived_coi + self.values))
        
        # Create a fill_value to replace NaN values in the columns used as the index in the pivot table.
        # Check if num_fill_value is already in the dataframe index values. If num_fill_value already exists
        # then increase num_fill_value by 1 until a value is found for num_fill_value that is not in the dataframe. 
        num_fill_value = -999
        while (df[idx] == num_fill_value).any().any():
            num_fill_value += 1
        # Fill in nan values for each col found in idx. 
        for col in idx:   
            if pd.api.types.is_numeric_dtype(df[col]):
                fill_value = num_fill_value
            if pd.api.types.is_string_dtype(df[col]):
                fill_value = 'NOT AVAILABLE'
                
            df[col]=df[col].fillna(fill_value)

        pivot_df=df.pivot_table(index=idx,
                              columns=self.columns,
                              values=self.values + derived_coi,
                              fill_value=np.nan,
                              aggfunc=lambda x: x
                              ).reset_index()
        

        # Replace fill_value  with  np.nan
        pivot_df[idx]=pivot_df[idx].replace({'NOT AVAILABLE': np.nan,
                                             num_fill_value : np.nan})
        # Set the index to be the org_index
        pivot_df = pivot_df.set_index('org_index')
                
        return (pivot_df)

    def __call__(self, tfm):
        for grp in tfm.dfs.keys():
            tfm.dfs[grp] = self.pivot(tfm.dfs[grp])
            tfm.dfs[grp].columns = self.renamed_cols(tfm.dfs[grp].columns)
            
    

# %% ../../nbs/handlers/helcom.ipynb 305
kw = ['oceanography', 'Earth Science > Oceans > Ocean Chemistry> Radionuclides',
      'Earth Science > Human Dimensions > Environmental Impacts > Nuclear Radiation Exposure',
      'Earth Science > Oceans > Ocean Chemistry > Ocean Tracers, Earth Science > Oceans > Marine Sediments',
      'Earth Science > Oceans > Ocean Chemistry, Earth Science > Oceans > Sea Ice > Isotopes',
      'Earth Science > Oceans > Water Quality > Ocean Contaminants',
      'Earth Science > Biological Classification > Animals/Vertebrates > Fish',
      'Earth Science > Biosphere > Ecosystems > Marine Ecosystems',
      'Earth Science > Biological Classification > Animals/Invertebrates > Mollusks',
      'Earth Science > Biological Classification > Animals/Invertebrates > Arthropods > Crustaceans',
      'Earth Science > Biological Classification > Plants > Macroalgae (Seaweeds)']


# %% ../../nbs/handlers/helcom.ipynb 306
def get_attrs(tfm, zotero_key, kw=kw):
    return GlobAttrsFeeder(tfm.dfs, cbs=[
        BboxCB(),
        DepthRangeCB(),
        TimeRangeCB(cfg()),
        ZoteroCB(zotero_key, cfg=cfg()),
        KeyValuePairCB('keywords', ', '.join(kw)),
        KeyValuePairCB('publisher_postprocess_logs', ', '.join(tfm.logs))
        ])()

# %% ../../nbs/handlers/helcom.ipynb 308
def enums_xtra(tfm, vars):
    "Retrieve a subset of the lengthy enum as 'species_t' for instance"
    enums = Enums(lut_src_dir=lut_path(), cdl_enums=cdl_cfg()['enums'])
    xtras = {}
    for var in vars:
        unique_vals = tfm.unique(var)
        if unique_vals.any():
            xtras[f'{var}_t'] = enums.filter(f'{var}_t', unique_vals)
    return xtras

# %% ../../nbs/handlers/helcom.ipynb 310
def encode(fname_in, fname_out_nc, nc_tpl_path, **kwargs):
    dfs = load_data(fname_in)
    tfm = Transformer(dfs, cbs=[LowerStripRdnNameCB(),
                                RemapRdnNameCB(),
                                ParseTimeCB(),
                                EncodeTimeCB(cfg()),                             
                                NormalizeUncCB(),
                                SanitizeValue(coi_val),                       
                                LookupBiotaSpeciesCB(get_maris_species),
                                LookupBiotaBodyPartCB(get_maris_bodypart),                          
                                LookupBiogroupCB(partial(get_biogroup_lut, species_lut_path())),
                                LookupSedimentCB(get_maris_sediments),
                                LookupUnitCB(),
                                LookupDetectionLimitCB(),    
                                RemapDataProviderSampleIdCB(),
                                RemapStationIdCB(),
                                RemapProfileIdCB(), 
                                RemapSedSliceTopBottomCB(),
                                RemapTaxonRepNameCB(),
                                LookupDryWetRatio(),
                                FormatCoordinates(coi_coordinates, ddmmmm2dddddd),
                                SanitizeLonLatCB(),
                                SelectAndRenameColumnCB(get_renaming_rules, encoding_type='netcdf'),
                                ReshapeLongToWide()
                                ])
    tfm()
    encoder = NetCDFEncoder(tfm.dfs, 
                            src_fname=nc_tpl_path,
                            dest_fname=fname_out_nc, 
                            global_attrs=get_attrs(tfm, zotero_key=zotero_key, kw=kw),
                            verbose=kwargs.get('verbose', False),
                            enums_xtra=enums_xtra(tfm, vars=['species', 'body_part'])
                           )
    encoder.encode()

# %% ../../nbs/handlers/helcom.ipynb 318
class LookupTimeFromEncodedTimeCB(Callback):
    """
    A callback to convert encoded time into a combined date and time column.
    """

    def __init__(self, cfg):
        """
        Initialize the LookupTimeFromEncodedTime callback.

        Args:
            cfg (dict): Configuration dictionary containing time units.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Apply the conversion to DataFrames in the transformer to create a combined date and time column.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            # Apply format_date and format_time to 'time' column, and combine into a single 'begperiod' column
            tfm.dfs[grp]['begperiod'] = tfm.dfs[grp]['begperiod'].apply(lambda x: self.format_datetime(x))

    def format_datetime(self, x):
        """
        Convert encoded time to a combined date and time string.

        Args:
            x (float): Encoded time value.

        Returns:
            str: Combined date and time string in 'dd-MMM-yyyy HH:mm:ss' format.
        """
        date_time = num2pydate(x, units=self.cfg['units']['time'])
        return date_time.strftime('%d/%b/%Y %H:%M:%S')


# %% ../../nbs/handlers/helcom.ipynb 323
class GetSampleTypeCB(Callback):
    """
    A callback to set the 'Sample type' column in DataFrames based on a lookup table.
    If the 'type_lut' is provided, it will use it to map group names to sample types.
    Otherwise, it defaults to using the group name in uppercase.
    """
    
    def __init__(self, type_lut=None):
        """
        Initialize the GetSampleTypeCB callback.

        Args:
            type_lut (dict, optional): A lookup table to map group names to sample types.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Apply the sample type lookup to DataFrames in the transformer.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for key in tfm.dfs.keys():
            df = tfm.dfs[key]
            
            # Determine the sample type
            sample_type = self._get_sample_type(key)
            
            # Set the 'Sample type' column
            df['Sample type'] = sample_type

    def _get_sample_type(self, group_name):
        """
        Determine the sample type for a given group name using the lookup table.

        Args:
            group_name (str): The name of the group.

        Returns:
            str: The sample type.
        """
        
        # Return the sample type from the lookup table
        return self.type_lut[group_name.upper()]


# %% ../../nbs/handlers/helcom.ipynb 327
def get_nucnames_lut():
    df = pd.read_excel(nuc_lut_path(), usecols=['nc_name','nuclide_id'])
    return df.set_index('nc_name').to_dict()['nuclide_id']

# %% ../../nbs/handlers/helcom.ipynb 328
class LookupNuclideByIdCB(Callback):
    """
    A callback to lookup and clean MARIS nuclide identifiers by replacing `nuclide` IDs
    with corresponding names from a lookup table.
    """
    
    def __init__(self, fn_lut=get_nucnames_lut):
        """
        Initialize the LookupNuclideByIdCB callback.

        Args:
            fn_lut (function): A function that returns a lookup table for nuclide names.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Apply the nuclide lookup to DataFrames in the transformer.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        for key in tfm.dfs.keys():
            df = tfm.dfs[key]
            self._lookup_and_clean_nuclides(df, lut)

    def _lookup_and_clean_nuclides(self, df, lut):
        """
        Replace nuclide IDs with names from the lookup table and clean the 'Nuclide' column.

        Args:
            df (pd.DataFrame): The DataFrame to process.
            lut (dict): The lookup table for nuclide names.
        """
        # Replace nuclide IDs with names from the lookup table
        df['nuclide_id'] = df['nuclide_id'].replace(lut)


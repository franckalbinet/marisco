"""Data pipeline (handler) to convert OSPAR data ([source](https://odims.ospar.org/en/)) to `NetCDF` format or `Open Refine` format."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/handlers/ospar.ipynb.

# %% auto 0
__all__ = ['fname_in', 'fname_out_nc', 'fname_out_csv', 'zotero_key', 'ref_id', 'lut_fname_smp_type', 'get_unique_nuclides',
           'varnames_lut_updates', 'unmatched_fixes_biota_species', 'get_maris_species', 'whole_animal_plant',
           'unmatched_fixes_biota_tissues', 'renaming_unit_rules', 'kw', 'load_data', 'get_nuc_id_lut',
           'RemapRdnNameCB', 'ParseTimeCB', 'SanitizeValue', 'unc_exp2stan', 'get_maris_lut', 'LookupBiotaSpeciesCB',
           'CorrectWholeBodyPartCB', 'LookupBiotaBodyPartCB', 'get_biogroup_lut', 'LookupBiogroupCB',
           'get_taxon_info_lut', 'LookupTaxonInformationCB', 'LookupUnitCB', 'LookupDetectionLimitCB',
           'RemapDataProviderSampleIdCB', 'RemapStationIdCB', 'RecordMeasurementNoteCB', 'RecordRefNoteCB',
           'RecordSampleNoteCB', 'ConvertLonLatCB', 'get_renaming_rules', 'SelectAndRenameColumnCB',
           'ReshapeLongToWide', 'get_attrs', 'enums_xtra', 'encode', 'encode_or']

# %% ../../nbs/handlers/ospar.ipynb 6
import pandas as pd 
import numpy as np
from tqdm import tqdm 
from functools import partial 
import fastcore.all as fc 
from pathlib import Path 
from dataclasses import asdict
from typing import List, Dict, Callable,  Tuple
from math import modf
from collections import OrderedDict

from ..utils import (has_valid_varname, match_worms, match_maris_lut, 
                           Match, get_unique_across_dfs)
from ..callbacks import (Callback, Transformer, EncodeTimeCB, AddSampleTypeIdColumnCB,
                               LowerStripRdnNameCB, SanitizeLonLatCB, CompareDfsAndTfmCB)
from ..metadata import (GlobAttrsFeeder, BboxCB, DepthRangeCB, TimeRangeCB, ZoteroCB, KeyValuePairCB)
from ..configs import (nuc_lut_path, nc_tpl_path, cfg, cache_path, cdl_cfg, Enums, lut_path,
                             species_lut_path, sediments_lut_path, bodyparts_lut_path, 
                             detection_limit_lut_path, filtered_lut_path, area_lut_path)

# Check if it is worths for these Callbacks to be abstracted in `callbacks.ipynb`
# OSPAR pipeline is very similar as HELCOM one so a lot of reuse possible.
from .helcom import (get_varnames_lut, get_nuc_id_lut,
                                     RemapRdnNameCB)

from ..serializers import NetCDFEncoder,  OpenRefineCsvEncoder
import warnings

# %% ../../nbs/handlers/ospar.ipynb 10
fname_in = '../../_data/accdb/ospar/csv'
fname_out_nc = '../../_data/output/191-OSPAR-2024.nc'
fname_out_csv = '../../_data/output/191-OSPAR-2024.csv'
zotero_key ='LQRA4MMK' # OSPAR MORS zotero key
ref_id = 191 # OSPAR reference id as defined by MARIS

# %% ../../nbs/handlers/ospar.ipynb 14
lut_fname_smp_type = {'Seawater data': 'seawater', 'Biota data': 'biota'}

# %% ../../nbs/handlers/ospar.ipynb 15
def load_data(src_dir:str, # Directory where the source CSV files are located
              lut:dict=lut_fname_smp_type # A dictionary with the file name as key and the sample type as value
              ) -> dict: # A dictionary with sample types as keys and their corresponding dataframes as values
    "Load `OSPAR` data and return the data in a dictionary of dataframes with the dictionary key as the sample type."
    return {
        sample_type: pd.read_csv(Path(src_dir) / f'{file_name}.csv', encoding='unicode_escape')
        for file_name, sample_type in lut.items()
    }

# %% ../../nbs/handlers/ospar.ipynb 31
get_unique_nuclides = partial(get_unique_across_dfs, col_name='NUCLIDE')


# %% ../../nbs/handlers/ospar.ipynb 34
varnames_lut_updates = {
            "239, 240 pu" :  'pu239_240_tot',
            "cs-137" :  'cs137',
            "241am" : 'am241',
            "228ra" : 'ra228',
            "3h" : 'h3',
            "99tc" : 'tc99' ,
            "cs-134" : 'cs134',
            "210pb" : 'pb210',
            "239,240pu" : 'pu239_240_tot',
            "238pu" : 'pu238',
            "137cs" : 'cs137',
            "226ra" : 'ra226',
            "ra-228" : 'ra228',
            "ra-226" : 'ra226',
            "210po" : 'po210'}

# %% ../../nbs/handlers/ospar.ipynb 39
def get_nuc_id_lut():
    df = pd.read_excel(nuc_lut_path(), usecols=['nc_name','nuclide_id'])
    return df.set_index('nc_name').to_dict()['nuclide_id']

# %% ../../nbs/handlers/ospar.ipynb 41
class RemapRdnNameCB(Callback):
    """Remap and standardize radionuclide names to MARIS radionuclide names and define nuclide ids."""
    
    def __init__(self, 
                 fn_lut: Callable[[Dict[str, pd.DataFrame]], Dict[str, str]] = partial(get_varnames_lut, lut=varnames_lut_updates),
                 nuc_id_lut: Callable[[], Dict[str, str]] = get_nuc_id_lut):
        """
        Initialize the RemapRdnNameCB with functions to generate lookup tables for radionuclide names 
        and nuclide IDs.

        Args:
            fn_lut (Callable, optional): A function that takes a dictionary of DataFrames and returns a lookup table 
                                         for remapping radionuclide names.
            nuc_id_lut (Callable, optional): A function that returns a lookup table for nuclide IDs.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """Apply lookup tables to remap radionuclide names and obtain nuclide IDs in DataFrames.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut(tfm.dfs)
        nuc_id_lut = self.nuc_id_lut()
        
        for grp in tfm.dfs:
            df = tfm.dfs[grp]
            # Drop rows where 'NUCLIDE' is NaN
            df.dropna(subset=['NUCLIDE'], inplace=True)
            self._remap_nuclide_names(df, lut)
            self._apply_nuclide_ids(df, nuc_id_lut)
                        
            
    def _remap_nuclide_names(self, df: pd.DataFrame, lut: Dict[str, str]):
        """
        Remap radionuclide names in the 'NUCLIDE' column of the DataFrame using the provided lookup table.

        Args:
            df (pd.DataFrame): DataFrame containing the 'NUCLIDE' column.
            lut (Dict[str, str]): Lookup table for remapping radionuclide names.
        """
        if 'NUCLIDE' in df.columns:
            df['NUCLIDE'] = df['NUCLIDE'].replace(lut)
        else:
            print(f"No 'NUCLIDE' column found in DataFrame of group {df.name}")

    def _apply_nuclide_ids(self, df: pd.DataFrame, nuc_id_lut: Dict[str, str]):
        """
        Apply nuclide IDs to the 'NUCLIDE' column using the provided nuclide ID lookup table.

        Args:
            df (pd.DataFrame): DataFrame containing the 'NUCLIDE' column.
            nuc_id_lut (Dict[str, str]): Lookup table for nuclide IDs.
        """
        if 'NUCLIDE' in df.columns:
            df['nuclide_id'] = df['NUCLIDE'].map(nuc_id_lut)
        else:
            print(f"No 'NUCLIDE' column found in DataFrame of group {df.name}")


# %% ../../nbs/handlers/ospar.ipynb 54
class ParseTimeCB(Callback):
    def __init__(self):
        fc.store_attr()
            
        
    def __call__(self, tfm):
        for grp in tfm.dfs.keys():
            df = tfm.dfs[grp]
            self._process_dates(df)
            self._define_beg_period(df)
            self._remove_nan(df)

    def _process_dates(self, df: pd.DataFrame):
        """
        Process and correct date and time information in the DataFrame.

        Args:
            df (pd.DataFrame): DataFrame containing the 'Sampling date' column.
        """
        if 'Sampling date' in df.columns:
            # Convert 'Sampling date' to datetime, ignoring errors to avoid NaNs
            df['time'] = pd.to_datetime(df['Sampling date'], format='%d/%m/%Y', errors='coerce')
        else:
            # Create 'time' column with NaT if 'Sampling date' doesn't exist
            df['time'] = pd.NaT                
                    
    def _define_beg_period(self, df: pd.DataFrame):
        """
        Create a standardized date representation for Open Refine.
        
        Args:
            df (pd.DataFrame): DataFrame containing the 'time' column.
        """
        df['begperiod'] = df['time']

    def _remove_nan(self, df: pd.DataFrame):
        """
        Remove rows with NaN entries in the 'time' column.
        
        Args:
            df (pd.DataFrame): DataFrame containing the 'time' column.
        """
        df.dropna(subset=['time'], inplace=True)



# %% ../../nbs/handlers/ospar.ipynb 67
class SanitizeValue(Callback):
    "Sanitize value by removing blank entries."

    def __init__(self):
        """
        Initialize the SanitizeValue callback.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Sanitize the DataFrames in the transformer by removing rows with blank values in specified columns.
        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            self._sanitize_dataframe(tfm.dfs[grp], grp)


    def _sanitize_dataframe(self, df: pd.DataFrame, grp: str):
        """
        Remove rows where value column (i.e. 'Activity or MDA') is blank and remap to 'value' column.

        Args:
            df (pd.DataFrame): DataFrame to sanitize.
            grp (str): Group name to determine column names.
        """
        value_col = 'Activity or MDA'
        if value_col in df.columns:
            df.dropna(subset=[value_col], inplace=True)
            df['value'] = df[value_col]
            

# %% ../../nbs/handlers/ospar.ipynb 75
# Make measurement and uncertainty units consistent
def unc_exp2stan(df: pd.DataFrame, unc_col: str) -> pd.Series:
    """
    Convert expanded uncertainty (k=2) to standard uncertainty (k=1).

    Args:
        df (pd.DataFrame): DataFrame containing uncertainty values.
        unc_col (str): Column name of the uncertainty values to be converted.

    Returns:
        pd.Series: Series of standard uncertainty values.
    """
    k = 2
    return df[unc_col] / k

# %% ../../nbs/handlers/ospar.ipynb 83
def get_maris_lut(df_biota,
                  fname_cache,  # For instance 'species_ospar.pkl'
                  data_provider_name_col: str,  # Data provider lookup column name of interest
                  maris_lut: Callable,  # Function retrieving MARIS source lookup table
                  maris_id: str,  # Id of MARIS lookup table nomenclature item to match
                  maris_name: str,  # Name of MARIS lookup table nomenclature item to match
                  unmatched_fixes={},
                  as_dataframe=False,
                  overwrite=False):
    """
    Generate a lookup table mapping data provider names to MARIS radionuclide names.

    Args:
        df_biota (pd.DataFrame): DataFrame containing biota data.
        fname_cache (str): Cache file name for storing the lookup table.
        data_provider_name_col (str): Column name of interest in the data provider's dataset.
        maris_lut (Callable): Function to retrieve MARIS source lookup table.
        maris_id (str): Id of MARIS lookup table nomenclature item to match.
        maris_name (str): Name of MARIS lookup table nomenclature item to match.
        unmatched_fixes (dict): Dictionary of unmatched names and their corrections.
        as_dataframe (bool): Whether to return the lookup table as a DataFrame.
        overwrite (bool): Whether to overwrite the cache file if it exists.

    Returns:
        dict or pd.DataFrame: Lookup table mapping data provider names to MARIS radionuclide names.
    """
    fname_cache = Path(cache_path()) / fname_cache
    maris_lut_table = maris_lut()

    if overwrite or not fname_cache.exists():
        lut = _generate_lookup_table(df_biota, data_provider_name_col, maris_lut_table, maris_id, maris_name, unmatched_fixes)
        fc.save_pickle(fname_cache, lut)
    else:
        lut = fc.load_pickle(fname_cache)

    if as_dataframe:
        return _convert_lut_to_dataframe(lut)
    else:
        return lut

def _generate_lookup_table(df_biota, data_provider_name_col, maris_lut_table, maris_id, maris_name, unmatched_fixes):
    """
    Generate the lookup table from the provided data.

    Args:
        df_biota (pd.DataFrame): DataFrame containing biota data.
        data_provider_name_col (str): Column name of interest in the data provider's dataset.
        maris_lut_table (pd.DataFrame): MARIS source lookup table.
        maris_id (str): Id of MARIS lookup table nomenclature item to match.
        maris_name (str): Name of MARIS lookup table nomenclature item to match.
        unmatched_fixes (dict): Dictionary of unmatched names and their corrections.

    Returns:
        dict: Lookup table mapping data provider names to MARIS radionuclide names.
    """
    lut = {}
    unique_names = df_biota[data_provider_name_col].unique()
    for name in tqdm(unique_names, total=len(unique_names), desc="Generating lookup table"):
        corrected_name = unmatched_fixes.get(name, name)
        corrected_name = _sanitize_name(corrected_name)
        result = match_maris_lut(maris_lut_table, corrected_name, maris_id, maris_name)
        match = Match(result.iloc[0][maris_id], result.iloc[0][maris_name], name, result.iloc[0]['score'])
        lut[name] = match
    return lut

def _sanitize_name(name):
    """
    Ensure the name is a string and convert it to lowercase, stripping any trailing spaces.

    Args:
        name (any): The name to sanitize.

    Returns:
        str: The sanitized name.
    """
    if isinstance(name, str):
        return name.lower().strip()
    else:
        return str(name).lower().strip()

def _convert_lut_to_dataframe(lut):
    """
    Convert the lookup table dictionary to a sorted DataFrame.

    Args:
        lut (dict): Lookup table mapping data provider names to MARIS radionuclide names.

    Returns:
        pd.DataFrame: Sorted DataFrame of the lookup table.
    """
    df_lut = pd.DataFrame({k: asdict(v) for k, v in lut.items()}).transpose()
    df_lut.index.name = 'source_id'
    return df_lut.sort_values(by='match_score', ascending=False)


# %% ../../nbs/handlers/ospar.ipynb 87
# key equals name in dfs['biota']. 
# value equals replacement name to use in match_maris_lut (i.e. name_to_match)
unmatched_fixes_biota_species = {}

# %% ../../nbs/handlers/ospar.ipynb 93
# LookupBiotaSpeciesCB filters 'Not available'. 
unmatched_fixes_biota_species = {'RHODYMENIA PSEUDOPALAMATA & PALMARIA PALMATA': 'Not available', # mix
 'Mixture of green, red and brown algae': 'Not available', #mix 
 'Solea solea (S.vulgaris)': 'Solea solea',
 'SOLEA SOLEA (S.VULGARIS)': 'Solea solea',
 'CERASTODERMA (CARDIUM) EDULE': 'Cerastoderma edule',
 'Cerastoderma (Cardium) Edule': 'Cerastoderma edule',
 'MONODONTA LINEATA': 'Phorcus lineatus',
 'NUCELLA LAPILLUS': 'Not available', # Droped. In worms 'Nucella lapillus (Linnaeus, 1758)', 
 'DICENTRARCHUS (MORONE) LABRAX': 'Dicentrarchus labrax',
 'Pleuronectiformes [order]': 'Pleuronectiformes',
 'RAJIDAE/BATOIDEA': 'Not available', #mix 
 'PALMARIA PALMATA': 'Not available', # Dropped. In worms 'Palmaria palmata (Linnaeus) F.Weber & D.Mohr, 1805',
 'Sepia spp.': 'Sepia',
 'Rhodymenia spp.': 'Rhodymenia',
 'unknown': 'Not available',
 'RAJA DIPTURUS BATIS': 'Dipturus batis',
 'Unknown': 'Not available',
 'Flatfish': 'Not available',
 'FUCUS SPP.': 'FUCUS',
 'Patella sp.': 'Patella',
 'Gadus sp.': 'Gadus',
 'FUCUS spp': 'FUCUS',
 'Tapes sp.': 'Tapes',
 'Thunnus sp.': 'Thunnus',
 'RHODYMENIA spp': 'RHODYMENIA',
 'Fucus sp.': 'Fucus',
 'PECTINIDAE': 'Not available', # Droped. In worms as PECTINIDAE is a family.
 'PLUERONECTES PLATESSA': 'Pleuronectes platessa',
 'Gaidropsarus argenteus': 'Gaidropsarus argentatus'}

# %% ../../nbs/handlers/ospar.ipynb 102
class LookupBiotaSpeciesCB(Callback):
    """Remap biota species to MARIS database format.This class updates the 'Species' column in the biota DataFrame by:
    - Replacing 'NaN' or 'Not available' values with corresponding biological groups.
    - Performing a lookup to remap species to MARIS format."""

    def __init__(self, fn_lut: Callable[[pd.DataFrame], dict], unmatched_fixes_biota_species: dict):
        """
        Initialize the LookupBiotaSpeciesCB with a lookup function and unmatched fixes.
        
        Args:
            fn_lut (Callable[[pd.DataFrame], dict]): Function that returns a lookup table dictionary.
            unmatched_fixes_biota_species (dict): Dictionary mapping species to their fixes.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply the callback to the biota DataFrame within the transformer.
        
        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        # Get the lookup table
        lut = self.fn_lut(df_biota=tfm.dfs['biota'])
        biota_df = tfm.dfs['biota']

        # Process the biota DataFrame
        biota_df = self._replace_nan_species_with_biological_group(biota_df)
        biota_df = self._perform_lookup(biota_df, lut)
        
        # Update the transformer DataFrame
        tfm.dfs['biota'] = biota_df

    def _replace_nan_species_with_biological_group(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Replace 'NaN' or 'Not available' values in 'Species' column with the corresponding 'Biological group'.
        
        Args:
            df (pd.DataFrame): DataFrame containing the 'Species' and 'Biological group' columns.

        Returns:
            pd.DataFrame: DataFrame with updated 'Species' column.
        """
        na_list = ['Not available']
        na_biota_species = {k for k, v in self.unmatched_fixes_biota_species.items() if v in na_list}
        
        # Replace 'Not available' and 'NaN' with 'Biological group'
        df['Species'] = df.apply(
            lambda row: row['Biological group'] if row['Species'] in na_biota_species or pd.isna(row['Species']) else row['Species'], axis=1)
        return df

    def _perform_lookup(self, df: pd.DataFrame, lut: dict) -> pd.DataFrame:
        """
        Perform lookup to remap species based on the provided lookup table.
        
        Args:
            df (pd.DataFrame): DataFrame containing the 'Species' column.
            lut (dict): Dictionary mapping species names to their MARIS IDs.

        Returns:
            pd.DataFrame: DataFrame with updated 'species' column based on the lookup table.
        """
        df['species'] = df['Species'].apply(lambda x: lut.get(x, Match(-1, None, x, None)).matched_id)
        return df


# %% ../../nbs/handlers/ospar.ipynb 103
get_maris_species = partial(get_maris_lut, 
                fname_cache='species_ospar.pkl', 
                data_provider_name_col='SCIENTIFIC NAME',
                maris_lut=species_lut_path,
                maris_id='species_id',
                maris_name='species',
                unmatched_fixes=unmatched_fixes_biota_species,
                as_dataframe=False,
                overwrite=False)

# %% ../../nbs/handlers/ospar.ipynb 110
whole_animal_plant = {'whole' : ['Whole','WHOLE', 'WHOLE FISH', 'Whole fisk', 'Whole fish'],
                      'Whole animal' : ['Molluscs','Fish','FISH','molluscs','fish','MOLLUSCS'],
                      'Whole plant' : ['Seaweed','seaweed','SEAWEED'] }

# %% ../../nbs/handlers/ospar.ipynb 111
class CorrectWholeBodyPartCB(Callback):
    """Update body parts labeled as 'whole' to either 'Whole animal' or 'Whole plant'."""
    
    def __init__(self, wap: Dict[str, List[str]] = whole_animal_plant):
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        self.correct_whole_body_part(tfm.dfs['biota'])

    def correct_whole_body_part(self, df: pd.DataFrame):
        df['body_part'] = df['Body Part']   
        self.update_body_part(df, self.wap['whole'], self.wap['Whole animal'], 'Whole animal')
        self.update_body_part(df, self.wap['whole'], self.wap['Whole plant'], 'Whole plant')

    def update_body_part(self, df: pd.DataFrame, whole_list: List[str], group_list: List[str], new_value: str):
        mask = (df['body_part'].isin(whole_list)) & (df['Biological group'].isin(group_list))
        df.loc[mask, 'body_part'] = new_value


# %% ../../nbs/handlers/ospar.ipynb 114
unmatched_fixes_biota_tissues = {}

# %% ../../nbs/handlers/ospar.ipynb 121
unmatched_fixes_biota_tissues = {
'Mix of muscle and whole fish without liver' : 'Not available', # Drop
 'Whole without head' : 'Whole animal eviscerated without head', # Drop? eviscerated? ,
 'Cod medallion' : 'Whole animal eviscerated without head',
 'FLESH' : 'Flesh without bones', # Drop? with or without bones?
 'Flesh' : 'Flesh without bones', # Drop? with or without bones?
 'UNKNOWN' : 'Not available',
 'FLESH WITHOUT BONE' : 'Flesh without bones'
}

# %% ../../nbs/handlers/ospar.ipynb 125
class LookupBiotaBodyPartCB(Callback):
    """Update body part id based on MARIS dbo_bodypar.xlsx"""

    def __init__(self, fn_lut: Callable, unmatched_fixes_biota_tissues: Dict[str, str]):
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        lut = self.fn_lut(df_biota=tfm.dfs['biota'])
        self.drop_nan_species(tfm.dfs['biota'])
        self.drop_unmatched(tfm.dfs['biota'])
        self.perform_lookup(tfm.dfs['biota'], lut)

    def drop_nan_species(self, df: pd.DataFrame):
        """
        Drop rows where 'body_part' is NaN.

        Args:
            df (pd.DataFrame): The DataFrame to process.
        """
        df.dropna(subset=['body_part'], inplace=True)

    def drop_unmatched(self, df: pd.DataFrame):
        """
        Drop rows where the 'body_part' is in the unmatched_fixes_biota_tissues list with value 'Not available'.

        Args:
            df (pd.DataFrame): The DataFrame to process.
        """
        na_list = ['Not available']
        na_biota_tissues = [k for k, v in self.unmatched_fixes_biota_tissues.items() if v in na_list]
        df.drop(df[df['body_part'].isin(na_biota_tissues)].index, inplace=True)

    def perform_lookup(self, df: pd.DataFrame, lut: Dict[str, 'Match']):
        """
        Perform lookup to update 'body_part' with matched IDs.

        Args:
            df (pd.DataFrame): The DataFrame to process.
            lut (Dict[str, Match]): The lookup table.
        """
        df['body_part'] = df['body_part'].apply(lambda x: lut[x].matched_id if x in lut else x)


# %% ../../nbs/handlers/ospar.ipynb 133
def get_biogroup_lut(maris_lut: str) -> dict:
    """
    Retrieve a lookup table for biogroup ids from a MARIS lookup table.

    Args:
        maris_lut (str): Path to the MARIS lookup table (Excel file).

    Returns:
        dict: A dictionary mapping species_id to biogroup_id.
    """
    species = pd.read_excel(maris_lut)
    return species[['species_id', 'biogroup_id']].set_index('species_id').to_dict()['biogroup_id']


# %% ../../nbs/handlers/ospar.ipynb 135
class LookupBiogroupCB(Callback):
    """Update biogroup id based on MARIS dbo_species.xlsx."""

    def __init__(self, fn_lut: Callable):
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        lut = self.fn_lut()
        self.update_bio_group(tfm.dfs['biota'], lut)

    def update_bio_group(self, df: pd.DataFrame, lut: dict):
        """
        Update the 'bio_group' column in the DataFrame based on the lookup table.

        Args:
            df (pd.DataFrame): The DataFrame to process.
            lut (Dict[str, Any]): The lookup table for updating 'bio_group'.
        """
        df['bio_group'] = df['species'].apply(lambda x: lut.get(x, -1))


# %% ../../nbs/handlers/ospar.ipynb 145
def get_taxon_info_lut(maris_lut: str) -> dict:
    """
    Retrieve a lookup table for Taxonname from a MARIS lookup table.

    Args:
        maris_lut (str): Path to the MARIS lookup table (Excel file).

    Returns:
        dict: A dictionary mapping species_id to biogroup_id.
    """
    species = pd.read_excel(maris_lut)
    return species[['species_id', 'Taxonname', 'Taxonrank','TaxonDB','TaxonDBID','TaxonDBURL']].set_index('species_id').to_dict()

# TODO include Commonname field after next MARIS data reconciling process.

# %% ../../nbs/handlers/ospar.ipynb 146
class LookupTaxonInformationCB(Callback):
    """Update taxon names based on MARIS species LUT (dbo_species.xlsx)."""
    def __init__(self, fn_lut: Callable[[], dict]):
        """
        Initialize the LookupTaxonNameCB with a function to generate the lookup table.

        Args:
            fn_lut (Callable[[], dict]): Function that returns the lookup table dictionary.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Update the 'taxon_name' column in the DataFrame using the lookup table and print unmatched species IDs.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        lut = self.fn_lut()
        
        
        self._set_taxon_rep_name(tfm.dfs['biota'])
        tfm.dfs['biota']['Taxonname'] =  tfm.dfs['biota']['species'].apply(lambda x: self._get_name_by_species_id(x, lut['Taxonname']))
        #df['Commonname'] = df['species'].apply(lambda x: self._get_name_by_species_id(x, lut['Commonname']))
        tfm.dfs['biota']['Taxonrank'] =  tfm.dfs['biota']['species'].apply(lambda x: self._get_name_by_species_id(x, lut['Taxonrank']))
        tfm.dfs['biota']['TaxonDB'] =  tfm.dfs['biota']['species'].apply(lambda x: self._get_name_by_species_id(x, lut['TaxonDB']))
        tfm.dfs['biota']['TaxonDBID'] =  tfm.dfs['biota']['species'].apply(lambda x: self._get_name_by_species_id(x, lut['TaxonDBID']))
        tfm.dfs['biota']['TaxonDBURL'] =  tfm.dfs['biota']['species'].apply(lambda x: self._get_name_by_species_id(x, lut['TaxonDBURL']))


    def _set_taxon_rep_name(self, df: pd.DataFrame):
        """
        Remap the 'TaxonRepName' column to the 'Species' column values.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        # Ensure both columns exist before attempting to remap
        if 'Species' in df.columns:
            df['TaxonRepName'] = df['Species']
        else:
            print("Warning: 'Species' column not found in DataFrame.")
            
            

    def _get_name_by_species_id(self, species_id: str, lut: dict) -> str:
        """
        Get the  name from the lookup table and print species ID if the taxon name is not found.

        Args:
            species_id (str): The species ID from the DataFrame.
            lut (dict): The lookup table dictionary.

        Returns:
            str: The name from the lookup table.
        """
        name = lut.get(species_id, 'Unknown')  # Default to 'Unknown' if not found
        if name == 'Unknown':
            print(f"Unmatched species ID: {species_id} for {lut.keys()[0]}")
        return name


# %% ../../nbs/handlers/ospar.ipynb 153
# Define unit names renaming rules
renaming_unit_rules = {'Bq/l': 1, #'Bq/m3'
                       'Bq/L': 1,
                       'BQ/L': 1,
                       'Bq/kg f.w.': 5, # Bq/kgw
                       'Bq/kg.fw' : 5,
                       'Bq/kg fw' : 5,
                       'Bq/kg f.w' : 5 
                       } 

# %% ../../nbs/handlers/ospar.ipynb 154
class LookupUnitCB(Callback):
    """Update the 'unit' column in DataFrames based on a lookup table.
    The class handles:
    - Assigning a default unit for NaN values in the 'Unit' column for specific groups.
    - Dropping rows with NaN values in the 'Unit' column.
    - Performing lookup to update the 'unit' column based on the provided lookup table."""

    def __init__(self, lut: dict = renaming_unit_rules):
        """
        Initialize the LookupUnitCB with a lookup table.

        Args:
            lut (dict): A dictionary used for lookup to update the 'unit' column.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply the callback to each DataFrame in the transformer.

        Args:
            tfm (Transformer): The transformer containing DataFrames to process.
        """
        for grp in tfm.dfs.keys():
            if grp == 'seawater':
                self._apply_units(tfm.dfs[grp])
            self._drop_na_units(tfm.dfs[grp])
            self._perform_lookup(tfm.dfs[grp])

    def _apply_units(self, df: pd.DataFrame):
        """
        Apply a default unit where the 'Unit' column is NaN.

        Args:
            df (pd.DataFrame): The DataFrame to process.
        """
        df.loc[df['Unit'].isnull(), 'Unit'] = 'Bq/l'

    def _drop_na_units(self, df: pd.DataFrame):
        """
        Drop rows where the 'Unit' column has NaN values.

        Args:
            df (pd.DataFrame): The DataFrame to process.
        """
        df.dropna(subset=['Unit'], inplace=True)

    def _perform_lookup(self, df: pd.DataFrame):
        """
        Perform lookup to update the 'unit' column based on the lookup table.

        Args:
            df (pd.DataFrame): The DataFrame to process.
        """
        df['unit'] = df['Unit'].apply(lambda x: self.lut.get(x, 'Unknown'))


# %% ../../nbs/handlers/ospar.ipynb 161
class LookupDetectionLimitCB(Callback):
    """Remap activity value, activity uncertainty, and detection limit to MARIS format.
    This class performs the following operations:
    - Reads a lookup table from an Excel file.
    - Copies and processes the 'Value type' column.
    - Fills NaN values with 'Not Available'.
    - Drops rows where 'Value type' is not in the lookup table.
    - Performs a lookup to update the 'detection_limit' column based on the lookup table.
    """

    def __init__(self, lut_path: str):
        """
        Initialize the LookupDetectionLimitCB with a path to the lookup table.

        Args:
            lut_path (str): The path to the Excel file containing the lookup table.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply the callback to each DataFrame in the transformer.

        Args:
            tfm (Transformer): The transformer containing DataFrames to process.
        """
        lut = self._load_lookup_table()
        for grp in tfm.dfs.keys():
            df = tfm.dfs[grp]
            df = self._copy_and_fill_na(df)
            df = self._correct_greater_than(df)  # Ensure to correct 'Value type' if necessary
            df = self._drop_na_rows(df, lut)
            self._perform_lookup(df, lut)
            tfm.dfs[grp] = df  # Update the DataFrame in the transformer

    def _load_lookup_table(self) -> dict:
        """
        Load the lookup table from the Excel file and create a mapping dictionary.

        Returns:
            dict: A dictionary mapping value types to detection limits.
        """
        df = pd.read_excel(self.lut_path)
        df = df.astype({'id': 'int'})
        return dict((v, k) for k, v in df.set_index('id')['name'].to_dict().items())

    def _copy_and_fill_na(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Copy the 'Value type' column and fill NaN values with 'Not Available'.

        Args:
            df (pd.DataFrame): The DataFrame to process.

        Returns:
            pd.DataFrame: The DataFrame with updated 'detection_limit' column.
        """
        df['detection_limit'] = df['Value type']
        df['detection_limit'].fillna('Not Available', inplace=True)
        return df
    
    def _correct_greater_than(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Correct the 'Value type' where it is '>' by changing it to '<'.

        Args:
            df (pd.DataFrame): The DataFrame to process.

        Returns:
            pd.DataFrame: The DataFrame with corrected 'Value type'.
        """
        df.loc[df['detection_limit'] == '>', 'detection_limit'] = '<'
        return df


    def _drop_na_rows(self, df: pd.DataFrame, lut: dict) -> pd.DataFrame:
        """
        Drop rows where the 'detection_limit' column has values not in the lookup table.

        Args:
            df (pd.DataFrame): The DataFrame to process.

        Returns:
            pd.DataFrame: The DataFrame with rows dropped where 'detection_limit' is not in the lookup table.
        """
        return df[df['detection_limit'].isin(lut.keys())]

    def _perform_lookup(self, df: pd.DataFrame, lut: dict):
        """
        Perform lookup to update the 'detection_limit' column based on the lookup table.

        Args:
            df (pd.DataFrame): The DataFrame to process.
            lut (dict): The lookup table dictionary.
        """
        df['detection_limit'] = df['detection_limit'].apply(lambda x: lut.get(x, 0))


# %% ../../nbs/handlers/ospar.ipynb 168
class RemapDataProviderSampleIdCB(Callback):
    """Remap 'KEY' column to 'samplabcode' in each DataFrame."""

    def __init__(self):
        """
        Initialize the RemapDataProviderSampleIdCB.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Remap 'KEY' column to 'samplabcode' in the DataFrames.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs:
            self._remap_sample_id(tfm.dfs[grp])
    
    def _remap_sample_id(self, df: pd.DataFrame):
        """
        Remap the 'KEY' column to 'samplabcode' in the DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['samplabcode'] = df['Sample ID']


# %% ../../nbs/handlers/ospar.ipynb 178
class RemapStationIdCB(Callback):
    """Remap Station ID to MARIS format."""

    def __init__(self):
        """
        Initialize the RemapStationIdCB with no specific parameters.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Iterate through all DataFrames in the transformer object and remap 'STATION' to 'station_id'.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        for grp in tfm.dfs.keys():
            self._remap_station_id(tfm.dfs[grp])

    def _remap_station_id(self, df: pd.DataFrame):
        """
        Remap 'STATION' column to 'station_id' in the given DataFrame.

        Args:
            df (pd.DataFrame): The DataFrame to modify.
        """
        df['station'] = df['Station ID'] + ', ' + df['Contracting Party']

# %% ../../nbs/handlers/ospar.ipynb 184
class RecordMeasurementNoteCB(Callback):
    """Record measurement notes by adding a 'measurenote' column to DataFrames."""
    
    def __init__(self):
        """
        Initialize the RecordMeasurementNoteCB.

        This class does not require additional arguments or setup for initialization.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply the callback to each DataFrame in the transformer to add the 'measurenote' column.

        Args:
            tfm (Transformer): The transformer object containing DataFrames to process.
        
        This method iterates over all DataFrames in the transformer and checks for the
        presence of the 'Measurement Comment' column. If found, it copies the values
        to a new 'measurenote' column. If not found, it prints a warning message.
        """
        for grp, df in tfm.dfs.items():
            if 'Measurement Comment' in df.columns:
                self._add_measurementnote(df)
            else:
                print(f"Warning: 'Measurement Comment' column not found in DataFrame for group '{grp}'")

    def _add_measurementnote(self, df: pd.DataFrame):
        """
        Add the 'measurenote' column to the DataFrame by mapping values from 'Measurement Comment'.

        Args:
            df (pd.DataFrame): DataFrame containing the 'Measurement Comment' column.
        
        The 'Measurement Comment' column values are copied to the new 'measurenote' column.
        """
        df['measurenote'] = df['Measurement Comment']


# %% ../../nbs/handlers/ospar.ipynb 190
class RecordRefNoteCB(Callback):
    """Record reference notes by adding a 'refnote' column to DataFrames."""
    
    def __init__(self):
        """
        Initialize the RecordRefNoteCB.

        This class does not require additional arguments or setup for initialization.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply the callback to each DataFrame in the transformer to add the 'refnote' column.

        Args:
            tfm (Transformer): The transformer object containing DataFrames to process.
        
        This method iterates over all DataFrames in the transformer and checks for the
        presence of the 'Reference Comment' column. If found, it copies the values
        to a new 'refnote' column. If not found, it prints a warning message.
        """
        for grp, df in tfm.dfs.items():
            if 'Reference Comment' in df.columns:
                self._add_refnote(df)
            else:
                print(f"Warning: 'Reference Comment' column not found in DataFrame for group '{grp}'")

    def _add_refnote(self, df: pd.DataFrame):
        """
        Add the 'refnote' column to the DataFrame by mapping values from 'Reference Comment'.

        Args:
            df (pd.DataFrame): DataFrame containing the 'Reference Comment' column.
        
        The 'Reference Comment' column values are copied to the new 'refnote' column.
        """
        df['refnote'] = df['Reference Comment']


# %% ../../nbs/handlers/ospar.ipynb 197
class RecordSampleNoteCB(Callback):
    """Record sample notes by adding a 'sampnote' column to DataFrames."""
    
    def __init__(self):
        """
        Initialize the RecordSampleNoteCB.

        This class does not require additional arguments or setup for initialization.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply the callback to each DataFrame in the transformer to add the 'sampnote' column.

        Args:
            tfm (Transformer): The transformer object containing DataFrames to process.
        
        This method iterates over all DataFrames in the transformer and checks for the
        presence of the 'Sample Comment' column. If found, it copies the values
        to a new 'sampnote' column. If not found, it prints a warning message.
        """
        for grp, df in tfm.dfs.items():
            if 'Sample Comment' in df.columns:
                self._add_samplenote(df)
            else:
                print(f"Warning: 'Sample Comment' column not found in DataFrame for group '{grp}'")

    def _add_samplenote(self, df: pd.DataFrame):
        """
        Add the 'sampnote' column to the DataFrame by mapping values from 'Sample Comment'.

        Args:
            df (pd.DataFrame): DataFrame containing the 'Measurement Comment' column.
        
        The 'Sample Comment' column values are copied to the new 'sampnote' column.
        """
        df['sampnote'] = df['Sample Comment']


# %% ../../nbs/handlers/ospar.ipynb 205
class ConvertLonLatCB(Callback):
    """Convert Longitude and Latitude values to decimal degrees (DDD.DDDDD°). This class processes DataFrames to convert latitude and longitude from degrees, minutes, and seconds 
    (DMS) format with direction indicators to decimal degrees format."""
    def __init__(self):
        """
        Initialize the ConvertLonLatCB class.
        """
        fc.store_attr()

    def __call__(self, tfm: 'Transformer'):
        """
        Apply the conversion to latitude and longitude in each DataFrame within the transformer.

        Args:
            tfm (Transformer): The transformer object containing DataFrames to process.
        
        This method processes each DataFrame to convert latitude and longitude values into decimal degrees.
        """
        for grp, df in tfm.dfs.items():
            df['lat'] = self._convert_latitude(df)
            df['lon'] = self._convert_longitude(df)

    def _convert_latitude(self, df: pd.DataFrame) -> pd.Series:
        """
        Convert latitude values from DMS format to decimal degrees.

        Args:
            df (pd.DataFrame): DataFrame containing latitude columns.

        Returns:
            pd.Series: Series with latitude values converted to decimal degrees.
        """
        return np.where(
            df['LatDir'].isin(['S']),
            self._dms_to_decimal(df['LatD'], df['LatM'], df['LatS']) * -1,
            self._dms_to_decimal(df['LatD'], df['LatM'], df['LatS'])
        )

    def _convert_longitude(self, df: pd.DataFrame) -> pd.Series:
        """
        Convert longitude values from DMS format to decimal degrees.

        Args:
            df (pd.DataFrame): DataFrame containing longitude columns.

        Returns:
            pd.Series: Series with longitude values converted to decimal degrees.
        """
        return np.where(
            df['LongDir'].isin(['W']),
            self._dms_to_decimal(df['LongD'], df['LongM'], df['LongS']) * -1,
            self._dms_to_decimal(df['LongD'], df['LongM'], df['LongS'])
        )

    def _dms_to_decimal(self, degrees: pd.Series, minutes: pd.Series, seconds: pd.Series) -> pd.Series:
        """
        Convert DMS (degrees, minutes, seconds) format to decimal degrees.

        Args:
            degrees (pd.Series): Series containing degree values.
            minutes (pd.Series): Series containing minute values.
            seconds (pd.Series): Series containing second values.

        Returns:
            pd.Series: Series with values converted to decimal degrees.
        """
        return degrees + minutes / 60 + seconds / 3600


# %% ../../nbs/handlers/ospar.ipynb 221
# Define columns of interest (keys) and renaming rules (values).
def get_renaming_rules(encoding_type='netcdf'):
    vars = cdl_cfg()['vars']
    if encoding_type == 'netcdf':
        return OrderedDict({
            ('seawater', 'biota', 'sediment'): {
                # DEFAULT
                'lat': vars['defaults']['lat']['name'],
                'lon': vars['defaults']['lon']['name'],
                'time': vars['defaults']['time']['name'],
                'NUCLIDE': 'nuclide',
                'detection_limit': vars['suffixes']['detection_limit']['name'],
                'unit': vars['suffixes']['unit']['name'],
                'value': 'value',
                'uncertainty': vars['suffixes']['uncertainty']['name'],
                #'counting_method': vars['suffixes']['counting_method']['name'],
                #'sampling_method': vars['suffixes']['sampling_method']['name'],
                #'preparation_method': vars['suffixes']['preparation_method']['name']
            },
            ('seawater',): {
                # SEAWATER
            },
            ('biota',): {
                # BIOTA
                'species': vars['bio']['species']['name'],
                'body_part': vars['bio']['body_part']['name'],
                'bio_group': vars['bio']['bio_group']['name']
            }
        })
    
    elif encoding_type == 'openrefine':
        return OrderedDict({
            ('seawater', 'biota', 'sediment'): {
                # DEFAULT
                'samptype_id': 'samptype_id',
                'lat': 'latitude',
                'lon': 'longitude',
                'station': 'station',
                'begperiod': 'begperiod',
                'samplabcode': 'samplabcode',
                #'endperiod': 'endperiod',
                'nuclide_id': 'nuclide_id',
                'detection_limit': 'detection',
                'unit': 'unit_id',
                'value': 'activity',
                'uncertainty': 'uncertaint',
                'sampnote': 'sampnote',
                'measurenote': 'measurenote',
                'refnote' : 'refnote'
            },
            ('seawater',) : {
                # SEAWATER
                #'volume': 'volume',
                #'filtpore': 'filtpore',
                #'acid': 'acid'
            },
            ('biota',) : {
                # BIOTA
                'species': 'species_id',
                'Taxonname': 'Taxonname',
                'TaxonRepName': 'TaxonRepName',
                #'Commonname': 'Commonname',
                'Taxonrank': 'Taxonrank',
                'TaxonDB': 'TaxonDB',
                'TaxonDBID': 'TaxonDBID',
                'TaxonDBURL': 'TaxonDBURL',
                'body_part': 'bodypar_id',
            }
        })
    else:
        print("Invalid encoding_type provided. Please use 'netcdf' or 'openrefine'.")
        return None

# %% ../../nbs/handlers/ospar.ipynb 222
class SelectAndRenameColumnCB(Callback):
    """A callback to select and rename columns in a DataFrame based on provided renaming rules
    for a specified encoding type. It also prints renaming rules that were not applied
    because their keys were not found in the DataFrame."""
    
    def __init__(self, fn_renaming_rules, encoding_type='netcdf', verbose=False):
        """
        Initialize the SelectAndRenameColumnCB callback.

        Args:
            fn_renaming_rules (function): A function that returns an OrderedDict of renaming rules.
            encoding_type (str): The encoding type ('netcdf' or 'openrefine') to determine which renaming rules to use.
            verbose (bool): Whether to print out renaming rules that were not applied.
        """
        fc.store_attr()

    def __call__(self, tfm):
        """
        Apply column selection and renaming to DataFrames in the transformer, and identify unused rules.

        Args:
            tfm (Transformer): The transformer object containing DataFrames.
        """
        try:
            renaming_rules = self.fn_renaming_rules(self.encoding_type)
        except ValueError as e:
            print(f"Error fetching renaming rules: {e}")
            return

        for group in tfm.dfs.keys():
            # Get relevant renaming rules for the current group
            group_rules = self._get_group_rules(renaming_rules, group)

            if not group_rules:
                continue

            # Apply renaming rules and track keys not found in the DataFrame
            df = tfm.dfs[group]
            df, not_found_keys = self._apply_renaming(df, group_rules)
            tfm.dfs[group] = df
            
            # Print any renaming rules that were not used
            if not_found_keys and self.verbose:
                print(f"\nGroup '{group}' has the following renaming rules not applied:")
                for old_col in not_found_keys:
                    print(f"Key '{old_col}' from renaming rules was not found in the DataFrame.")

    def _get_group_rules(self, renaming_rules, group):
        """
        Retrieve and merge renaming rules for the specified group based on the encoding type.

        Args:
            renaming_rules (OrderedDict): OrderedDict of all renaming rules.
            group (str): Group name to filter rules.

        Returns:
            OrderedDict: An OrderedDict of renaming rules applicable to the specified group.
        """
        relevant_rules = [rules for key, rules in renaming_rules.items() if group in key]
        merged_rules = OrderedDict()
        for rules in relevant_rules:
            merged_rules.update(rules)
        return merged_rules

    def _apply_renaming(self, df, rename_rules):
        """
        Select columns based on renaming rules and apply renaming, only for existing columns,
        while maintaining the order of the dictionary columns.

        Args:
            df (pd.DataFrame): DataFrame to modify.
            rename_rules (OrderedDict): OrderedDict of column renaming rules.

        Returns:
            tuple: A tuple containing:
                - The DataFrame with columns renamed and filtered.
                - A set of column names from renaming rules that were not found in the DataFrame.
        """
        existing_columns = set(df.columns)
        valid_rules = OrderedDict((old_col, new_col) for old_col, new_col in rename_rules.items() if old_col in existing_columns)

        # Create a list to maintain the order of columns
        columns_to_keep = [col for col in rename_rules.keys() if col in existing_columns]
        columns_to_keep += [new_col for old_col, new_col in valid_rules.items() if new_col in df.columns]

        df = df[list(OrderedDict.fromkeys(columns_to_keep))]

        # Apply renaming
        df.rename(columns=valid_rules, inplace=True)

        # Determine which keys were not found
        not_found_keys = set(rename_rules.keys()) - existing_columns
        return df, not_found_keys


# %% ../../nbs/handlers/ospar.ipynb 227
class ReshapeLongToWide(Callback):
    "Convert data from long to wide with renamed columns."
    def __init__(self, columns=['nuclide'], values=['value']):
        fc.store_attr()
        # Retrieve all possible derived vars (e.g 'unc', 'dl', ...) from configs
        self.derived_cols = [value['name'] for value in cdl_cfg()['vars']['suffixes'].values()]
    
    def renamed_cols(self, cols):
        "Flatten columns name"
        return [inner if outer == "value" else f'{inner}{outer}'
                if inner else outer
                for outer, inner in cols]

    def pivot(self, df):
        # Among all possible 'derived cols' select the ones present in df
        derived_coi = [col for col in self.derived_cols if col in df.columns]
        df.index.name = 'org_index'
        df=df.reset_index()
        idx = list(set(df.columns) - set(self.columns + derived_coi + self.values))
        
        # Create a fill_value to replace NaN values in the columns used as the index in the pivot table.
        # Check if num_fill_value is already in the dataframe index values. If num_fill_value already exists
        # then increase num_fill_value by 1 until a value is found for num_fill_value that is not in the dataframe. 
        num_fill_value = -999
        while (df[idx] == num_fill_value).any().any():
            num_fill_value += 1
        # Fill in nan values for each col found in idx. 
        for col in idx:   
            if pd.api.types.is_numeric_dtype(df[col]):
                fill_value = num_fill_value
            if pd.api.types.is_string_dtype(df[col]):
                fill_value = 'NOT AVAILABLE'
                
            df[col]=df[col].fillna(fill_value)

        pivot_df=df.pivot_table(index=idx,
                              columns=self.columns,
                              values=self.values + derived_coi,
                              fill_value=np.nan,
                              aggfunc=lambda x: x
                              ).reset_index()
        

        # Replace fill_value  with  np.nan
        pivot_df[idx]=pivot_df[idx].replace({'NOT AVAILABLE': np.nan,
                                             num_fill_value : np.nan})
        # Set the index to be the org_index
        pivot_df = pivot_df.set_index('org_index')
                
        return (pivot_df)

    def __call__(self, tfm):
        for grp in tfm.dfs.keys():
            tfm.dfs[grp] = self.pivot(tfm.dfs[grp])
            tfm.dfs[grp].columns = self.renamed_cols(tfm.dfs[grp].columns)

# %% ../../nbs/handlers/ospar.ipynb 236
kw = ['oceanography', 'Earth Science > Oceans > Ocean Chemistry> Radionuclides',
      'Earth Science > Human Dimensions > Environmental Impacts > Nuclear Radiation Exposure',
      'Earth Science > Oceans > Ocean Chemistry > Ocean Tracers, Earth Science > Oceans > Marine Sediments',
      'Earth Science > Oceans > Ocean Chemistry, Earth Science > Oceans > Sea Ice > Isotopes',
      'Earth Science > Oceans > Water Quality > Ocean Contaminants',
      'Earth Science > Biological Classification > Animals/Vertebrates > Fish',
      'Earth Science > Biosphere > Ecosystems > Marine Ecosystems',
      'Earth Science > Biological Classification > Animals/Invertebrates > Mollusks',
      'Earth Science > Biological Classification > Animals/Invertebrates > Arthropods > Crustaceans',
      'Earth Science > Biological Classification > Plants > Macroalgae (Seaweeds)']


# %% ../../nbs/handlers/ospar.ipynb 237
def get_attrs(tfm, zotero_key, kw=kw):
    return GlobAttrsFeeder(tfm.dfs, cbs=[
        BboxCB(),
        DepthRangeCB(),
        TimeRangeCB(cfg()),
        ZoteroCB(zotero_key, cfg=cfg()),
        KeyValuePairCB('keywords', ', '.join(kw)),
        KeyValuePairCB('publisher_postprocess_logs', ', '.join(tfm.logs))
        ])()

# %% ../../nbs/handlers/ospar.ipynb 239
def enums_xtra(tfm, vars):
    "Retrieve a subset of the lengthy enum as 'species_t' for instance"
    enums = Enums(lut_src_dir=lut_path(), cdl_enums=cdl_cfg()['enums'])
    xtras = {}
    for var in vars:
        unique_vals = tfm.unique(var)
        if unique_vals.any():
            xtras[f'{var}_t'] = enums.filter(f'{var}_t', unique_vals)
    return xtras

# %% ../../nbs/handlers/ospar.ipynb 241
def encode(fname_in, fname_out_nc, nc_tpl_path, **kwargs):
    dfs = load_data(fname_in)
    tfm = Transformer(dfs, cbs=[
                                GetSampleTypeCB(type_lut),
                                LowerStripRdnNameCB(),
                                RemapRdnNameCB(),
                                ParseTimeCB(),
                                EncodeTimeCB(cfg()),        
                                SanitizeValue(),                       
                                NormalizeUncCB(unc_exp2stan),
                                LookupBiotaSpeciesCB(get_maris_species, unmatched_fixes_biota_species),
                                CorrectWholeBodyPartCB(),
                                LookupBiotaBodyPartCB(get_maris_bodypart, unmatched_fixes_biota_tissues),
                                LookupBiogroupCB(partial(get_biogroup_lut, species_lut_path())),
                                LookupTaxonInformationCB(partial(get_taxon_info_lut, species_lut_path())),
                                LookupUnitCB(renaming_unit_rules),
                                LookupDetectionLimitCB(detection_limit_lut_path()),
                                RemapDataProviderSampleIdCB(),
                                RemapStationIdCB(),
                                RecordMeasurementNoteCB(),
                                RecordRefNoteCB(),
                                RecordSampleNoteCB(),   
                                ConvertLonLatCB(),                    
                                SanitizeLonLatCB(),
                                SelectAndRenameColumnCB(get_renaming_rules, encoding_type='netcdf'),
                                ReshapeLongToWide(),
                                ])
    tfm()
    encoder = NetCDFEncoder(tfm.dfs, 
                            src_fname=nc_tpl_path,
                            dest_fname=fname_out_nc, 
                            global_attrs=get_attrs(tfm, zotero_key=zotero_key, kw=kw),
                            verbose=kwargs.get('verbose', False),
                            enums_xtra=enums_xtra(tfm, vars=['species', 'body_part'])
                           )
    encoder.encode()

# %% ../../nbs/handlers/ospar.ipynb 250
def encode_or(fname_in, fname_out_csv, ref_id, **kwargs):
    dfs = load_data(fname_in)
    tfm = Transformer(dfs, cbs=[
                                GetSampleTypeCB(type_lut),
                                LowerStripRdnNameCB(),
                                RemapRdnNameCB(),
                                ParseTimeCB(),
                                EncodeTimeCB(cfg()),        
                                SanitizeValue(),                       
                                NormalizeUncCB(unc_exp2stan),
                                LookupBiotaSpeciesCB(get_maris_species, unmatched_fixes_biota_species),
                                CorrectWholeBodyPartCB(),
                                LookupBiotaBodyPartCB(get_maris_bodypart, unmatched_fixes_biota_tissues),
                                LookupBiogroupCB(partial(get_biogroup_lut, species_lut_path())),
                                LookupTaxonInformationCB(partial(get_taxon_info_lut, species_lut_path())),
                                LookupUnitCB(renaming_unit_rules),
                                LookupDetectionLimitCB(detection_limit_lut_path()),
                                RemapDataProviderSampleIdCB(),
                                RemapStationIdCB(),
                                RecordMeasurementNoteCB(),
                                RecordRefNoteCB(),
                                RecordSampleNoteCB(),   
                                ConvertLonLatCB(),                    
                                SanitizeLonLatCB(),
                                SelectAndRenameColumnCB(get_renaming_rules, encoding_type='openrefine', verbose=True),
                                CompareDfsAndTfmCB(dfs)
                                ])
    tfm()

    encoder = OpenRefineCsvEncoder(tfm.dfs, 
                                    dest_fname=fname_out_csv, 
                                    ref_id = ref_id,
                                    verbose = True
                                )
    encoder.encode()

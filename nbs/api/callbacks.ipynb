{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp callbacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34f38641",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "> Callback used in handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4934cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "import fastcore.all as fc\n",
    "from operator import attrgetter\n",
    "from cftime import date2num\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from marisco.configs import cfg, cdl_cfg, grp_names\n",
    "from functools import partial \n",
    "from typing import List, Dict, Callable, Tuple, Any, Optional\n",
    "from pathlib import Path \n",
    "\n",
    "from marisco.configs import get_lut, nuc_lut_path\n",
    "from marisco.utils import Match\n",
    "from typing import Any, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91324c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from marisco.configs import cdl_cfg, CONFIGS_CDL\n",
    "from marisco.utils import test_dfs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d7034",
   "metadata": {},
   "source": [
    "## Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962281da",
   "metadata": {},
   "source": [
    "The `Transformer` class is designed to facilitate the application of a series of callbacks to a set of dataframes. It provides a structured way to apply transformations (i.e `Callback`) to the data, with a focus on flexibility and ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Callback(): \n",
    "    \"Base class for callbacks.\"\n",
    "    order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61702d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def run_cbs(\n",
    "    cbs: List[Callback], # List of callbacks to run\n",
    "    obj: Any # Object to pass to the callbacks\n",
    "    ):\n",
    "    \"Run the callbacks in the order they are specified.\"\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        if cb.__doc__: obj.logs.append(cb.__doc__)\n",
    "        cb(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Transformer():\n",
    "    \"Transform the dataframe(s) according to the specified callbacks.\"\n",
    "    def __init__(self, \n",
    "                 data: Union[Dict[str, pd.DataFrame], pd.DataFrame], # Data to be transformed\n",
    "                 cbs: Optional[List[Callback]]=None, # List of callbacks to run\n",
    "                 inplace: bool=False # Whether to modify the dataframe(s) in place\n",
    "                 ): \n",
    "        fc.store_attr()\n",
    "        self.is_single_df = isinstance(data, pd.DataFrame)\n",
    "        self.df, self.dfs = self._prepare_data(data, inplace)\n",
    "        self.logs = []\n",
    "            \n",
    "    def _prepare_data(self, data, inplace):\n",
    "        if self.is_single_df:\n",
    "            return (data if inplace else data.copy()), None\n",
    "        else:\n",
    "            return None, (data if inplace else {k: v.copy() for k, v in data.items()})\n",
    "    \n",
    "    def unique(self, col_name: str) -> np.ndarray:\n",
    "        \"Distinct values of a specific column present in all groups.\"\n",
    "        if self.is_single_df:\n",
    "            values = self.df.get(col_name, pd.Series()).dropna().values\n",
    "        else:\n",
    "            columns = [df.get(col_name) for df in self.dfs.values() if df.get(col_name) is not None]\n",
    "            values = np.concatenate([col.dropna().values for col in columns]) if columns else []\n",
    "        return np.unique(values)\n",
    "        \n",
    "    def __call__(self):\n",
    "        \"Transform the dataframe(s) according to the specified callbacks.\"\n",
    "        if self.cbs: run_cbs(self.cbs, self)\n",
    "        return self.df if self.dfs is None else self.dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab87dd9",
   "metadata": {},
   "source": [
    "Below, a few examples of how to use the `Transformer` class.\n",
    "Let's define first a test callback that adds `1` to the `depth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCB(Callback):\n",
    "    \"A test callback to add 1 to the depth.\"\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for grp, df in tfm.dfs.items(): \n",
    "            df['depth'] = df['depth'].apply(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24bcbc",
   "metadata": {},
   "source": [
    "And apply it to the following dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ee79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {'biota': pd.DataFrame({'id': [0, 1, 2], 'species': [0, 2, 0], 'depth': [2, 3, 4]}),\n",
    "       'seawater': pd.DataFrame({'id': [0, 1, 2], 'depth': [3, 4, 5]})}\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[TestCB()])\n",
    "dfs_test = tfm()\n",
    "\n",
    "fc.test_eq(dfs_test['biota']['depth'].to_list(), [3, 4, 5])\n",
    "fc.test_eq(dfs_test['seawater']['depth'].to_list(), [4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bde01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCB(Callback):\n",
    "    \"A test callback to add 1 to the depth.\"\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        tfm.df['depth'] = tfm.df['depth'].apply(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id': [0, 1, 2], 'species': [0, 2, 0], 'depth': [2, 3, 4]})\n",
    "\n",
    "tfm = Transformer(df, cbs=[TestCB()])\n",
    "df_test = tfm()\n",
    "\n",
    "fc.test_eq(df_test['depth'].to_list(), [3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca917c61",
   "metadata": {},
   "source": [
    "## Geographical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db90009",
   "metadata": {},
   "source": [
    "This section gathers callbacks that are used to transform the geographical coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class SanitizeLonLatCB(Callback):\n",
    "    \"Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.\"\n",
    "    def __init__(self, \n",
    "                 lon_col: str='lon', # Longitude column name\n",
    "                 lat_col: str='lat', # Latitude column name\n",
    "                 verbose: bool=False # Whether to print the number of invalid longitude & latitude values\n",
    "                 ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for grp, df in tfm.dfs.items():\n",
    "            # Convert `,` separator to `.` separator\n",
    "            df[self.lon_col] = df[self.lon_col].apply(lambda x: float(str(x).replace(',', '.')))\n",
    "            df[self.lat_col] = df[self.lat_col].apply(lambda x: float(str(x).replace(',', '.')))\n",
    "            \n",
    "            # Mask zero values\n",
    "            mask_zeroes = (df[self.lon_col] == 0) & (df[self.lat_col] == 0) \n",
    "            nZeroes = mask_zeroes.sum()\n",
    "            if nZeroes and self.verbose: \n",
    "                print(f'The \"{grp}\" group contains {nZeroes} data points whose ({self.lon_col}, {self.lat_col}) = (0, 0)')\n",
    "            \n",
    "            # Mask out of bounds values\n",
    "            mask_goob = (df[self.lon_col] < -180) | (df[self.lon_col] > 180) | (df[self.lat_col] < -90) | (df[self.lat_col] > 90)\n",
    "            nGoob = mask_goob.sum()\n",
    "            if nGoob and self.verbose: \n",
    "                print(f'The \"{grp}\" group contains {nGoob} data points with unrealistic {self.lon_col} or {self.lat_col} values.')\n",
    "                \n",
    "            tfm.dfs[grp] = df.loc[~(mask_zeroes | mask_goob)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that measurements located at (0,0) get removed\n",
    "dfs = {'biota': pd.DataFrame({'lon': [0, 1, 0], 'lat': [0, 2, 0]})}\n",
    "tfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\n",
    "tfm()['biota']\n",
    "\n",
    "expected = [1., 2.]\n",
    "fc.test_eq(tfm()['biota'].iloc[0].to_list(), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378851b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that comma decimal separator get replaced by point instead\n",
    "dfs = {'biota': pd.DataFrame({'lon': ['45,2'], 'lat': ['43,1']})}\n",
    "tfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\n",
    "tfm()['biota']\n",
    "\n",
    "expected = [45.2, 43.1]\n",
    "fc.test_eq(tfm()['biota'].iloc[0].to_list(), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd0096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that out of bounds lon or lat get removed\n",
    "dfs = {'biota': pd.DataFrame({'lon': [-190, 190, 1, 2, 1.1], 'lat': [1, 2, 91, -91, 2.2]})}\n",
    "tfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\n",
    "tfm()['biota']\n",
    "\n",
    "expected = [1.1, 2.2]\n",
    "fc.test_eq(tfm()['biota'].iloc[0].to_list(), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d96a0",
   "metadata": {},
   "source": [
    "## Add columns\n",
    "This section gathers callbacks that are used to add required columns to the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ceffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class AddSampleTypeIdColumnCB(Callback):\n",
    "    def __init__(self, \n",
    "                 cdl_cfg: Callable=cdl_cfg, # Callable to get the CDL config dictionary\n",
    "                 col_name: str='samptype_id' # Column name to store the sample type id\n",
    "                 ): \n",
    "        \"Add a column with the sample type id as defined in the CDL.\"\n",
    "        fc.store_attr()\n",
    "        self.lut = {v['name']: v['id'] for v in cdl_cfg()['grps'].values()}\n",
    "        \n",
    "    def __call__(self, tfm):\n",
    "        for grp, df in tfm.dfs.items(): df[self.col_name] = self.lut[grp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635aaaf6",
   "metadata": {},
   "source": [
    "Let's test the callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cb3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {v['name']: pd.DataFrame({'col_test': [0, 1, 2]}) for v in CONFIGS_CDL['grps'].values()}\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[AddSampleTypeIdColumnCB(cdl_cfg=lambda: CONFIGS_CDL)])\n",
    "dfs_test = tfm()\n",
    "\n",
    "for v in CONFIGS_CDL['grps'].values():\n",
    "    fc.test_eq(dfs_test[v['name']]['samptype_id'].unique().item(), v['id']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dbfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class AddNuclideIdColumnCB(Callback):\n",
    "    def __init__(self, \n",
    "                 col_value: str, # Column name containing the nuclide name\n",
    "                 lut_fname_fn: Callable=nuc_lut_path, # Function returning the lut path\n",
    "                 col_name: str='nuclide_id' # Column name to store the nuclide id\n",
    "                 ): \n",
    "        \"Add a column with the nuclide id.\"\n",
    "        fc.store_attr()\n",
    "        self.lut = get_lut(lut_fname_fn().parent, lut_fname_fn().name, \n",
    "                           key='nc_name', value='nuclide_id', reverse=False)\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for grp, df in tfm.dfs.items(): \n",
    "            df[self.col_name] = df[self.col_value].map(self.lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {v['name']: pd.DataFrame({'Nuclide': ['cs137', 'pu239_240_tot']}) for v in CONFIGS_CDL['grps'].values()}\n",
    "\n",
    "lut_fname_fn = lambda: Path('./files/lut/dbo_nuclide.xlsx')\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[AddNuclideIdColumnCB(col_value='Nuclide', lut_fname_fn=lut_fname_fn)])\n",
    "tfm()['seawater']\n",
    "\n",
    "expected = [33, 77]\n",
    "for grp in tfm.dfs.keys():\n",
    "    fc.test_eq(tfm.dfs[grp]['nuclide_id'].to_list(), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98192b7",
   "metadata": {},
   "source": [
    "## Map & Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c905654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapCB(Callback):\n",
    "    \"Generic MARIS remapping callback.\"\n",
    "    def __init__(self, \n",
    "                 fn_lut: Callable, # Function that returns the lookup table dictionary\n",
    "                 col_remap: str, # Name of the column to remap\n",
    "                 col_src: str, # Name of the column with the source values\n",
    "                 dest_grps: list[str]|str=grp_names(), # List of destination groups\n",
    "                 default_value: Any = -1, # Default value for unmatched entries\n",
    "                 verbose: bool = False, # Whether to print unmatched values\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        self.lut = None\n",
    "        \n",
    "        if isinstance(dest_grps, str): \n",
    "            self.dest_grps = [dest_grps]\n",
    "        self.__doc__ = f\"Remap values from '{self.col_src}' to '{self.col_remap}' for groups: {', '.join(self.dest_grps) if len(self.dest_grps) > 1 else self.dest_grps[0]}.\"\n",
    "        \n",
    "    def __call__(self, tfm):\n",
    "        self.lut = self.fn_lut()\n",
    "        for grp in self.dest_grps:\n",
    "            if grp in tfm.dfs:\n",
    "                self._remap_group(tfm.dfs[grp])\n",
    "\n",
    "    def _remap_group(self, df: pd.DataFrame):\n",
    "        df[self.col_remap] = df[self.col_src].apply(self._remap_value)\n",
    "\n",
    "    def _remap_value(self, value: str) -> Any:\n",
    "        value = value.strip() if isinstance(value, str) else value\n",
    "        match = self.lut.get(value, Match(self.default_value, None, None, None))\n",
    "        if isinstance(match, Match):\n",
    "            if match.matched_id == self.default_value and self.verbose:\n",
    "                print(f\"Unmatched value: {value}\")\n",
    "            return match.matched_id \n",
    "        else:\n",
    "            return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1917a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class LowerStripNameCB(Callback):\n",
    "    \"Convert values to lowercase and strip any trailing spaces.\"\n",
    "    def __init__(self, \n",
    "                 col_src: str, # Source column name e.g. 'Nuclide'\n",
    "                 col_dst: str=None, # Destination column name\n",
    "                 fn_transform: Callable=lambda x: x.lower().strip() # Transformation function\n",
    "                 ):\n",
    "        fc.store_attr()\n",
    "        self.__doc__ = f\"Convert values from '{col_src}' to lowercase, strip spaces, and store in '{col_dst}'.\"\n",
    "        if not col_dst: self.col_dst = col_src\n",
    "        \n",
    "    def _safe_transform(self, value):\n",
    "        \"Ensure value is not NA and apply transformation function.\"\n",
    "        return value if pd.isna(value) else self.fn_transform(str(value))\n",
    "            \n",
    "    def __call__(self, tfm):\n",
    "        for key in tfm.dfs.keys():\n",
    "            tfm.dfs[key][self.col_dst] = tfm.dfs[key][self.col_src].apply(self._safe_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06003b9",
   "metadata": {},
   "source": [
    "Let's test the callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {'seawater': pd.DataFrame({'Nuclide': ['CS137', '226RA']})}\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='Nuclide', col_dst='NUCLIDE')])\n",
    "fc.test_eq(tfm()['seawater']['NUCLIDE'].to_list(), ['cs137', '226ra'])\n",
    "\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='Nuclide')])\n",
    "fc.test_eq(tfm()['seawater']['Nuclide'].to_list(), ['cs137', '226ra'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b9b06a",
   "metadata": {},
   "source": [
    "The point is when (semi-automatic) remapping names generally:\n",
    "\n",
    "1. we need first to guess (fuzzy matching or other) the right nuclide name.\n",
    "2. Then manually check the result and eventually update the lookup table.\n",
    "3.  Finally we can apply the lookup table to the dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ae6fa",
   "metadata": {},
   "source": [
    "## Change structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701be72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemoveAllNAValuesCB(Callback):\n",
    "    \"Remove rows with all NA values.\"\n",
    "    def __init__(self, \n",
    "                 cols_to_check: Dict[str, str] # A dictionary with the sample type as key and the column name to check as value\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm):\n",
    "        for k in tfm.dfs.keys():\n",
    "            col_to_check = self.cols_to_check[k]\n",
    "            mask = tfm.dfs[k][col_to_check].isnull().all(axis=1)\n",
    "            tfm.dfs[k] = tfm.dfs[k][~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32c3bc",
   "metadata": {},
   "source": [
    "Many data providers use a long format (e.g `lat, lon, radionuclide, value, unc, ...`) to store their data. When encoding as `netCDF`, it is often required to use a wide format (e.g `lat, lon, nuclide1_value, nuclide1_unc, nuclide2_value, nuclide2_unc, ...`). The class `ReshapeLongToWide` is designed to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b409a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ReshapeLongToWide(Callback):\n",
    "    \"Convert data from long to wide with renamed columns.\"\n",
    "    def __init__(self, \n",
    "                 columns: List[str]=['nuclide'], # Columns to use as index\n",
    "                 values: List[str]=['value'], # Columns to use as values\n",
    "                 num_fill_value: int=-999, # Fill value for numeric columns\n",
    "                 str_fill_value='STR FILL VALUE'\n",
    "                 ):\n",
    "        fc.store_attr()\n",
    "        self.derived_cols = self._get_derived_cols()\n",
    "    \n",
    "    def _get_derived_cols(self):\n",
    "        \"Retrieve all possible derived vars (e.g 'unc', 'dl', ...) from configs.\"\n",
    "        return [value['name'] for value in cdl_cfg()['vars']['suffixes'].values()]\n",
    "\n",
    "    def renamed_cols(self, cols):\n",
    "        \"Flatten columns name.\"\n",
    "        return [inner if outer == \"value\" else f'{inner}{outer}' if inner else outer\n",
    "                for outer, inner in cols]\n",
    "\n",
    "    def _get_unique_fill_value(self, df, idx):\n",
    "        \"Get a unique fill value for NaN replacement.\"\n",
    "        fill_value = self.num_fill_value\n",
    "        while (df[idx] == fill_value).any().any():\n",
    "            fill_value -= 1\n",
    "        return fill_value\n",
    "\n",
    "    def _fill_nan_values(self, df, idx):\n",
    "        \"Fill NaN values in index columns.\"\n",
    "        num_fill_value = self._get_unique_fill_value(df, idx)\n",
    "        for col in idx:\n",
    "            fill_value = num_fill_value if pd.api.types.is_numeric_dtype(df[col]) else self.str_fill_value\n",
    "            df[col] = df[col].fillna(fill_value)\n",
    "        return df, num_fill_value\n",
    "\n",
    "    def pivot(self, df):\n",
    "        derived_coi = [col for col in self.derived_cols if col in df.columns]\n",
    "        # In past implementation we added an index column before pivoting \n",
    "        # TO BE REMOVED\n",
    "        # making all rows (compound_idx) unique.\n",
    "        # df.index.name = 'org_index'\n",
    "        # df = df.reset_index()\n",
    "        idx = list(set(df.columns) - set(self.columns + derived_coi + self.values))\n",
    "        \n",
    "        df, num_fill_value = self._fill_nan_values(df, idx)\n",
    "\n",
    "        pivot_df = df.pivot_table(index=idx,\n",
    "                                  columns=self.columns,\n",
    "                                  values=self.values + derived_coi,\n",
    "                                  fill_value=np.nan,\n",
    "                                  aggfunc=lambda x: x).reset_index()\n",
    "\n",
    "        pivot_df[idx] = pivot_df[idx].replace({self.str_fill_value: np.nan, num_fill_value: np.nan})\n",
    "        pivot_df = self.set_index(pivot_df)\n",
    "        return pivot_df\n",
    "\n",
    "    def set_index(self, df):\n",
    "        \"Set the index of the dataframe.\"\n",
    "        # TODO: Consider implementing a universal unique index\n",
    "        # by hashing the compound index columns (lat, lon, time, depth, etc.)\n",
    "        df.index.name = 'org_index'\n",
    "        return df\n",
    "    \n",
    "    def __call__(self, tfm):\n",
    "        for grp in tfm.dfs.keys():\n",
    "            tfm.dfs[grp] = self.pivot(tfm.dfs[grp])\n",
    "            tfm.dfs[grp].columns = self.renamed_cols(tfm.dfs[grp].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8887de6",
   "metadata": {},
   "source": [
    "Example of usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930af6a8",
   "metadata": {},
   "source": [
    "* **Case 1**: `compound_idx` (in our case made of `lon`, `lat`, `time`, `depth`, ...) are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5416fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound_idx</th>\n",
       "      <th>cs137_unc</th>\n",
       "      <th>pu239_240_tot_unc</th>\n",
       "      <th>cs137</th>\n",
       "      <th>pu239_240_tot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          compound_idx  cs137_unc  pu239_240_tot_unc  cs137  pu239_240_tot\n",
       "org_index                                                                 \n",
       "0                    a        0.1                NaN    1.0            NaN\n",
       "1                    b        0.2                NaN    2.0            NaN\n",
       "2                    c        NaN                0.3    NaN            3.0\n",
       "3                    d        NaN                0.4    NaN            4.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs_test = {'seawater': pd.DataFrame({\n",
    "    'compound_idx': ['a', 'b', 'c', 'd'], \n",
    "    'nuclide': ['cs137', 'cs137', 'pu239_240_tot', 'pu239_240_tot'], \n",
    "    'value': [1, 2, 3, 4],\n",
    "    '_unc': [0.1, 0.2, 0.3, 0.4]})}\n",
    "\n",
    "tfm = Transformer(dfs_test, cbs=[ReshapeLongToWide()])\n",
    "tfm()['seawater']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f00ed0",
   "metadata": {},
   "source": [
    "* **Case 2**: compound_idx are not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e271af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound_idx</th>\n",
       "      <th>cs134_unc</th>\n",
       "      <th>cs137_unc</th>\n",
       "      <th>pu239_240_tot_unc</th>\n",
       "      <th>cs134</th>\n",
       "      <th>cs137</th>\n",
       "      <th>pu239_240_tot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>org_index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          compound_idx  cs134_unc  cs137_unc  pu239_240_tot_unc  cs134  cs137  \\\n",
       "org_index                                                                       \n",
       "0                    a        0.2        0.1                NaN    2.0    1.0   \n",
       "1                    c        NaN        NaN                0.3    NaN    NaN   \n",
       "2                    d        NaN        NaN                0.4    NaN    NaN   \n",
       "\n",
       "           pu239_240_tot  \n",
       "org_index                 \n",
       "0                    NaN  \n",
       "1                    3.0  \n",
       "2                    4.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs_test = {'seawater': pd.DataFrame({\n",
    "    'compound_idx': ['a', 'a', 'c', 'd'], \n",
    "    'nuclide': ['cs137', 'cs134', 'pu239_240_tot', 'pu239_240_tot'], \n",
    "    'value': [1, 2, 3, 4],\n",
    "    '_unc': [0.1, 0.2, 0.3, 0.4]})}\n",
    "\n",
    "tfm = Transformer(dfs_test, cbs=[ReshapeLongToWide()])\n",
    "tfm()['seawater']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c95e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51972/3206494790.py:48: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  pivot_df = df.pivot_table(index=idx,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must produce aggregated value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m dfs_expected \u001b[38;5;241m=\u001b[39m fc\u001b[38;5;241m.\u001b[39mload_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./files/pkl/dfs_reshape_test_expected.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m tfm \u001b[38;5;241m=\u001b[39m Transformer(dfs_test, cbs\u001b[38;5;241m=\u001b[39m[ReshapeLongToWide()])\n\u001b[0;32m----> 6\u001b[0m dfs_output \u001b[38;5;241m=\u001b[39m \u001b[43mtfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m test_dfs(dfs_output, dfs_expected)    \n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransform the dataframe(s) according to the specified callbacks.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs: \u001b[43mrun_cbs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mrun_cbs\u001b[0;34m(cbs, obj)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(cbs, key\u001b[38;5;241m=\u001b[39mattrgetter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m: obj\u001b[38;5;241m.\u001b[39mlogs\u001b[38;5;241m.\u001b[39mappend(cb\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 67\u001b[0m, in \u001b[0;36mReshapeLongToWide.__call__\u001b[0;34m(self, tfm)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tfm):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m grp \u001b[38;5;129;01min\u001b[39;00m tfm\u001b[38;5;241m.\u001b[39mdfs\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m---> 67\u001b[0m         tfm\u001b[38;5;241m.\u001b[39mdfs[grp] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m         tfm\u001b[38;5;241m.\u001b[39mdfs[grp]\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrenamed_cols(tfm\u001b[38;5;241m.\u001b[39mdfs[grp]\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m, in \u001b[0;36mReshapeLongToWide.pivot\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     44\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m+\u001b[39m derived_coi \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues))\n\u001b[1;32m     46\u001b[0m df, num_fill_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fill_nan_values(df, idx)\n\u001b[0;32m---> 48\u001b[0m pivot_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mderived_coi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                          \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     54\u001b[0m pivot_df[idx] \u001b[38;5;241m=\u001b[39m pivot_df[idx]\u001b[38;5;241m.\u001b[39mreplace({\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_fill_value: np\u001b[38;5;241m.\u001b[39mnan, num_fill_value: np\u001b[38;5;241m.\u001b[39mnan})\n\u001b[1;32m     55\u001b[0m pivot_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_index(pivot_df)\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/frame.py:9482\u001b[0m, in \u001b[0;36mDataFrame.pivot_table\u001b[0;34m(self, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m   9465\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   9466\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   9467\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot_table\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9478\u001b[0m     sort: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   9479\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   9480\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot_table\n\u001b[0;32m-> 9482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9483\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9487\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9493\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/reshape/pivot.py:102\u001b[0m, in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m     99\u001b[0m     table \u001b[38;5;241m=\u001b[39m concat(pieces, keys\u001b[38;5;241m=\u001b[39mkeys, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43m__internal_pivot_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmargins_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\u001b[38;5;241m.\u001b[39m__finalize__(data, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/reshape/pivot.py:183\u001b[0m, in \u001b[0;36m__internal_pivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed, sort)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    174\u001b[0m     ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouped\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mgroupings\n\u001b[1;32m    175\u001b[0m ):\n\u001b[1;32m    176\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default value of observed=False is deprecated and will change \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto observed=True in a future version of pandas. Specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m agged \u001b[38;5;241m=\u001b[39m \u001b[43mgrouped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43maggfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropna \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(agged, ABCDataFrame) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agged\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[1;32m    186\u001b[0m     agged \u001b[38;5;241m=\u001b[39m agged\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1466\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;66;03m# grouper specific aggregations\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39mnkeys \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1465\u001b[0m     \u001b[38;5;66;03m# test_groupby_as_index_series_scalar gets here with 'not self.as_index'\u001b[39;00m\n\u001b[0;32m-> 1466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_agg_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args \u001b[38;5;129;01mor\u001b[39;00m kwargs:\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;66;03m# test_pass_args_kwargs gets here (with and without as_index)\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;66;03m# can't return early\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_frame(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1532\u001b[0m, in \u001b[0;36mDataFrameGroupBy._python_agg_general\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m output: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, ArrayLike] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, ser) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(obj\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m-> 1532\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1533\u001b[0m     output[idx] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m   1535\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor(output)\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/groupby/ops.py:863\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;66;03m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;66;03m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/groupby/ops.py:889\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    885\u001b[0m res \u001b[38;5;241m=\u001b[39m extract_result(res)\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initialized:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;66;03m# We only do this validation on the first iteration\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[43mcheck_result_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m     initialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    892\u001b[0m result[i] \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/groupby/ops.py:88\u001b[0m, in \u001b[0;36mcheck_result_array\u001b[0;34m(obj, dtype)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;66;03m# If it is object dtype, the function can be a reduction/aggregation\u001b[39;00m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m#  and still return an ndarray e.g. test_agg_over_numpy_arrays\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust produce aggregated value\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Must produce aggregated value"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs_test = fc.load_pickle('./files/pkl/dfs_reshape_test_in.pkl')\n",
    "dfs_expected = fc.load_pickle('./files/pkl/dfs_reshape_test_expected.pkl')\n",
    "\n",
    "tfm = Transformer(dfs_test, cbs=[ReshapeLongToWide()])\n",
    "dfs_output = tfm()\n",
    "test_dfs(dfs_output, dfs_expected)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf07327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class CompareDfsAndTfmCB(Callback):\n",
    "    \"Create a dataframe of dropped data. Data included in the `dfs` not in the `tfm`.\"\n",
    "    def __init__(self, \n",
    "                 dfs: Dict[str, pd.DataFrame] # Original dataframes\n",
    "                 ): \n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer) -> None:\n",
    "        self._initialize_tfm_attributes(tfm)\n",
    "        for grp in tfm.dfs.keys():\n",
    "            dropped_df = self._get_dropped_data(grp, tfm)\n",
    "            tfm.dfs_dropped[grp] = dropped_df\n",
    "            tfm.compare_stats[grp] = self._compute_stats(grp, tfm)\n",
    "\n",
    "    def _initialize_tfm_attributes(self, tfm: Transformer) -> None:\n",
    "        tfm.dfs_dropped = {}\n",
    "        tfm.compare_stats = {}\n",
    "\n",
    "    def _get_dropped_data(self, \n",
    "                          grp: str, # The group key\n",
    "                          tfm: Transformer # The transformation object containing `dfs`\n",
    "                         ) -> pd.DataFrame: # Dataframe with dropped rows\n",
    "        \"Get the data that is present in `dfs` but not in `tfm.dfs`.\"\n",
    "        index_diff = self.dfs[grp].index.difference(tfm.dfs[grp].index)\n",
    "        return self.dfs[grp].loc[index_diff]\n",
    "    \n",
    "    def _compute_stats(self, \n",
    "                       grp: str, # The group key\n",
    "                       tfm: Transformer # The transformation object containing `dfs`\n",
    "                      ) -> Dict[str, int]: # Dictionary with comparison statistics\n",
    "        \"Compute comparison statistics between `dfs` and `tfm.dfs`.\"\n",
    "        return {\n",
    "            'Number of rows in dfs': len(self.dfs[grp].index),\n",
    "            'Number of rows in tfm.dfs': len(tfm.dfs[grp].index),\n",
    "            'Number of dropped rows': len(tfm.dfs_dropped[grp].index),\n",
    "            'Number of rows in tfm.dfs + Number of dropped rows': len(tfm.dfs[grp].index) + len(tfm.dfs_dropped[grp].index)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a31ca8",
   "metadata": {},
   "source": [
    "`CompareDfsAndTfmCB` compares the original dataframes to the transformed dataframe. A dictionary of dataframes, `tfm.dfs_dropped`, is created to include the data present in the original dataset but absent from the transformed data. `tfm.compare_stats` provides a quick overview of the number of rows in both the original dataframes and the transformed dataframe. \n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d96f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seawater':    a  b\n",
      "2  3  6, 'sediment':    a  b\n",
      "1  2  5\n",
      "2  3  6}\n"
     ]
    }
   ],
   "source": [
    "dfs_test = {\n",
    "    'seawater': pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]}),\n",
    "    'sediment': pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]}),\n",
    "}\n",
    "\n",
    "class TestTfmCB(Callback):\n",
    "    def __call__(self, tfm):\n",
    "        for key in tfm.dfs.keys():\n",
    "            df = tfm.dfs[key]\n",
    "            drop_idxs = [0, 1] if key == 'seawater' else [0]\n",
    "            df.drop(drop_idxs, inplace=True)\n",
    "            \n",
    "tfm = Transformer(dfs_test, cbs=[\n",
    "    TestTfmCB(), \n",
    "    CompareDfsAndTfmCB(dfs_test)], inplace=False)\n",
    "\n",
    "print(tfm())\n",
    "\n",
    "fc.test_eq(tfm.compare_stats['seawater']['Number of dropped rows'], 2)\n",
    "fc.test_eq(tfm.compare_stats['sediment']['Number of dropped rows'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a7408",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9de40",
   "metadata": {},
   "source": [
    "These callbacks are used to transform the time variable according to netCDF CF standards. For instance, the `EncodeTimeCB` callback is used to encode the time variable as an integer representing seconds since a reference date as specified in `configs.ipynb` `CONFIGS_CDL` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a817ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class EncodeTimeCB(Callback):\n",
    "    \"Encode time as `int` representing seconds since xxx.\"    \n",
    "    def __init__(self, \n",
    "                 cfg: dict, # Configuration dictionary\n",
    "                 verbose: bool=False # Whether to print the number of invalid time entries\n",
    "                 ): \n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm): \n",
    "        def format_time(x): \n",
    "            return date2num(x, units=self.cfg['units']['time'])\n",
    "        \n",
    "        for k in tfm.dfs.keys():\n",
    "            # If invalid time entries.\n",
    "            if tfm.dfs[k]['time'].isna().any():\n",
    "                if self.verbose:\n",
    "                    invalid_time_df=tfm.dfs[k][tfm.dfs[k]['time'].isna()]\n",
    "                    print (f'{len(invalid_time_df.index)} of {len(tfm.dfs[k].index)} entries for `time` are invalid for {k}.')\n",
    "                # Filter nan values\n",
    "                tfm.dfs[k] = tfm.dfs[k][tfm.dfs[k]['time'].notna()]\n",
    "            \n",
    "            tfm.dfs[k]['time'] = tfm.dfs[k]['time'].apply(format_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

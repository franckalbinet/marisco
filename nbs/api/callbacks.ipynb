{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp callbacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34f38641",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "> Callback used in handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4934cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "import fastcore.all as fc\n",
    "from operator import attrgetter\n",
    "from cftime import date2num\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from marisco.configs import cfg, cdl_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91324c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from marisco.configs import cdl_cfg\n",
    "from marisco.utils import CompareDfsAndTfmCB\n",
    "from marisco.utils import test_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d7034",
   "metadata": {},
   "source": [
    "## Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Callback(): order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61702d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def run_cbs(cbs, obj=None):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        if cb.__doc__: obj.logs.append(cb.__doc__)\n",
    "        cb(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Transformer():\n",
    "    def __init__(self, dfs, cbs=None): \n",
    "        self.cbs = cbs\n",
    "        #self.dfs = {k: v.copy() for k, v in dfs.items()}\n",
    "\n",
    "        self.dfs = self._copy(dfs)\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def _copy(self, dfs):\n",
    "        if isinstance(dfs, dict):\n",
    "            return {k: v.copy() for k, v in dfs.items()}\n",
    "        else:\n",
    "            return dfs.copy()\n",
    "        \n",
    "    def callback(self):\n",
    "        run_cbs(self.cbs, self)\n",
    "        \n",
    "    def unique(self, col_name):\n",
    "        \"Distinct values of a specific column present in all groups\"\n",
    "        columns = [df.get(col_name) for df in self.dfs.values() if df.get(col_name) is not None]\n",
    "        values = np.concatenate(columns) if columns else []\n",
    "        return np.unique(values)\n",
    "        \n",
    "    def __call__(self):\n",
    "        if self.cbs: self.callback()\n",
    "        return self.dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab87dd9",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ee79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = {'biota': pd.DataFrame({'id': [0, 1, 2], 'species': [0, 2, 0], 'depth': [2, 3, 4]}),\n",
    "       'seawater': pd.DataFrame({'id': [0, 1, 2], 'depth': [3, 4, 5]})}\n",
    "tfm = Transformer(dfs); tfm()\n",
    "tfm.unique('species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b00a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm.unique('non_existing_var')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca917c61",
   "metadata": {},
   "source": [
    "## Geographical coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class SanitizeLonLatCB(Callback):\n",
    "    \"Drop row when both longitude & latitude equal 0. Drop unrealistic longitude & latitude values. Convert longitude & latitude `,` separator to `.` separator.\"\n",
    "    def __init__(self, verbose=False): fc.store_attr()\n",
    "    def __call__(self, tfm):\n",
    "        for grp, df in tfm.dfs.items():\n",
    "            \" Convert `,` separator to `.` separator\"\n",
    "            df['lon'] = [float(str(x).replace(',', '.')) for x in df['lon']]\n",
    "            df['lat'] = [float(str(x).replace(',', '.')) for x in df['lat']]\n",
    "            \n",
    "            # mask zero values\n",
    "            mask_zeroes = (df.lon == 0) & (df.lat == 0) \n",
    "            nZeroes = mask_zeroes.sum()\n",
    "            if nZeroes and self.verbose: \n",
    "                print(f'The \"{grp}\" group contains {nZeroes} data points whose (lon, lat) = (0, 0)')\n",
    "            \n",
    "            # mask gps out of bounds, goob. \n",
    "            mask_goob = (df.lon < -180) | (df.lon > 180) | (df.lat < -90) | (df.lat > 90)\n",
    "            nGoob = mask_goob.sum()\n",
    "            if nGoob and self.verbose: \n",
    "                print(f'The \"{grp}\" group contains {nGoob} data points whose lon or lat are unrealistic. Outside -90 to 90 for latitude and -180 to 180 for longitude.')\n",
    "                \n",
    "            tfm.dfs[grp] = df.loc[~(mask_zeroes | mask_goob)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that measurements located at (0,0) get removed\n",
    "dfs = {'biota': pd.DataFrame({'lon': [0, 1, 0], 'lat': [0, 2, 0]})}\n",
    "tfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\n",
    "tfm()['biota']\n",
    "\n",
    "expected = [1., 2.]\n",
    "fc.test_eq(tfm()['biota'].iloc[0].to_list(), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378851b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that comma decimal separator get replaced by point instead\n",
    "dfs = {'biota': pd.DataFrame({'lon': ['45,2'], 'lat': ['43,1']})}\n",
    "tfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\n",
    "tfm()['biota']\n",
    "\n",
    "expected = [45.2, 43.1]\n",
    "fc.test_eq(tfm()['biota'].iloc[0].to_list(), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd0096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that out of bounds lon or lat get removed\n",
    "dfs = {'biota': pd.DataFrame({'lon': [-190, 190, 1, 2, 1.1], 'lat': [1, 2, 91, -91, 2.2]})}\n",
    "tfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\n",
    "tfm()['biota']\n",
    "\n",
    "expected = [1.1, 2.2]\n",
    "fc.test_eq(tfm()['biota'].iloc[0].to_list(), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d96a0",
   "metadata": {},
   "source": [
    "## Remapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ae6fa",
   "metadata": {},
   "source": [
    "## Structural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32c3bc",
   "metadata": {},
   "source": [
    "Many data providers use a long format (e.g `lat, lon, radionuclide, value, unc, ...`) to store their data. When encoding as `netCDF`, it is often required to use a wide format (e.g `lat, lon, nuclide1_value, nuclide1_unc, nuclide2_value, nuclide2_unc, ...`). The class `ReshapeLongToWide` is designed to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b409a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ReshapeLongToWide(Callback):\n",
    "    def __init__(self, columns=['nuclide'], values=['value'], \n",
    "                 num_fill_value=-999, str_fill_value='STR FILL VALUE'):\n",
    "        \"Convert data from long to wide with renamed columns.\"\n",
    "        fc.store_attr()\n",
    "        self.derived_cols = self._get_derived_cols()\n",
    "    \n",
    "    def _get_derived_cols(self):\n",
    "        \"Retrieve all possible derived vars (e.g 'unc', 'dl', ...) from configs.\"\n",
    "        return [value['name'] for value in cdl_cfg()['vars']['suffixes'].values()]\n",
    "\n",
    "    def renamed_cols(self, cols):\n",
    "        \"Flatten columns name.\"\n",
    "        return [inner if outer == \"value\" else f'{inner}{outer}' if inner else outer\n",
    "                for outer, inner in cols]\n",
    "\n",
    "    def _get_unique_fill_value(self, df, idx):\n",
    "        \"Get a unique fill value for NaN replacement.\"\n",
    "        fill_value = self.num_fill_value\n",
    "        while (df[idx] == fill_value).any().any():\n",
    "            fill_value -= 1\n",
    "        return fill_value\n",
    "\n",
    "    def _fill_nan_values(self, df, idx):\n",
    "        \"Fill NaN values in index columns.\"\n",
    "        num_fill_value = self._get_unique_fill_value(df, idx)\n",
    "        for col in idx:\n",
    "            fill_value = num_fill_value if pd.api.types.is_numeric_dtype(df[col]) else self.str_fill_value\n",
    "            df[col] = df[col].fillna(fill_value)\n",
    "        return df, num_fill_value\n",
    "\n",
    "    def pivot(self, df):\n",
    "        derived_coi = [col for col in self.derived_cols if col in df.columns]\n",
    "        df.index.name = 'org_index'\n",
    "        df = df.reset_index()\n",
    "        idx = list(set(df.columns) - set(self.columns + derived_coi + self.values))\n",
    "        \n",
    "        df, num_fill_value = self._fill_nan_values(df, idx)\n",
    "\n",
    "        pivot_df = df.pivot_table(index=idx,\n",
    "                                  columns=self.columns,\n",
    "                                  values=self.values + derived_coi,\n",
    "                                  fill_value=np.nan,\n",
    "                                  aggfunc=lambda x: x).reset_index()\n",
    "\n",
    "        pivot_df[idx] = pivot_df[idx].replace({self.str_fill_value: np.nan, num_fill_value: np.nan})\n",
    "        return pivot_df.set_index('org_index')\n",
    "\n",
    "    def __call__(self, tfm):\n",
    "        for grp in tfm.dfs.keys():\n",
    "            tfm.dfs[grp] = self.pivot(tfm.dfs[grp])\n",
    "            tfm.dfs[grp].columns = self.renamed_cols(tfm.dfs[grp].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_test = fc.load_pickle('./files/pkl/dfs_reshape_test_in.pkl')\n",
    "dfs_expected = fc.load_pickle('./files/pkl/dfs_reshape_test_expected.pkl')\n",
    "\n",
    "tfm = Transformer(dfs_test, cbs=[ReshapeLongToWide()])\n",
    "dfs_output = tfm()\n",
    "test_dfs(dfs_output, dfs_expected)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a7408",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a817ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class EncodeTimeCB(Callback):\n",
    "    \"Encode time as `int` representing seconds since xxx\"    \n",
    "    def __init__(self, cfg , verbose=False): fc.store_attr()\n",
    "    def __call__(self, tfm): \n",
    "        def format_time(x): \n",
    "            return date2num(x, units=self.cfg['units']['time'])\n",
    "        \n",
    "        for k in tfm.dfs.keys():\n",
    "            # If invalid time entries.\n",
    "            if tfm.dfs[k]['time'].isna().any():\n",
    "                if self.verbose:\n",
    "                    invalid_time_df=tfm.dfs[k][tfm.dfs[k]['time'].isna()]\n",
    "                    print (f'{len(invalid_time_df.index)} of {len(tfm.dfs[k].index)} entries for `time` are invalid for {k}.')\n",
    "                # Filter nan values\n",
    "                tfm.dfs[k] = tfm.dfs[k][tfm.dfs[k]['time'].notna()]\n",
    "            \n",
    "            tfm.dfs[k]['time'] = tfm.dfs[k]['time'].apply(format_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp callbacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34f38641",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "> Callback used in handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a293345",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "import fastcore.all as fc\n",
    "from operator import attrgetter\n",
    "from cftime import date2num\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial \n",
    "from pathlib import Path \n",
    "from typing import List, Dict, Callable, Tuple, Any, Optional, Union\n",
    "\n",
    "from marisco.configs import (\n",
    "    get_lut, \n",
    "    nuc_lut_path, \n",
    "    nc_tpl_path,\n",
    "    get_time_units,\n",
    "    NC_GROUPS,\n",
    "    SMP_TYPE_LUT,\n",
    "    cfg, \n",
    "    # cdl_cfg\n",
    ")\n",
    "\n",
    "from marisco.utils import Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91324c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# from marisco.configs import cdl_cfg, CONFIGS_CDL\n",
    "from marisco.utils import test_dfs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d7034",
   "metadata": {},
   "source": [
    "## Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962281da",
   "metadata": {},
   "source": [
    "The `Transformer` class is designed to facilitate the application of a series of callbacks to a set of dataframes. It provides a structured way to apply transformations (i.e `Callback`) to the data, with a focus on flexibility and ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Callback(): \n",
    "    \"Base class for callbacks.\"\n",
    "    order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61702d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def run_cbs(\n",
    "    cbs: List[Callback], # List of callbacks to run\n",
    "    obj: Any # Object to pass to the callbacks\n",
    "    ):\n",
    "    \"Run the callbacks in the order they are specified.\"\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        if cb.__doc__: obj.logs.append(cb.__doc__)\n",
    "        cb(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class Transformer():\n",
    "    \"Transform the dataframe(s) according to the specified callbacks.\"\n",
    "    def __init__(self, \n",
    "                 data: Union[Dict[str, pd.DataFrame], pd.DataFrame], # Data to be transformed\n",
    "                 cbs: Optional[List[Callback]]=None, # List of callbacks to run\n",
    "                 inplace: bool=False # Whether to modify the dataframe(s) in place\n",
    "                 ): \n",
    "        fc.store_attr()\n",
    "        self.is_single_df = isinstance(data, pd.DataFrame)\n",
    "        self.df, self.dfs = self._prepare_data(data, inplace)\n",
    "        self.logs = []\n",
    "            \n",
    "    def _prepare_data(self, data, inplace):\n",
    "        if self.is_single_df:\n",
    "            return (data if inplace else data.copy()), None\n",
    "        else:\n",
    "            return None, (data if inplace else {k: v.copy() for k, v in data.items()})\n",
    "    \n",
    "    def unique(self, col_name: str) -> np.ndarray:\n",
    "        \"Distinct values of a specific column present in all groups.\"\n",
    "        if self.is_single_df:\n",
    "            values = self.df.get(col_name, pd.Series()).dropna().values\n",
    "        else:\n",
    "            columns = [df.get(col_name) for df in self.dfs.values() if df.get(col_name) is not None]\n",
    "            values = np.concatenate([col.dropna().values for col in columns]) if columns else []\n",
    "        return np.unique(values)\n",
    "        \n",
    "    def __call__(self):\n",
    "        \"Transform the dataframe(s) according to the specified callbacks.\"\n",
    "        if self.cbs: run_cbs(self.cbs, self)\n",
    "        return self.df if self.dfs is None else self.dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab87dd9",
   "metadata": {},
   "source": [
    "Below, a few examples of how to use the `Transformer` class.\n",
    "Let's define first a test callback that adds `1` to the `depth`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCB(Callback):\n",
    "    \"A test callback to add 1 to the depth.\"\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for grp, df in tfm.dfs.items(): \n",
    "            df['depth'] = df['depth'].apply(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24bcbc",
   "metadata": {},
   "source": [
    "And apply it to the following dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ee79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {'biota': pd.DataFrame({'id': [0, 1, 2], 'species': [0, 2, 0], 'depth': [2, 3, 4]}),\n",
    "       'seawater': pd.DataFrame({'id': [0, 1, 2], 'depth': [3, 4, 5]})}\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[TestCB()])\n",
    "dfs_test = tfm()\n",
    "\n",
    "fc.test_eq(dfs_test['biota']['depth'].to_list(), [3, 4, 5])\n",
    "fc.test_eq(dfs_test['seawater']['depth'].to_list(), [4, 5, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bde01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCB(Callback):\n",
    "    \"A test callback to add 1 to the depth.\"\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        tfm.df['depth'] = tfm.df['depth'].apply(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id': [0, 1, 2], 'species': [0, 2, 0], 'depth': [2, 3, 4]})\n",
    "\n",
    "tfm = Transformer(df, cbs=[TestCB()])\n",
    "df_test = tfm()\n",
    "\n",
    "fc.test_eq(df_test['depth'].to_list(), [3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca917c61",
   "metadata": {},
   "source": [
    "## Geographical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db90009",
   "metadata": {},
   "source": [
    "This section gathers callbacks that are used to transform the geographical coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class SanitizeLonLatCB(Callback):\n",
    "    \"Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.\"\n",
    "    def __init__(self, \n",
    "                 lon_col: str='LON', # Longitude column name\n",
    "                 lat_col: str='LAT', # Latitude column name\n",
    "                 verbose: bool=False # Whether to print the number of invalid longitude & latitude values\n",
    "                 ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for grp, df in tfm.dfs.items():\n",
    "            # Convert `,` separator to `.` separator\n",
    "            df[self.lon_col] = df[self.lon_col].apply(lambda x: float(str(x).replace(',', '.')))\n",
    "            df[self.lat_col] = df[self.lat_col].apply(lambda x: float(str(x).replace(',', '.')))\n",
    "            \n",
    "            # Mask zero values\n",
    "            mask_zeroes = (df[self.lon_col] == 0) & (df[self.lat_col] == 0) \n",
    "            nZeroes = mask_zeroes.sum()\n",
    "            if nZeroes and self.verbose: \n",
    "                print(f'The \"{grp}\" group contains {nZeroes} data points whose ({self.lon_col}, {self.lat_col}) = (0, 0)')\n",
    "            \n",
    "            # Mask out of bounds values\n",
    "            mask_goob = (df[self.lon_col] < -180) | (df[self.lon_col] > 180) | (df[self.lat_col] < -90) | (df[self.lat_col] > 90)\n",
    "            nGoob = mask_goob.sum()\n",
    "            if nGoob and self.verbose: \n",
    "                print(f'The \"{grp}\" group contains {nGoob} data points with unrealistic {self.lon_col} or {self.lat_col} values.')\n",
    "                \n",
    "            tfm.dfs[grp] = df.loc[~(mask_zeroes | mask_goob)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that measurements located at (0,0) get removed\n",
    "dfs = {'BIOTA': pd.DataFrame({'LON': [0, 1, 0], 'LAT': [0, 2, 0]})}\n",
    "tfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\n",
    "tfm()['BIOTA']\n",
    "\n",
    "expected = [1., 2.]\n",
    "fc.test_eq(tfm()['BIOTA'].iloc[0].to_list(), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378851b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that comma decimal separator get replaced by point instead\n",
    "dfs = {'BIOTA': pd.DataFrame({'LON': ['45,2'], 'LAT': ['43,1']})}\n",
    "tfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\n",
    "tfm()['BIOTA']\n",
    "\n",
    "expected = [45.2, 43.1]\n",
    "fc.test_eq(tfm()['BIOTA'].iloc[0].to_list(), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd0096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that out of bounds lon or lat get removed\n",
    "dfs = {'BIOTA': pd.DataFrame({'LON': [-190, 190, 1, 2, 1.1], 'LAT': [1, 2, 91, -91, 2.2]})}\n",
    "tfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\n",
    "tfm()['BIOTA']\n",
    "\n",
    "expected = [1.1, 2.2]\n",
    "fc.test_eq(tfm()['BIOTA'].iloc[0].to_list(), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98192b7",
   "metadata": {},
   "source": [
    "## Map & Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c905654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapCB(Callback):\n",
    "    \"Generic MARIS remapping callback.\"\n",
    "    def __init__(self, \n",
    "                 fn_lut: Callable, # Function that returns the lookup table dictionary\n",
    "                 col_remap: str, # Name of the column to remap\n",
    "                 col_src: str, # Name of the column with the source values\n",
    "                 dest_grps: list[str]|str=NC_GROUPS.keys(), # List of destination groups\n",
    "                 default_value: Any = -1 # Default value for unmatched entries\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        self.lut = None\n",
    "        \n",
    "        if isinstance(dest_grps, str):\n",
    "            self.dest_grps = [dest_grps]\n",
    "        # Format the documentation string based on the type and content of dest_grps\n",
    "        if isinstance(self.dest_grps, list):\n",
    "            if len(self.dest_grps) > 1:\n",
    "                grp_str = ', '.join(self.dest_grps[:-1]) + ' and ' + self.dest_grps[-1]\n",
    "            else:\n",
    "                grp_str = self.dest_grps[0]\n",
    "        else:\n",
    "            grp_str = self.dest_grps\n",
    "                \n",
    "        self.__doc__ = f\"Remap values from '{col_src}' to '{col_remap}' for groups: {grp_str}.\"\n",
    "\n",
    "    def __call__(self, tfm):\n",
    "        self.lut = self.fn_lut()\n",
    "        for grp in self.dest_grps:\n",
    "            if grp in tfm.dfs:\n",
    "                self._remap_group(tfm.dfs[grp])\n",
    "            else:\n",
    "                print(f\"Group {grp} not found in the dataframes.\")\n",
    "\n",
    "    def _remap_group(self, df: pd.DataFrame):\n",
    "        df[self.col_remap] = df[self.col_src].apply(self._remap_value)\n",
    "\n",
    "    def _remap_value(self, value: str) -> Any:\n",
    "        value = value.strip() if isinstance(value, str) else value\n",
    "        match = self.lut.get(value, Match(self.default_value, None, None, None))\n",
    "        if isinstance(match, Match):\n",
    "            if match.matched_id == self.default_value:\n",
    "                print(f\"Unmatched value: {value}\")\n",
    "            return match.matched_id \n",
    "        else:\n",
    "            return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1917a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class LowerStripNameCB(Callback):\n",
    "    \"Convert values to lowercase and strip any trailing spaces.\"\n",
    "    def __init__(self, \n",
    "                 col_src: str, # Source column name e.g. 'Nuclide'\n",
    "                 col_dst: str=None, # Destination column name\n",
    "                 fn_transform: Callable=lambda x: x.lower().strip() # Transformation function\n",
    "                 ):\n",
    "        fc.store_attr()\n",
    "        self.__doc__ = f\"Convert '{col_src}' column values to lowercase, strip spaces, and store in '{col_dst}' column.\"\n",
    "        if not col_dst: self.col_dst = col_src\n",
    "        \n",
    "    def _safe_transform(self, value):\n",
    "        \"Ensure value is not NA and apply transformation function.\"\n",
    "        return value if pd.isna(value) else self.fn_transform(str(value))\n",
    "            \n",
    "    def __call__(self, tfm):\n",
    "        for key in tfm.dfs.keys():\n",
    "            tfm.dfs[key][self.col_dst] = tfm.dfs[key][self.col_src].apply(self._safe_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06003b9",
   "metadata": {},
   "source": [
    "Let's test the callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {'seawater': pd.DataFrame({'Nuclide': ['CS137', '226RA']})}\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='Nuclide', col_dst='NUCLIDE')])\n",
    "fc.test_eq(tfm()['seawater']['NUCLIDE'].to_list(), ['cs137', '226ra'])\n",
    "\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='Nuclide')])\n",
    "fc.test_eq(tfm()['seawater']['Nuclide'].to_list(), ['cs137', '226ra'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b9b06a",
   "metadata": {},
   "source": [
    "The point is when (semi-automatic) remapping names generally:\n",
    "\n",
    "1. we need first to guess (fuzzy matching or other) the right nuclide name.\n",
    "2. Then manually check the result and eventually update the lookup table.\n",
    "3.  Finally we can apply the lookup table to the dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ae6fa",
   "metadata": {},
   "source": [
    "## Change structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class AddSampleTypeIdColumnCB(Callback):\n",
    "    def __init__(self, \n",
    "                 lut: dict=SMP_TYPE_LUT, # Lookup table for sample type\n",
    "                 col_name: str='samptype_id' # Column name to store the sample type id\n",
    "                 ): \n",
    "        \"Add a column with the sample type id as defined in the CDL.\"\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm):\n",
    "        for grp, df in tfm.dfs.items():             \n",
    "            df[self.col_name] = self.lut[grp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2202db",
   "metadata": {},
   "source": [
    "Let's test the callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b44b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {smp_type: pd.DataFrame({'col_test': [0, 1, 2]}) for smp_type in SMP_TYPE_LUT.keys()};\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[AddSampleTypeIdColumnCB()])\n",
    "dfs_test = tfm()\n",
    "\n",
    "for smp_type in SMP_TYPE_LUT.keys():\n",
    "    fc.test_eq(dfs_test[smp_type]['samptype_id'].unique().item(), SMP_TYPE_LUT[smp_type]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46801203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class AddNuclideIdColumnCB(Callback):\n",
    "    def __init__(self, \n",
    "                 col_value: str, # Column name containing the nuclide name\n",
    "                 lut_fname_fn: Callable=nuc_lut_path, # Function returning the lut path\n",
    "                 col_name: str='nuclide_id' # Column name to store the nuclide id\n",
    "                 ): \n",
    "        \"Add a column with the nuclide id.\"\n",
    "        fc.store_attr()\n",
    "        self.lut = get_lut(lut_fname_fn().parent, lut_fname_fn().name, \n",
    "                           key='nc_name', value='nuclide_id', reverse=False)\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for grp, df in tfm.dfs.items(): \n",
    "            df[self.col_name] = df[self.col_value].map(self.lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe41950",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {smp_type: pd.DataFrame({'Nuclide': ['cs137', 'pu239_240_tot']}) for smp_type in SMP_TYPE_LUT.keys()};\n",
    "\n",
    "lut_fname_fn = lambda: Path('./files/lut/dbo_nuclide.xlsx')\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[AddNuclideIdColumnCB(col_value='Nuclide', lut_fname_fn=lut_fname_fn)])\n",
    "tfm()['SEAWATER']\n",
    "\n",
    "expected = [33, 77]\n",
    "for grp in tfm.dfs.keys():\n",
    "    fc.test_eq(tfm.dfs[grp]['nuclide_id'].to_list(), expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class SelectColumnsCB(Callback):\n",
    "    \"Select columns of interest.\"\n",
    "    def __init__(self, \n",
    "                 cois: dict # Columns of interest\n",
    "                 ): \n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm):\n",
    "        \"Select columns of interest.\"\n",
    "        for grp, df in tfm.dfs.items(): \n",
    "            tfm.dfs[grp] = df.loc[:, self.cois.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c28eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RenameColumnsCB(Callback):\n",
    "    \"Renaming variables to MARIS standard names.\"\n",
    "    def __init__(self,\n",
    "                 renaming_rules: dict # Renaming rules\n",
    "                 ): \n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm):\n",
    "        for grp in tfm.dfs.keys(): \n",
    "            tfm.dfs[grp].rename(columns=self.renaming_rules, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701be72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemoveAllNAValuesCB(Callback):\n",
    "    \"Remove rows with all NA values.\"\n",
    "    def __init__(self, \n",
    "                 cols_to_check: Dict[str, str] # A dictionary with the sample type as key and the column name to check as value\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm):\n",
    "        for k in tfm.dfs.keys():\n",
    "            col_to_check = self.cols_to_check[k]\n",
    "            mask = tfm.dfs[k][col_to_check].isnull().all(axis=1)\n",
    "            tfm.dfs[k] = tfm.dfs[k][~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a32c3bc",
   "metadata": {},
   "source": [
    "[TO BE REMOVED] Many data providers use a long format (e.g `lat, lon, radionuclide, value, unc, ...`) to store their data. When encoding as `netCDF`, it is often required to use a wide format (e.g `lat, lon, nuclide1_value, nuclide1_unc, nuclide2_value, nuclide2_unc, ...`). The class `ReshapeLongToWide` is designed to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b409a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ReshapeLongToWide(Callback):\n",
    "#     \"Convert data from long to wide with renamed columns.\"\n",
    "#     def __init__(self, \n",
    "#                  columns: List[str]=['nuclide'], # Columns to use as index\n",
    "#                  values: List[str]=['value'], # Columns to use as values\n",
    "#                  num_fill_value: int=-999, # Fill value for numeric columns\n",
    "#                  str_fill_value='STR FILL VALUE'\n",
    "#                  ):\n",
    "#         fc.store_attr()\n",
    "#         self.derived_cols = self._get_derived_cols()\n",
    "    \n",
    "#     def _get_derived_cols(self):\n",
    "#         \"Retrieve all possible derived vars (e.g 'unc', 'dl', ...) from configs.\"\n",
    "#         return [value['name'] for value in cdl_cfg()['vars']['suffixes'].values()]\n",
    "\n",
    "#     def renamed_cols(self, cols):\n",
    "#         \"Flatten columns name.\"\n",
    "#         return [inner if outer == \"value\" else f'{inner}{outer}' if inner else outer\n",
    "#                 for outer, inner in cols]\n",
    "\n",
    "#     def _get_unique_fill_value(self, df, idx):\n",
    "#         \"Get a unique fill value for NaN replacement.\"\n",
    "#         fill_value = self.num_fill_value\n",
    "#         while (df[idx] == fill_value).any().any():\n",
    "#             fill_value -= 1\n",
    "#         return fill_value\n",
    "\n",
    "#     def _fill_nan_values(self, df, idx):\n",
    "#         \"Fill NaN values in index columns.\"\n",
    "#         num_fill_value = self._get_unique_fill_value(df, idx)\n",
    "#         for col in idx:\n",
    "#             fill_value = num_fill_value if pd.api.types.is_numeric_dtype(df[col]) else self.str_fill_value\n",
    "#             df[col] = df[col].fillna(fill_value)\n",
    "#         return df, num_fill_value\n",
    "\n",
    "#     def pivot(self, df):\n",
    "#         derived_coi = [col for col in self.derived_cols if col in df.columns]\n",
    "#         # In past implementation we added an index column before pivoting \n",
    "#         # TO BE REMOVED\n",
    "#         # making all rows (compound_idx) unique.\n",
    "#         # df.index.name = 'org_index'\n",
    "#         # df = df.reset_index()\n",
    "#         idx = list(set(df.columns) - set(self.columns + derived_coi + self.values))\n",
    "        \n",
    "#         df, num_fill_value = self._fill_nan_values(df, idx)\n",
    "\n",
    "#         pivot_df = df.pivot_table(index=idx,\n",
    "#                                   columns=self.columns,\n",
    "#                                   values=self.values + derived_coi,\n",
    "                                  \n",
    "                                  \n",
    "#                                   aggfunc=lambda x: x\n",
    "#                                   ).reset_index()\n",
    "\n",
    "#         pivot_df[idx] = pivot_df[idx].replace({self.str_fill_value: np.nan, num_fill_value: np.nan})\n",
    "#         pivot_df = self.set_index(pivot_df)\n",
    "#         return pivot_df\n",
    "\n",
    "#         def set_index(self, df):\n",
    "#             \"Set the index of the dataframe.\"\n",
    "#             # TODO: Consider implementing a universal unique index\n",
    "#             # by hashing the compound index columns (lat, lon, time, depth, etc.)\n",
    "#             df.index.name = 'org_index'\n",
    "#             return df\n",
    "    \n",
    "#     def __call__(self, tfm):\n",
    "#         for grp in tfm.dfs.keys():\n",
    "#             tfm.dfs[grp] = self.pivot(tfm.dfs[grp])\n",
    "#             tfm.dfs[grp].columns = self.renamed_cols(tfm.dfs[grp].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8887de6",
   "metadata": {},
   "source": [
    "[TO BE DELETED] Example of usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930af6a8",
   "metadata": {},
   "source": [
    "* **Case 1**: `compound_idx` (in our case made of `lon`, `lat`, `time`, `depth`, ...) are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5416fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| eval: false\n",
    "# dfs_test = {'seawater': pd.DataFrame({\n",
    "#     'compound_idx': ['a', 'b', 'c', 'd'], \n",
    "#     'nuclide': ['cs137', 'cs137', 'pu239_240_tot', 'pu239_240_tot'], \n",
    "#     'value': [1, 2, 3, 4],\n",
    "#     '_unc': [0.1, 0.2, 0.3, 0.4]})}\n",
    "\n",
    "# tfm = Transformer(dfs_test, cbs=[ReshapeLongToWide()])\n",
    "# tfm()['seawater']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f00ed0",
   "metadata": {},
   "source": [
    "* **Case 2**: compound_idx are not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e271af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| eval: false\n",
    "# dfs_test = {'seawater': pd.DataFrame({\n",
    "#     'compound_idx': ['a', 'a', 'c', 'd'], \n",
    "#     'nuclide': ['cs137', 'cs134', 'pu239_240_tot', 'pu239_240_tot'], \n",
    "#     'value': [1, 2, 3, 4],\n",
    "#     '_unc': [0.1, 0.2, 0.3, 0.4]})}\n",
    "\n",
    "# tfm = Transformer(dfs_test, cbs=[ReshapeLongToWide()])\n",
    "# tfm()['seawater']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c95e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| eval: false\n",
    "# dfs_test = fc.load_pickle('./files/pkl/dfs_reshape_test_in.pkl')\n",
    "# dfs_expected = fc.load_pickle('./files/pkl/dfs_reshape_test_expected.pkl')\n",
    "\n",
    "# tfm = Transformer(dfs_test, cbs=[ReshapeLongToWide()])\n",
    "# dfs_output = tfm()\n",
    "# test_dfs(dfs_output, dfs_expected)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf07327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class CompareDfsAndTfmCB(Callback):\n",
    "    \"Create a dataframe of dropped data. Data included in the `dfs` not in the `tfm`.\"\n",
    "    def __init__(self, \n",
    "                 dfs: Dict[str, pd.DataFrame] # Original dataframes\n",
    "                 ): \n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer) -> None:\n",
    "        self._initialize_tfm_attributes(tfm)\n",
    "        for grp in tfm.dfs.keys():\n",
    "            dropped_df = self._get_dropped_data(grp, tfm)\n",
    "            tfm.dfs_dropped[grp] = dropped_df\n",
    "            tfm.compare_stats[grp] = self._compute_stats(grp, tfm)\n",
    "\n",
    "    def _initialize_tfm_attributes(self, tfm: Transformer) -> None:\n",
    "        tfm.dfs_dropped = {}\n",
    "        tfm.compare_stats = {}\n",
    "\n",
    "    def _get_dropped_data(self, \n",
    "                          grp: str, # The group key\n",
    "                          tfm: Transformer # The transformation object containing `dfs`\n",
    "                         ) -> pd.DataFrame: # Dataframe with dropped rows\n",
    "        \"Get the data that is present in `dfs` but not in `tfm.dfs`.\"\n",
    "        index_diff = self.dfs[grp].index.difference(tfm.dfs[grp].index)\n",
    "        return self.dfs[grp].loc[index_diff]\n",
    "    \n",
    "    def _compute_stats(self, \n",
    "                       grp: str, # The group key\n",
    "                       tfm: Transformer # The transformation object containing `dfs`\n",
    "                      ) -> Dict[str, int]: # Dictionary with comparison statistics\n",
    "        \"Compute comparison statistics between `dfs` and `tfm.dfs`.\"\n",
    "        return {\n",
    "            'Number of rows in dfs': len(self.dfs[grp].index),\n",
    "            'Number of rows in tfm.dfs': len(tfm.dfs[grp].index),\n",
    "            'Number of rows removed': len(tfm.dfs_dropped[grp].index),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a31ca8",
   "metadata": {},
   "source": [
    "`CompareDfsAndTfmCB` compares the original dataframes to the transformed dataframe. A dictionary of dataframes, `tfm.dfs_dropped`, is created to include the data present in the original dataset but absent from the transformed data. `tfm.compare_stats` provides a quick overview of the number of rows in both the original dataframes and the transformed dataframe. \n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d96f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_test = {\n",
    "#     'seawater': pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]}),\n",
    "#     'sediment': pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]}),\n",
    "# }\n",
    "\n",
    "# class TestTfmCB(Callback):\n",
    "#     def __call__(self, tfm):\n",
    "#         for key in tfm.dfs.keys():\n",
    "#             df = tfm.dfs[key]\n",
    "#             drop_idxs = [0, 1] if key == 'seawater' else [0]\n",
    "#             df.drop(drop_idxs, inplace=True)\n",
    "            \n",
    "# tfm = Transformer(dfs_test, cbs=[\n",
    "#     TestTfmCB(), \n",
    "#     CompareDfsAndTfmCB(dfs_test)], inplace=False)\n",
    "\n",
    "# print(tfm())\n",
    "\n",
    "# fc.test_eq(tfm.compare_stats['seawater']['Number of dropped rows'], 2)\n",
    "# fc.test_eq(tfm.compare_stats['sediment']['Number of dropped rows'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class UniqueIndexCB(Callback):\n",
    "    \"Set unique index for each group.\"\n",
    "    def __init__(self,\n",
    "                 index_name='ID'):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm):\n",
    "        for k in tfm.dfs.keys():\n",
    "            # Reset the index of the DataFrame and drop the old index\n",
    "            tfm.dfs[k] = tfm.dfs[k].reset_index(drop=True)\n",
    "            # Reset the index again and set the name of the new index to `ìndex_name``\n",
    "            tfm.dfs[k] = tfm.dfs[k].reset_index(names=[self.index_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a7408",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9de40",
   "metadata": {},
   "source": [
    "These callbacks are used to transform the time variable according to netCDF CF standards. For instance, the `EncodeTimeCB` callback is used to encode the time variable as an integer representing seconds since a reference date as specified in `maris.cdl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class EncodeTimeCB(Callback):\n",
    "    \"Encode time as seconds since epoch.\"    \n",
    "    def __init__(self, \n",
    "                 col_time: str='TIME',\n",
    "                 fn_units: Callable=get_time_units # Function returning the time units\n",
    "                 ): \n",
    "        fc.store_attr()\n",
    "        self.units = fn_units()\n",
    "    \n",
    "    def __call__(self, tfm): \n",
    "        for grp, df in tfm.dfs.items():\n",
    "            n_missing = df[self.col_time].isna().sum()\n",
    "            if n_missing:\n",
    "                print(f\"Warning: {n_missing} missing time value(s) in {grp}\")\n",
    "            \n",
    "            # Remove NaN times and convert to seconds since epoch\n",
    "            tfm.dfs[grp] = tfm.dfs[grp][tfm.dfs[grp][self.col_time].notna()]\n",
    "            tfm.dfs[grp][self.col_time] = tfm.dfs[grp][self.col_time].apply(lambda x: date2num(x, units=self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e22765",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_test = {\n",
    "    'SEAWATER': pd.DataFrame({\n",
    "        'TIME': [pd.Timestamp(f'2023-01-0{t}') for t in [1, 2]],\n",
    "        'value': [1, 2]\n",
    "        }),\n",
    "    'SEDIMENT': pd.DataFrame({\n",
    "        'TIME': [pd.Timestamp(f'2023-01-0{t}') for t in [3, 4]],\n",
    "        'value': [3, 4]\n",
    "        }),\n",
    "}\n",
    "\n",
    "units = 'seconds since 1970-01-01 00:00:00.0'\n",
    "tfm = Transformer(dfs_test, cbs=[\n",
    "    EncodeTimeCB(fn_units=lambda: units)\n",
    "    ], inplace=False)\n",
    "dfs_result = tfm()\n",
    "\n",
    "fc.test_eq(dfs_result['SEAWATER'].TIME.dtype, 'int64')\n",
    "fc.test_eq(dfs_result['SEDIMENT'].TIME.dtype, 'int64')\n",
    "\n",
    "\n",
    "fc.test_eq(dfs_result['SEAWATER'].TIME, dfs_test['SEAWATER'].TIME.apply(lambda x: date2num(x, units=units)))\n",
    "fc.test_eq(dfs_result['SEDIMENT'].TIME, dfs_test['SEDIMENT'].TIME.apply(lambda x: date2num(x, units=units)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

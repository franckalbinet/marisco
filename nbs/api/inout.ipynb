{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ddae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34f38641",
   "metadata": {},
   "source": [
    "# Input/Output\n",
    "> Files reader and writer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4934cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import tomli_w\n",
    "import tomli\n",
    "from typing import Dict, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def write_toml(fname: str, cfg: Dict[str, Any]):\n",
    "    \"Write a TOML file from a dictionary.\"\n",
    "    none_keys = [k for k, v in flatten_dict(cfg).items() if v is None]\n",
    "    if none_keys:\n",
    "        print(f\"Warning: The following config keys have None values: {', '.join(none_keys)}\")\n",
    "        \n",
    "    print(f'Creating {fname}')\n",
    "    with open(fname, \"wb\") as f:\n",
    "        tomli_w.dump(cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ceebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def flatten_dict(d: Dict[str, Any], parent_key: str = '', sep: str = '.') -> Dict[str, Any]:\n",
    "    \"\"\"Flatten a nested dictionary.\"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a75525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def read_toml(fname):\n",
    "    \"Read a TOML file into a dictionary.\"\n",
    "    with open(fname, \"rb\") as f:\n",
    "        config = tomli.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bfdf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from marisco.configs import get_time_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aa112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def nc_to_dfs(\n",
    "    fname: str # Path to NetCDF file\n",
    "    ) -> dict: # Dictionary with group names as keys and pandas DataFrames as values\n",
    "    \"Convert a NetCDF (with groups) file to a dictionary of dataframes.\"\n",
    "    dfs = {}\n",
    "    \n",
    "    with Dataset(fname, 'r') as nc:\n",
    "        # Process each group in the NetCDF file\n",
    "        for group_name in nc.groups:\n",
    "            group = nc.groups[group_name]\n",
    "            \n",
    "            # Get all variables in the group\n",
    "            data = {}\n",
    "            for var_name in group.variables:\n",
    "                # Skip dimension variables (like 'id')\n",
    "                if var_name not in group.dimensions:\n",
    "                    data[var_name] = group.variables[var_name][:]\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Convert time from seconds since epoch if present\n",
    "            if 'time' in df.columns:\n",
    "                df['time'] = pd.to_datetime(df['time'], unit='s')\n",
    "                \n",
    "            dfs[group_name.upper()] = df\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Example usage:\n",
    "fname = Path('../../_data/output/dump/100-HELCOM-MORS-2018.nc')\n",
    "dfs = nc_to_dfs(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caf86ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BIOTA', 'SEAWATER', 'SEDIMENT'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d10c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61535, 17)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['BIOTA'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fde2e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp netcdf_to_refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert MARIS NetCDF to OpenRefine CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A data pipeline that converts MARIS NetCDF files into MARIS Standard OpenRefine CSV format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module converts NetCDF files into CSV files that follow the MARIS Standard [OpenRefine](https://openrefine.org/) format. While MARISCO has replaced OpenRefine in the data cleaning and preparation pipeline, the MARIS master database still requires input files to conform to this CSV format specification. The conversion is performed using the marisco library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "For new MARIS users, please refer to [field definitions\n",
    "](https://github.com/franckalbinet/marisco/blob/main/nbs/metadata/field-definition.ipynb) for detailed information about Maris fields.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "> Required packages and internal modules for data format transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "# from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import fastcore.all as fc\n",
    "from typing import Dict,Callable\n",
    "\n",
    "from marisco.configs import (\n",
    "    NC_VARS,\n",
    "    CSV_VARS,\n",
    "    CSV_DTYPES,\n",
    "    Enums,\n",
    "    lut_path,\n",
    "    species_lut_path,\n",
    "    detection_limit_lut_path, # used for feedback. \n",
    "    filtered_lut_path,\n",
    "    cfg\n",
    ")\n",
    "\n",
    "from marisco.utils import (\n",
    "    ExtractNetcdfContents,\n",
    ")\n",
    "\n",
    "from marisco.callbacks import (\n",
    "    Callback,\n",
    "    Transformer,\n",
    "    DecodeTimeCB,\n",
    "    AddSampleTypeIdColumnCB\n",
    ")  \n",
    "    \n",
    "from marisco.decoders import (\n",
    "    NetCDFDecoder\n",
    "    )\n",
    "from marisco.metadata import (\n",
    "    ZoteroItem\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "fname_in =  Path('../../_data/output/100-HELCOM-MORS-2024.nc')\n",
    "fname_out = fname_in.with_suffix('.csv')\n",
    "output_format = 'openrefine_csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from standardized MARIS NetCDF files using ExtractNetcdfContents. The NetCDF files follow CF conventions and include standardized variable names and metadata according to MARIS specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the dictionary of dataframes extracted from the NetCDF file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BIOTA', 'SEAWATER', 'SEDIMENT'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents.dfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example of the DataFrame extracted from the NetCDF file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LON</th>\n",
       "      <th>LAT</th>\n",
       "      <th>SMP_DEPTH</th>\n",
       "      <th>TOT_DEPTH</th>\n",
       "      <th>TIME</th>\n",
       "      <th>SMP_ID</th>\n",
       "      <th>NUCLIDE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>UNC</th>\n",
       "      <th>DL</th>\n",
       "      <th>FILT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.333300</td>\n",
       "      <td>60.083302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1337731200</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.696</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.333300</td>\n",
       "      <td>60.083302</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1337731200</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.980</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.150000</td>\n",
       "      <td>59.433300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1339891200</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5.100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.983299</td>\n",
       "      <td>60.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1337817600</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>4.930</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.983299</td>\n",
       "      <td>60.250000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1337817600</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>22.200001</td>\n",
       "      <td>1</td>\n",
       "      <td>3.996</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LON        LAT  SMP_DEPTH  TOT_DEPTH        TIME  SMP_ID  NUCLIDE  \\\n",
       "0  29.333300  60.083302        0.0        NaN  1337731200       0       33   \n",
       "1  29.333300  60.083302       29.0        NaN  1337731200       1       33   \n",
       "2  23.150000  59.433300        0.0        NaN  1339891200       2       33   \n",
       "3  27.983299  60.250000        0.0        NaN  1337817600       3       33   \n",
       "4  27.983299  60.250000       39.0        NaN  1337817600       4       33   \n",
       "\n",
       "       VALUE  UNIT    UNC  DL  FILT  \n",
       "0   5.300000     1  1.696   1     0  \n",
       "1  19.900000     1  3.980   1     0  \n",
       "2  25.500000     1  5.100   1     0  \n",
       "3  17.000000     1  4.930   1     0  \n",
       "4  22.200001     1  3.996   1     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(contents.dfs['SEAWATER'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example of the dictionary of enums extracted from the NetCDF file as a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables in SEAWATER group: dict_keys(['nuclide', 'unit', 'dl', 'filt'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOT APPLICABLE</th>\n",
       "      <th>NOT AVAILABLE</th>\n",
       "      <th>h3</th>\n",
       "      <th>be7</th>\n",
       "      <th>c14</th>\n",
       "      <th>k40</th>\n",
       "      <th>cr51</th>\n",
       "      <th>mn54</th>\n",
       "      <th>co57</th>\n",
       "      <th>co58</th>\n",
       "      <th>co60</th>\n",
       "      <th>zn65</th>\n",
       "      <th>sr89</th>\n",
       "      <th>sr90</th>\n",
       "      <th>zr95</th>\n",
       "      <th>nb95</th>\n",
       "      <th>tc99</th>\n",
       "      <th>ru103</th>\n",
       "      <th>ru106</th>\n",
       "      <th>rh106</th>\n",
       "      <th>ag106m</th>\n",
       "      <th>ag108</th>\n",
       "      <th>ag108m</th>\n",
       "      <th>ag110m</th>\n",
       "      <th>sb124</th>\n",
       "      <th>sb125</th>\n",
       "      <th>te129m</th>\n",
       "      <th>i129</th>\n",
       "      <th>i131</th>\n",
       "      <th>cs127</th>\n",
       "      <th>cs134</th>\n",
       "      <th>cs137</th>\n",
       "      <th>ba140</th>\n",
       "      <th>la140</th>\n",
       "      <th>ce141</th>\n",
       "      <th>ce144</th>\n",
       "      <th>pm147</th>\n",
       "      <th>eu154</th>\n",
       "      <th>eu155</th>\n",
       "      <th>pb210</th>\n",
       "      <th>pb212</th>\n",
       "      <th>pb214</th>\n",
       "      <th>bi207</th>\n",
       "      <th>bi211</th>\n",
       "      <th>bi214</th>\n",
       "      <th>po210</th>\n",
       "      <th>rn220</th>\n",
       "      <th>rn222</th>\n",
       "      <th>ra223</th>\n",
       "      <th>ra224</th>\n",
       "      <th>ra225</th>\n",
       "      <th>ra226</th>\n",
       "      <th>ra228</th>\n",
       "      <th>ac228</th>\n",
       "      <th>th227</th>\n",
       "      <th>th228</th>\n",
       "      <th>th232</th>\n",
       "      <th>th234</th>\n",
       "      <th>pa234</th>\n",
       "      <th>u234</th>\n",
       "      <th>u235</th>\n",
       "      <th>u238</th>\n",
       "      <th>np237</th>\n",
       "      <th>np239</th>\n",
       "      <th>pu238</th>\n",
       "      <th>pu239</th>\n",
       "      <th>pu240</th>\n",
       "      <th>pu241</th>\n",
       "      <th>am240</th>\n",
       "      <th>am241</th>\n",
       "      <th>cm242</th>\n",
       "      <th>cm243</th>\n",
       "      <th>cm244</th>\n",
       "      <th>cs134_137_tot</th>\n",
       "      <th>pu239_240_tot</th>\n",
       "      <th>pu239_240_iii_iv_tot</th>\n",
       "      <th>pu239_240_v_vi_tot</th>\n",
       "      <th>cm243_244_tot</th>\n",
       "      <th>pu238_pu239_240_tot_ratio</th>\n",
       "      <th>am241_pu239_240_tot_ratio</th>\n",
       "      <th>cs137_134_ratio</th>\n",
       "      <th>cd109</th>\n",
       "      <th>eu152</th>\n",
       "      <th>fe59</th>\n",
       "      <th>gd153</th>\n",
       "      <th>ir192</th>\n",
       "      <th>pu238_240_tot</th>\n",
       "      <th>rb86</th>\n",
       "      <th>sc46</th>\n",
       "      <th>sn113</th>\n",
       "      <th>sn117m</th>\n",
       "      <th>tl208</th>\n",
       "      <th>mo99</th>\n",
       "      <th>tc99m</th>\n",
       "      <th>ru105</th>\n",
       "      <th>te129</th>\n",
       "      <th>te132</th>\n",
       "      <th>i132</th>\n",
       "      <th>i135</th>\n",
       "      <th>cs136</th>\n",
       "      <th>tbeta</th>\n",
       "      <th>talpha</th>\n",
       "      <th>i133</th>\n",
       "      <th>th230</th>\n",
       "      <th>pa231</th>\n",
       "      <th>u236</th>\n",
       "      <th>ag111</th>\n",
       "      <th>in116m</th>\n",
       "      <th>te123m</th>\n",
       "      <th>sb127</th>\n",
       "      <th>ba133</th>\n",
       "      <th>ce139</th>\n",
       "      <th>tl201</th>\n",
       "      <th>hg203</th>\n",
       "      <th>na22</th>\n",
       "      <th>pa234m</th>\n",
       "      <th>am243</th>\n",
       "      <th>se75</th>\n",
       "      <th>sr85</th>\n",
       "      <th>y88</th>\n",
       "      <th>ce140</th>\n",
       "      <th>bi212</th>\n",
       "      <th>u236_238_ratio</th>\n",
       "      <th>i125</th>\n",
       "      <th>ba137m</th>\n",
       "      <th>u232</th>\n",
       "      <th>pa233</th>\n",
       "      <th>ru106_rh106_tot</th>\n",
       "      <th>tu</th>\n",
       "      <th>tbeta40k</th>\n",
       "      <th>fe55</th>\n",
       "      <th>ce144_pr144_tot</th>\n",
       "      <th>pu240_pu239_ratio</th>\n",
       "      <th>u233</th>\n",
       "      <th>pu239_242_tot</th>\n",
       "      <th>ac227</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>44</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>48</td>\n",
       "      <td>49</td>\n",
       "      <td>50</td>\n",
       "      <td>51</td>\n",
       "      <td>52</td>\n",
       "      <td>53</td>\n",
       "      <td>54</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>81</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>87</td>\n",
       "      <td>88</td>\n",
       "      <td>89</td>\n",
       "      <td>90</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>93</td>\n",
       "      <td>94</td>\n",
       "      <td>95</td>\n",
       "      <td>96</td>\n",
       "      <td>97</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>105</td>\n",
       "      <td>106</td>\n",
       "      <td>107</td>\n",
       "      <td>108</td>\n",
       "      <td>109</td>\n",
       "      <td>110</td>\n",
       "      <td>111</td>\n",
       "      <td>112</td>\n",
       "      <td>113</td>\n",
       "      <td>114</td>\n",
       "      <td>116</td>\n",
       "      <td>117</td>\n",
       "      <td>122</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "      <td>126</td>\n",
       "      <td>127</td>\n",
       "      <td>128</td>\n",
       "      <td>129</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>133</td>\n",
       "      <td>134</td>\n",
       "      <td>135</td>\n",
       "      <td>136</td>\n",
       "      <td>137</td>\n",
       "      <td>138</td>\n",
       "      <td>139</td>\n",
       "      <td>140</td>\n",
       "      <td>141</td>\n",
       "      <td>142</td>\n",
       "      <td>143</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  NOT APPLICABLE NOT AVAILABLE h3 be7 c14 k40 cr51 mn54 co57 co58 co60 zn65  \\\n",
       "0             -1             0  1   2   3   4    5    6    7    8    9   10   \n",
       "\n",
       "  sr89 sr90 zr95 nb95 tc99 ru103 ru106 rh106 ag106m ag108 ag108m ag110m sb124  \\\n",
       "0   11   12   13   14   15    16    17    18     19    20     21     22    23   \n",
       "\n",
       "  sb125 te129m i129 i131 cs127 cs134 cs137 ba140 la140 ce141 ce144 pm147  \\\n",
       "0    24     25   28   29    30    31    33    34    35    36    37    38   \n",
       "\n",
       "  eu154 eu155 pb210 pb212 pb214 bi207 bi211 bi214 po210 rn220 rn222 ra223  \\\n",
       "0    39    40    41    42    43    44    45    46    47    48    49    50   \n",
       "\n",
       "  ra224 ra225 ra226 ra228 ac228 th227 th228 th232 th234 pa234 u234 u235 u238  \\\n",
       "0    51    52    53    54    55    56    57    59    60    61   62   63   64   \n",
       "\n",
       "  np237 np239 pu238 pu239 pu240 pu241 am240 am241 cm242 cm243 cm244  \\\n",
       "0    65    66    67    68    69    70    71    72    73    74    75   \n",
       "\n",
       "  cs134_137_tot pu239_240_tot pu239_240_iii_iv_tot pu239_240_v_vi_tot  \\\n",
       "0            76            77                   78                 79   \n",
       "\n",
       "  cm243_244_tot pu238_pu239_240_tot_ratio am241_pu239_240_tot_ratio  \\\n",
       "0            80                        81                        82   \n",
       "\n",
       "  cs137_134_ratio cd109 eu152 fe59 gd153 ir192 pu238_240_tot rb86 sc46 sn113  \\\n",
       "0              83    84    85   86    87    88            89   90   91    92   \n",
       "\n",
       "  sn117m tl208 mo99 tc99m ru105 te129 te132 i132 i135 cs136 tbeta talpha i133  \\\n",
       "0     93    94   95    96    97    98    99  100  101   102   103    104  105   \n",
       "\n",
       "  th230 pa231 u236 ag111 in116m te123m sb127 ba133 ce139 tl201 hg203 na22  \\\n",
       "0   106   107  108   109    110    111   112   113   114   116   117  122   \n",
       "\n",
       "  pa234m am243 se75 sr85  y88 ce140 bi212 u236_238_ratio i125 ba137m u232  \\\n",
       "0    123   124  126  127  128   129   130            131  132    133  134   \n",
       "\n",
       "  pa233 ru106_rh106_tot   tu tbeta40k fe55 ce144_pr144_tot pu240_pu239_ratio  \\\n",
       "0   135             136  137      138  139             140               141   \n",
       "\n",
       "  u233 pu239_242_tot ac227  \n",
       "0  142           143   144  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "grp='SEAWATER'\n",
    "print(f'Variables in {grp} group: {contents.enum_dicts[grp].keys()}')\n",
    "var='nuclide'\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(pd.DataFrame.from_dict(contents.enum_dicts[grp][var], orient='index').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the global attributes extracted from the NetCDF file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few attributes from global attributes: [('id', '26VMZZ2Q'), ('title', 'Environmental database - Helsinki Commission Monitoring of Radioactive Substances'), ('summary', 'MORS Environment database has been used to collate data resulting from monitoring of environmental radioactivity in the Baltic Sea based on HELCOM Recommendation 26/3.\\n\\nThe database is structured according to HELCOM Guidelines on Monitoring of Radioactive Substances (https://www.helcom.fi/wp-content/uploads/2019/08/Guidelines-for-Monitoring-of-Radioactive-Substances.pdf), which specifies reporting format, database structure, data types and obligatory parameters used for reporting data under Recommendation 26/3.\\n\\nThe database is updated and quality assured annually by HELCOM MORS EG.'), ('keywords', 'oceanography, Earth Science > Oceans > Ocean Chemistry> Radionuclides, Earth Science > Human Dimensions > Environmental Impacts > Nuclear Radiation Exposure, Earth Science > Oceans > Ocean Chemistry > Ocean Tracers, Earth Science > Oceans > Marine Sediments, Earth Science > Oceans > Ocean Chemistry, Earth Science > Oceans > Sea Ice > Isotopes, Earth Science > Oceans > Water Quality > Ocean Contaminants, Earth Science > Biological Classification > Animals/Vertebrates > Fish, Earth Science > Biosphere > Ecosystems > Marine Ecosystems, Earth Science > Biological Classification > Animals/Invertebrates > Mollusks, Earth Science > Biological Classification > Animals/Invertebrates > Arthropods > Crustaceans, Earth Science > Biological Classification > Plants > Macroalgae (Seaweeds)'), ('history', 'TBD')]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "print(\"First few attributes from global attributes:\", list(contents.global_attrs.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the custom maps extracted from the NetCDF file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom maps in SEAWATER group: dict_keys([])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "grp='SEAWATER'\n",
    "print(f'Custom maps in {grp} group: {contents.custom_maps[grp].keys()}')\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(pd.DataFrame.from_dict(contents.custom_maps[grp], orient='index'))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate NetCDF Enumerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that enumerated values in the NetCDF file match current MARIS lookup tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "## FEEDBACK TO DATA PROVIDERS\n",
    "\n",
    "The enumeration validation process is a diagnostic step that identifies inconsistencies between NetCDF enumerations and MARIS lookup tables. While this validation does not modify the dataset, it generates detailed feedback about any mismatches or undefined values. \n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ValidateEnumsCB(Callback):\n",
    "    \"Validate enumeration mappings between NetCDF file and MARIS lookup tables.\"\n",
    "\n",
    "    def __init__(self, contents, maris_enums, verbose=False):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm):\n",
    "        for group_name, enum_dict in self.contents.enum_dicts.items():\n",
    "            self._validate_group(group_name, enum_dict)\n",
    "\n",
    "    def _validate_group(self, group_name, enum_dict):\n",
    "        \n",
    "        for var_name, nc_enum_dict in enum_dict.items():\n",
    "            if self.verbose:\n",
    "                print(f\"Validating variable {var_name} from NetCDF group {group_name}.\")\n",
    "            var_name = self._get_original_var_name(var_name)\n",
    "            if self.verbose:\n",
    "                print(f\"Standardized variable name to MARISCO naming convention: {var_name}\")\n",
    "\n",
    "            if var_name not in self.maris_enums.types:\n",
    "                if self.verbose:\n",
    "                    print(f\"Variable {var_name} not found in MARISCO enums.\")\n",
    "                continue\n",
    "\n",
    "            self._compare_mappings(nc_enum_dict, self.maris_enums.types[var_name], group_name, var_name)\n",
    "\n",
    "    def _get_original_var_name(self, var_name):\n",
    "        return next((var for var, nc_var in NC_VARS.items() if nc_var == var_name), var_name)\n",
    "\n",
    "    def _compare_mappings(self, nc_dict, maris_enum, group_name, var_name):        \n",
    "        for key, value in nc_dict.items():\n",
    "            value=int(value)\n",
    "            if key not in maris_enum or maris_enum[key] != value:\n",
    "                print(f\"\\nWarning: Enum mismatch: {var_name} in {group_name}.\")\n",
    "                print(f\"   NetCDF value: {key} -> {value}\")\n",
    "                print(f\"   MARISCO standard enum lookup value: {key} -> {maris_enum.get(key, 'Not found')}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)\n",
    "tfm = Transformer(\n",
    "    data= contents.dfs,\n",
    "    custom_maps=contents.custom_maps,\n",
    "    cbs=[\n",
    "        ValidateEnumsCB(\n",
    "            contents = contents,\n",
    "            maris_enums=Enums(lut_src_dir=lut_path())\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Non Compatible Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``RemoveNonCompatibleVariablesCB`` callback filters out variables from the NetCDF format that are not listed in the VARS configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RemoveNonCompatibleVariablesCB(Callback):\n",
    "    \"Remove variables not listed in VARS configuration.\"\n",
    "    def __init__(self, \n",
    "                vars: Dict[str, str] = CSV_VARS,  # Dictionary mapping OR vars to NC vars\n",
    "                verbose: bool = False,\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Remove non-OR variables from all dataframes.\"\"\"\n",
    "        for group_name in tfm.dfs:\n",
    "            tfm.dfs[group_name] = self._remove_non_vars(tfm.dfs[group_name], group_name)\n",
    "            \n",
    "    def _remove_non_vars(self, df: pd.DataFrame, group_name:str ) -> pd.DataFrame:\n",
    "        \"\"\"Remove variables not in vars and print removed columns if verbose.\"\"\"\n",
    "        current_cols = set(df.columns)\n",
    "        vars_cols = set(self.vars.keys())\n",
    "        cols_to_remove = current_cols - vars_cols\n",
    "        \n",
    "        if self.verbose and cols_to_remove:\n",
    "            print(f\"Removing variables that are not compatible with vars provided. \\nRemoving {', '.join(cols_to_remove)} from {group_name} dataset.\")\n",
    "                        \n",
    "        return df.drop(columns=cols_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing variables that are not compatible with vars provided. \n",
      "Removing BIO_GROUP from BIOTA dataset.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)\n",
    "tfm = Transformer(\n",
    "    data=contents.dfs,\n",
    "    custom_maps=contents.custom_maps,\n",
    "    cbs=[\n",
    "        RemoveNonCompatibleVariablesCB(vars=CSV_VARS, verbose=True),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Taxon Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "TAXON_MAP = {\n",
    "    'Taxonname': 'TAXONNAME',\n",
    "    'Taxonrank': 'TAXONRANK',\n",
    "    'TaxonDB': 'TAXONDB',\n",
    "    'TaxonDBID': 'TAXONDBID',\n",
    "    'TaxonDBURL': 'TAXONDBURL'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_taxon_info_lut(maris_lut: str, key_names: dict = TAXON_MAP) -> dict:\n",
    "    \"Create lookup dictionary for taxon information from MARIS species lookup table.\"\n",
    "    species = pd.read_excel(maris_lut)\n",
    "    # Select columns and rename them to standardized format\n",
    "    columns = ['species_id'] + list(key_names.keys())\n",
    "    df = species[columns].rename(columns=key_names)\n",
    "    return df.set_index('species_id').to_dict()\n",
    "\n",
    "lut_taxon = lambda: get_taxon_info_lut(maris_lut=species_lut_path(), key_names=TAXON_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AddTaxonInformationCB(Callback):\n",
    "    \"\"\"Add taxon information to BIOTA group based on species lookup table.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                fn_lut: Callable = lut_taxon,  # Function that returns taxon lookup dictionary\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Delegate tasks to add taxon information to the BIOTA group.\"\"\"\n",
    "        if not self.check_biota_group_exists(tfm):\n",
    "            return\n",
    "        \n",
    "        df = tfm.dfs['BIOTA']\n",
    "        if not self.check_species_column_exists(df):\n",
    "            return\n",
    "        \n",
    "        self.add_taxon_columns(df)\n",
    "\n",
    "    def check_biota_group_exists(self, tfm: Transformer) -> bool:\n",
    "        \"\"\"Check if 'BIOTA' group exists in the dataframes.\"\"\"\n",
    "        if 'BIOTA' not in tfm.dfs:\n",
    "            if self.verbose:\n",
    "                print(\"No BIOTA group found, skipping taxon information\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def check_species_column_exists(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Check if 'SPECIES' column exists in the BIOTA dataframe.\"\"\"\n",
    "        if 'SPECIES' not in df.columns:\n",
    "            if self.verbose:\n",
    "                print(\"No SPECIES column found in BIOTA dataframe, skipping taxon information\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def add_taxon_columns(self, df: pd.DataFrame):\n",
    "        \"\"\"Add taxon information columns to the BIOTA dataframe.\"\"\"\n",
    "        lut = self.fn_lut()\n",
    "        \n",
    "        # Add each column from the lookup table\n",
    "        for col in lut.keys():\n",
    "            df[col] = df['SPECIES'].map(lut[col]).fillna('Unknown')\n",
    "        \n",
    "        self.report_unmatched_species(df)\n",
    "\n",
    "    def report_unmatched_species(self, df: pd.DataFrame):\n",
    "        \"\"\"Report any species IDs not found in the lookup table.\"\"\"\n",
    "        unmatched = df[df['TAXONNAME'] == 'Unknown']['SPECIES'].unique()\n",
    "        if self.verbose and len(unmatched) > 0:\n",
    "            print(f\"Warning: Species IDs not found in lookup table: {', '.join(map(str, unmatched))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               TAXONNAME TAXONRANK   TAXONDB TAXONDBID  \\\n",
      "0           Gadus morhua   species  Wikidata   Q199788   \n",
      "1           Gadus morhua   species  Wikidata   Q199788   \n",
      "2           Gadus morhua   species  Wikidata   Q199788   \n",
      "3           Gadus morhua   species  Wikidata   Q199788   \n",
      "4           Gadus morhua   species  Wikidata   Q199788   \n",
      "...                  ...       ...       ...       ...   \n",
      "16089  Fucus vesiculosus   species  Wikidata   Q754755   \n",
      "16090  Fucus vesiculosus   species  Wikidata   Q754755   \n",
      "16091     Mytilus edulis   species  Wikidata    Q27855   \n",
      "16092     Mytilus edulis   species  Wikidata    Q27855   \n",
      "16093     Mytilus edulis   species  Wikidata    Q27855   \n",
      "\n",
      "                                  TAXONDBURL  \n",
      "0      https://www.wikidata.org/wiki/Q199788  \n",
      "1      https://www.wikidata.org/wiki/Q199788  \n",
      "2      https://www.wikidata.org/wiki/Q199788  \n",
      "3      https://www.wikidata.org/wiki/Q199788  \n",
      "4      https://www.wikidata.org/wiki/Q199788  \n",
      "...                                      ...  \n",
      "16089  https://www.wikidata.org/wiki/Q754755  \n",
      "16090  https://www.wikidata.org/wiki/Q754755  \n",
      "16091   https://www.wikidata.org/wiki/Q27855  \n",
      "16092   https://www.wikidata.org/wiki/Q27855  \n",
      "16093   https://www.wikidata.org/wiki/Q27855  \n",
      "\n",
      "[16094 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)\n",
    "tfm = Transformer(\n",
    "    data=contents.dfs,\n",
    "    custom_maps=contents.custom_maps,\n",
    "    cbs=[\n",
    "        AddTaxonInformationCB(\n",
    "            fn_lut=lut_taxon\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "print(tfm.dfs['BIOTA'][['TAXONNAME','TAXONRANK','TAXONDB','TAXONDBID','TAXONDBURL']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2012-09-23\n",
      "1       2012-09-23\n",
      "2       2012-09-23\n",
      "3       2012-09-23\n",
      "4       2012-09-23\n",
      "           ...    \n",
      "16089   2022-05-10\n",
      "16090   2022-05-10\n",
      "16091   2022-09-15\n",
      "16092   2022-09-15\n",
      "16093   2022-09-15\n",
      "Name: TIME, Length: 16094, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)\n",
    "tfm = Transformer(\n",
    "    data=contents.dfs,\n",
    "    custom_maps=contents.custom_maps,\n",
    "    cbs=[\n",
    "        DecodeTimeCB(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "\n",
    "print(tfm.dfs['BIOTA']['TIME'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sample Type ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)\n",
    "tfm = Transformer(\n",
    "    data=contents.dfs,\n",
    "    custom_maps=contents.custom_maps,\n",
    "    cbs=[\n",
    "        AddSampleTypeIdColumnCB(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "print(tfm.dfs['SEAWATER']['SAMPLE_TYPE'].unique())\n",
    "print(tfm.dfs['BIOTA']['SAMPLE_TYPE'].unique())\n",
    "print(tfm.dfs['SEDIMENT']['SAMPLE_TYPE'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Reference ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the `ref_id` (i.e., Zotero Archive Location). The `ZoteroArchiveLocationCB` performs a lookup of the Zotero Archive Location based on the `Zotero key` defined in the global attributes of the MARIS NetCDF file as `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26VMZZ2Q'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents.global_attrs['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AddZoteroArchiveLocationCB(Callback):\n",
    "    \"Fetch and append 'Loc. in Archive' from Zotero to DataFrame.\"\n",
    "    def __init__(self, attrs: str, cfg: dict):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm):\n",
    "        \n",
    "        zotero_key = self.attrs['id']\n",
    "        item = ZoteroItem(zotero_key, self.cfg['zotero'])\n",
    "        if item.exist():\n",
    "            loc_in_archive = item.item['data']['archiveLocation'] \n",
    "            for grp, df in tfm.dfs.items():\n",
    "                df['REF_ID'] = int(loc_in_archive)\n",
    "        else:\n",
    "            print(f\"Warning: Zotero item {self.item_id} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)\n",
    "tfm = Transformer(\n",
    "    data=contents.dfs,\n",
    "    custom_maps=contents.custom_maps,\n",
    "    cbs=[\n",
    "        AddZoteroArchiveLocationCB(contents.global_attrs, cfg=cfg()),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print(tfm.dfs['SEAWATER']['REF_ID'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remap encoded custom maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetCDF variables can store custom mappings as attributes, which provide a way to map between encoded values and their human-readable representations. The `RemapCustomMapsCB` callback handles this conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RemapCustomMapsCB(Callback):\n",
    "    \"Remap encoded custom maps to decoded values.\"\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm):\n",
    "        \"\"\"Remap encoded custom maps to decoded values.\"\"\"\n",
    "        \n",
    "        for grp in tfm.dfs:\n",
    "            for var in tfm.dfs[grp].columns:\n",
    "                if var in tfm.custom_maps[grp]:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Remapping {var} from {grp} group\")\n",
    "                    \n",
    "                    # Convert column to int type to ensure proper mapping\n",
    "                    tfm.dfs[grp][var] = tfm.dfs[grp][var].astype(int)\n",
    "                    \n",
    "                    # Create reverse mapping dictionary\n",
    "                    reverse_custom_map = {int(v): k for k, v in tfm.custom_maps[grp][var].items()}\n",
    "                    \n",
    "                    # Apply mapping\n",
    "                    tfm.dfs[grp][var] = tfm.dfs[grp][var].map(reverse_custom_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of remapped custom maps:\n",
      "BIOTA\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    1\n",
      "Name: SMP_ID, dtype: uint64\n",
      "SEAWATER\n",
      "0    0\n",
      "1    1\n",
      "2    2\n",
      "3    3\n",
      "4    4\n",
      "Name: SMP_ID, dtype: uint64\n",
      "SEDIMENT\n",
      "0    0\n",
      "1    1\n",
      "2    2\n",
      "3    3\n",
      "4    4\n",
      "Name: SMP_ID, dtype: uint64\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)\n",
    "tfm = Transformer(\n",
    "    data=contents.dfs,\n",
    "    custom_maps=contents.custom_maps,\n",
    "    cbs=[\n",
    "        RemapCustomMapsCB(verbose=True),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print('Example of remapped custom maps:')\n",
    "for grp in tfm.dfs:\n",
    "    print(grp)\n",
    "    print(tfm.dfs[grp]['SMP_ID'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remap to Open Refine specific mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "## FEEDBACK FOR NEXT VERSION \n",
    "\n",
    "[To be further clarified]\n",
    "\n",
    "The current approach of remapping to OR-specific mappings should be reconsidered. Considering that we already utilize MARISCO lookup tables in NetCDF for creating enums, it would be beneficial to extend their use to OpenRefine data formats as well. By doing so, we could eliminate the need for OpenRefine-specific mappings, streamlining the data transformation process. Lets review the lookup tables used to create the enums for NetCDF:\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL enums: {'Not applicable': -1, 'Not available': 0, 'Detected value': 1, 'Detection limit': 2, 'Not detected': 3, 'Derived': 4}\n",
      "FILT enums: {'Not applicable': -1, 'Not available': 0, 'Yes': 1, 'No': 2}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "enums = Enums(lut_src_dir=lut_path())\n",
    "print(f'DL enums: {enums.types[\"DL\"]}')\n",
    "print(f'FILT enums: {enums.types[\"FILT\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the detection limit lookup table (LUT), as shown below, the values required for the OpenRefine CSV format are listed under the 'name' column, whereas the enums utilize the 'name_sanitized' column. Additionally, for the filtered LUT, also shown below, the values do not align consistently with the OpenRefine CSV format, which uses (`Y`, `N`, `NA`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>name_sanitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>=</td>\n",
       "      <td>Detected value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>Detection limit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>ND</td>\n",
       "      <td>Not detected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>DE</td>\n",
       "      <td>Derived</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            name   name_sanitized\n",
       "0  -1  Not applicable   Not applicable\n",
       "1   0   Not Available    Not available\n",
       "2   1               =   Detected value\n",
       "3   2               <  Detection limit\n",
       "4   3              ND     Not detected\n",
       "5   4              DE          Derived"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dl_lut = pd.read_excel(detection_limit_lut_path())\n",
    "dl_lut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Not applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            name\n",
       "0  -1  Not applicable\n",
       "1   0   Not available\n",
       "2   1             Yes\n",
       "3   2              No"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "filtered_lut = pd.read_excel(filtered_lut_path())\n",
    "filtered_lut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create OpenRefine specific mappings for the detection limit and filtered data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "or_mappings={'DL':\n",
    "                {0:'ND',1:'=',2:'<'},\n",
    "            'FILT':\n",
    "                {0:'NA',1:'Y',2:'N'},\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RemapToORSpecificMappingsCB remaps the values of the detection limit and filtered data to the OpenRefine CSV format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RemapToORSpecificMappingsCB(Callback):\n",
    "    \"Convert values using OR mappings if columns exist in dataframe.\"\n",
    "    def __init__(self, \n",
    "                or_mappings: Dict[str, Dict] = or_mappings,  # Dictionary of column mappings, \n",
    "                output_format: str = 'openrefine_csv',\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Apply OR mappings to all dataframes.\"\"\"\n",
    "        for group_name in tfm.dfs:\n",
    "            if self.verbose:\n",
    "                print(f\"\\nProcessing {group_name} group...\")\n",
    "            tfm.dfs[group_name] = self._apply_mappings(tfm.dfs[group_name])\n",
    "            \n",
    "    def _apply_mappings(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply OR mappings to columns that exist in the dataframe.\"\"\"\n",
    "        for col, mapping in self.or_mappings.items():\n",
    "            if col in df.columns:\n",
    "                if self.verbose:\n",
    "                    print(f\"    Mapping values for column: {col}\")\n",
    "                df[col] = df[col].map(mapping)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in BIOTA for columns ['DL']:\n",
      "DL: ['<' '=' 'ND']\n",
      "\n",
      "Unique values in SEAWATER for columns ['DL', 'FILT']:\n",
      "DL: ['=' '<' 'ND']\n",
      "FILT: ['NA' 'N' 'Y']\n",
      "\n",
      "Unique values in SEDIMENT for columns ['DL']:\n",
      "DL: ['=' '<' 'ND']\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)\n",
    "tfm = Transformer(\n",
    "    data= contents.dfs,\n",
    "    custom_maps=contents.custom_maps,\n",
    "    cbs=[\n",
    "        RemapToORSpecificMappingsCB(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "\n",
    "# Loop through each group in the 'dfs' dictionary\n",
    "for group_name, df in tfm.dfs.items():\n",
    "    # Check if the group dataframe contains any of the columns specified in or_mappings.keys()\n",
    "    relevant_columns = [col for col in or_mappings.keys() if col in df.columns]\n",
    "    if relevant_columns:\n",
    "        # Print the unique values from the relevant columns\n",
    "        print(f\"\\nUnique values in {group_name} for columns {relevant_columns}:\")\n",
    "        for col in relevant_columns:\n",
    "            print(f\"{col}: {df[col].unique()}\")\n",
    "    else:\n",
    "        print(f\"No relevant columns found in {group_name} based on or_mappings keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remap to CSV data type format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CSV_DTYPES` (defined in configs.ipynb) defines a state for each variable that contains a lookup table (i.e. enums). The state is either 'decoded' or 'encoded'. Lets review the variable states as a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AREA</th>\n",
       "      <th>NUCLIDE</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>DL</th>\n",
       "      <th>FILT</th>\n",
       "      <th>COUNT_MET</th>\n",
       "      <th>SAMP_MET</th>\n",
       "      <th>PREP_MET</th>\n",
       "      <th>SPECIES</th>\n",
       "      <th>BODY_PART</th>\n",
       "      <th>SED_TYPE</th>\n",
       "      <th>LAB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>decoded</td>\n",
       "      <td>encoded</td>\n",
       "      <td>encoded</td>\n",
       "      <td>decoded</td>\n",
       "      <td>decoded</td>\n",
       "      <td>encoded</td>\n",
       "      <td>encoded</td>\n",
       "      <td>encoded</td>\n",
       "      <td>encoded</td>\n",
       "      <td>encoded</td>\n",
       "      <td>encoded</td>\n",
       "      <td>encoded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AREA  NUCLIDE     UNIT       DL     FILT COUNT_MET SAMP_MET  \\\n",
       "state  decoded  encoded  encoded  decoded  decoded   encoded  encoded   \n",
       "\n",
       "      PREP_MET  SPECIES BODY_PART SED_TYPE      LAB  \n",
       "state  encoded  encoded   encoded  encoded  encoded  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "with pd.option_context('display.max_columns', None, 'display.max_colwidth', None):\n",
    "    display(pd.DataFrame.from_dict(CSV_DTYPES, orient='index').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['AREA', 'BIO_GROUP', 'BODY_PART', 'COUNT_MET', 'DL', 'FILT', 'NUCLIDE', 'PREP_MET', 'SAMP_MET', 'SED_TYPE', 'SPECIES', 'UNIT', 'LAB'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "enums = Enums(lut_src_dir=lut_path())\n",
    "enums.types.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_excluded_enums(output_format: str = 'openrefine_csv') -> dict:\n",
    "    \"Get excluded enums based on output format.\"\n",
    "    return or_mappings if output_format == 'openrefine_csv' else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataFormatConversionCB(Callback):\n",
    "    \"\"\"\n",
    "    A callback to convert DataFrame enum values between encoded and decoded formats based on specified settings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 dtypes: Dict,  # Dictionary defining data types and states for each lookup table\n",
    "                 excluded_mappings: Callable = get_excluded_enums,  # Dictionary of columns to exclude from conversion\n",
    "                 output_format: str = 'openrefine_csv',\n",
    "                 verbose: bool = False  # Flag for verbose output\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm):\n",
    "        \"\"\"\n",
    "        Apply the data format conversion to each DataFrame within the Transformer.\n",
    "        \"\"\"\n",
    "        self.load_enums()\n",
    "        \n",
    "        for group_name, df in tfm.dfs.items():\n",
    "            tfm.dfs[group_name] = self.process_dataframe(group_name, df)\n",
    "\n",
    "    def load_enums(self):\n",
    "        \"\"\"\n",
    "        Load enums from the lookup path.\n",
    "        \"\"\"\n",
    "        self.enums = Enums(lut_path())\n",
    "        if self.verbose:\n",
    "            print(f\"Loaded enums: {self.enums.types.keys()}\")\n",
    "\n",
    "    def process_dataframe(self, group_name: str, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Process each DataFrame to convert columns to the target state.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if column in self.dtypes and column not in self.excluded_mappings(self.output_format):\n",
    "                if self.dtypes[column]['state'] == 'decoded':\n",
    "                    if self.verbose:\n",
    "                        print(f\"Decoding column: {column}\")\n",
    "                    if column in self.enums.types:\n",
    "                        # Apply the mapping from encoded to decoded values\n",
    "                        df[column] = df[column].map(self.enums.types[column])\n",
    "                        if self.verbose:\n",
    "                            print(f\"Decoded column: {column}\")\n",
    "                    else:\n",
    "                        if self.verbose:\n",
    "                            print(f\"No enum mapping found for column: {column}, skipping decoding.\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing variables that are not compatible with vars provided. \n",
      "Removing BIO_GROUP from BIOTA dataset.\n",
      "Loaded enums: dict_keys(['AREA', 'BIO_GROUP', 'BODY_PART', 'COUNT_MET', 'DL', 'FILT', 'NUCLIDE', 'PREP_MET', 'SAMP_MET', 'SED_TYPE', 'SPECIES', 'UNIT', 'LAB'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BIOTA':              LON        LAT  SMP_DEPTH        TIME  SMP_ID  NUCLIDE  \\\n",
       " 0      12.316667  54.283333        NaN  1348358400       0       31   \n",
       " 1      12.316667  54.283333        NaN  1348358400       0        4   \n",
       " 2      12.316667  54.283333        NaN  1348358400       0        9   \n",
       " 3      12.316667  54.283333        NaN  1348358400       0       33   \n",
       " 4      12.316667  54.283333        NaN  1348358400       1       31   \n",
       " ...          ...        ...        ...         ...     ...      ...   \n",
       " 16089  21.395000  61.241501        2.0  1652140800    4789       33   \n",
       " 16090  21.395000  61.241501        2.0  1652140800    4789        9   \n",
       " 16091  21.385000  61.343334        NaN  1663200000    4790        4   \n",
       " 16092  21.385000  61.343334        NaN  1663200000    4790       33   \n",
       " 16093  21.385000  61.343334        NaN  1663200000    4790       12   \n",
       " \n",
       "             VALUE  UNIT       UNC  DL  SPECIES  BODY_PART       DRYWT  WETWT  \\\n",
       " 0        0.010140     5       NaN   2       99         52  174.934433  948.0   \n",
       " 1      135.300003     5  4.830210   1       99         52  174.934433  948.0   \n",
       " 2        0.013980     5       NaN   2       99         52  174.934433  948.0   \n",
       " 3        4.338000     5  0.150962   1       99         52  174.934433  948.0   \n",
       " 4        0.009614     5       NaN   2       99         52  177.935120  964.0   \n",
       " ...           ...   ...       ...  ..      ...        ...         ...    ...   \n",
       " 16089   13.700000     4  0.520600   1       96         55         NaN    NaN   \n",
       " 16090    0.500000     4  0.045500   1       96         55         NaN    NaN   \n",
       " 16091   50.700001     4  4.106700   1      129          1         NaN    NaN   \n",
       " 16092    0.880000     4  0.140800   1      129          1         NaN    NaN   \n",
       " 16093    6.600000     4  0.349800   1      129          1         NaN    NaN   \n",
       " \n",
       "        PERCENTWT  \n",
       " 0        0.18453  \n",
       " 1        0.18453  \n",
       " 2        0.18453  \n",
       " 3        0.18453  \n",
       " 4        0.18458  \n",
       " ...          ...  \n",
       " 16089        NaN  \n",
       " 16090        NaN  \n",
       " 16091        NaN  \n",
       " 16092        NaN  \n",
       " 16093        NaN  \n",
       " \n",
       " [16094 rows x 15 columns],\n",
       " 'SEAWATER':              LON        LAT  SMP_DEPTH  TOT_DEPTH        TIME  SMP_ID  \\\n",
       " 0      29.333300  60.083302        0.0        NaN  1337731200       0   \n",
       " 1      29.333300  60.083302       29.0        NaN  1337731200       1   \n",
       " 2      23.150000  59.433300        0.0        NaN  1339891200       2   \n",
       " 3      27.983299  60.250000        0.0        NaN  1337817600       3   \n",
       " 4      27.983299  60.250000       39.0        NaN  1337817600       4   \n",
       " ...          ...        ...        ...        ...         ...     ...   \n",
       " 21468  13.499833  54.600334        0.0       47.0  1686441600    9724   \n",
       " 21469  13.499833  54.600334       45.0       47.0  1686441600    9725   \n",
       " 21470  14.200833  54.600334        0.0       11.0  1686614400    9731   \n",
       " 21471  14.665500  54.600334        0.0       20.0  1686614400    9732   \n",
       " 21472  14.330000  54.600334        0.0       17.0  1686614400    9734   \n",
       " \n",
       "        NUCLIDE       VALUE  UNIT        UNC  DL  FILT  \n",
       " 0           33    5.300000     1   1.696000   1     0  \n",
       " 1           33   19.900000     1   3.980000   1     0  \n",
       " 2           33   25.500000     1   5.100000   1     0  \n",
       " 3           33   17.000000     1   4.930000   1     0  \n",
       " 4           33   22.200001     1   3.996000   1     0  \n",
       " ...        ...         ...   ...        ...  ..   ...  \n",
       " 21468        1  702.838074     1  51.276207   1     0  \n",
       " 21469        1  725.855713     1  52.686260   1     0  \n",
       " 21470        1  648.992920     1  48.154419   1     0  \n",
       " 21471        1  627.178406     1  46.245316   1     0  \n",
       " 21472        1  605.715088     1  45.691143   1     0  \n",
       " \n",
       " [21473 rows x 12 columns],\n",
       " 'SEDIMENT':              LON        LAT  TOT_DEPTH        TIME  SMP_ID  NUCLIDE  \\\n",
       " 0      27.799999  60.466667       25.0  1337904000       0       33   \n",
       " 1      27.799999  60.466667       25.0  1337904000       1       33   \n",
       " 2      27.799999  60.466667       25.0  1337904000       2       33   \n",
       " 3      27.799999  60.466667       25.0  1337904000       3       33   \n",
       " 4      27.799999  60.466667       25.0  1337904000       4       33   \n",
       " ...          ...        ...        ...         ...     ...      ...   \n",
       " 70444  15.537800  54.617832       62.0  1654646400   14121       67   \n",
       " 70445  15.537800  54.617832       62.0  1654646400   14121       77   \n",
       " 70446  15.537800  54.617832       62.0  1654646400   14122        4   \n",
       " 70447  15.537800  54.617832       62.0  1654646400   14122       33   \n",
       " 70448  15.537800  54.617832       62.0  1654646400   14122       77   \n",
       " \n",
       "              VALUE  UNIT         UNC  DL  SED_TYPE   TOP  BOTTOM  PERCENTWT  \n",
       " 0      1200.000000     3  240.000000   1         0  15.0    20.0        NaN  \n",
       " 1       250.000000     3   50.000000   1         0  20.0    25.0        NaN  \n",
       " 2       140.000000     3   29.400000   1         0  25.0    30.0        NaN  \n",
       " 3        79.000000     3   15.800000   1         0  30.0    35.0        NaN  \n",
       " 4        29.000000     3    6.960000   1         0  35.0    40.0        NaN  \n",
       " ...            ...   ...         ...  ..       ...   ...     ...        ...  \n",
       " 70444     0.044000     2    0.015312   1        10  15.0    17.0   0.257642  \n",
       " 70445     2.500000     2    0.185000   1        10  15.0    17.0   0.257642  \n",
       " 70446  5873.000000     2  164.444000   1        10  17.0    19.0   0.263965  \n",
       " 70447    21.200001     2    2.162400   1        10  17.0    19.0   0.263965  \n",
       " 70448     0.370000     2    0.048100   1        10  17.0    19.0   0.263965  \n",
       " \n",
       " [70449 rows x 14 columns]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)\n",
    "tfm = Transformer(\n",
    "    contents.dfs,\n",
    "    cbs=[\n",
    "        RemoveNonCompatibleVariablesCB(vars=CSV_VARS, verbose=True),\n",
    "        DataFormatConversionCB(\n",
    "            dtypes=CSV_DTYPES,\n",
    "            excluded_mappings = get_excluded_enums,\n",
    "            output_format='openrefine_csv',\n",
    "            verbose=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "tfm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Head of the transformed `SEAWATER` DataFrame:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LON</th>\n",
       "      <th>LAT</th>\n",
       "      <th>SMP_DEPTH</th>\n",
       "      <th>TOT_DEPTH</th>\n",
       "      <th>TIME</th>\n",
       "      <th>SMP_ID</th>\n",
       "      <th>NUCLIDE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>UNC</th>\n",
       "      <th>DL</th>\n",
       "      <th>FILT</th>\n",
       "      <th>SAMPLE_TYPE</th>\n",
       "      <th>REF_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.333300</td>\n",
       "      <td>60.083302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-05-23</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.696</td>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.333300</td>\n",
       "      <td>60.083302</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-05-23</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>19.900000</td>\n",
       "      <td>1</td>\n",
       "      <td>3.980</td>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.150000</td>\n",
       "      <td>59.433300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-06-17</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5.100</td>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.983299</td>\n",
       "      <td>60.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-05-24</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>4.930</td>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27.983299</td>\n",
       "      <td>60.250000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-05-24</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>22.200001</td>\n",
       "      <td>1</td>\n",
       "      <td>3.996</td>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         LON        LAT  SMP_DEPTH  TOT_DEPTH       TIME  SMP_ID  NUCLIDE  \\\n",
       "0  29.333300  60.083302        0.0        NaN 2012-05-23       0       33   \n",
       "1  29.333300  60.083302       29.0        NaN 2012-05-23       1       33   \n",
       "2  23.150000  59.433300        0.0        NaN 2012-06-17       2       33   \n",
       "3  27.983299  60.250000        0.0        NaN 2012-05-24       3       33   \n",
       "4  27.983299  60.250000       39.0        NaN 2012-05-24       4       33   \n",
       "\n",
       "       VALUE  UNIT    UNC DL FILT  SAMPLE_TYPE  REF_ID  \n",
       "0   5.300000     1  1.696  =   NA            1     100  \n",
       "1  19.900000     1  3.980  =   NA            1     100  \n",
       "2  25.500000     1  5.100  =   NA            1     100  \n",
       "3  17.000000     1  4.930  =   NA            1     100  \n",
       "4  22.200001     1  3.996  =   NA            1     100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Head of the transformed `BIOTA` DataFrame:</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LON</th>\n",
       "      <th>LAT</th>\n",
       "      <th>SMP_DEPTH</th>\n",
       "      <th>TIME</th>\n",
       "      <th>SMP_ID</th>\n",
       "      <th>NUCLIDE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>UNC</th>\n",
       "      <th>DL</th>\n",
       "      <th>...</th>\n",
       "      <th>DRYWT</th>\n",
       "      <th>WETWT</th>\n",
       "      <th>PERCENTWT</th>\n",
       "      <th>TAXONNAME</th>\n",
       "      <th>TAXONRANK</th>\n",
       "      <th>TAXONDB</th>\n",
       "      <th>TAXONDBID</th>\n",
       "      <th>TAXONDBURL</th>\n",
       "      <th>SAMPLE_TYPE</th>\n",
       "      <th>REF_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.316667</td>\n",
       "      <td>54.283333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-09-23</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>...</td>\n",
       "      <td>174.934433</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.18453</td>\n",
       "      <td>Gadus morhua</td>\n",
       "      <td>species</td>\n",
       "      <td>Wikidata</td>\n",
       "      <td>Q199788</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q199788</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.316667</td>\n",
       "      <td>54.283333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-09-23</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.300003</td>\n",
       "      <td>5</td>\n",
       "      <td>4.830210</td>\n",
       "      <td>=</td>\n",
       "      <td>...</td>\n",
       "      <td>174.934433</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.18453</td>\n",
       "      <td>Gadus morhua</td>\n",
       "      <td>species</td>\n",
       "      <td>Wikidata</td>\n",
       "      <td>Q199788</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q199788</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.316667</td>\n",
       "      <td>54.283333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-09-23</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.013980</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>...</td>\n",
       "      <td>174.934433</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.18453</td>\n",
       "      <td>Gadus morhua</td>\n",
       "      <td>species</td>\n",
       "      <td>Wikidata</td>\n",
       "      <td>Q199788</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q199788</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.316667</td>\n",
       "      <td>54.283333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-09-23</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>4.338000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.150962</td>\n",
       "      <td>=</td>\n",
       "      <td>...</td>\n",
       "      <td>174.934433</td>\n",
       "      <td>948.0</td>\n",
       "      <td>0.18453</td>\n",
       "      <td>Gadus morhua</td>\n",
       "      <td>species</td>\n",
       "      <td>Wikidata</td>\n",
       "      <td>Q199788</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q199788</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.316667</td>\n",
       "      <td>54.283333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-09-23</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>...</td>\n",
       "      <td>177.935120</td>\n",
       "      <td>964.0</td>\n",
       "      <td>0.18458</td>\n",
       "      <td>Gadus morhua</td>\n",
       "      <td>species</td>\n",
       "      <td>Wikidata</td>\n",
       "      <td>Q199788</td>\n",
       "      <td>https://www.wikidata.org/wiki/Q199788</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         LON        LAT  SMP_DEPTH       TIME  SMP_ID  NUCLIDE       VALUE  \\\n",
       "0  12.316667  54.283333        NaN 2012-09-23       0       31    0.010140   \n",
       "1  12.316667  54.283333        NaN 2012-09-23       0        4  135.300003   \n",
       "2  12.316667  54.283333        NaN 2012-09-23       0        9    0.013980   \n",
       "3  12.316667  54.283333        NaN 2012-09-23       0       33    4.338000   \n",
       "4  12.316667  54.283333        NaN 2012-09-23       1       31    0.009614   \n",
       "\n",
       "   UNIT       UNC DL  ...       DRYWT  WETWT  PERCENTWT     TAXONNAME  \\\n",
       "0     5       NaN  <  ...  174.934433  948.0    0.18453  Gadus morhua   \n",
       "1     5  4.830210  =  ...  174.934433  948.0    0.18453  Gadus morhua   \n",
       "2     5       NaN  <  ...  174.934433  948.0    0.18453  Gadus morhua   \n",
       "3     5  0.150962  =  ...  174.934433  948.0    0.18453  Gadus morhua   \n",
       "4     5       NaN  <  ...  177.935120  964.0    0.18458  Gadus morhua   \n",
       "\n",
       "   TAXONRANK   TAXONDB TAXONDBID                             TAXONDBURL  \\\n",
       "0    species  Wikidata   Q199788  https://www.wikidata.org/wiki/Q199788   \n",
       "1    species  Wikidata   Q199788  https://www.wikidata.org/wiki/Q199788   \n",
       "2    species  Wikidata   Q199788  https://www.wikidata.org/wiki/Q199788   \n",
       "3    species  Wikidata   Q199788  https://www.wikidata.org/wiki/Q199788   \n",
       "4    species  Wikidata   Q199788  https://www.wikidata.org/wiki/Q199788   \n",
       "\n",
       "  SAMPLE_TYPE REF_ID  \n",
       "0           2    100  \n",
       "1           2    100  \n",
       "2           2    100  \n",
       "3           2    100  \n",
       "4           2    100  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| eval: false\n",
    "contents = ExtractNetcdfContents(fname_in)\n",
    "output_format = 'openrefine_csv'\n",
    "tfm = Transformer(\n",
    "    data=contents.dfs,\n",
    "    custom_maps=contents.custom_maps,\n",
    "    cbs=[\n",
    "        ValidateEnumsCB(\n",
    "            contents = contents,\n",
    "            maris_enums=Enums(lut_src_dir=lut_path())\n",
    "        ),\n",
    "        RemoveNonCompatibleVariablesCB(vars=CSV_VARS) ,\n",
    "        RemapCustomMapsCB(),\n",
    "        RemapToORSpecificMappingsCB(output_format=output_format),\n",
    "        AddTaxonInformationCB(\n",
    "            fn_lut=lut_taxon\n",
    "        ),\n",
    "        DecodeTimeCB(),\n",
    "        AddSampleTypeIdColumnCB(),\n",
    "        AddZoteroArchiveLocationCB(contents.global_attrs, cfg=cfg()),\n",
    "        DataFormatConversionCB(\n",
    "            dtypes=CSV_DTYPES,\n",
    "            excluded_mappings = get_excluded_enums,\n",
    "            output_format=output_format,\n",
    "        ) \n",
    "        ]\n",
    ")\n",
    "tfm()\n",
    "for grp in ['SEAWATER', 'BIOTA']:\n",
    "    display(Markdown(f\"<b>Head of the transformed `{grp}` DataFrame:</b>\"))\n",
    "    with pd.option_context('display.max_rows', None):\n",
    "        display(tfm.dfs[grp].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decode(\n",
    "    fname_in: str, # Input file name\n",
    "    dest_out: str | None = None, # Output file name (optional)\n",
    "    output_format: str = 'openrefine_csv',\n",
    "    remap_vars: Dict[str, str] = CSV_VARS,\n",
    "    remap_dtypes: Dict[str, str] = CSV_DTYPES,\n",
    "    verbose: bool = False,\n",
    "    **kwargs # Additional arguments\n",
    "    ) -> None:\n",
    "    \"Decode data from NetCDF.\"\n",
    "    valid_output_formats=['openrefine_csv', 'decoded_csv']\n",
    "    \n",
    "    if output_format not in valid_output_formats:\n",
    "        print (f'Invalid output format. Allowed formats: {valid_output_formats}')\n",
    "        return \n",
    "    \n",
    "    if output_format == 'decoded_csv':\n",
    "        remap_dtypes = {k: {'state': 'decoded'} for k in remap_dtypes.keys()}\n",
    "        \n",
    "    contents = ExtractNetcdfContents(fname_in)\n",
    "    tfm = Transformer(\n",
    "        data=contents.dfs,\n",
    "        custom_maps=contents.custom_maps,\n",
    "        cbs=[\n",
    "        ValidateEnumsCB(\n",
    "            contents = contents,\n",
    "            maris_enums=Enums(lut_src_dir=lut_path())\n",
    "        ),\n",
    "        RemoveNonCompatibleVariablesCB(vars=remap_vars),\n",
    "        RemapCustomMapsCB(),\n",
    "        RemapToORSpecificMappingsCB(output_format=output_format),\n",
    "        AddTaxonInformationCB(\n",
    "            fn_lut=lut_taxon\n",
    "        ),\n",
    "        DecodeTimeCB(),\n",
    "        AddSampleTypeIdColumnCB(),\n",
    "        AddZoteroArchiveLocationCB(contents.global_attrs, cfg=cfg()),\n",
    "        DataFormatConversionCB(\n",
    "            dtypes=remap_dtypes,\n",
    "            excluded_mappings = get_excluded_enums,\n",
    "            output_format=output_format\n",
    "        ) \n",
    "        ]\n",
    "    )    \n",
    "    \n",
    "    tfm()\n",
    "    decoder = NetCDFDecoder( \n",
    "                            dfs=tfm.dfs,\n",
    "                            fname_in=fname_in,  \n",
    "                            dest_out=dest_out,                           \n",
    "                            output_format='csv',\n",
    "                            remap_vars=CSV_VARS,\n",
    "                            verbose=verbose\n",
    "                    )\n",
    "    decoder.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "fname = Path('../../_data/output/100-HELCOM-MORS-2024.nc')\n",
    "decode(fname_in=fname, dest_out=fname.with_suffix(''), output_format='openrefine_csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

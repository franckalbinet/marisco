{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp handlers.data_format_transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data format transformation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A data pipeline handler that transforms MARIS data between different formats. The primary focus is on converting NetCDF data into human-readable formats (such as CSV and Excel) and MARIS Standard Open-Refine format while preserving data integrity and maintaining standardized variable names and units. This handler implements a modular transformation pipeline using callbacks for each processing step, ensuring flexibility and extensibility in data handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "For new MARIS users, please refer to [field definitions\n",
    "](https://github.com/franckalbinet/marisco/blob/main/nbs/metadata/field-definition.ipynb) for detailed information about Maris fields.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "> Required packages and internal modules for data format transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2827/2494999833.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#| export\n",
    "from pathlib import Path\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "from fastcore.basics import patch, store_attr\n",
    "import fastcore.all as fc\n",
    "from typing import Dict, Callable\n",
    "\n",
    "from marisco.configs import (\n",
    "    NC_VARS,\n",
    "    OR_VARS,\n",
    "    NC_GROUPS,\n",
    "    OR_DTYPES,\n",
    "    Enums,\n",
    "    lut_path,\n",
    "    species_lut_path,\n",
    "    cfg\n",
    ")\n",
    "\n",
    "from marisco.utils import (\n",
    "    get_netcdf_properties\n",
    ")\n",
    "\n",
    "from marisco.callbacks import (\n",
    "    Callback,\n",
    "    Transformer,\n",
    "    DecodeTimeCB,\n",
    "    AddSampleTypeIdColumnCB\n",
    ")  \n",
    "    \n",
    "from marisco.decoders import (\n",
    "        NetCDFDecoder\n",
    "    )\n",
    "from marisco.metadata import (\n",
    "    ZoteroItem\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "fname_in =  Path('../../_data/output/100-HELCOM-MORS-2024.nc')\n",
    "fname_out = fname_in.with_suffix('.csv')\n",
    "ref_id = 100 # HELCOM MORS reference id as defined by MARIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and validate data from standardized MARIS NetCDF files. The NetCDF files follow CF conventions and include standardized variable names, units, and metadata according to MARIS specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def load_to_dataframes(fname:str, verbose: bool = False):\n",
    "    \"\"\"Load NetCDF groups into DataFrames with standardized column names.\"\"\"\n",
    "    dfs = {}\n",
    "    with Dataset(fname, 'r') as nc:\n",
    "        for group_name in nc.groups:\n",
    "            group = nc.groups[group_name]\n",
    "            # Get all variables in the group\n",
    "            data = {}\n",
    "            for var_name, var in group.variables.items():\n",
    "                if var_name not in group.dimensions:  # Skip dimension variables\n",
    "                    data[var_name] = var[:]\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            # Rename columns using NC_VARS mapping\n",
    "            rename_map = {nc_var: col for col, nc_var in NC_VARS.items() \n",
    "                         if nc_var in df.columns}\n",
    "            df = df.rename(columns=rename_map)\n",
    "            dfs[group_name.upper()] = df\n",
    "            if verbose:\n",
    "                print(f\"Loaded group {group_name} with columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded group biota with columns: ['LON', 'LAT', 'SMP_DEPTH', 'TIME', 'NUCLIDE', 'VALUE', 'UNIT', 'UNC', 'DL', 'BIO_GROUP', 'SPECIES', 'BODY_PART', 'DRYWT', 'WETWT', 'PERCENTWT']\n",
      "Loaded group seawater with columns: ['LON', 'LAT', 'SMP_DEPTH', 'TOT_DEPTH', 'TIME', 'NUCLIDE', 'VALUE', 'UNIT', 'UNC', 'DL', 'FILT']\n",
      "Loaded group sediment with columns: ['LON', 'LAT', 'TOT_DEPTH', 'TIME', 'NUCLIDE', 'VALUE', 'UNIT', 'UNC', 'DL', 'SED_TYPE', 'TOP', 'BOTTOM', 'PERCENTWT']\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "dfs = load_to_dataframes(fname_in, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Non Open Refine Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemoveNonORVarsCB(Callback):\n",
    "    \"Remove variables not defined in OR_VARS configuration.\"\n",
    "    def __init__(self, \n",
    "                or_vars: Dict[str, str] = OR_VARS,  # Dictionary mapping OR vars to NC vars\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Remove non-OR variables from all dataframes.\"\"\"\n",
    "        for group_name in tfm.dfs:\n",
    "            if self.verbose:\n",
    "                print(f\"\\nProcessing {group_name} group...\")\n",
    "            tfm.dfs[group_name] = self._remove_non_or_vars(tfm.dfs[group_name])\n",
    "            \n",
    "    def _remove_non_or_vars(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Remove columns not in OR_VARS and print removed columns if verbose.\"\"\"\n",
    "        current_cols = set(df.columns)\n",
    "        or_cols = set(self.or_vars.keys())\n",
    "        cols_to_remove = current_cols - or_cols\n",
    "        \n",
    "        if self.verbose and cols_to_remove:\n",
    "            print(f\"    Removing columns: {', '.join(cols_to_remove)}\")\n",
    "            \n",
    "        return df.drop(columns=cols_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BIOTA group...\n",
      "    Removing columns: BIO_GROUP\n",
      "\n",
      "Processing SEAWATER group...\n",
      "\n",
      "Processing SEDIMENT group...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        RemoveNonORVarsCB(verbose=True),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate NetCDF Enumerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that enumerated values in the NetCDF file match MARIS lookup tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: The enumeration validation process is a diagnostic step that identifies inconsistencies between NetCDF enumerations and MARIS lookup tables. While this validation does not modify the dataset, it generates detailed feedback about any mismatches or undefined values. \n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ValidateEnumsCB(Callback):\n",
    "    \"Validate enumeration mappings between NetCDF file and MARIS lookup tables.\"\n",
    "    def __init__(self, \n",
    "                src_fname: str,  # Path to NetCDF file\n",
    "                enums: Enums,    # MARIS lookup table enums\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Process each group in the NetCDF file and validate its enums.\"\"\"\n",
    "        with Dataset(self.src_fname, 'r') as nc:\n",
    "            for group_name in nc.groups:\n",
    "                group = nc.groups[group_name]\n",
    "                self._validate_group(group, group_name)\n",
    "    \n",
    "    def _validate_group(self, group, group_name: str):\n",
    "        \"\"\"Validate enum mappings for a specific group.\"\"\"\n",
    "        for var_name, var in group.variables.items():\n",
    "            if not hasattr(var.datatype, 'enum_dict'): \n",
    "                continue\n",
    "            \n",
    "            nc_enum_dict = var.datatype.enum_dict\n",
    "            if self.verbose:\n",
    "                print(f\"nc_enum_dict [{var_name}]:\", nc_enum_dict)\n",
    "\n",
    "            # Get original column name from NC_VARS mapping\n",
    "            original_col = next((col for col, nc_var in NC_VARS.items() \n",
    "                               if nc_var == var_name), None)\n",
    "            if not original_col: \n",
    "                continue\n",
    "\n",
    "            # Compare enum mappings\n",
    "            self._compare_mappings(\n",
    "                nc_enum_dict,\n",
    "                self.enums.types[original_col],\n",
    "                group_name,\n",
    "                var_name,\n",
    "                original_col\n",
    "            )\n",
    "    \n",
    "    def _compare_mappings(self, nc_dict: dict, lut_dict: dict, \n",
    "                         group_name: str, var_name: str, col_name: str):\n",
    "        \"\"\"Compare NetCDF enum dictionary with lookup table dictionary.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"lut_enum [{col_name}]:\", lut_dict)\n",
    "            \n",
    "        # Check for mismatches between NetCDF and lookup table\n",
    "        for key, value in nc_dict.items():\n",
    "            if key not in lut_dict or lut_dict[key] != value:\n",
    "                print(f\"\\nWarning: Enum mismatch in {group_name}/{var_name}\")\n",
    "                print(f\"NetCDF value: {key} -> {value}\")\n",
    "                print(f\"Lookup value: {key} -> {lut_dict.get(key, 'Not found')}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        ValidateEnumsCB(\n",
    "            src_fname=fname_in,\n",
    "            enums=Enums(lut_src_dir=lut_path()),\n",
    "            #verbose=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate NetCDF Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that variable names in the NetCDF file match those used in MARIS ternminogy, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ValidateNetCDFVarsCB(Callback):\n",
    "    \" Validate that all variables in the NetCDF file are included in NC_VARS mapping. Identifies and reports any unmapped variables.\"\n",
    "    def __init__(self, \n",
    "                src_fname: str,  # Path to NetCDF file\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Check each group's variables against NC_VARS mapping.\"\"\"\n",
    "        unmapped_vars = {}\n",
    "        \n",
    "        with Dataset(self.src_fname, 'r') as nc:\n",
    "            for group_name in nc.groups:\n",
    "                group = nc.groups[group_name]\n",
    "                group_vars = set(group.variables.keys())\n",
    "                mapped_vars = {v for k, v in NC_VARS.items()}\n",
    "                unmapped = group_vars - mapped_vars - {'id'}  # Exclude dimension variables\n",
    "                \n",
    "                if unmapped:\n",
    "                    unmapped_vars[group_name] = unmapped\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\nWarning: Unmapped variables in group {group_name}:\")\n",
    "                        print(f\"Variables: {unmapped}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        ValidateNetCDFVarsCB(\n",
    "            src_fname=fname_in,\n",
    "            verbose=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Taxon information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "TAXON_KEY_MAP = {\n",
    "    'Taxonname': 'TAXONNAME',\n",
    "    'Taxonrank': 'TAXONRANK',\n",
    "    'TaxonDB': 'TAXONDB',\n",
    "    'TaxonDBID': 'TAXONDBID',\n",
    "    'TaxonDBURL': 'TAXONDBURL'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_taxon_info_lut(maris_lut: str, key_names: dict = TAXON_KEY_MAP) -> dict:\n",
    "    \"Create lookup dictionary for taxon information from MARIS species lookup table.\"\n",
    "    species = pd.read_excel(maris_lut)\n",
    "    # Select columns and rename them to standardized format\n",
    "    columns = ['species_id'] + list(key_names.keys())\n",
    "    df = species[columns].rename(columns=key_names)\n",
    "    return df.set_index('species_id').to_dict()\n",
    "\n",
    "lut_taxon = lambda: get_taxon_info_lut(maris_lut=species_lut_path(), key_names=TAXON_KEY_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class AddTaxonInformationCB(Callback):\n",
    "    \"Add taxon information to BIOTA group based on species lookup table.\"\n",
    "    def __init__(self, \n",
    "                fn_lut: Callable = lut_taxon,  # Function that returns taxon lookup dictionary\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Add taxon information columns to BIOTA group.\"\"\"\n",
    "        if 'BIOTA' not in tfm.dfs:\n",
    "            if self.verbose:\n",
    "                print(\"No BIOTA group found, skipping taxon information\")\n",
    "            return\n",
    "            \n",
    "        df = tfm.dfs['BIOTA']\n",
    "        if 'SPECIES' not in df.columns:\n",
    "            if self.verbose:\n",
    "                print(\"No SPECIES column found in BIOTA dataframe, skipping taxon information\")\n",
    "            return\n",
    "        \n",
    "        lut = self.fn_lut()\n",
    "        \n",
    "        # Add each column from the lookup table\n",
    "        for col in lut.keys():\n",
    "            df[col] = df['SPECIES'].map(lut[col]).fillna('Unknown')\n",
    "            \n",
    "        if self.verbose:\n",
    "            unmatched = df[df['TAXONNAME'] == 'Unknown']['SPECIES'].unique()\n",
    "            if len(unmatched) > 0:\n",
    "                print(f\"Warning: Species IDs not found in lookup table: {', '.join(map(str, unmatched))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               TAXONNAME TAXONRANK   TAXONDB TAXONDBID  \\\n",
      "0           Gadus morhua   species  Wikidata   Q199788   \n",
      "1           Gadus morhua   species  Wikidata   Q199788   \n",
      "2           Gadus morhua   species  Wikidata   Q199788   \n",
      "3           Gadus morhua   species  Wikidata   Q199788   \n",
      "4           Gadus morhua   species  Wikidata   Q199788   \n",
      "...                  ...       ...       ...       ...   \n",
      "16089  Fucus vesiculosus   species  Wikidata   Q754755   \n",
      "16090  Fucus vesiculosus   species  Wikidata   Q754755   \n",
      "16091     Mytilus edulis   species  Wikidata    Q27855   \n",
      "16092     Mytilus edulis   species  Wikidata    Q27855   \n",
      "16093     Mytilus edulis   species  Wikidata    Q27855   \n",
      "\n",
      "                                  TAXONDBURL  \n",
      "0      https://www.wikidata.org/wiki/Q199788  \n",
      "1      https://www.wikidata.org/wiki/Q199788  \n",
      "2      https://www.wikidata.org/wiki/Q199788  \n",
      "3      https://www.wikidata.org/wiki/Q199788  \n",
      "4      https://www.wikidata.org/wiki/Q199788  \n",
      "...                                      ...  \n",
      "16089  https://www.wikidata.org/wiki/Q754755  \n",
      "16090  https://www.wikidata.org/wiki/Q754755  \n",
      "16091   https://www.wikidata.org/wiki/Q27855  \n",
      "16092   https://www.wikidata.org/wiki/Q27855  \n",
      "16093   https://www.wikidata.org/wiki/Q27855  \n",
      "\n",
      "[16094 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        AddTaxonInformationCB(\n",
    "            fn_lut=lut_taxon\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "print(tfm.dfs['BIOTA'][['TAXONNAME','TAXONRANK','TAXONDB','TAXONDBID','TAXONDBURL']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remap OR mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** This operation must take place before `ConvertToHumanReadableCB` as it relies on the data being in its encoded form for accurate mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "or_mappings={'DL':\n",
    "                {0:'ND',1:'=',2:'<'},\n",
    "            'FILT':\n",
    "                {0:'NA',1:'Y',2:'N'},\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapToORMappingsCB(Callback):\n",
    "    \"Convert values using OR mappings if columns exist in dataframe.\"\n",
    "    def __init__(self, \n",
    "                or_mappings: Dict[str, Dict] = or_mappings,  # Dictionary of column mappings\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def _apply_mappings(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply OR mappings to columns that exist in the dataframe.\"\"\"\n",
    "        for col, mapping in self.or_mappings.items():\n",
    "            if col in df.columns:\n",
    "                if self.verbose:\n",
    "                    print(f\"    Mapping values for column: {col}\")\n",
    "                df[col] = df[col].map(mapping)\n",
    "        return df\n",
    "    \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Apply OR mappings to all dataframes.\"\"\"\n",
    "        for group_name in tfm.dfs:\n",
    "            if self.verbose:\n",
    "                print(f\"\\nProcessing {group_name} group...\")\n",
    "            tfm.dfs[group_name] = self._apply_mappings(tfm.dfs[group_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BIOTA group...\n",
      "    Mapping values for column: DL\n",
      "\n",
      "Processing SEAWATER group...\n",
      "    Mapping values for column: DL\n",
      "    Mapping values for column: FILT\n",
      "\n",
      "Processing SEDIMENT group...\n",
      "    Mapping values for column: DL\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DL</th>\n",
       "      <th>FILT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21468</th>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21469</th>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21470</th>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21471</th>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21472</th>\n",
       "      <td>=</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21473 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      DL FILT\n",
       "0      =   NA\n",
       "1      =   NA\n",
       "2      =   NA\n",
       "3      =   NA\n",
       "4      =   NA\n",
       "...   ..  ...\n",
       "21468  =   NA\n",
       "21469  =   NA\n",
       "21470  =   NA\n",
       "21471  =   NA\n",
       "21472  =   NA\n",
       "\n",
       "[21473 rows x 2 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        RemapToORMappingsCB(verbose=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "tfm.dfs['SEAWATER'][list(or_mappings.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remap to human readable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapToHumanReadableCB(Callback):\n",
    "    \"Convert enum values in DataFrames to their human-readable format,but only for variables defined as 'human_readable' in OR_DTYPES and not present in or_mappings.\"\n",
    "    def __init__(self, \n",
    "                src_fname: str,  # Path to NetCDF file\n",
    "                or_dtypes: Dict = OR_DTYPES,  # Dictionary defining variable types\n",
    "                or_mappings: Dict = or_mappings,  # Dictionary of value mappings\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Convert numeric enum values to human-readable strings for specified variables.\"\"\"\n",
    "        with Dataset(self.src_fname, 'r') as nc:\n",
    "            for group_name, df in tfm.dfs.items():\n",
    "                nc_group_name = NC_GROUPS[group_name]\n",
    "                group = nc.groups[nc_group_name]\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f'Processing {group_name} enums ...')\n",
    "                \n",
    "                # Process each variable that has an enum\n",
    "                for var_name, var in group.variables.items():\n",
    "                    if hasattr(var.datatype, 'enum_dict'):\n",
    "                        # Get the original column name from NC_VARS mapping\n",
    "                        original_col = next((col for col, nc_var in NC_VARS.items() \n",
    "                                          if nc_var == var_name), None)\n",
    "                        \n",
    "                        # Only convert if variable is human_readable and not in or_mappings\n",
    "                        if (original_col and \n",
    "                            original_col in df.columns and \n",
    "                            original_col not in self.or_mappings and\n",
    "                            self.or_dtypes[original_col]['type'] == 'human_readable'):\n",
    "                            \n",
    "                            if self.verbose:\n",
    "                                print(f\"Converting '{original_col}' to human readable format\")\n",
    "                                print(f\"Enum values: {var.datatype.enum_dict}\")\n",
    "                            \n",
    "                            enum_dict = {v: k for k, v in var.datatype.enum_dict.items()}\n",
    "                            tfm.dfs[group_name][original_col] = df[original_col].map(enum_dict)\n",
    "                            \n",
    "                            if self.verbose:\n",
    "                                print(f\"Converted {original_col} in {group_name}\")\n",
    "                                print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BIOTA enums ...\n",
      "Processing SEAWATER enums ...\n",
      "Processing SEDIMENT enums ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        RemoveNonORVarsCB(),\n",
    "        RemapToHumanReadableCB(\n",
    "            src_fname=fname_in,\n",
    "            verbose=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2012-09-23\n",
      "1       2012-09-23\n",
      "2       2012-09-23\n",
      "3       2012-09-23\n",
      "4       2012-09-23\n",
      "           ...    \n",
      "16089   2022-05-10\n",
      "16090   2022-05-10\n",
      "16091   2022-09-15\n",
      "16092   2022-09-15\n",
      "16093   2022-09-15\n",
      "Name: TIME, Length: 16094, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        DecodeTimeCB(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "\n",
    "print(tfm.dfs['BIOTA']['TIME'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sample Type ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        AddSampleTypeIdColumnCB(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "print(tfm.dfs['SEAWATER']['samptype_id'].unique())\n",
    "print(tfm.dfs['BIOTA']['samptype_id'].unique())\n",
    "print(tfm.dfs['SEDIMENT']['samptype_id'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Reference ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the `ref_id` (i.e., Zotero Archive Location) of the Maris data. The `ZoteroArchiveLocationCB` performs a lookup of the Zotero Archive Location based on the `Zotero key` defined in the global attributes of the MARIS NetCDF file as `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "class AddZoteroArchiveLocationCB(Callback):\n",
    "    \"Fetch and append 'Loc. in Archive' from Zotero to DataFrame.\"\n",
    "    def __init__(self, src_fname: str, cfg: dict):\n",
    "        self.src_fname = src_fname\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def __call__(self, tfm):\n",
    "        \n",
    "        zotero_key = get_netcdf_properties(self.src_fname)['global_attributes']['id']\n",
    "        item = ZoteroItem(zotero_key, self.cfg['zotero'])\n",
    "        if item.exist():\n",
    "            loc_in_archive = item.item['data']['archiveLocation'] \n",
    "            for grp, df in tfm.dfs.items():\n",
    "                df['REF_ID'] = int(loc_in_archive)\n",
    "        else:\n",
    "            print(f\"Warning: Zotero item {self.item_id} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        AddZoteroArchiveLocationCB(src_fname=fname_in, cfg=cfg()),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print(tfm.dfs['SEAWATER']['REF_ID'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             LON        LAT  SMP_DEPTH       TIME  NUCLIDE       VALUE  UNIT  \\\n",
      "0      12.316667  54.283333        NaN 2012-09-23       31    0.010140     5   \n",
      "1      12.316667  54.283333        NaN 2012-09-23        4  135.300003     5   \n",
      "2      12.316667  54.283333        NaN 2012-09-23        9    0.013980     5   \n",
      "3      12.316667  54.283333        NaN 2012-09-23       33    4.338000     5   \n",
      "4      12.316667  54.283333        NaN 2012-09-23       31    0.009614     5   \n",
      "...          ...        ...        ...        ...      ...         ...   ...   \n",
      "14868  19.000000  54.583302       61.0 2018-02-26       53    0.043000     5   \n",
      "14869  15.500000  54.333302       65.0 2018-02-13        4   98.000000     5   \n",
      "14870  15.500000  54.333302       65.0 2018-02-13       33    3.690000     5   \n",
      "14871  15.500000  54.333302       65.0 2018-02-13       53    0.049000     5   \n",
      "14872  19.433300  54.363899        NaN 2018-10-03       33    0.830000     5   \n",
      "\n",
      "            UNC DL  SPECIES  BODY_PART       DRYWT  WETWT  PERCENTWT  \\\n",
      "0           NaN  <       99         52  174.934433  948.0    0.18453   \n",
      "1      4.830210  =       99         52  174.934433  948.0    0.18453   \n",
      "2           NaN  <       99         52  174.934433  948.0    0.18453   \n",
      "3      0.150962  =       99         52  174.934433  948.0    0.18453   \n",
      "4           NaN  <       99         52  177.935120  964.0    0.18458   \n",
      "...         ... ..      ...        ...         ...    ...        ...   \n",
      "14868  0.011008  =      191          3  120.000000  500.0    0.24000   \n",
      "14869  1.097600  =      191         52  112.500000  500.0    0.22500   \n",
      "14870  0.078597  =      191         52  112.500000  500.0    0.22500   \n",
      "14871  0.007007  =      191         52  112.500000  500.0    0.22500   \n",
      "14872  0.229993  =      247         52         NaN  120.0        NaN   \n",
      "\n",
      "                TAXONNAME TAXONRANK   TAXONDB TAXONDBID  \\\n",
      "0            Gadus morhua   species  Wikidata   Q199788   \n",
      "1            Gadus morhua   species  Wikidata   Q199788   \n",
      "2            Gadus morhua   species  Wikidata   Q199788   \n",
      "3            Gadus morhua   species  Wikidata   Q199788   \n",
      "4            Gadus morhua   species  Wikidata   Q199788   \n",
      "...                   ...       ...       ...       ...   \n",
      "14868  Platichthys flesus   species  Wikidata   Q214034   \n",
      "14869  Platichthys flesus   species  Wikidata   Q214034   \n",
      "14870  Platichthys flesus   species  Wikidata   Q214034   \n",
      "14871  Platichthys flesus   species  Wikidata   Q214034   \n",
      "14872   Perca fluviatilis   species  Wikidata   Q166812   \n",
      "\n",
      "                                  TAXONDBURL  samptype_id  \n",
      "0      https://www.wikidata.org/wiki/Q199788            2  \n",
      "1      https://www.wikidata.org/wiki/Q199788            2  \n",
      "2      https://www.wikidata.org/wiki/Q199788            2  \n",
      "3      https://www.wikidata.org/wiki/Q199788            2  \n",
      "4      https://www.wikidata.org/wiki/Q199788            2  \n",
      "...                                      ...          ...  \n",
      "14868  https://www.wikidata.org/wiki/Q214034            2  \n",
      "14869  https://www.wikidata.org/wiki/Q214034            2  \n",
      "14870  https://www.wikidata.org/wiki/Q214034            2  \n",
      "14871  https://www.wikidata.org/wiki/Q214034            2  \n",
      "14872  https://www.wikidata.org/wiki/Q166812            2  \n",
      "\n",
      "[14873 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        RemoveNonORVarsCB(),\n",
    "        ValidateEnumsCB(\n",
    "            src_fname=fname_in,\n",
    "            enums=Enums(lut_src_dir=lut_path())\n",
    "            ),\n",
    "        ValidateNetCDFVarsCB(\n",
    "            src_fname=fname_in\n",
    "            ),\n",
    "        AddTaxonInformationCB(\n",
    "            fn_lut=lut_taxon\n",
    "            ),  \n",
    "        RemapToORMappingsCB(or_mappings),            \n",
    "        RemapToHumanReadableCB(\n",
    "            src_fname=fname_in),\n",
    "        DecodeTimeCB(),\n",
    "        AddSampleTypeIdColumnCB(),\n",
    "        AddZoteroArchiveLocationCB(src_fname=fname_in, cfg=cfg())\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print(tfm.dfs['BIOTA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding NETCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decode(\n",
    "    fname_in: str, # Input file name\n",
    "    dest_out: str | None = None, # Output file name (optional)\n",
    "    output_format: str = 'csv',\n",
    "    remap_vars: Dict[str, str] = OR_VARS,\n",
    "    verbose: bool = False,\n",
    "    **kwargs # Additional arguments\n",
    "    ) -> None:\n",
    "    \"Decode data from NetCDF.\"\n",
    "    dfs = load_to_dataframes(fname_in)\n",
    "    print (dfs)\n",
    "    tfm = Transformer(\n",
    "        dfs,\n",
    "        cbs=[\n",
    "            RemoveNonORVarsCB(),\n",
    "            ValidateEnumsCB(\n",
    "                src_fname=fname_in,\n",
    "                enums=Enums(lut_src_dir=lut_path())\n",
    "                ),\n",
    "            ValidateNetCDFVarsCB(\n",
    "                src_fname=fname_in\n",
    "                ),\n",
    "            \n",
    "            AddTaxonInformationCB(\n",
    "                fn_lut=lut_taxon\n",
    "                ),  \n",
    "            RemapToORMappingsCB(or_mappings),            \n",
    "            RemapToHumanReadableCB(\n",
    "                src_fname=fname_in),\n",
    "            DecodeTimeCB(),\n",
    "            AddSampleTypeIdColumnCB(),\n",
    "            AddZoteroArchiveLocationCB(src_fname=fname_in, cfg=cfg())\n",
    "        ]\n",
    "    )    \n",
    "    \n",
    "    tfm()\n",
    "    decoder = NetCDFDecoder( \n",
    "                            dfs=tfm.dfs,\n",
    "                            fname_in=fname_in,  \n",
    "                            dest_out=dest_out,                           \n",
    "                            output_format='csv',\n",
    "                            remap_vars=OR_VARS,\n",
    "                            verbose=verbose\n",
    "                    )\n",
    "    decoder.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BIOTA':              LON        LAT  SMP_DEPTH        TIME  NUCLIDE       VALUE  UNIT  \\\n",
      "0      12.316667  54.283333        NaN  1348358400       31    0.010140     5   \n",
      "1      12.316667  54.283333        NaN  1348358400        4  135.300003     5   \n",
      "2      12.316667  54.283333        NaN  1348358400        9    0.013980     5   \n",
      "3      12.316667  54.283333        NaN  1348358400       33    4.338000     5   \n",
      "4      12.316667  54.283333        NaN  1348358400       31    0.009614     5   \n",
      "...          ...        ...        ...         ...      ...         ...   ...   \n",
      "16089  21.395000  61.241501        2.0  1652140800       33   13.700000     4   \n",
      "16090  21.395000  61.241501        2.0  1652140800        9    0.500000     4   \n",
      "16091  21.385000  61.343334        NaN  1663200000        4   50.700001     4   \n",
      "16092  21.385000  61.343334        NaN  1663200000       33    0.880000     4   \n",
      "16093  21.385000  61.343334        NaN  1663200000       12    6.600000     4   \n",
      "\n",
      "            UNC  DL  BIO_GROUP  SPECIES  BODY_PART       DRYWT  WETWT  \\\n",
      "0           NaN   2          4       99         52  174.934433  948.0   \n",
      "1      4.830210   1          4       99         52  174.934433  948.0   \n",
      "2           NaN   2          4       99         52  174.934433  948.0   \n",
      "3      0.150962   1          4       99         52  174.934433  948.0   \n",
      "4           NaN   2          4       99         52  177.935120  964.0   \n",
      "...         ...  ..        ...      ...        ...         ...    ...   \n",
      "16089  0.520600   1         11       96         55         NaN    NaN   \n",
      "16090  0.045500   1         11       96         55         NaN    NaN   \n",
      "16091  4.106700   1         14      129          1         NaN    NaN   \n",
      "16092  0.140800   1         14      129          1         NaN    NaN   \n",
      "16093  0.349800   1         14      129          1         NaN    NaN   \n",
      "\n",
      "       PERCENTWT  \n",
      "0        0.18453  \n",
      "1        0.18453  \n",
      "2        0.18453  \n",
      "3        0.18453  \n",
      "4        0.18458  \n",
      "...          ...  \n",
      "16089        NaN  \n",
      "16090        NaN  \n",
      "16091        NaN  \n",
      "16092        NaN  \n",
      "16093        NaN  \n",
      "\n",
      "[16094 rows x 15 columns], 'SEAWATER':              LON        LAT  SMP_DEPTH  TOT_DEPTH        TIME  NUCLIDE  \\\n",
      "0      29.333300  60.083302        0.0        NaN  1337731200       33   \n",
      "1      29.333300  60.083302       29.0        NaN  1337731200       33   \n",
      "2      23.150000  59.433300        0.0        NaN  1339891200       33   \n",
      "3      27.983299  60.250000        0.0        NaN  1337817600       33   \n",
      "4      27.983299  60.250000       39.0        NaN  1337817600       33   \n",
      "...          ...        ...        ...        ...         ...      ...   \n",
      "21468  13.499833  54.600334        0.0       47.0  1686441600        1   \n",
      "21469  13.499833  54.600334       45.0       47.0  1686441600        1   \n",
      "21470  14.200833  54.600334        0.0       11.0  1686614400        1   \n",
      "21471  14.665500  54.600334        0.0       20.0  1686614400        1   \n",
      "21472  14.330000  54.600334        0.0       17.0  1686614400        1   \n",
      "\n",
      "            VALUE  UNIT        UNC  DL  FILT  \n",
      "0        5.300000     1   1.696000   1     0  \n",
      "1       19.900000     1   3.980000   1     0  \n",
      "2       25.500000     1   5.100000   1     0  \n",
      "3       17.000000     1   4.930000   1     0  \n",
      "4       22.200001     1   3.996000   1     0  \n",
      "...           ...   ...        ...  ..   ...  \n",
      "21468  702.838074     1  51.276207   1     0  \n",
      "21469  725.855713     1  52.686260   1     0  \n",
      "21470  648.992920     1  48.154419   1     0  \n",
      "21471  627.178406     1  46.245316   1     0  \n",
      "21472  605.715088     1  45.691143   1     0  \n",
      "\n",
      "[21473 rows x 11 columns], 'SEDIMENT':              LON        LAT  TOT_DEPTH        TIME  NUCLIDE        VALUE  \\\n",
      "0      27.799999  60.466702       25.0  1337904000       33  1200.000000   \n",
      "1      27.799999  60.466702       25.0  1337904000       33   250.000000   \n",
      "2      27.799999  60.466702       25.0  1337904000       33   140.000000   \n",
      "3      27.799999  60.466702       25.0  1337904000       33    79.000000   \n",
      "4      27.799999  60.466702       25.0  1337904000       33    29.000000   \n",
      "...          ...        ...        ...         ...      ...          ...   \n",
      "70444  15.537800  54.617802       62.0  1654646400       67     0.044000   \n",
      "70445  15.537800  54.617802       62.0  1654646400       77     2.500000   \n",
      "70446  15.537800  54.617802       62.0  1654646400        4  5873.000000   \n",
      "70447  15.537800  54.617802       62.0  1654646400       33    21.200001   \n",
      "70448  15.537800  54.617802       62.0  1654646400       77     0.370000   \n",
      "\n",
      "       UNIT         UNC  DL  SED_TYPE   TOP  BOTTOM  PERCENTWT  \n",
      "0         4  240.000000   1         0  15.0    20.0        NaN  \n",
      "1         4   50.000000   1         0  20.0    25.0        NaN  \n",
      "2         4   29.400000   1         0  25.0    30.0        NaN  \n",
      "3         4   15.800000   1         0  30.0    35.0        NaN  \n",
      "4         4    6.960000   1         0  35.0    40.0        NaN  \n",
      "...     ...         ...  ..       ...   ...     ...        ...  \n",
      "70444     4    0.015312   1        10  15.0    17.0   0.257642  \n",
      "70445     4    0.185000   1        10  15.0    17.0   0.257642  \n",
      "70446     4  164.444000   1        10  17.0    19.0   0.263965  \n",
      "70447     4    2.162400   1        10  17.0    19.0   0.263965  \n",
      "70448     4    0.048100   1        10  17.0    19.0   0.263965  \n",
      "\n",
      "[70449 rows x 13 columns]}\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "fname = Path('../../_data/output/100-HELCOM-MORS-2024.nc')\n",
    "decode(fname_in=fname, dest_out=fname.with_suffix(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BIOTA':              LON        LAT  SMP_DEPTH        TIME  NUCLIDE       VALUE  UNIT  \\\n",
      "0      12.316667  54.283333        NaN  1348358400       31    0.010140     5   \n",
      "1      12.316667  54.283333        NaN  1348358400        4  135.300003     5   \n",
      "2      12.316667  54.283333        NaN  1348358400        9    0.013980     5   \n",
      "3      12.316667  54.283333        NaN  1348358400       33    4.338000     5   \n",
      "4      12.316667  54.283333        NaN  1348358400       31    0.009614     5   \n",
      "...          ...        ...        ...         ...      ...         ...   ...   \n",
      "16089  21.395000  61.241501        2.0  1652140800       33   13.700000     4   \n",
      "16090  21.395000  61.241501        2.0  1652140800        9    0.500000     4   \n",
      "16091  21.385000  61.343334        NaN  1663200000        4   50.700001     4   \n",
      "16092  21.385000  61.343334        NaN  1663200000       33    0.880000     4   \n",
      "16093  21.385000  61.343334        NaN  1663200000       12    6.600000     4   \n",
      "\n",
      "            UNC  DL  BIO_GROUP  SPECIES  BODY_PART       DRYWT  WETWT  \\\n",
      "0           NaN   2          4       99         52  174.934433  948.0   \n",
      "1      4.830210   1          4       99         52  174.934433  948.0   \n",
      "2           NaN   2          4       99         52  174.934433  948.0   \n",
      "3      0.150962   1          4       99         52  174.934433  948.0   \n",
      "4           NaN   2          4       99         52  177.935120  964.0   \n",
      "...         ...  ..        ...      ...        ...         ...    ...   \n",
      "16089  0.520600   1         11       96         55         NaN    NaN   \n",
      "16090  0.045500   1         11       96         55         NaN    NaN   \n",
      "16091  4.106700   1         14      129          1         NaN    NaN   \n",
      "16092  0.140800   1         14      129          1         NaN    NaN   \n",
      "16093  0.349800   1         14      129          1         NaN    NaN   \n",
      "\n",
      "       PERCENTWT  \n",
      "0        0.18453  \n",
      "1        0.18453  \n",
      "2        0.18453  \n",
      "3        0.18453  \n",
      "4        0.18458  \n",
      "...          ...  \n",
      "16089        NaN  \n",
      "16090        NaN  \n",
      "16091        NaN  \n",
      "16092        NaN  \n",
      "16093        NaN  \n",
      "\n",
      "[16094 rows x 15 columns], 'SEAWATER':              LON        LAT  SMP_DEPTH  TOT_DEPTH        TIME  NUCLIDE  \\\n",
      "0      29.333300  60.083302        0.0        NaN  1337731200       33   \n",
      "1      29.333300  60.083302       29.0        NaN  1337731200       33   \n",
      "2      23.150000  59.433300        0.0        NaN  1339891200       33   \n",
      "3      27.983299  60.250000        0.0        NaN  1337817600       33   \n",
      "4      27.983299  60.250000       39.0        NaN  1337817600       33   \n",
      "...          ...        ...        ...        ...         ...      ...   \n",
      "21468  13.499833  54.600334        0.0       47.0  1686441600        1   \n",
      "21469  13.499833  54.600334       45.0       47.0  1686441600        1   \n",
      "21470  14.200833  54.600334        0.0       11.0  1686614400        1   \n",
      "21471  14.665500  54.600334        0.0       20.0  1686614400        1   \n",
      "21472  14.330000  54.600334        0.0       17.0  1686614400        1   \n",
      "\n",
      "            VALUE  UNIT        UNC  DL  FILT  \n",
      "0        5.300000     1   1.696000   1     0  \n",
      "1       19.900000     1   3.980000   1     0  \n",
      "2       25.500000     1   5.100000   1     0  \n",
      "3       17.000000     1   4.930000   1     0  \n",
      "4       22.200001     1   3.996000   1     0  \n",
      "...           ...   ...        ...  ..   ...  \n",
      "21468  702.838074     1  51.276207   1     0  \n",
      "21469  725.855713     1  52.686260   1     0  \n",
      "21470  648.992920     1  48.154419   1     0  \n",
      "21471  627.178406     1  46.245316   1     0  \n",
      "21472  605.715088     1  45.691143   1     0  \n",
      "\n",
      "[21473 rows x 11 columns], 'SEDIMENT':              LON        LAT  TOT_DEPTH        TIME  NUCLIDE        VALUE  \\\n",
      "0      27.799999  60.466702       25.0  1337904000       33  1200.000000   \n",
      "1      27.799999  60.466702       25.0  1337904000       33   250.000000   \n",
      "2      27.799999  60.466702       25.0  1337904000       33   140.000000   \n",
      "3      27.799999  60.466702       25.0  1337904000       33    79.000000   \n",
      "4      27.799999  60.466702       25.0  1337904000       33    29.000000   \n",
      "...          ...        ...        ...         ...      ...          ...   \n",
      "70444  15.537800  54.617802       62.0  1654646400       67     0.044000   \n",
      "70445  15.537800  54.617802       62.0  1654646400       77     2.500000   \n",
      "70446  15.537800  54.617802       62.0  1654646400        4  5873.000000   \n",
      "70447  15.537800  54.617802       62.0  1654646400       33    21.200001   \n",
      "70448  15.537800  54.617802       62.0  1654646400       77     0.370000   \n",
      "\n",
      "       UNIT         UNC  DL  SED_TYPE   TOP  BOTTOM  PERCENTWT  \n",
      "0         4  240.000000   1         0  15.0    20.0        NaN  \n",
      "1         4   50.000000   1         0  20.0    25.0        NaN  \n",
      "2         4   29.400000   1         0  25.0    30.0        NaN  \n",
      "3         4   15.800000   1         0  30.0    35.0        NaN  \n",
      "4         4    6.960000   1         0  35.0    40.0        NaN  \n",
      "...     ...         ...  ..       ...   ...     ...        ...  \n",
      "70444     4    0.015312   1        10  15.0    17.0   0.257642  \n",
      "70445     4    0.185000   1        10  15.0    17.0   0.257642  \n",
      "70446     4  164.444000   1        10  17.0    19.0   0.263965  \n",
      "70447     4    2.162400   1        10  17.0    19.0   0.263965  \n",
      "70448     4    0.048100   1        10  17.0    19.0   0.263965  \n",
      "\n",
      "[70449 rows x 13 columns]}\n"
     ]
    }
   ],
   "source": [
    "decode(fname_in=fname, dest_out=fname.with_suffix(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

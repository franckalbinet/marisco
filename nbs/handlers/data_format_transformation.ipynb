{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp handlers.data_format_transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data format transformation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A data pipeline handler that transforms MARIS data from NetCDF to CSV. The primary focus is on converting NetCDF data into MARIS Standard Open-Refine format while preserving data integrity. This handler implements a modular transformation pipeline using callbacks for each processing step, ensuring flexibility and extensibility in data handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "For new MARIS users, please refer to [field definitions\n",
    "](https://github.com/franckalbinet/marisco/blob/main/nbs/metadata/field-definition.ipynb) for detailed information about Maris fields.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "> Required packages and internal modules for data format transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53037/2716152763.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "import fastcore.all as fc\n",
    "from typing import Dict, Callable\n",
    "\n",
    "from marisco.configs import (\n",
    "    NC_VARS,\n",
    "    OR_VARS,\n",
    "    NC_GROUPS,\n",
    "    OR_DTYPES,\n",
    "    Enums,\n",
    "    lut_path,\n",
    "    species_lut_path,\n",
    "    cfg\n",
    ")\n",
    "\n",
    "from marisco.utils import (\n",
    "    get_netcdf_properties\n",
    ")\n",
    "\n",
    "from marisco.callbacks import (\n",
    "    Callback,\n",
    "    Transformer,\n",
    "    DecodeTimeCB,\n",
    "    AddSampleTypeIdColumnCB\n",
    ")  \n",
    "    \n",
    "from marisco.decoders import (\n",
    "        NetCDFDecoder\n",
    "    )\n",
    "from marisco.metadata import (\n",
    "    ZoteroItem\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "fname_in =  Path('../../_data/output/100-HELCOM-MORS-2024.nc')\n",
    "fname_out = fname_in.with_suffix('.csv')\n",
    "output_format = 'openrefine_csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from standardized MARIS NetCDF files. The NetCDF files follow CF conventions and include standardized variable names and metadata according to MARIS specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def load_data(fname:str, verbose: bool = False):\n",
    "    \"\"\"Load NetCDF groups into DataFrames with standardized column names.\"\"\"\n",
    "    dfs = {}\n",
    "    with Dataset(fname, 'r') as nc:\n",
    "        for group_name in nc.groups:\n",
    "            group = nc.groups[group_name]\n",
    "            # Get all variables in the group\n",
    "            data = {var_name: var[:] for var_name, var in group.variables.items() if var_name not in group.dimensions}\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            # Rename columns using NC_VARS mapping\n",
    "            rename_map = {nc_var: col for col, nc_var in NC_VARS.items() \n",
    "                         if nc_var in df.columns}\n",
    "            # here update to infrom if df.columns includes names that are not included in NC_VARS. Just print \n",
    "            \n",
    "            \n",
    "            df = df.rename(columns=rename_map)\n",
    "            dfs[group_name.upper()] = df\n",
    "            if verbose:\n",
    "                print(f\"Loaded group {group_name} with columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # herer extract enum_dicts \n",
    "        \n",
    "        #here get global attibutes \n",
    "        \n",
    "        \n",
    "    \n",
    "    return dfs, enum_dicts, globattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded group biota with columns: ['LON', 'LAT', 'SMP_DEPTH', 'TIME', 'NUCLIDE', 'VALUE', 'UNIT', 'UNC', 'DL', 'BIO_GROUP', 'SPECIES', 'BODY_PART', 'DRYWT', 'WETWT', 'PERCENTWT']\n",
      "Loaded group seawater with columns: ['LON', 'LAT', 'SMP_DEPTH', 'TOT_DEPTH', 'TIME', 'NUCLIDE', 'VALUE', 'UNIT', 'UNC', 'DL', 'FILT']\n",
      "Loaded group sediment with columns: ['LON', 'LAT', 'TOT_DEPTH', 'TIME', 'NUCLIDE', 'VALUE', 'UNIT', 'UNC', 'DL', 'SED_TYPE', 'TOP', 'BOTTOM', 'PERCENTWT']\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "dfs = load_to_dataframes(fname_in, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate NetCDF Enumerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that enumerated values in the NetCDF file match current MARIS lookup tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: The enumeration validation process is a diagnostic step that identifies inconsistencies between NetCDF enumerations and MARIS lookup tables. While this validation does not modify the dataset, it generates detailed feedback about any mismatches or undefined values. \n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ValidateEnumsCB(Callback):\n",
    "    \"Validate enumeration mappings between NetCDF file and MARIS lookup tables.\"\n",
    "    def __init__(self, \n",
    "                src_fname: str,  # Path to NetCDF file\n",
    "                enums: Enums,    # MARIS lookup table enums\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Process each group in the NetCDF file and validate its enums.\"\"\"\n",
    "        with Dataset(self.src_fname, 'r') as nc:\n",
    "            for group_name in nc.groups:\n",
    "                group = nc.groups[group_name]\n",
    "                self._validate_group(group, group_name)\n",
    "    \n",
    "    def _validate_group(self, group, group_name: str):\n",
    "        \"\"\"Validate enum mappings for a specific group.\"\"\"\n",
    "        for var_name, var in group.variables.items():\n",
    "            if not hasattr(var.datatype, 'enum_dict'): \n",
    "                continue\n",
    "            \n",
    "            nc_enum_dict = var.datatype.enum_dict\n",
    "            if self.verbose:\n",
    "                print(f\"nc_enum_dict [{var_name}]:\", nc_enum_dict)\n",
    "\n",
    "            # Get original column name from NC_VARS mapping\n",
    "            original_col = next((col for col, nc_var in NC_VARS.items() \n",
    "                               if nc_var == var_name), None)\n",
    "            if not original_col: \n",
    "                continue\n",
    "\n",
    "            # Compare enum mappings\n",
    "            self._compare_mappings(\n",
    "                nc_enum_dict,\n",
    "                self.enums.types[original_col],\n",
    "                group_name,\n",
    "                var_name,\n",
    "                original_col\n",
    "            )\n",
    "    \n",
    "    def _compare_mappings(self, nc_dict: dict, lut_dict: dict, \n",
    "                         group_name: str, var_name: str, col_name: str):\n",
    "        \"\"\"Compare NetCDF enum dictionary with lookup table dictionary.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"lut_enum [{col_name}]:\", lut_dict)\n",
    "            \n",
    "        # Check for mismatches between NetCDF and lookup table\n",
    "        for key, value in nc_dict.items():\n",
    "            if key not in lut_dict or lut_dict[key] != value:\n",
    "                print(f\"\\nWarning: Enum mismatch in {group_name}/{var_name}\")\n",
    "                print(f\"NetCDF value: {key} -> {value}\")\n",
    "                print(f\"Lookup value: {key} -> {lut_dict.get(key, 'Not found')}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        ValidateEnumsCB(\n",
    "            src_fname=fname_in,\n",
    "            enums=Enums(lut_src_dir=lut_path()),\n",
    "            #verbose=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Non Open Refine Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``RemoveNonORVarsCB`` class filters out variables from the NetCDF format that do not align with the requirements for MARIS's OpenRefine data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemoveNonORVarsCB(Callback):\n",
    "    \"Remove variables not defined in OR_VARS configuration.\"\n",
    "    def __init__(self, \n",
    "                or_vars: Dict[str, str] = OR_VARS,  # Dictionary mapping OR vars to NC vars\n",
    "                verbose: bool = False,\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Remove non-OR variables from all dataframes.\"\"\"\n",
    "        for group_name in tfm.dfs:\n",
    "            tfm.dfs[group_name] = self._remove_non_or_vars(tfm.dfs[group_name], group_name)\n",
    "            \n",
    "    def _remove_non_or_vars(self, df: pd.DataFrame, group_name:str ) -> pd.DataFrame:\n",
    "        \"\"\"Remove columns not in OR_VARS and print removed columns if verbose.\"\"\"\n",
    "        current_cols = set(df.columns)\n",
    "        or_cols = set(self.or_vars.keys())\n",
    "        cols_to_remove = current_cols - or_cols\n",
    "        \n",
    "        if self.verbose and cols_to_remove:\n",
    "            print(f\"Removing variables that are not compatible with MARIS's OpenRefine processing. \\nRemoving {', '.join(cols_to_remove)} from {group_name} dataset.\")\n",
    "                        \n",
    "        return df.drop(columns=cols_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing variables that are not compatible with MARIS's OpenRefine processing. \n",
      "Removing BIO_GROUP from BIOTA dataset.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        RemoveNonORVarsCB(verbose=True),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Taxon Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "TAXON_KEY_MAP = {\n",
    "    'Taxonname': 'TAXONNAME',\n",
    "    'Taxonrank': 'TAXONRANK',\n",
    "    'TaxonDB': 'TAXONDB',\n",
    "    'TaxonDBID': 'TAXONDBID',\n",
    "    'TaxonDBURL': 'TAXONDBURL'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_taxon_info_lut(maris_lut: str, key_names: dict = TAXON_KEY_MAP) -> dict:\n",
    "    \"Create lookup dictionary for taxon information from MARIS species lookup table.\"\n",
    "    species = pd.read_excel(maris_lut)\n",
    "    # Select columns and rename them to standardized format\n",
    "    columns = ['species_id'] + list(key_names.keys())\n",
    "    df = species[columns].rename(columns=key_names)\n",
    "    return df.set_index('species_id').to_dict()\n",
    "\n",
    "lut_taxon = lambda: get_taxon_info_lut(maris_lut=species_lut_path(), key_names=TAXON_KEY_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class AddTaxonInformationCB(Callback):\n",
    "    \"\"\"Add taxon information to BIOTA group based on species lookup table.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                fn_lut: Callable = lut_taxon,  # Function that returns taxon lookup dictionary\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Delegate tasks to add taxon information to the BIOTA group.\"\"\"\n",
    "        if not self.check_biota_group_exists(tfm):\n",
    "            return\n",
    "        \n",
    "        df = tfm.dfs['BIOTA']\n",
    "        if not self.check_species_column_exists(df):\n",
    "            return\n",
    "        \n",
    "        self.add_taxon_columns(df)\n",
    "\n",
    "    def check_biota_group_exists(self, tfm: Transformer) -> bool:\n",
    "        \"\"\"Check if 'BIOTA' group exists in the dataframes.\"\"\"\n",
    "        if 'BIOTA' not in tfm.dfs:\n",
    "            if self.verbose:\n",
    "                print(\"No BIOTA group found, skipping taxon information\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def check_species_column_exists(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Check if 'SPECIES' column exists in the BIOTA dataframe.\"\"\"\n",
    "        if 'SPECIES' not in df.columns:\n",
    "            if self.verbose:\n",
    "                print(\"No SPECIES column found in BIOTA dataframe, skipping taxon information\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def add_taxon_columns(self, df: pd.DataFrame):\n",
    "        \"\"\"Add taxon information columns to the BIOTA dataframe.\"\"\"\n",
    "        lut = self.fn_lut()\n",
    "        \n",
    "        # Add each column from the lookup table\n",
    "        for col in lut.keys():\n",
    "            df[col] = df['SPECIES'].map(lut[col]).fillna('Unknown')\n",
    "        \n",
    "        self.report_unmatched_species(df)\n",
    "\n",
    "    def report_unmatched_species(self, df: pd.DataFrame):\n",
    "        \"\"\"Report any species IDs not found in the lookup table.\"\"\"\n",
    "        unmatched = df[df['TAXONNAME'] == 'Unknown']['SPECIES'].unique()\n",
    "        if self.verbose and len(unmatched) > 0:\n",
    "            print(f\"Warning: Species IDs not found in lookup table: {', '.join(map(str, unmatched))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               TAXONNAME TAXONRANK   TAXONDB TAXONDBID  \\\n",
      "0           Gadus morhua   species  Wikidata   Q199788   \n",
      "1           Gadus morhua   species  Wikidata   Q199788   \n",
      "2           Gadus morhua   species  Wikidata   Q199788   \n",
      "3           Gadus morhua   species  Wikidata   Q199788   \n",
      "4           Gadus morhua   species  Wikidata   Q199788   \n",
      "...                  ...       ...       ...       ...   \n",
      "16089  Fucus vesiculosus   species  Wikidata   Q754755   \n",
      "16090  Fucus vesiculosus   species  Wikidata   Q754755   \n",
      "16091     Mytilus edulis   species  Wikidata    Q27855   \n",
      "16092     Mytilus edulis   species  Wikidata    Q27855   \n",
      "16093     Mytilus edulis   species  Wikidata    Q27855   \n",
      "\n",
      "                                  TAXONDBURL  \n",
      "0      https://www.wikidata.org/wiki/Q199788  \n",
      "1      https://www.wikidata.org/wiki/Q199788  \n",
      "2      https://www.wikidata.org/wiki/Q199788  \n",
      "3      https://www.wikidata.org/wiki/Q199788  \n",
      "4      https://www.wikidata.org/wiki/Q199788  \n",
      "...                                      ...  \n",
      "16089  https://www.wikidata.org/wiki/Q754755  \n",
      "16090  https://www.wikidata.org/wiki/Q754755  \n",
      "16091   https://www.wikidata.org/wiki/Q27855  \n",
      "16092   https://www.wikidata.org/wiki/Q27855  \n",
      "16093   https://www.wikidata.org/wiki/Q27855  \n",
      "\n",
      "[16094 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        AddTaxonInformationCB(\n",
    "            fn_lut=lut_taxon\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "print(tfm.dfs['BIOTA'][['TAXONNAME','TAXONRANK','TAXONDB','TAXONDBID','TAXONDBURL']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remap to OR mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** This operation must take place before `ConvertToHumanReadableCB` as it relies on the data being in its encoded form for accurate mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "or_mappings={'DL':\n",
    "                {0:'ND',1:'=',2:'<'},\n",
    "            'FILT':\n",
    "                {0:'NA',1:'Y',2:'N'},\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapToORMappingsCB(Callback):\n",
    "    \"Convert values using OR mappings if columns exist in dataframe.\"\n",
    "    def __init__(self, \n",
    "                or_mappings: Dict[str, Dict] = or_mappings,  # Dictionary of column mappings, \n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Apply OR mappings to all dataframes.\"\"\"\n",
    "        for group_name in tfm.dfs:\n",
    "            if self.verbose:\n",
    "                print(f\"\\nProcessing {group_name} group...\")\n",
    "            tfm.dfs[group_name] = self._apply_mappings(tfm.dfs[group_name])\n",
    "            \n",
    "    def _apply_mappings(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply OR mappings to columns that exist in the dataframe.\"\"\"\n",
    "        for col, mapping in self.or_mappings.items():\n",
    "            if col in df.columns:\n",
    "                if self.verbose:\n",
    "                    print(f\"    Mapping values for column: {col}\")\n",
    "                df[col] = df[col].map(mapping)\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in BIOTA for columns ['DL']:\n",
      "DL: ['<' '=' 'ND']\n",
      "\n",
      "Unique values in SEAWATER for columns ['DL', 'FILT']:\n",
      "DL: ['=' '<' 'ND']\n",
      "FILT: ['NA' 'N' 'Y']\n",
      "\n",
      "Unique values in SEDIMENT for columns ['DL']:\n",
      "DL: ['=' '<' 'ND']\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        RemapToORMappingsCB(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "\n",
    "# Loop through each group in the 'dfs' dictionary\n",
    "for group_name, df in tfm.dfs.items():\n",
    "    # Check if the group dataframe contains any of the columns specified in or_mappings.keys()\n",
    "    relevant_columns = [col for col in or_mappings.keys() if col in df.columns]\n",
    "    if relevant_columns:\n",
    "        # Print the unique values from the relevant columns\n",
    "        print(f\"\\nUnique values in {group_name} for columns {relevant_columns}:\")\n",
    "        for col in relevant_columns:\n",
    "            print(f\"{col}: {df[col].unique()}\")\n",
    "    else:\n",
    "        print(f\"No relevant columns found in {group_name} based on or_mappings keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remap to human readable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LON': {'type': 'human_readable'},\n",
       " 'LAT': {'type': 'human_readable'},\n",
       " 'SMP_DEPTH': {'type': 'human_readable'},\n",
       " 'TOT_DEPTH': {'type': 'human_readable'},\n",
       " 'TIME': {'type': 'human_readable'},\n",
       " 'AREA': {'type': 'human_readable'},\n",
       " 'NUCLIDE': {'type': 'encoded'},\n",
       " 'VALUE': {'type': 'human_readable'},\n",
       " 'UNIT': {'type': 'encoded'},\n",
       " 'UNC': {'type': 'human_readable'},\n",
       " 'DL': {'type': 'human_readable'},\n",
       " 'FILT': {'type': 'human_readable'},\n",
       " 'COUNT_MET': {'type': 'encoded'},\n",
       " 'SAMP_MET': {'type': 'encoded'},\n",
       " 'PREP_MET': {'type': 'encoded'},\n",
       " 'VOL': {'type': 'human_readable'},\n",
       " 'SAL': {'type': 'human_readable'},\n",
       " 'TEMP': {'type': 'human_readable'},\n",
       " 'SPECIES': {'type': 'encoded'},\n",
       " 'BODY_PART': {'type': 'encoded'},\n",
       " 'SED_TYPE': {'type': 'encoded'},\n",
       " 'TOP': {'type': 'human_readable'},\n",
       " 'BOTTOM': {'type': 'human_readable'},\n",
       " 'DRYWT': {'type': 'human_readable'},\n",
       " 'WETWT': {'type': 'human_readable'},\n",
       " 'PERCENTWT': {'type': 'human_readable'},\n",
       " 'LAB': {'type': 'encoded'},\n",
       " 'PROFILE_ID': {'type': 'human_readable'},\n",
       " 'SAMPLE_TYPE': {'type': 'human_readable'},\n",
       " 'TAXONNAME': {'type': 'human_readable'},\n",
       " 'TAXONREPNAME': {'type': 'human_readable'},\n",
       " 'TAXONRANK': {'type': 'human_readable'},\n",
       " 'TAXONDB': {'type': 'human_readable'},\n",
       " 'TAXONDBID': {'type': 'human_readable'},\n",
       " 'TaxonDBURL': {'type': 'human_readable'},\n",
       " 'REF_ID': {'type': 'human_readable'}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OR_DTYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapToHumanReadableCB(Callback):\n",
    "    \"\"\"Convert enum values in DataFrames to their human-readable format, but only for variables defined as 'human_readable' in OR_DTYPES and not present in or_mappings.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 or_dtypes: Dict = OR_DTYPES,  # Dictionary defining variable types\n",
    "                 or_mappings: Dict = or_mappings,  # Dictionary of value mappings\n",
    "                 output_format: str = 'openrefine_csv',\n",
    "                 verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Delegate the conversion of numeric enum values to human-readable strings for specified variables.\"\"\"\n",
    "        if self.output_format != 'openrefine_csv':\n",
    "            self.or_mappings = {}\n",
    "        \n",
    "        for group_name, df in tfm.dfs.items():\n",
    "            self.process_group(group_name, df)\n",
    "\n",
    "    def process_group(self, group_name, df):\n",
    "        \"\"\"Process each group to convert enums to human-readable format.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Processing {group_name} enums ...')\n",
    "        \n",
    "        self.convert_variables(group_name, df)\n",
    "\n",
    "    def convert_variables(self, group_name, df):\n",
    "        \"\"\"Convert variables within a group to their human-readable format if applicable.\"\"\"\n",
    "        for col_name in df.columns:\n",
    "            if col_name in self.or_dtypes and self.or_dtypes[col_name]['type'] == 'human_readable':\n",
    "                self.convert_variable(col_name, group_name, df)\n",
    "\n",
    "    def convert_variable(self, col_name, group_name, df):\n",
    "        \"\"\"Convert a single variable to human-readable format based on conditions.\"\"\"\n",
    "        if col_name not in self.or_mappings and self.output_format == 'openrefine_csv':\n",
    "            if self.verbose:\n",
    "                print(f\"Converting '{col_name}' to human readable format\")\n",
    "            \n",
    "            # Assuming enum_dict is somehow accessible here, possibly through a modified approach or additional data structure\n",
    "            enum_dict = self.get_enum_dict(col_name)\n",
    "            df[col_name] = df[col_name].map(enum_dict).fillna('Unknown')\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"Converted {col_name} in {group_name}\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "            \n",
    "    def get_enum_dict(self, col_name):\n",
    "        \"\"\"Retrieve or simulate the enum dictionary for a column.\"\"\"\n",
    "        # Placeholder for actual enum dictionary retrieval logic\n",
    "        return {1: 'Type A', 2: 'Type B', 3: 'Type C'}  # Example mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing BIOTA enums ...\n",
      "Processing SEAWATER enums ...\n",
      "Processing SEDIMENT enums ...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "output_format = 'csv' #'openrefine_csv'\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        RemoveNonORVarsCB(),\n",
    "        RemapToHumanReadableCB(\n",
    "            output_format=output_format,\n",
    "            verbose=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2012-09-23\n",
      "1       2012-09-23\n",
      "2       2012-09-23\n",
      "3       2012-09-23\n",
      "4       2012-09-23\n",
      "           ...    \n",
      "16089   2022-05-10\n",
      "16090   2022-05-10\n",
      "16091   2022-09-15\n",
      "16092   2022-09-15\n",
      "16093   2022-09-15\n",
      "Name: TIME, Length: 16094, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        DecodeTimeCB(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "\n",
    "print(tfm.dfs['BIOTA']['TIME'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Sample Type ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[2]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        AddSampleTypeIdColumnCB(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "print(tfm.dfs['SEAWATER']['samptype_id'].unique())\n",
    "print(tfm.dfs['BIOTA']['samptype_id'].unique())\n",
    "print(tfm.dfs['SEDIMENT']['samptype_id'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Reference ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the `ref_id` (i.e., Zotero Archive Location) of the Maris data. The `ZoteroArchiveLocationCB` performs a lookup of the Zotero Archive Location based on the `Zotero key` defined in the global attributes of the MARIS NetCDF file as `id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| export\n",
    "class AddZoteroArchiveLocationCB(Callback):\n",
    "    \"Fetch and append 'Loc. in Archive' from Zotero to DataFrame.\"\n",
    "    def __init__(self, src_fname: str, cfg: dict):\n",
    "        self.src_fname = src_fname\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def __call__(self, tfm):\n",
    "        \n",
    "        zotero_key = get_netcdf_properties(self.src_fname)['global_attributes']['id']\n",
    "        item = ZoteroItem(zotero_key, self.cfg['zotero'])\n",
    "        if item.exist():\n",
    "            loc_in_archive = item.item['data']['archiveLocation'] \n",
    "            for grp, df in tfm.dfs.items():\n",
    "                df['REF_ID'] = int(loc_in_archive)\n",
    "        else:\n",
    "            print(f\"Warning: Zotero item {self.item_id} does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        AddZoteroArchiveLocationCB(src_fname=fname_in, cfg=cfg()),\n",
    "    ]\n",
    ")\n",
    "tfm()\n",
    "print(tfm.dfs['SEAWATER']['REF_ID'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ValidateEnumsCB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dfs \u001b[38;5;241m=\u001b[39m load_to_dataframes(fname_in)\n\u001b[1;32m      3\u001b[0m output_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#'openrefine_csv' # 'csv'\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m cbs\u001b[38;5;241m=\u001b[39m[\u001b[43mValidateEnumsCB\u001b[49m(\n\u001b[1;32m      5\u001b[0m             src_fname\u001b[38;5;241m=\u001b[39mfname_in,\n\u001b[1;32m      6\u001b[0m             enums\u001b[38;5;241m=\u001b[39mEnums(lut_src_dir\u001b[38;5;241m=\u001b[39mlut_path())\n\u001b[1;32m      7\u001b[0m             ),\n\u001b[1;32m      8\u001b[0m         ValidateNetCDFVarsCB(\n\u001b[1;32m      9\u001b[0m             src_fname\u001b[38;5;241m=\u001b[39mfname_in\n\u001b[1;32m     10\u001b[0m             ),\n\u001b[1;32m     11\u001b[0m         AddTaxonInformationCB(\n\u001b[1;32m     12\u001b[0m             fn_lut\u001b[38;5;241m=\u001b[39mlut_taxon\n\u001b[1;32m     13\u001b[0m             ),            \n\u001b[1;32m     14\u001b[0m         RemapToHumanReadableCB(\n\u001b[1;32m     15\u001b[0m             src_fname\u001b[38;5;241m=\u001b[39mfname_in, \n\u001b[1;32m     16\u001b[0m             output_format\u001b[38;5;241m=\u001b[39moutput_format\n\u001b[1;32m     17\u001b[0m             ),\n\u001b[1;32m     18\u001b[0m         DecodeTimeCB(),\n\u001b[1;32m     19\u001b[0m         AddSampleTypeIdColumnCB(),\n\u001b[1;32m     20\u001b[0m         AddZoteroArchiveLocationCB(src_fname\u001b[38;5;241m=\u001b[39mfname_in, cfg\u001b[38;5;241m=\u001b[39mcfg())\n\u001b[1;32m     21\u001b[0m     ]\n\u001b[1;32m     23\u001b[0m or_cbs\u001b[38;5;241m=\u001b[39m [RemoveNonORVarsCB(),\n\u001b[1;32m     24\u001b[0m             RemapToORMappingsCB(\n\u001b[1;32m     25\u001b[0m             or_mappings\u001b[38;5;241m=\u001b[39mor_mappings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m          \n\u001b[1;32m     31\u001b[0m          ]\n\u001b[1;32m     34\u001b[0m tfm \u001b[38;5;241m=\u001b[39m Transformer(dfs,cbs)  \n",
      "\u001b[0;31mNameError\u001b[0m: name 'ValidateEnumsCB' is not defined"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "output_format = 'csv' #'openrefine_csv' # 'csv'\n",
    "\n",
    "cbs_validation=[ValidateEnumsCB(\n",
    "            src_fname=fname_in,\n",
    "            enums=Enums(lut_src_dir=lut_path())\n",
    "            ),\n",
    "        ValidateNetCDFVarsCB(\n",
    "            src_fname=fname_in\n",
    "            )]\n",
    "\n",
    "cbs_or=[RemoveNonORVarsCB(),\n",
    "            RemapToORMappingsCB(\n",
    "            or_mappings=or_mappings,\n",
    "            ),\n",
    "            \n",
    "            ]\n",
    "\n",
    "\n",
    "cbs_general=[AddTaxonInformationCB(\n",
    "            fn_lut=lut_taxon\n",
    "            ),            \n",
    "        RemapToHumanReadableCB(\n",
    "            src_fname=fname_in, \n",
    "            output_format=output_format\n",
    "            ),\n",
    "        DecodeTimeCB(),\n",
    "        AddSampleTypeIdColumnCB(),\n",
    "        AddZoteroArchiveLocationCB(src_fname=fname_in, cfg=cfg())\n",
    "    ]\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "         \n",
    "         \n",
    "         ]\n",
    "\n",
    "\n",
    "tfm = Transformer(dfs,cbs)  \n",
    "tfm()\n",
    "print(tfm.dfs['BIOTA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm.dfs['SEAWATER']['DL'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding NETCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decode(\n",
    "    fname_in: str, # Input file name\n",
    "    dest_out: str | None = None, # Output file name (optional)\n",
    "    output_format: str = 'openrefine_csv',\n",
    "    remap_vars: Dict[str, str] = OR_VARS,\n",
    "    verbose: bool = False,\n",
    "    **kwargs # Additional arguments\n",
    "    ) -> None:\n",
    "    \"Decode data from NetCDF.\"\n",
    "    dfs = load_to_dataframes(fname_in)\n",
    "\n",
    "    valid_output_formats=['openrefine_csv', 'csv']\n",
    "    if output_format not in valid_output_formats:\n",
    "        print (f'Invalid output format. Allowed formats: {valid_output_formats}')\n",
    "        return \n",
    "    \n",
    "    tfm = Transformer(\n",
    "        dfs,\n",
    "        cbs=[\n",
    "            RemoveNonORVarsCB(\n",
    "                output_format=output_format\n",
    "                ),\n",
    "            ValidateEnumsCB(\n",
    "                src_fname=fname_in,\n",
    "                enums=Enums(lut_src_dir=lut_path())\n",
    "                ),\n",
    "            ValidateNetCDFVarsCB(\n",
    "                src_fname=fname_in\n",
    "                ),\n",
    "            \n",
    "            AddTaxonInformationCB(\n",
    "                fn_lut=lut_taxon\n",
    "                ),  \n",
    "            RemapToORMappingsCB(\n",
    "                or_mappings=or_mappings,\n",
    "                output_format=output_format\n",
    "                ),            \n",
    "            RemapToHumanReadableCB(\n",
    "                src_fname=fname_in, \n",
    "                output_format=output_format\n",
    "                ),\n",
    "            DecodeTimeCB(),\n",
    "            AddSampleTypeIdColumnCB(),\n",
    "            AddZoteroArchiveLocationCB(src_fname=fname_in, cfg=cfg())\n",
    "        ]\n",
    "    )    \n",
    "    \n",
    "    tfm()\n",
    "    decoder = NetCDFDecoder( \n",
    "                            dfs=tfm.dfs,\n",
    "                            fname_in=fname_in,  \n",
    "                            dest_out=dest_out,                           \n",
    "                            output_format='csv',\n",
    "                            remap_vars=OR_VARS,\n",
    "                            verbose=verbose\n",
    "                    )\n",
    "    decoder.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "fname = Path('../../_data/output/100-HELCOM-MORS-2024.nc')\n",
    "decode(fname_in=fname, dest_out=fname.with_suffix(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(fname_in=fname, dest_out=fname.with_suffix(''))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp handlers.data_format_transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data format transformation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A data pipeline handler that transforms MARIS data between different formats. The primary focus is converting NetCDF data into human-readable formats (like CSV, Excel) while preserving data integrity and maintaining standardized variable names and units. This handler implements a modular transformation pipeline using callbacks for each processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "For new MARIS users, please refer to [Understanding MARIS Data Formats (NetCDF and Open Refine)](https://github.com/franckalbinet/marisco/tree/main/install_configure_guide) for detailed information.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "> Required packages and internal modules for data format transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from netCDF4 import Dataset\n",
    "import pandas as pd\n",
    "from fastcore.basics import patch, store_attr\n",
    "import fastcore.all as fc\n",
    "from typing import Dict\n",
    "\n",
    "from marisco.configs import (\n",
    "    NC_VARS,\n",
    "    OR_VARS,\n",
    "    NC_GROUPS,\n",
    "    Enums,\n",
    "    lut_path\n",
    ")\n",
    "\n",
    "from marisco.callbacks import (\n",
    "    Callback,\n",
    "    Transformer,\n",
    "    DecodeTimeCB\n",
    ")  \n",
    "    \n",
    "from marisco.decoders import (\n",
    "        NetCDFDecoder\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "fname_in =  Path('../../_data/output/100-HELCOM-MORS-2024.nc')\n",
    "fname_out = fname_in.with_suffix('.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and validate data from standardized MARIS NetCDF files. The NetCDF files follow CF conventions and include standardized variable names, units, and metadata according to MARIS specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_dataframes(fname:str, verbose: bool = False):\n",
    "    \"\"\"Load NetCDF groups into DataFrames with standardized column names.\"\"\"\n",
    "    dfs = {}\n",
    "    with Dataset(fname, 'r') as nc:\n",
    "        for group_name in nc.groups:\n",
    "            group = nc.groups[group_name]\n",
    "            # Get all variables in the group\n",
    "            data = {}\n",
    "            for var_name, var in group.variables.items():\n",
    "                if var_name not in group.dimensions:  # Skip dimension variables\n",
    "                    data[var_name] = var[:]\n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            # Rename columns using NC_VARS mapping\n",
    "            rename_map = {nc_var: col for col, nc_var in NC_VARS.items() \n",
    "                         if nc_var in df.columns}\n",
    "            df = df.rename(columns=rename_map)\n",
    "            dfs[group_name.upper()] = df\n",
    "            if verbose:\n",
    "                print(f\"Loaded group {group_name} with columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded group biota with columns: ['LON', 'LAT', 'SMP_DEPTH', 'TIME', 'NUCLIDE', 'VALUE', 'UNIT', 'DL', 'BIO_GROUP', 'SPECIES', 'BODY_PART', 'DRYWT', 'WETWT']\n",
      "Loaded group seawater with columns: ['LON', 'LAT', 'SMP_DEPTH', 'TOT_DEPTH', 'TIME', 'NUCLIDE', 'VALUE', 'UNIT', 'DL', 'FILT']\n",
      "Loaded group sediment with columns: ['LON', 'LAT', 'TOT_DEPTH', 'TIME', 'AREA', 'NUCLIDE', 'VALUE', 'UNIT', 'DL', 'SED_TYPE', 'TOP', 'BOTTOM']\n"
     ]
    }
   ],
   "source": [
    "dfs = load_to_dataframes(fname_in, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate NetCDF Enumerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that enumerated values in the NetCDF file match MARIS lookup tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: The enumeration validation process is a diagnostic step that identifies inconsistencies between NetCDF enumerations and MARIS lookup tables. While this validation does not modify the dataset, it generates detailed feedback about any mismatches or undefined values. \n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ValidateEnumsCB(Callback):\n",
    "    \"Validate enumeration mappings between NetCDF file and MARIS lookup tables.\"\n",
    "    def __init__(self, \n",
    "                src_fname: str,  # Path to NetCDF file\n",
    "                enums: Enums,    # MARIS lookup table enums\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Process each group in the NetCDF file and validate its enums.\"\"\"\n",
    "        with Dataset(self.src_fname, 'r') as nc:\n",
    "            for group_name in nc.groups:\n",
    "                group = nc.groups[group_name]\n",
    "                self._validate_group(group, group_name)\n",
    "    \n",
    "    def _validate_group(self, group, group_name: str):\n",
    "        \"\"\"Validate enum mappings for a specific group.\"\"\"\n",
    "        for var_name, var in group.variables.items():\n",
    "            if not hasattr(var.datatype, 'enum_dict'): \n",
    "                continue\n",
    "            \n",
    "            nc_enum_dict = var.datatype.enum_dict\n",
    "            if self.verbose:\n",
    "                print(f\"nc_enum_dict [{var_name}]:\", nc_enum_dict)\n",
    "\n",
    "            # Get original column name from NC_VARS mapping\n",
    "            original_col = next((col for col, nc_var in NC_VARS.items() \n",
    "                               if nc_var == var_name), None)\n",
    "            if not original_col: \n",
    "                continue\n",
    "\n",
    "            # Compare enum mappings\n",
    "            self._compare_mappings(\n",
    "                nc_enum_dict,\n",
    "                self.enums.types[original_col],\n",
    "                group_name,\n",
    "                var_name,\n",
    "                original_col\n",
    "            )\n",
    "    \n",
    "    def _compare_mappings(self, nc_dict: dict, lut_dict: dict, \n",
    "                         group_name: str, var_name: str, col_name: str):\n",
    "        \"\"\"Compare NetCDF enum dictionary with lookup table dictionary.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f\"lut_enum [{col_name}]:\", lut_dict)\n",
    "            \n",
    "        # Check for mismatches between NetCDF and lookup table\n",
    "        for key, value in nc_dict.items():\n",
    "            if key not in lut_dict or lut_dict[key] != value:\n",
    "                print(f\"\\nWarning: Enum mismatch in {group_name}/{var_name}\")\n",
    "                print(f\"NetCDF value: {key} -> {value}\")\n",
    "                print(f\"Lookup value: {key} -> {lut_dict.get(key, 'Not found')}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIOTA':              LON        LAT  SMP_DEPTH        TIME  NUCLIDE       VALUE  UNIT  \\\n",
       " 0      12.316667  54.283333        NaN  1348358400       31    0.010140     5   \n",
       " 1      12.316667  54.283333        NaN  1348358400        4  135.300003     5   \n",
       " 2      12.316667  54.283333        NaN  1348358400        9    0.013980     5   \n",
       " 3      12.316667  54.283333        NaN  1348358400       33    4.338000     5   \n",
       " 4      12.316667  54.283333        NaN  1348358400       31    0.009614     5   \n",
       " ...          ...        ...        ...         ...      ...         ...   ...   \n",
       " 14868  19.000000  54.583302       61.0  1519603200       53    0.043000     5   \n",
       " 14869  15.500000  54.333302       65.0  1518480000        4   98.000000     5   \n",
       " 14870  15.500000  54.333302       65.0  1518480000       33    3.690000     5   \n",
       " 14871  15.500000  54.333302       65.0  1518480000       53    0.049000     5   \n",
       " 14872  19.433300  54.363899        NaN  1538524800       33    0.830000     5   \n",
       " \n",
       "        DL  BIO_GROUP  SPECIES  BODY_PART       DRYWT  WETWT  \n",
       " 0       2          4       99         52  174.934433  948.0  \n",
       " 1       1          4       99         52  174.934433  948.0  \n",
       " 2       2          4       99         52  174.934433  948.0  \n",
       " 3       1          4       99         52  174.934433  948.0  \n",
       " 4       2          4       99         52  177.935120  964.0  \n",
       " ...    ..        ...      ...        ...         ...    ...  \n",
       " 14868   1          4      191          3  120.000000  500.0  \n",
       " 14869   1          4      191         52  112.500000  500.0  \n",
       " 14870   1          4      191         52  112.500000  500.0  \n",
       " 14871   1          4      191         52  112.500000  500.0  \n",
       " 14872   1          4      247         52         NaN  120.0  \n",
       " \n",
       " [14873 rows x 13 columns],\n",
       " 'SEAWATER':              LON        LAT  SMP_DEPTH  TOT_DEPTH        TIME  NUCLIDE  \\\n",
       " 0      29.333300  60.083302        0.0        NaN  1337731200       33   \n",
       " 1      29.333300  60.083302       29.0        NaN  1337731200       33   \n",
       " 2      23.150000  59.433300        0.0        NaN  1339891200       33   \n",
       " 3      27.983299  60.250000        0.0        NaN  1337817600       33   \n",
       " 4      27.983299  60.250000       39.0        NaN  1337817600       33   \n",
       " ...          ...        ...        ...        ...         ...      ...   \n",
       " 20237  14.200000  54.006802        4.0       12.0  1434931200       12   \n",
       " 20238  14.667200  54.499500        4.0       20.0  1435017600       12   \n",
       " 20239  14.334200  54.750500        4.0       17.0  1435017600       12   \n",
       " 20240  13.500200  54.916500        4.0       47.0  1435104000       12   \n",
       " 20241  13.500200  54.916500       45.0       47.0  1435104000       12   \n",
       " \n",
       "            VALUE  UNIT  DL  FILT  \n",
       " 0       5.300000     1   1     0  \n",
       " 1      19.900000     1   1     0  \n",
       " 2      25.500000     1   1     0  \n",
       " 3      17.000000     1   1     0  \n",
       " 4      22.200001     1   1     0  \n",
       " ...          ...   ...  ..   ...  \n",
       " 20237   6.600000     1   1     1  \n",
       " 20238   6.900000     1   1     1  \n",
       " 20239   6.800000     1   1     1  \n",
       " 20240   7.300000     1   1     1  \n",
       " 20241   5.500000     1   1     1  \n",
       " \n",
       " [20242 rows x 10 columns],\n",
       " 'SEDIMENT':            LON        LAT  TOT_DEPTH        TIME  AREA  NUCLIDE        VALUE  \\\n",
       " 0      24.0000  59.666698       71.0  1339891200     0       53    35.000000   \n",
       " 1      24.0000  59.666698       71.0  1339891200     0       53    36.000000   \n",
       " 2      28.8433  59.860001       23.0  1344556800     0       53    38.000000   \n",
       " 3      28.8433  59.860001       23.0  1344556800     0       53    36.000000   \n",
       " 4      28.8433  59.860001       23.0  1344556800     0       53    30.000000   \n",
       " ...        ...        ...        ...         ...   ...      ...          ...   \n",
       " 63863  21.0830  59.035999      171.0  1465430400     0       33     8.916443   \n",
       " 63864  21.0830  59.035999      171.0  1465430400     0       33     5.992929   \n",
       " 63865  19.7297  61.066700      131.0  1464480000     0       33  2164.945801   \n",
       " 63866  19.7297  61.066700      131.0  1464480000     0       33  2523.279053   \n",
       " 63867  19.7297  61.066700      131.0  1464480000     0       33  3929.780029   \n",
       " \n",
       "        UNIT  DL  SED_TYPE   TOP  BOTTOM  \n",
       " 0         4   1         0  15.0    20.0  \n",
       " 1         4   1         0  20.0    27.0  \n",
       " 2         4   1         0   0.0     2.0  \n",
       " 3         4   1         0   2.0     4.0  \n",
       " 4         4   1         0   4.0     6.0  \n",
       " ...     ...  ..       ...   ...     ...  \n",
       " 63863     4   1        50  18.0    20.0  \n",
       " 63864     4   1        50  20.0    22.0  \n",
       " 63865     4   1        59   0.0     2.0  \n",
       " 63866     4   1        51   2.0     4.0  \n",
       " 63867     4   1        50   4.0     6.0  \n",
       " \n",
       " [63868 rows x 12 columns]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        ValidateEnumsCB(\n",
    "            src_fname=fname_in,\n",
    "            enums=Enums(lut_src_dir=lut_path()),\n",
    "            #verbose=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate NetCDF Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that variable names in the NetCDF file match those used in MARIS ternminogy, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ValidateNetCDFVarsCB(Callback):\n",
    "    \" Validate that all variables in the NetCDF file are included in NC_VARS mapping. Identifies and reports any unmapped variables.\"\n",
    "    def __init__(self, \n",
    "                src_fname: str,  # Path to NetCDF file\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Check each group's variables against NC_VARS mapping.\"\"\"\n",
    "        unmapped_vars = {}\n",
    "        \n",
    "        with Dataset(self.src_fname, 'r') as nc:\n",
    "            for group_name in nc.groups:\n",
    "                group = nc.groups[group_name]\n",
    "                group_vars = set(group.variables.keys())\n",
    "                mapped_vars = {v for k, v in NC_VARS.items()}\n",
    "                unmapped = group_vars - mapped_vars - {'id'}  # Exclude dimension variables\n",
    "                \n",
    "                if unmapped:\n",
    "                    unmapped_vars[group_name] = unmapped\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\nWarning: Unmapped variables in group {group_name}:\")\n",
    "                        print(f\"Variables: {unmapped}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIOTA':              LON        LAT  SMP_DEPTH        TIME  NUCLIDE       VALUE  UNIT  \\\n",
       " 0      12.316667  54.283333        NaN  1348358400       31    0.010140     5   \n",
       " 1      12.316667  54.283333        NaN  1348358400        4  135.300003     5   \n",
       " 2      12.316667  54.283333        NaN  1348358400        9    0.013980     5   \n",
       " 3      12.316667  54.283333        NaN  1348358400       33    4.338000     5   \n",
       " 4      12.316667  54.283333        NaN  1348358400       31    0.009614     5   \n",
       " ...          ...        ...        ...         ...      ...         ...   ...   \n",
       " 14868  19.000000  54.583302       61.0  1519603200       53    0.043000     5   \n",
       " 14869  15.500000  54.333302       65.0  1518480000        4   98.000000     5   \n",
       " 14870  15.500000  54.333302       65.0  1518480000       33    3.690000     5   \n",
       " 14871  15.500000  54.333302       65.0  1518480000       53    0.049000     5   \n",
       " 14872  19.433300  54.363899        NaN  1538524800       33    0.830000     5   \n",
       " \n",
       "        DL  BIO_GROUP  SPECIES  BODY_PART       DRYWT  WETWT  \n",
       " 0       2          4       99         52  174.934433  948.0  \n",
       " 1       1          4       99         52  174.934433  948.0  \n",
       " 2       2          4       99         52  174.934433  948.0  \n",
       " 3       1          4       99         52  174.934433  948.0  \n",
       " 4       2          4       99         52  177.935120  964.0  \n",
       " ...    ..        ...      ...        ...         ...    ...  \n",
       " 14868   1          4      191          3  120.000000  500.0  \n",
       " 14869   1          4      191         52  112.500000  500.0  \n",
       " 14870   1          4      191         52  112.500000  500.0  \n",
       " 14871   1          4      191         52  112.500000  500.0  \n",
       " 14872   1          4      247         52         NaN  120.0  \n",
       " \n",
       " [14873 rows x 13 columns],\n",
       " 'SEAWATER':              LON        LAT  SMP_DEPTH  TOT_DEPTH        TIME  NUCLIDE  \\\n",
       " 0      29.333300  60.083302        0.0        NaN  1337731200       33   \n",
       " 1      29.333300  60.083302       29.0        NaN  1337731200       33   \n",
       " 2      23.150000  59.433300        0.0        NaN  1339891200       33   \n",
       " 3      27.983299  60.250000        0.0        NaN  1337817600       33   \n",
       " 4      27.983299  60.250000       39.0        NaN  1337817600       33   \n",
       " ...          ...        ...        ...        ...         ...      ...   \n",
       " 20237  14.200000  54.006802        4.0       12.0  1434931200       12   \n",
       " 20238  14.667200  54.499500        4.0       20.0  1435017600       12   \n",
       " 20239  14.334200  54.750500        4.0       17.0  1435017600       12   \n",
       " 20240  13.500200  54.916500        4.0       47.0  1435104000       12   \n",
       " 20241  13.500200  54.916500       45.0       47.0  1435104000       12   \n",
       " \n",
       "            VALUE  UNIT  DL  FILT  \n",
       " 0       5.300000     1   1     0  \n",
       " 1      19.900000     1   1     0  \n",
       " 2      25.500000     1   1     0  \n",
       " 3      17.000000     1   1     0  \n",
       " 4      22.200001     1   1     0  \n",
       " ...          ...   ...  ..   ...  \n",
       " 20237   6.600000     1   1     1  \n",
       " 20238   6.900000     1   1     1  \n",
       " 20239   6.800000     1   1     1  \n",
       " 20240   7.300000     1   1     1  \n",
       " 20241   5.500000     1   1     1  \n",
       " \n",
       " [20242 rows x 10 columns],\n",
       " 'SEDIMENT':            LON        LAT  TOT_DEPTH        TIME  AREA  NUCLIDE        VALUE  \\\n",
       " 0      24.0000  59.666698       71.0  1339891200     0       53    35.000000   \n",
       " 1      24.0000  59.666698       71.0  1339891200     0       53    36.000000   \n",
       " 2      28.8433  59.860001       23.0  1344556800     0       53    38.000000   \n",
       " 3      28.8433  59.860001       23.0  1344556800     0       53    36.000000   \n",
       " 4      28.8433  59.860001       23.0  1344556800     0       53    30.000000   \n",
       " ...        ...        ...        ...         ...   ...      ...          ...   \n",
       " 63863  21.0830  59.035999      171.0  1465430400     0       33     8.916443   \n",
       " 63864  21.0830  59.035999      171.0  1465430400     0       33     5.992929   \n",
       " 63865  19.7297  61.066700      131.0  1464480000     0       33  2164.945801   \n",
       " 63866  19.7297  61.066700      131.0  1464480000     0       33  2523.279053   \n",
       " 63867  19.7297  61.066700      131.0  1464480000     0       33  3929.780029   \n",
       " \n",
       "        UNIT  DL  SED_TYPE   TOP  BOTTOM  \n",
       " 0         4   1         0  15.0    20.0  \n",
       " 1         4   1         0  20.0    27.0  \n",
       " 2         4   1         0   0.0     2.0  \n",
       " 3         4   1         0   2.0     4.0  \n",
       " 4         4   1         0   4.0     6.0  \n",
       " ...     ...  ..       ...   ...     ...  \n",
       " 63863     4   1        50  18.0    20.0  \n",
       " 63864     4   1        50  20.0    22.0  \n",
       " 63865     4   1        59   0.0     2.0  \n",
       " 63866     4   1        51   2.0     4.0  \n",
       " 63867     4   1        50   4.0     6.0  \n",
       " \n",
       " [63868 rows x 12 columns]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        ValidateNetCDFVarsCB(\n",
    "            src_fname=fname_in,\n",
    "            verbose=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remap to human readable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ConvertToHumanReadableCB(Callback):\n",
    "    \"\"\"\n",
    "    Convert enum values in DataFrames to their human-readable format.\n",
    "    Uses the enum dictionary keys as the human-readable values.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                src_fname: str,  # Path to NetCDF file\n",
    "                verbose: bool = False\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "        \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Convert numeric enum values to human-readable strings.\"\"\"\n",
    "        with Dataset(self.src_fname, 'r') as nc:\n",
    "            for group_name, df in tfm.dfs.items():\n",
    "                nc_group_name = NC_GROUPS[group_name]\n",
    "                group = nc.groups[nc_group_name]\n",
    "                \n",
    "                # Process each variable that has an enum\n",
    "                for var_name, var in group.variables.items():\n",
    "                    if hasattr(var.datatype, 'enum_dict'):\n",
    "                        # Get the original column name from NC_VARS mapping\n",
    "                        original_col = next((col for col, nc_var in NC_VARS.items() \n",
    "                                          if nc_var == var_name), None)\n",
    "                        \n",
    "                        if original_col and original_col in df.columns:\n",
    "                            enum_dict = {v: k for k, v in var.datatype.enum_dict.items()}\n",
    "                            tfm.dfs[group_name][original_col] = df[original_col].map(enum_dict)\n",
    "                            \n",
    "                            if self.verbose:\n",
    "                                print(f\"Converted {original_col} to human readable format in {group_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted NUCLIDE to human readable format in BIOTA\n",
      "Converted UNIT to human readable format in BIOTA\n",
      "Converted DL to human readable format in BIOTA\n",
      "Converted BIO_GROUP to human readable format in BIOTA\n",
      "Converted SPECIES to human readable format in BIOTA\n",
      "Converted BODY_PART to human readable format in BIOTA\n",
      "Converted NUCLIDE to human readable format in SEAWATER\n",
      "Converted UNIT to human readable format in SEAWATER\n",
      "Converted DL to human readable format in SEAWATER\n",
      "Converted FILT to human readable format in SEAWATER\n",
      "Converted AREA to human readable format in SEDIMENT\n",
      "Converted NUCLIDE to human readable format in SEDIMENT\n",
      "Converted UNIT to human readable format in SEDIMENT\n",
      "Converted DL to human readable format in SEDIMENT\n",
      "Converted SED_TYPE to human readable format in SEDIMENT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BIOTA':              LON        LAT  SMP_DEPTH        TIME NUCLIDE       VALUE  \\\n",
       " 0      12.316667  54.283333        NaN  1348358400   cs134    0.010140   \n",
       " 1      12.316667  54.283333        NaN  1348358400     k40  135.300003   \n",
       " 2      12.316667  54.283333        NaN  1348358400    co60    0.013980   \n",
       " 3      12.316667  54.283333        NaN  1348358400   cs137    4.338000   \n",
       " 4      12.316667  54.283333        NaN  1348358400   cs134    0.009614   \n",
       " ...          ...        ...        ...         ...     ...         ...   \n",
       " 14868  19.000000  54.583302       61.0  1519603200   ra226    0.043000   \n",
       " 14869  15.500000  54.333302       65.0  1518480000     k40   98.000000   \n",
       " 14870  15.500000  54.333302       65.0  1518480000   cs137    3.690000   \n",
       " 14871  15.500000  54.333302       65.0  1518480000   ra226    0.049000   \n",
       " 14872  19.433300  54.363899        NaN  1538524800   cs137    0.830000   \n",
       " \n",
       "              UNIT               DL BIO_GROUP             SPECIES  \\\n",
       " 0      Bq per kgw  Detection limit      Fish        Gadus morhua   \n",
       " 1      Bq per kgw   Detected value      Fish        Gadus morhua   \n",
       " 2      Bq per kgw  Detection limit      Fish        Gadus morhua   \n",
       " 3      Bq per kgw   Detected value      Fish        Gadus morhua   \n",
       " 4      Bq per kgw  Detection limit      Fish        Gadus morhua   \n",
       " ...           ...              ...       ...                 ...   \n",
       " 14868  Bq per kgw   Detected value      Fish  Platichthys flesus   \n",
       " 14869  Bq per kgw   Detected value      Fish  Platichthys flesus   \n",
       " 14870  Bq per kgw   Detected value      Fish  Platichthys flesus   \n",
       " 14871  Bq per kgw   Detected value      Fish  Platichthys flesus   \n",
       " 14872  Bq per kgw   Detected value      Fish   Perca fluviatilis   \n",
       " \n",
       "                                    BODY_PART       DRYWT  WETWT  \n",
       " 0                        Flesh without bones  174.934433  948.0  \n",
       " 1                        Flesh without bones  174.934433  948.0  \n",
       " 2                        Flesh without bones  174.934433  948.0  \n",
       " 3                        Flesh without bones  174.934433  948.0  \n",
       " 4                        Flesh without bones  177.935120  964.0  \n",
       " ...                                      ...         ...    ...  \n",
       " 14868  Whole animal eviscerated without head  120.000000  500.0  \n",
       " 14869                    Flesh without bones  112.500000  500.0  \n",
       " 14870                    Flesh without bones  112.500000  500.0  \n",
       " 14871                    Flesh without bones  112.500000  500.0  \n",
       " 14872                    Flesh without bones         NaN  120.0  \n",
       " \n",
       " [14873 rows x 13 columns],\n",
       " 'SEAWATER':              LON        LAT  SMP_DEPTH  TOT_DEPTH        TIME NUCLIDE  \\\n",
       " 0      29.333300  60.083302        0.0        NaN  1337731200   cs137   \n",
       " 1      29.333300  60.083302       29.0        NaN  1337731200   cs137   \n",
       " 2      23.150000  59.433300        0.0        NaN  1339891200   cs137   \n",
       " 3      27.983299  60.250000        0.0        NaN  1337817600   cs137   \n",
       " 4      27.983299  60.250000       39.0        NaN  1337817600   cs137   \n",
       " ...          ...        ...        ...        ...         ...     ...   \n",
       " 20237  14.200000  54.006802        4.0       12.0  1434931200    sr90   \n",
       " 20238  14.667200  54.499500        4.0       20.0  1435017600    sr90   \n",
       " 20239  14.334200  54.750500        4.0       17.0  1435017600    sr90   \n",
       " 20240  13.500200  54.916500        4.0       47.0  1435104000    sr90   \n",
       " 20241  13.500200  54.916500       45.0       47.0  1435104000    sr90   \n",
       " \n",
       "            VALUE       UNIT              DL           FILT  \n",
       " 0       5.300000  Bq per m3  Detected value  Not available  \n",
       " 1      19.900000  Bq per m3  Detected value  Not available  \n",
       " 2      25.500000  Bq per m3  Detected value  Not available  \n",
       " 3      17.000000  Bq per m3  Detected value  Not available  \n",
       " 4      22.200001  Bq per m3  Detected value  Not available  \n",
       " ...          ...        ...             ...            ...  \n",
       " 20237   6.600000  Bq per m3  Detected value            Yes  \n",
       " 20238   6.900000  Bq per m3  Detected value            Yes  \n",
       " 20239   6.800000  Bq per m3  Detected value            Yes  \n",
       " 20240   7.300000  Bq per m3  Detected value            Yes  \n",
       " 20241   5.500000  Bq per m3  Detected value            Yes  \n",
       " \n",
       " [20242 rows x 10 columns],\n",
       " 'SEDIMENT':            LON        LAT  TOT_DEPTH        TIME           AREA NUCLIDE  \\\n",
       " 0      24.0000  59.666698       71.0  1339891200  Not available   ra226   \n",
       " 1      24.0000  59.666698       71.0  1339891200  Not available   ra226   \n",
       " 2      28.8433  59.860001       23.0  1344556800  Not available   ra226   \n",
       " 3      28.8433  59.860001       23.0  1344556800  Not available   ra226   \n",
       " 4      28.8433  59.860001       23.0  1344556800  Not available   ra226   \n",
       " ...        ...        ...        ...         ...            ...     ...   \n",
       " 63863  21.0830  59.035999      171.0  1465430400  Not available   cs137   \n",
       " 63864  21.0830  59.035999      171.0  1465430400  Not available   cs137   \n",
       " 63865  19.7297  61.066700      131.0  1464480000  Not available   cs137   \n",
       " 63866  19.7297  61.066700      131.0  1464480000  Not available   cs137   \n",
       " 63867  19.7297  61.066700      131.0  1464480000  Not available   cs137   \n",
       " \n",
       "              VALUE        UNIT              DL       SED_TYPE   TOP  BOTTOM  \n",
       " 0        35.000000  Bq per kgd  Detected value  Not available  15.0    20.0  \n",
       " 1        36.000000  Bq per kgd  Detected value  Not available  20.0    27.0  \n",
       " 2        38.000000  Bq per kgd  Detected value  Not available   0.0     2.0  \n",
       " 3        36.000000  Bq per kgd  Detected value  Not available   2.0     4.0  \n",
       " 4        30.000000  Bq per kgd  Detected value  Not available   4.0     6.0  \n",
       " ...            ...         ...             ...            ...   ...     ...  \n",
       " 63863     8.916443  Bq per kgd  Detected value   Glacial clay  18.0    20.0  \n",
       " 63864     5.992929  Bq per kgd  Detected value   Glacial clay  20.0    22.0  \n",
       " 63865  2164.945801  Bq per kgd  Detected value       Soft mud   0.0     2.0  \n",
       " 63866  2523.279053  Bq per kgd  Detected value      Soft clay   2.0     4.0  \n",
       " 63867  3929.780029  Bq per kgd  Detected value   Glacial clay   4.0     6.0  \n",
       " \n",
       " [63868 rows x 12 columns]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        ConvertToHumanReadableCB(\n",
    "            src_fname=fname_in,\n",
    "            verbose=True\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       2012-09-23\n",
      "1       2012-09-23\n",
      "2       2012-09-23\n",
      "3       2012-09-23\n",
      "4       2012-09-23\n",
      "           ...    \n",
      "14868   2018-02-26\n",
      "14869   2018-02-13\n",
      "14870   2018-02-13\n",
      "14871   2018-02-13\n",
      "14872   2018-10-03\n",
      "Name: TIME, Length: 14873, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        DecodeTimeCB(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "\n",
    "print(tfm.dfs['BIOTA']['TIME'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             LON        LAT  SMP_DEPTH       TIME NUCLIDE       VALUE  \\\n",
      "0      12.316667  54.283333        NaN 2012-09-23   cs134    0.010140   \n",
      "1      12.316667  54.283333        NaN 2012-09-23     k40  135.300003   \n",
      "2      12.316667  54.283333        NaN 2012-09-23    co60    0.013980   \n",
      "3      12.316667  54.283333        NaN 2012-09-23   cs137    4.338000   \n",
      "4      12.316667  54.283333        NaN 2012-09-23   cs134    0.009614   \n",
      "...          ...        ...        ...        ...     ...         ...   \n",
      "14868  19.000000  54.583302       61.0 2018-02-26   ra226    0.043000   \n",
      "14869  15.500000  54.333302       65.0 2018-02-13     k40   98.000000   \n",
      "14870  15.500000  54.333302       65.0 2018-02-13   cs137    3.690000   \n",
      "14871  15.500000  54.333302       65.0 2018-02-13   ra226    0.049000   \n",
      "14872  19.433300  54.363899        NaN 2018-10-03   cs137    0.830000   \n",
      "\n",
      "             UNIT               DL BIO_GROUP             SPECIES  \\\n",
      "0      Bq per kgw  Detection limit      Fish        Gadus morhua   \n",
      "1      Bq per kgw   Detected value      Fish        Gadus morhua   \n",
      "2      Bq per kgw  Detection limit      Fish        Gadus morhua   \n",
      "3      Bq per kgw   Detected value      Fish        Gadus morhua   \n",
      "4      Bq per kgw  Detection limit      Fish        Gadus morhua   \n",
      "...           ...              ...       ...                 ...   \n",
      "14868  Bq per kgw   Detected value      Fish  Platichthys flesus   \n",
      "14869  Bq per kgw   Detected value      Fish  Platichthys flesus   \n",
      "14870  Bq per kgw   Detected value      Fish  Platichthys flesus   \n",
      "14871  Bq per kgw   Detected value      Fish  Platichthys flesus   \n",
      "14872  Bq per kgw   Detected value      Fish   Perca fluviatilis   \n",
      "\n",
      "                                   BODY_PART       DRYWT  WETWT  \n",
      "0                        Flesh without bones  174.934433  948.0  \n",
      "1                        Flesh without bones  174.934433  948.0  \n",
      "2                        Flesh without bones  174.934433  948.0  \n",
      "3                        Flesh without bones  174.934433  948.0  \n",
      "4                        Flesh without bones  177.935120  964.0  \n",
      "...                                      ...         ...    ...  \n",
      "14868  Whole animal eviscerated without head  120.000000  500.0  \n",
      "14869                    Flesh without bones  112.500000  500.0  \n",
      "14870                    Flesh without bones  112.500000  500.0  \n",
      "14871                    Flesh without bones  112.500000  500.0  \n",
      "14872                    Flesh without bones         NaN  120.0  \n",
      "\n",
      "[14873 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_to_dataframes(fname_in)\n",
    "tfm = Transformer(\n",
    "    dfs,\n",
    "    cbs=[\n",
    "        ValidateEnumsCB(\n",
    "            src_fname=fname_in,\n",
    "            enums=Enums(lut_src_dir=lut_path())\n",
    "            ),\n",
    "        ValidateNetCDFVarsCB(\n",
    "            src_fname=fname_in\n",
    "            ),            \n",
    "        ConvertToHumanReadableCB(\n",
    "            src_fname=fname_in),      \n",
    "        DecodeTimeCB()\n",
    "    ]\n",
    ")\n",
    "\n",
    "tfm()\n",
    "\n",
    "print(tfm.dfs['BIOTA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<M8[ns]')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm.dfs['BIOTA']['TIME'].dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding NETCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decode(\n",
    "    fname_in: str, # Input file name\n",
    "    dest_out: str, # Output file name\n",
    "    output_format: str = 'csv',\n",
    "    remap_vars: Dict[str, str] = OR_VARS,\n",
    "    **kwargs # Additional arguments\n",
    "    ) -> None:\n",
    "    \"Decode data from NetCDF.\"\n",
    "    dfs = load_to_dataframes(fname_in)\n",
    "    tfm = Transformer(\n",
    "        dfs,\n",
    "        cbs=[\n",
    "            ValidateEnumsCB(\n",
    "                src_fname=fname_in,\n",
    "                enums=Enums(lut_src_dir=lut_path())\n",
    "                ),\n",
    "            ValidateNetCDFVarsCB(\n",
    "                src_fname=fname_in\n",
    "                ),            \n",
    "            ConvertToHumanReadableCB(\n",
    "                src_fname=fname_in),      \n",
    "            DecodeTimeCB()\n",
    "        ]\n",
    "    )    \n",
    "    \n",
    "    tfm()\n",
    "    decoder = NetCDFDecoder( \n",
    "                            dfs=dfs,\n",
    "                            fname_in=fname_in,  \n",
    "                            dest_out=dest_out,                           \n",
    "                            output_format='csv',\n",
    "                            remap_vars=OR_VARS,\n",
    "                            verbose=False\n",
    "                    )\n",
    "    decoder.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "fname = Path('../../_data/output/100-HELCOM-MORS-2024.nc')\n",
    "decode(fname_in=fname, dest_out=fname.with_suffix(''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVIEW time output!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

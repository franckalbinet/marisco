{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp handlers.helcom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416a6a41",
   "metadata": {},
   "source": [
    "# HELCOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709cfb6",
   "metadata": {},
   "source": [
    "> This data pipeline, known as a \"handler\" in Marisco terminology, is designed to clean, standardize, and encode [HELCOM data](https://helcom.fi/about-us) into `NetCDF` format. The handler processes raw HELCOM data, applying various transformations and lookups to align it with `MARIS` data standards.\n",
    "\n",
    "Key functions of this handler:\n",
    "\n",
    "- **Cleans** and **normalizes** raw HELCOM data\n",
    "- **Applies standardized nomenclature** and units\n",
    "- **Encodes the processed data** into `NetCDF` format compatible with MARIS requirements\n",
    "\n",
    "This handler is a crucial component in the Marisco data processing workflow, ensuring HELCOM data is properly integrated into the MARIS database.\n",
    "\n",
    "\n",
    "Note: *Additionally, an optional encoder (pipeline) is provided below to process data into a `.csv` format compatible with the MARIS master database. This feature is maintained for legacy purposes, as data ingestion was previously performed using OpenRefine.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801c877",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "For new MARIS users, please refer to [Understanding MARIS Data Formats (NetCDF and Open Refine)](https://github.com/franckalbinet/marisco/tree/main/install_configure_guide) for detailed information.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b121843",
   "metadata": {},
   "source": [
    "The present notebook pretends to be an instance of [Literate Programming](https://www.wikiwand.com/en/articles/Literate_programming) in the sense that it is a narrative that includes code snippets that are interspersed with explanations. When a function or a class needs to be exported in a dedicated python module (in our case `marisco/handlers/helcom.py`) the code snippet is added to the module using `#| exports` as provided by the wonderful [nbdev](https://nbdev.readthedocs.io/en/latest/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db45fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "#from functools import partial \n",
    "import fastcore.all as fc \n",
    "from pathlib import Path \n",
    "#from dataclasses import asdict\n",
    "from typing import List, Dict, Callable, Tuple, Any \n",
    "from collections import OrderedDict, defaultdict\n",
    "import re\n",
    "from functools import partial\n",
    "\n",
    "from marisco.utils import (\n",
    "    #has_valid_varname, \n",
    "    #match_worms, \n",
    "    Remapper, \n",
    "    ddmm_to_dd,\n",
    "    #match_maris_lut, \n",
    "    Match, \n",
    "    get_unique_across_dfs\n",
    ")\n",
    "\n",
    "from marisco.callbacks import (\n",
    "    Callback, \n",
    "    Transformer, \n",
    "    EncodeTimeCB, \n",
    "    AddSampleTypeIdColumnCB,\n",
    "    AddNuclideIdColumnCB, \n",
    "    LowerStripNameCB, \n",
    "    SanitizeLonLatCB, \n",
    "    #ReshapeLongToWide, \n",
    "    CompareDfsAndTfmCB, \n",
    "    RemapCB\n",
    ")\n",
    "\n",
    "from marisco.metadata import (\n",
    "    GlobAttrsFeeder, \n",
    "    BboxCB, \n",
    "    DepthRangeCB, \n",
    "    TimeRangeCB, \n",
    "    ZoteroCB, \n",
    "    KeyValuePairCB\n",
    ")\n",
    "\n",
    "from marisco.configs import (\n",
    "    nuc_lut_path, \n",
    "    nc_tpl_path, \n",
    "    cfg, \n",
    "    #cache_path, \n",
    "    #cdl_cfg, \n",
    "    Enums, \n",
    "    lut_path, \n",
    "    species_lut_path, \n",
    "    sediments_lut_path, \n",
    "    bodyparts_lut_path, \n",
    "    detection_limit_lut_path, \n",
    "    filtered_lut_path, \n",
    "    #area_lut_path, \n",
    "    get_lut, \n",
    "    unit_lut_path,\n",
    "    base_path, # not needed here, included to troubleshoot cdl_cfg\n",
    "    prepmet_lut_path,\n",
    "    sampmet_lut_path,\n",
    "    counmet_lut_path, \n",
    "    NC_VARS\n",
    ")\n",
    "\n",
    "from marisco.encoders import (\n",
    "    NetCDFEncoder, \n",
    "    #OpenRefineCsvEncoder\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045eeae",
   "metadata": {},
   "source": [
    "## Configuration & file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b476d",
   "metadata": {},
   "source": [
    "- **fname_in**: path to the folder containing the HELCOM data in CSV format. The path can be defined as a relative path. \n",
    "\n",
    "- **fname_out_nc**: path and filename for the NetCDF output.The path can be defined as a relative path. \n",
    "\n",
    "- **Zotero key**: used to retrieve attributes related to the dataset from [Zotero](https://www.zotero.org/). The MARIS datasets include a [library](https://maris.iaea.org/datasets) available on [Zotero](https://www.zotero.org/groups/2432820/maris/library). \n",
    "\n",
    "- **ref_id**: refers to the location in Archive of the Zotero library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "fname_in = '../../_data/accdb/mors/csv'\n",
    "fname_out_nc = '../../_data/output/100-HELCOM-MORS-2024.nc'\n",
    "zotero_key ='26VMZZ2Q' # HELCOM MORS zotero key\n",
    "ref_id = 100 # HELCOM MORS reference id as defined by MARIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88d99c",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbbc83f",
   "metadata": {},
   "source": [
    "[Helcom MORS (Monitoring of Radioactive Substances in the Baltic Sea) data](https://helcom.fi/about-us) is provided as a Microsoft Access database. \n",
    "[`Mdbtools`](https://github.com/mdbtools/mdbtools) can be used to convert the tables of the Microsoft Access database to `.csv` files on Unix-like OS.\n",
    "Metadata for the HELCOM MORS dataset is available [here](https://metadata.helcom.fi/geonetwork/srv/fin/catalog.search#/metadata/2fdd2d46-0329-40e3-bf96-cb08c7206a24).\n",
    "\n",
    "**Example steps**:\n",
    "\n",
    "\n",
    "1. [Download data](https://metadata.helcom.fi/geonetwork/srv/fin/catalog.search#/metadata/2fdd2d46-0329-40e3-bf96-cb08c7206a24)\n",
    "\n",
    "2. Install mdbtools via VScode Terminal: \n",
    "\n",
    "    ```\n",
    "    sudo apt-get -y install mdbtools\n",
    "    ```\n",
    "\n",
    "3. Install unzip via VScode Terminal:\n",
    "\n",
    "    ```\n",
    "    sudo apt-get -y install unzip\n",
    "    ```\n",
    "\n",
    "4. In `VS Code` terminal (for instance), navigate to the marisco data folder:\n",
    "\n",
    "    ```\n",
    "    cd /home/marisco/downloads/marisco/_data/accdb/mors_19840101_20211231\n",
    "    ```\n",
    "\n",
    "5. Unzip `MORS_ENVIRONMENT.zip`:\n",
    "\n",
    "    ```\n",
    "    unzip MORS_ENVIRONMENT.zip \n",
    "    ```\n",
    "\n",
    "6. Run `preprocess.sh` to generate the required data files:\n",
    "\n",
    "    ```\n",
    "    ./preprocess.sh MORS_ENVIRONMENT.zip\n",
    "    ```\n",
    "\n",
    "7. Content of `preprocess.sh` script:\n",
    "\n",
    "    ```\n",
    "    #!/bin/bash\n",
    "\n",
    "    # Example of use: ./preprocess.sh MORS_ENVIRONMENT.zip\n",
    "    unzip $1\n",
    "    dbname=$(ls *.accdb)\n",
    "    mkdir csv\n",
    "    for table in $(mdb-tables -1 \"$dbname\"); do\n",
    "        echo \"Export table $table\"\n",
    "        mdb-export \"$dbname\" \"$table\" > \"csv/$table.csv\"\n",
    "    done\n",
    "    ```\n",
    "\n",
    "Once converted to `.csv` files, the data is ready to be loaded into a dictionary of dataframes.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "default_smp_types = {  \n",
    "    'BIO': 'BIOTA', \n",
    "    'SEA': 'SEAWATER', \n",
    "    'SED': 'SEDIMENT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def load_data(src_dir: str|Path, \n",
    "              smp_types: dict = default_smp_types \n",
    "             ) -> Dict[str, pd.DataFrame]: \n",
    "    \"Load HELCOM data and return the data in a dictionary of dataframes with the dictionary key as the sample type.\"\n",
    "    src_path = Path(src_dir)\n",
    "    \n",
    "    def load_and_merge(file_prefix: str) -> pd.DataFrame:\n",
    "        try:\n",
    "            df_meas = pd.read_csv(src_path / f'{file_prefix}02.csv')\n",
    "            df_smp = pd.read_csv(src_path / f'{file_prefix}01.csv')\n",
    "            return pd.merge(df_meas, df_smp, on='KEY', how='left')\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error loading files for {file_prefix}: {e}\")\n",
    "            return pd.DataFrame()  # Return an empty DataFrame if files are not found\n",
    "    \n",
    "    return {smp_type: load_and_merge(file_prefix) for file_prefix, smp_type in smp_types.items()}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e48dc6",
   "metadata": {},
   "source": [
    "`dfs` is a dictionary of dataframes created from the Helcom dataset located at the path `fname_in`. The data to be included in each dataframe is sorted by sample type. Each dictionary is defined with a key equal to the sample type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4bf289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys/sample types:  dict_keys(['BIOTA', 'SEAWATER', 'SEDIMENT'])\n",
      "BIOTA columns:  Index(['KEY', 'NUCLIDE', 'METHOD', '< VALUE_Bq/kg', 'VALUE_Bq/kg', 'BASIS',\n",
      "       'ERROR%', 'NUMBER', 'DATE_OF_ENTRY_x', 'COUNTRY', 'LABORATORY',\n",
      "       'SEQUENCE', 'DATE', 'YEAR', 'MONTH', 'DAY', 'STATION',\n",
      "       'LATITUDE ddmmmm', 'LATITUDE dddddd', 'LONGITUDE ddmmmm',\n",
      "       'LONGITUDE dddddd', 'SDEPTH', 'RUBIN', 'BIOTATYPE', 'TISSUE', 'NO',\n",
      "       'LENGTH', 'WEIGHT', 'DW%', 'LOI%', 'MORS_SUBBASIN', 'HELCOM_SUBBASIN',\n",
      "       'DATE_OF_ENTRY_y'],\n",
      "      dtype='object')\n",
      "SEAWATER columns:  Index(['KEY', 'NUCLIDE', 'METHOD', '< VALUE_Bq/m³', 'VALUE_Bq/m³', 'ERROR%_m³',\n",
      "       'DATE_OF_ENTRY_x', 'COUNTRY', 'LABORATORY', 'SEQUENCE', 'DATE', 'YEAR',\n",
      "       'MONTH', 'DAY', 'STATION', 'LATITUDE (ddmmmm)', 'LATITUDE (dddddd)',\n",
      "       'LONGITUDE (ddmmmm)', 'LONGITUDE (dddddd)', 'TDEPTH', 'SDEPTH', 'SALIN',\n",
      "       'TTEMP', 'FILT', 'MORS_SUBBASIN', 'HELCOM_SUBBASIN', 'DATE_OF_ENTRY_y'],\n",
      "      dtype='object')\n",
      "SEDIMENT columns:  Index(['KEY', 'NUCLIDE', 'METHOD', '< VALUE_Bq/kg', 'VALUE_Bq/kg', 'ERROR%_kg',\n",
      "       '< VALUE_Bq/m²', 'VALUE_Bq/m²', 'ERROR%_m²', 'DATE_OF_ENTRY_x',\n",
      "       'COUNTRY', 'LABORATORY', 'SEQUENCE', 'DATE', 'YEAR', 'MONTH', 'DAY',\n",
      "       'STATION', 'LATITUDE (ddmmmm)', 'LATITUDE (dddddd)',\n",
      "       'LONGITUDE (ddmmmm)', 'LONGITUDE (dddddd)', 'DEVICE', 'TDEPTH',\n",
      "       'UPPSLI', 'LOWSLI', 'AREA', 'SEDI', 'OXIC', 'DW%', 'LOI%',\n",
      "       'MORS_SUBBASIN', 'HELCOM_SUBBASIN', 'SUM_LINK', 'DATE_OF_ENTRY_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "print('keys/sample types: ', dfs.keys())\n",
    "for key in dfs.keys():\n",
    "    print(f'{key} columns: ', dfs[key].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687eade",
   "metadata": {},
   "source": [
    "## Add sample type column (REMOVE STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7d15c",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**TODO**: The `samptype_id` column is added to the dataframe for legacy reasons (again Open Refine output). Soon we will use a 'decoder' to replace the open refine encoder and the openrefine csv will be created from the netcdf file. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984410e",
   "metadata": {},
   "source": [
    "The sample types (`SEAWATER`, `BIOTA`, `SEDIMENT`, ...) are encoded group names in the NetCDF file produced.\n",
    "\n",
    "To maintain compatibility with legacy systems, where we create a `csv` and parse using OpenRefine, sample type IDs are included in each DataFrame. This is acheived using the `AddSampleTypeIdColumnCB` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebeed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#| eval: false\\ndfs = load_data(fname_in)\\ntfm = Transformer(dfs, cbs=[AddSampleTypeIdColumnCB(),\\n                            CompareDfsAndTfmCB(dfs)\\n                            ])\\ntfm()\\nprint(tfm.dfs['SEAWATER'][['KEY', 'samptype_id']].head())\\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\\n\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[AddSampleTypeIdColumnCB(),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "tfm()\n",
    "print(tfm.dfs['SEAWATER'][['KEY', 'samptype_id']].head())\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "142ddab3",
   "metadata": {},
   "source": [
    "## Normalize nuclide names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a2311cd",
   "metadata": {},
   "source": [
    "### Lower & strip nuclide names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b4ceb",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: Some nuclide names contain one or multiple trailing spaces.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d84ed7",
   "metadata": {},
   "source": [
    "This is demonstrated below for the `NUCLIDE` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2306ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index      value  n_chars  stripped_chars\n",
      "3       3   AM241           8               5\n",
      "16     16    SR90           7               4\n",
      "21     21   K40             8               3\n",
      "27     27     CS137         6               5\n",
      "37     37     SR90          6               4\n",
      "55     55   CS137           8               5\n",
      "57     57    TC99           7               4\n",
      "66     66  CS137            9               5\n",
      "78     78      SR90         5               4\n",
      "80     80   PU238           8               5\n",
      "88     88   CO60            8               4\n",
      "89     89   SR90            8               4\n",
      "93     93   CS134           8               5\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "df = get_unique_across_dfs(load_data(fname_in), 'NUCLIDE', as_df=True, include_nchars=True)\n",
    "df['stripped_chars'] = df['value'].str.strip().str.replace(' ', '').str.len()\n",
    "print(df[df['n_chars'] != df['stripped_chars']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518174ba",
   "metadata": {},
   "source": [
    "To fix this issue, we use the `LowerStripNameCB` callback. For each dataframe in the dictionary of dataframes, it corrects the nuclide name by converting it lowercase, striping any leading or trailing whitespace(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fa068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIOTA nuclides: \n",
      "['cs134' 'k40' 'co60' 'cs137' 'sr90' 'ag108m' 'mn54' 'co58' 'ag110m'\n",
      " 'zn65' 'sb125' 'pu239240' 'ru106' 'be7' 'ce144' 'pb210' 'po210' 'sb124'\n",
      " 'sr89' 'zr95' 'te129m' 'ru103' 'nb95' 'ce141' 'la140' 'i131' 'ba140'\n",
      " 'pu238' 'u235' 'bi214' 'pb214' 'pb212' 'tl208' 'ac228' 'ra223' 'eu155'\n",
      " 'ra226' 'gd153' 'sn113' 'fe59' 'tc99' 'co57' 'sn117m' 'eu152' 'sc46'\n",
      " 'rb86' 'ra224' 'th232' 'cs134137' 'am241' 'ra228' 'th228' 'k-40' 'cs138'\n",
      " 'cs139' 'cs140' 'cs141' 'cs142' 'cs143' 'cs144' 'cs145' 'cs146']\n",
      "SEAWATER nuclides: \n",
      "['cs137' 'sr90' 'h3' 'cs134' 'pu238' 'pu239240' 'am241' 'cm242' 'cm244'\n",
      " 'tc99' 'k40' 'ru103' 'sr89' 'sb125' 'nb95' 'ru106' 'zr95' 'ag110m'\n",
      " 'cm243244' 'ba140' 'ce144' 'u234' 'u238' 'co60' 'pu239' 'pb210' 'po210'\n",
      " 'np237' 'pu240' 'mn54']\n",
      "SEDIMENT nuclides: \n",
      "['ra226' 'cs137' 'ra228' 'k40' 'sr90' 'cs134137' 'cs134' 'pu239240'\n",
      " 'pu238' 'co60' 'ru103' 'ru106' 'sb125' 'ag110m' 'ce144' 'am241' 'be7'\n",
      " 'th228' 'pb210' 'co58' 'mn54' 'zr95' 'ba140' 'po210' 'ra224' 'nb95'\n",
      " 'pu238240' 'pu241' 'pu239' 'eu155' 'ir192' 'th232' 'cd109' 'sb124' 'zn65'\n",
      " 'th234' 'tl208' 'pb212' 'pb214' 'bi214' 'ac228' 'ra223' 'u235' 'bi212']\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='NUCLIDE')])\n",
    "\n",
    "for key in tfm().keys():\n",
    "    print(f'{key} nuclides: ')\n",
    "    print(tfm()[key]['NUCLIDE'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52c9d0fe",
   "metadata": {},
   "source": [
    "### Remap nuclide names to MARIS data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58baf14",
   "metadata": {},
   "source": [
    "Below, we map nuclide names used by HELCOM to the MARIS standard nuclide names. \n",
    "\n",
    "Remapping data provider nomenclatures to MARIS standards is a recurrent operation and is done in a semi-automated manner according to the following pattern:\n",
    "\n",
    "1. **Inspect** data provider nomenclature:\n",
    "2. **Match** automatically against MARIS nomenclature (using a fuzzy matching algorithm); \n",
    "3. **Fix** potential mismatches; \n",
    "4. **Apply** the lookup table to the dataframe.\n",
    "\n",
    "We will refer to this process as **IMFA** (**I**nspect, **M**atch, **F**ix, **A**pply)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4b31bc",
   "metadata": {},
   "source": [
    "The `get_unique_across_dfs` function is a utility in MARISCO that retrieves unique values from a specified column across all DataFrames. \n",
    "Note that there is one DataFrame for each sample type, such as biota, sediment, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ee8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <td>cs139</td>\n",
       "      <td>gd153</td>\n",
       "      <td>tc99</td>\n",
       "      <td>ag110m</td>\n",
       "      <td>pb214</td>\n",
       "      <td>th232</td>\n",
       "      <td>sn117m</td>\n",
       "      <td>k40</td>\n",
       "      <td>ra228</td>\n",
       "      <td>ce144</td>\n",
       "      <td>...</td>\n",
       "      <td>po210</td>\n",
       "      <td>pu239</td>\n",
       "      <td>ce141</td>\n",
       "      <td>ru103</td>\n",
       "      <td>cd109</td>\n",
       "      <td>mn54</td>\n",
       "      <td>nb95</td>\n",
       "      <td>cs143</td>\n",
       "      <td>pu241</td>\n",
       "      <td>u235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1     2       3      4      5       6    7      8      9   \\\n",
       "index      0      1     2       3      4      5       6    7      8      9   \n",
       "value  cs139  gd153  tc99  ag110m  pb214  th232  sn117m  k40  ra228  ce144   \n",
       "\n",
       "       ...     67     68     69     70     71    72    73     74     75    76  \n",
       "index  ...     67     68     69     70     71    72    73     74     75    76  \n",
       "value  ...  po210  pu239  ce141  ru103  cd109  mn54  nb95  cs143  pu241  u235  \n",
       "\n",
       "[2 rows x 77 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='NUCLIDE')])\n",
    "\n",
    "dfs_output = tfm()\n",
    "\n",
    "# Transpose to display the dataframe horizontally\n",
    "get_unique_across_dfs(dfs_output, col_name='NUCLIDE', as_df=True).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614c1bdf",
   "metadata": {},
   "source": [
    "Let's now create an instance of a [fuzzy matching algorithm](https://www.wikiwand.com/en/articles/Approximate_string_matching) `Remapper`. This instance will match the nuclide names of the HELCOM dataset to the MARIS standard nuclide names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbc619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "remapper = Remapper(provider_lut_df=get_unique_across_dfs(dfs_output, col_name='NUCLIDE', as_df=True),\n",
    "                    maris_lut_fn=nuc_lut_path,\n",
    "                    maris_col_id='nuclide_id',\n",
    "                    maris_col_name='nc_name',\n",
    "                    provider_col_to_match='value',\n",
    "                    provider_col_key='value',\n",
    "                    fname_cache='nuclides_helcom.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e0ea0c",
   "metadata": {},
   "source": [
    "Lets try to match HELCOM nuclide names to MARIS standard nuclide names as automatically as possible. The `match_score` column allows to assess the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb645c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 77/77 [00:01<00:00, 39.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 entries matched the criteria, while 14 entries had a match score of 1 or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matched_maris_name</th>\n",
       "      <th>source_name</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cs134137</th>\n",
       "      <td>cs137</td>\n",
       "      <td>cs134137</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pu238240</th>\n",
       "      <td>pu240</td>\n",
       "      <td>pu238240</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cm243244</th>\n",
       "      <td>cm242</td>\n",
       "      <td>cm243244</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pu239240</th>\n",
       "      <td>pu239</td>\n",
       "      <td>pu239240</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs145</th>\n",
       "      <td>ce140</td>\n",
       "      <td>cs145</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs142</th>\n",
       "      <td>ce140</td>\n",
       "      <td>cs142</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs143</th>\n",
       "      <td>ce140</td>\n",
       "      <td>cs143</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs139</th>\n",
       "      <td>ce139</td>\n",
       "      <td>cs139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs144</th>\n",
       "      <td>cs134</td>\n",
       "      <td>cs144</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k-40</th>\n",
       "      <td>k40</td>\n",
       "      <td>k-40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs141</th>\n",
       "      <td>ce141</td>\n",
       "      <td>cs141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs138</th>\n",
       "      <td>cs134</td>\n",
       "      <td>cs138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs140</th>\n",
       "      <td>ce140</td>\n",
       "      <td>cs140</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs146</th>\n",
       "      <td>cs136</td>\n",
       "      <td>cs146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           matched_maris_name source_name  match_score\n",
       "source_key                                            \n",
       "cs134137                cs137    cs134137            3\n",
       "pu238240                pu240    pu238240            3\n",
       "cm243244                cm242    cm243244            3\n",
       "pu239240                pu239    pu239240            3\n",
       "cs145                   ce140       cs145            2\n",
       "cs142                   ce140       cs142            2\n",
       "cs143                   ce140       cs143            2\n",
       "cs139                   ce139       cs139            1\n",
       "cs144                   cs134       cs144            1\n",
       "k-40                      k40        k-40            1\n",
       "cs141                   ce141       cs141            1\n",
       "cs138                   cs134       cs138            1\n",
       "cs140                   ce140       cs140            1\n",
       "cs146                   cs136       cs146            1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "remapper.generate_lookup_table(as_df=True)\n",
    "remapper.select_match(match_score_threshold=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5cb838",
   "metadata": {},
   "source": [
    "We can now manually inspect the unmatched nuclide names and create a table to correct them to the MARIS standard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "fixes_nuclide_names = {\n",
    "    'cs134137': 'cs134_137_tot',\n",
    "    'cm243244': 'cm243_244_tot',\n",
    "    'pu239240': 'pu239_240_tot',\n",
    "    'pu238240': 'pu238_240_tot',\n",
    "    'cs143': 'cs137',\n",
    "    'cs145': 'cs137',\n",
    "    'cs142': 'cs137',\n",
    "    'cs141': 'cs137',\n",
    "    'cs144': 'cs137',\n",
    "    'k-40': 'k40',\n",
    "    'cs140': 'cs137',\n",
    "    'cs146': 'cs137',\n",
    "    'cs139': 'cs137',\n",
    "    'cs138': 'cs137'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd575e7e",
   "metadata": {},
   "source": [
    "We now include the table `fixes_nuclide_names`, which applies manual corrections to the nuclide names before the remapping process. \n",
    "The `generate_lookup_table` function has an `overwrite` parameter (default is `True`), which, when set to `True`, creates a pickle file cache of the lookup table. We can now test the remapping process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73410b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 77/77 [00:02<00:00, 36.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 entries matched the criteria, while 0 entries had a match score of 1 or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "remapper.generate_lookup_table(as_df=True, fixes=fixes_nuclide_names)\n",
    "fc.test_eq(len(remapper.select_match(match_score_threshold=1, verbose=True)), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1276f",
   "metadata": {},
   "source": [
    "Test passes! We can now create a callback `RemapNuclideNameCB` to remap the nuclide names. Note that we pass `overwrite=False` to the `Remapper` constructor to now use the cached version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a189ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "# Create a lookup table for nuclide names\n",
    "lut_nuclides = lambda df: Remapper(provider_lut_df=df,\n",
    "                                   maris_lut_fn=nuc_lut_path,\n",
    "                                   maris_col_id='nuclide_id',\n",
    "                                   maris_col_name='nc_name',\n",
    "                                   provider_col_to_match='value',\n",
    "                                   provider_col_key='value',\n",
    "                                   fname_cache='nuclides_helcom.pkl').generate_lookup_table(fixes=fixes_nuclide_names, \n",
    "                                                                                            as_df=False, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0281b22",
   "metadata": {},
   "source": [
    "We now create the callback `RemapNuclideNameCB`, which will remap the nuclide names using the `lut_nuclides` lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d47237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapNuclideNameCB(Callback):\n",
    "    \"Remap data provider nuclide names to MARIS nuclide names.\"\n",
    "    def __init__(self, \n",
    "                 fn_lut: Callable # Function that returns the lookup table dictionary\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        df_uniques = get_unique_across_dfs(tfm.dfs, col_name='NUCLIDE', as_df=True)\n",
    "        #lut = {k: v.matched_maris_name for k, v in self.fn_lut(df_uniques).items()}    \n",
    "        lut = {k: v.matched_id for k, v in self.fn_lut(df_uniques).items()}    \n",
    "        for k in tfm.dfs.keys():\n",
    "            tfm.dfs[k]['NUCLIDE'] = tfm.dfs[k]['NUCLIDE'].replace(lut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce649d7a",
   "metadata": {},
   "source": [
    "Let's see it in action, along with the `RemapRdnNameCB` callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9a9ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31,  4,  9, 33, 12, 21,  6,  8, 22, 10, 24, 77, 17,  2, 37, 41, 47,\n",
       "       23, 11, 13, 25, 16, 14, 36, 35, 29, 34, 67, 63, 46, 43, 42, 94, 55,\n",
       "       50, 40, 53, 87, 92, 86, 15,  7, 93, 85, 91, 90, 51, 59, 76, 72, 54,\n",
       "       57])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='NUCLIDE'),\n",
    "                            RemapNuclideNameCB(lut_nuclides)\n",
    "                            ])\n",
    "dfs_out = tfm()\n",
    "\n",
    "# For instance\n",
    "dfs_out['BIOTA'].NUCLIDE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91ba2d3",
   "metadata": {},
   "source": [
    "### Add Nuclide Id column (REMOVE STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc13a5",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**TODO**: The `nuclide_id` column is added to the dataframe for legacy reasons (again Open Refine output). Soon we will use a 'decoder' to replace the open refine encoder and the openrefine csv will be created from the netcdf file. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a6c352",
   "metadata": {},
   "source": [
    "The `nuclide_id` column is added to the dataframe for legacy reasons (again Open Refine output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#| eval: false\\ndfs = load_data(fname_in)\\ntfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='NUCLIDE'),\\n                            RemapNuclideNameCB(lut_nuclides),\\n                            AddNuclideIdColumnCB(col_value='NUCLIDE')\\n                            ])\\ndfs_out = tfm()\\n\\n# For instance\\ndfs_out['biota'][['NUCLIDE', 'nuclide_id']]\\n\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='NUCLIDE'),\n",
    "                            RemapNuclideNameCB(lut_nuclides),\n",
    "                            AddNuclideIdColumnCB(col_value='NUCLIDE')\n",
    "                            ])\n",
    "dfs_out = tfm()\n",
    "\n",
    "# For instance\n",
    "dfs_out['biota'][['NUCLIDE', 'nuclide_id']]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9e1f4",
   "metadata": {},
   "source": [
    "## Standardize Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24856dc5",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: Time/date is provide in the `DATE`, `YEAR`\n",
    ", `MONTH`, `DAY` columns. Note that the `DATE` contains missing values as indicated below. When missing, we fallback on the `YEAR`, `MONTH`, `DAY` columns. Note also that sometimes `DAY` and `MONTH` contain 0. In this case we systematically set them to 1.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612873e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIOTA DATE null values:  84\n",
      "SEAWATER DATE null values:  494\n",
      "SEDIMENT DATE null values:  741\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "for key in dfs.keys():\n",
    "    print(f'{key} DATE null values: ', dfs[key]['DATE'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae547a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ParseTimeCB(Callback):\n",
    "    \"Parse and standardize time information in the dataframe.\"\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for df in tfm.dfs.values():\n",
    "            self._process_dates(df)\n",
    "            # self._define_beg_period(df) # REMOVE STEP - we will use an OPEN REFINE decoder to replace the open refine encoder. \n",
    "\n",
    "    def _process_dates(self, df: pd.DataFrame) -> None:\n",
    "        \"Process and correct date and time information in the DataFrame.\"\n",
    "        df['TIME'] = self._parse_date(df)\n",
    "        self._handle_missing_dates(df)\n",
    "        self._fill_missing_time(df)\n",
    "\n",
    "    def _parse_date(self, df: pd.DataFrame) -> pd.Series:\n",
    "        \"Parse the DATE column if present.\"\n",
    "        return pd.to_datetime(df['DATE'], format='%m/%d/%y %H:%M:%S', errors='coerce')\n",
    "\n",
    "    def _handle_missing_dates(self, df: pd.DataFrame):\n",
    "        \"Handle cases where DAY or MONTH is 0 or missing.\"\n",
    "        df.loc[df[\"DAY\"] == 0, \"DAY\"] = 1\n",
    "        df.loc[df[\"MONTH\"] == 0, \"MONTH\"] = 1\n",
    "        \n",
    "        missing_day_month = (df[\"DAY\"].isna()) & (df[\"MONTH\"].isna()) & (df[\"YEAR\"].notna())\n",
    "        df.loc[missing_day_month, [\"DAY\", \"MONTH\"]] = 1\n",
    "\n",
    "    def _fill_missing_time(self, df: pd.DataFrame) -> None:\n",
    "        \"Fill missing time values using YEAR, MONTH, and DAY columns.\"\n",
    "        missing_time = df['TIME'].isna()\n",
    "        df.loc[missing_time, 'TIME'] = pd.to_datetime(\n",
    "            df.loc[missing_time, ['YEAR', 'MONTH', 'DAY']], \n",
    "            format='%Y%m%d', \n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "    ''' REMOVE STEP - we will use an OPEN REFINE decoder to replace the open refine encoder. \n",
    "    def _define_beg_period(self, df: pd.DataFrame) -> None:\n",
    "        \"Create a standardized date representation for Open Refine.\"\n",
    "        df['begperiod'] = df['TIME']\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c34819",
   "metadata": {},
   "source": [
    "Apply the transformer for callbacks `ParseTimeCB`. Then, print the `TIME` data for `seawater`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b90d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           BIOTA  SEAWATER  SEDIMENT\n",
      "Number of rows in dfs      14893     20318     37347\n",
      "Number of rows in tfm.dfs  14893     20318     37347\n",
      "Number of rows removed         0         0         0 \n",
      "\n",
      "            TIME\n",
      "0     2012-05-23\n",
      "1     2012-05-23\n",
      "2     2012-06-17\n",
      "3     2012-05-24\n",
      "4     2012-05-24\n",
      "...          ...\n",
      "20313 2015-06-22\n",
      "20314 2015-06-23\n",
      "20315 2015-06-23\n",
      "20316 2015-06-24\n",
      "20317 2015-06-24\n",
      "\n",
      "[20318 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[ParseTimeCB(),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "tfm()\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n",
    "print(tfm.dfs['SEAWATER'][['TIME']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd488a",
   "metadata": {},
   "source": [
    "The NetCDF time format requires that time be encoded as the number of milliseconds since a specified origin. In our case, the origin is `1970-01-01`, as indicated in the `cdl.toml` file under the `[vars.defaults.time.attrs]` section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b2966",
   "metadata": {},
   "source": [
    "`EncodeTimeCB` converts the HELCOM `time` format to the MARIS NetCDF `time` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8edc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1 missing time value(s) in SEDIMENT\n",
      "                           BIOTA  SEAWATER  SEDIMENT\n",
      "Number of rows in dfs      14893     20318     37347\n",
      "Number of rows in tfm.dfs  14893     20318     37346\n",
      "Number of rows removed         0         0         1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[ParseTimeCB(),\n",
    "                            EncodeTimeCB(),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "tfm()\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef4f4b",
   "metadata": {},
   "source": [
    "## Sanitize value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de49e39",
   "metadata": {},
   "source": [
    "We allocate each column containing measurement values (named differently across sample types) into a single column `VALUE` and remove NA where needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "coi_val = {'SEAWATER' : {'VALUE': 'VALUE_Bq/m³'},\n",
    "           'BIOTA':  {'VALUE': 'VALUE_Bq/kg'},\n",
    "           'SEDIMENT': {'VALUE': 'VALUE_Bq/kg'}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class SanitizeValue(Callback):\n",
    "    \"Sanitize value/measurement by removing blank entries and populating `value` column.\"\n",
    "    def __init__(self, \n",
    "                 coi: Dict[str, Dict[str, str]] # Columns of interest. Format: {group_name: {'val': 'column_name'}}\n",
    "                 ): \n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for grp, df in tfm.dfs.items():\n",
    "            value_col = self.coi[grp]['VALUE']\n",
    "            df.dropna(subset=[value_col], inplace=True)\n",
    "            df['VALUE'] = df[value_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb7a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           BIOTA  SEAWATER  SEDIMENT\n",
      "Number of rows in dfs      14893     20318     37347\n",
      "Number of rows in tfm.dfs  14873     20242     37090\n",
      "Number of rows removed        20        76       257 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[SanitizeValue(coi_val),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "\n",
    "tfm()\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be199c49",
   "metadata": {},
   "source": [
    "## Normalize uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515714b",
   "metadata": {},
   "source": [
    "Function `unc_rel2stan` converts uncertainty from relative uncertainty to standard uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76077d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def unc_rel2stan(\n",
    "    df: pd.DataFrame, # DataFrame containing measurement and uncertainty columns\n",
    "    meas_col: str, # Name of the column with measurement values\n",
    "    unc_col: str # Name of the column with relative uncertainty values (percentages)\n",
    ") -> pd.Series: # Series with calculated absolute uncertainties\n",
    "    \"Convert relative uncertainty to absolute uncertainty.\"\n",
    "    return df.apply(lambda row: row[unc_col] * row[meas_col] / 100, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2917d107",
   "metadata": {},
   "source": [
    "For each sample type in the Helcom dataset, the `UNCERTAINTY` is provided as a relative uncertainty. The column names for both the `VALUE` and the `UNCERTAINTY` vary by sample type. The `coi_units_unc` dictionary defines the column names for the `VALUE` and `UNCERTAINTY` for each sample type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "# Columns of interest\n",
    "coi_units_unc = [('SEAWATER', 'VALUE_Bq/m³', 'ERROR%_m³'),\n",
    "                 ('BIOTA', 'VALUE_Bq/kg', 'ERROR%'),\n",
    "                 ('SEDIMENT', 'VALUE_Bq/kg', 'ERROR%_kg')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c9a4b",
   "metadata": {},
   "source": [
    "NormalizeUncCB callback normalizes the ``UNCERTAINTY`` by converting from relative uncertainty to standard uncertainty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf262ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class NormalizeUncCB(Callback):\n",
    "    \"Convert from relative error % to standard uncertainty.\"\n",
    "    def __init__(self, \n",
    "                 fn_convert_unc: Callable=unc_rel2stan, # Function converting relative uncertainty to absolute uncertainty\n",
    "                 coi: List[Tuple[str, str, str]]=coi_units_unc # List of columns of interest\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "    \n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for grp, val, unc in self.coi:\n",
    "            if grp in tfm.dfs:\n",
    "                df = tfm.dfs[grp]\n",
    "                df['UNCERTAINTY'] = self.fn_convert_unc(df, val, unc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545b262",
   "metadata": {},
   "source": [
    "Apply the transformer for callback ``NormalizeUncCB``. Then, print the value (i.e. activity per unit ) and standard uncertainty for each sample type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e14e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VALUE  UNCERTAINTY\n",
      "0    5.3        1.696\n",
      "1   19.9        3.980\n",
      "2   25.5        5.100\n",
      "3   17.0        4.930\n",
      "4   22.2        3.996\n",
      "        VALUE  UNCERTAINTY\n",
      "0    0.010140          NaN\n",
      "1  135.300000     4.830210\n",
      "2    0.013980          NaN\n",
      "3    4.338000     0.150962\n",
      "4    0.009614          NaN\n",
      "   VALUE  UNCERTAINTY\n",
      "0   35.0         9.10\n",
      "1   36.0         7.92\n",
      "2   38.0         9.12\n",
      "3   36.0         9.00\n",
      "4   30.0         6.90\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[NormalizeUncCB(),\n",
    "                            SanitizeValue(coi_val)])\n",
    "tfm()\n",
    "print(tfm.dfs['SEAWATER'][['VALUE', 'UNCERTAINTY']][:5])\n",
    "print(tfm.dfs['BIOTA'][['VALUE', 'UNCERTAINTY']][:5])\n",
    "print(tfm.dfs['SEDIMENT'][['VALUE', 'UNCERTAINTY']][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392b0cb",
   "metadata": {},
   "source": [
    "## Remap Biota species"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfda9f9",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: RUBIN contains codes that are not found in the HELCOM biota dataset. \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbe37ba",
   "metadata": {},
   "source": [
    "For example, 'CH HI;BA', its not in the HELCOM biota dataset. Lets return the uniue RUBIN of the HELCOM biota dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33953cd",
   "metadata": {},
   "source": [
    "Other unused RUBIN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e11297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unused RUBIN names: ['CH HI;BA', 'SOLE SOL']\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "unique_rubin = dfs['BIOTA']['RUBIN'].unique()\n",
    "unique_rubin_set = set(unique_rubin)\n",
    "rubin_lut = list(pd.read_csv(Path(fname_in) / 'RUBIN_NAME.csv')['RUBIN'])\n",
    "unused_rubins = [rune for rune in rubin_lut if rune not in unique_rubin_set]\n",
    "print(\"Unused RUBIN names:\", unused_rubins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7129a",
   "metadata": {},
   "source": [
    "We will remap the HELCOM `RUBIN` column to the MARIS `SPECIES` column using the **IMFA** (**I**nspect, **M**atch, **F**ix, **A**pply) pattern. First lets **inspect** the `RUBIN_NAME.csv` file provided by HELCOM, which describes the nomenclature of biota species.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d3b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RUBIN_ID</th>\n",
       "      <th>RUBIN</th>\n",
       "      <th>SCIENTIFIC NAME</th>\n",
       "      <th>ENGLISH NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>ABRA BRA</td>\n",
       "      <td>ABRAMIS BRAMA</td>\n",
       "      <td>BREAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>ANGU ANG</td>\n",
       "      <td>ANGUILLA ANGUILLA</td>\n",
       "      <td>EEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>ARCT ISL</td>\n",
       "      <td>ARCTICA ISLANDICA</td>\n",
       "      <td>ISLAND CYPRINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>ASTE RUB</td>\n",
       "      <td>ASTERIAS RUBENS</td>\n",
       "      <td>COMMON STARFISH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>CARD EDU</td>\n",
       "      <td>CARDIUM EDULE</td>\n",
       "      <td>COCKLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RUBIN_ID     RUBIN    SCIENTIFIC NAME     ENGLISH NAME\n",
       "0        11  ABRA BRA      ABRAMIS BRAMA            BREAM\n",
       "1        12  ANGU ANG  ANGUILLA ANGUILLA              EEL\n",
       "2        13  ARCT ISL  ARCTICA ISLANDICA   ISLAND CYPRINE\n",
       "3        14  ASTE RUB    ASTERIAS RUBENS  COMMON STARFISH\n",
       "4        15  CARD EDU      CARDIUM EDULE           COCKLE"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "pd.read_csv(Path(fname_in) / 'RUBIN_NAME.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d23db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/home/marisco/.marisco/lut/dbo_species_2024_11_19.xlsx')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "species_lut_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1858121",
   "metadata": {},
   "source": [
    "Now we try to **MATCH** the `SCIENTIFIC NAME` column of HELCOM biota dataset to the `species` column of the MARIS species lookup table, again using a `Remapper` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45da37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 43/43 [00:07<00:00,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 entries matched the criteria, while 8 entries had a match score of 1 or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matched_maris_name</th>\n",
       "      <th>source_name</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STIZ LUC</th>\n",
       "      <td>Sander lucioperca</td>\n",
       "      <td>STIZOSTEDION LUCIOPERCA</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAMI SAC</th>\n",
       "      <td>Laminaria japonica</td>\n",
       "      <td>LAMINARIA SACCHARINA</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CARD EDU</th>\n",
       "      <td>Cardiidae</td>\n",
       "      <td>CARDIUM EDULE</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CH HI;BA</th>\n",
       "      <td>Macoma balthica</td>\n",
       "      <td>CHARA BALTICA</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENCH CIM</th>\n",
       "      <td>Echinodermata</td>\n",
       "      <td>ENCHINODERMATA CIM</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PSET MAX</th>\n",
       "      <td>Pinctada maxima</td>\n",
       "      <td>PSETTA MAXIMA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACO BAL</th>\n",
       "      <td>Macoma balthica</td>\n",
       "      <td>MACOMA BALTICA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STUC PEC</th>\n",
       "      <td>Stuckenia pectinata</td>\n",
       "      <td>STUCKENIA PECTINATE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             matched_maris_name              source_name  match_score\n",
       "source_key                                                           \n",
       "STIZ LUC      Sander lucioperca  STIZOSTEDION LUCIOPERCA           10\n",
       "LAMI SAC     Laminaria japonica     LAMINARIA SACCHARINA            7\n",
       "CARD EDU              Cardiidae            CARDIUM EDULE            6\n",
       "CH HI;BA        Macoma balthica            CHARA BALTICA            6\n",
       "ENCH CIM          Echinodermata       ENCHINODERMATA CIM            5\n",
       "PSET MAX        Pinctada maxima            PSETTA MAXIMA            5\n",
       "MACO BAL        Macoma balthica           MACOMA BALTICA            1\n",
       "STUC PEC    Stuckenia pectinata      STUCKENIA PECTINATE            1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "remapper = Remapper(provider_lut_df=pd.read_csv(Path(fname_in) / 'RUBIN_NAME.csv'),\n",
    "                    maris_lut_fn=species_lut_path,\n",
    "                    maris_col_id='species_id',\n",
    "                    maris_col_name='species',\n",
    "                    provider_col_to_match='SCIENTIFIC NAME',\n",
    "                    provider_col_key='RUBIN',\n",
    "                    fname_cache='species_helcom.pkl'\n",
    "                    )\n",
    "\n",
    "remapper.generate_lookup_table(as_df=True)\n",
    "remapper.select_match(match_score_threshold=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3b8aa",
   "metadata": {},
   "source": [
    "Below, we will correct the entries that were not properly matched by the `Remapper` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8290222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "fixes_biota_species = {\n",
    "    'CHARA BALTICA': 'NOT AVAILABLE', # CHARA BALTICA (RUBIN: CH HI;BA) is not listed in the biota data. \n",
    "    'CARDIUM EDULE': 'Cerastoderma edule',\n",
    "    'LAMINARIA SACCHARINA': 'Saccharina latissima',\n",
    "    'PSETTA MAXIMA': 'Scophthalmus maximus',\n",
    "    'STIZOSTEDION LUCIOPERCA': 'Sander luciopercas'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781f210",
   "metadata": {},
   "source": [
    "And give the ``remapper`` another try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb07a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 43/43 [00:07<00:00,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 entries matched the criteria, while 4 entries had a match score of 1 or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matched_maris_name</th>\n",
       "      <th>source_name</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENCH CIM</th>\n",
       "      <td>Echinodermata</td>\n",
       "      <td>ENCHINODERMATA CIM</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MACO BAL</th>\n",
       "      <td>Macoma balthica</td>\n",
       "      <td>MACOMA BALTICA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STIZ LUC</th>\n",
       "      <td>Sander lucioperca</td>\n",
       "      <td>STIZOSTEDION LUCIOPERCA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STUC PEC</th>\n",
       "      <td>Stuckenia pectinata</td>\n",
       "      <td>STUCKENIA PECTINATE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             matched_maris_name              source_name  match_score\n",
       "source_key                                                           \n",
       "ENCH CIM          Echinodermata       ENCHINODERMATA CIM            5\n",
       "MACO BAL        Macoma balthica           MACOMA BALTICA            1\n",
       "STIZ LUC      Sander lucioperca  STIZOSTEDION LUCIOPERCA            1\n",
       "STUC PEC    Stuckenia pectinata      STUCKENIA PECTINATE            1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "remapper.generate_lookup_table(fixes=fixes_biota_species)\n",
    "remapper.select_match(match_score_threshold=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17064a5",
   "metadata": {},
   "source": [
    "Visual inspection of the remaining unperfectly matched entries seem acceptable to proceed. \n",
    "\n",
    "We can now use the generic `RemapCB` callback to perform the remapping of the `RUBIN` column to the `species` column after having defined the lookup table `lut_biota`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "lut_biota = lambda: Remapper(provider_lut_df=pd.read_csv(Path(fname_in) / 'RUBIN_NAME.csv'),\n",
    "                             maris_lut_fn=species_lut_path,\n",
    "                             maris_col_id='species_id',\n",
    "                             maris_col_name='species',\n",
    "                             provider_col_to_match='SCIENTIFIC NAME',\n",
    "                             provider_col_key='RUBIN',\n",
    "                             fname_cache='species_helcom.pkl'\n",
    "                             ).generate_lookup_table(fixes=fixes_biota_species, as_df=False, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7321b5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  99  243   50  139  270  192  191  284   84  269  122   96  287  279\n",
      "  278  288  286  244  129  275  271  285  283  247  120   59  280  274\n",
      "  273  290  289  272  277  276   21  282  110  281  245  704 1524]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "    RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='RUBIN', dest_grps='BIOTA')\n",
    "    ])\n",
    "tfm()\n",
    "tfm.dfs['BIOTA'].columns\n",
    "# For instance:\n",
    "print(tfm.dfs['BIOTA']['SPECIES'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c74e492",
   "metadata": {},
   "source": [
    "## Remap Biota tissues\n",
    "Let's inspect the `TISSUE.csv` file provided by HELCOM describing the tissue nomenclature. Biota tissue is known as `body part` in the maris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38df50b-46a9-4a2d-9379-e670eb0d0bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TISSUE</th>\n",
       "      <th>TISSUE_DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>WHOLE FISH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>WHOLE FISH WITHOUT ENTRAILS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>WHOLE FISH WITHOUT HEAD AND ENTRAILS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>FLESH WITH BONES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>FLESH WITHOUT BONES (FILETS)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TISSUE                    TISSUE_DESCRIPTION\n",
       "0       1                            WHOLE FISH\n",
       "1       2           WHOLE FISH WITHOUT ENTRAILS\n",
       "2       3  WHOLE FISH WITHOUT HEAD AND ENTRAILS\n",
       "3       4                      FLESH WITH BONES\n",
       "4       5          FLESH WITHOUT BONES (FILETS)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "pd.read_csv('../../_data/accdb/mors/csv/TISSUE.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613f239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/29 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 29/29 [00:00<00:00, 93.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 entries matched the criteria, while 8 entries had a match score of 1 or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matched_maris_name</th>\n",
       "      <th>source_name</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Flesh without bones</td>\n",
       "      <td>WHOLE FISH WITHOUT HEAD AND ENTRAILS</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flesh without bones</td>\n",
       "      <td>WHOLE FISH WITHOUT ENTRAILS</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Soft parts</td>\n",
       "      <td>SKIN/EPIDERMIS</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Flesh without bones</td>\n",
       "      <td>FLESH WITHOUT BONES (FILETS)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whole animal</td>\n",
       "      <td>WHOLE FISH</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Brain</td>\n",
       "      <td>ENTRAILS</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stomach and intestine</td>\n",
       "      <td>STOMACH + INTESTINE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Whole animal</td>\n",
       "      <td>WHOLE ANIMALS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               matched_maris_name                           source_name  \\\n",
       "source_key                                                                \n",
       "3             Flesh without bones  WHOLE FISH WITHOUT HEAD AND ENTRAILS   \n",
       "2             Flesh without bones           WHOLE FISH WITHOUT ENTRAILS   \n",
       "8                      Soft parts                        SKIN/EPIDERMIS   \n",
       "5             Flesh without bones          FLESH WITHOUT BONES (FILETS)   \n",
       "1                    Whole animal                            WHOLE FISH   \n",
       "12                          Brain                              ENTRAILS   \n",
       "15          Stomach and intestine                   STOMACH + INTESTINE   \n",
       "41                   Whole animal                         WHOLE ANIMALS   \n",
       "\n",
       "            match_score  \n",
       "source_key               \n",
       "3                    20  \n",
       "2                    13  \n",
       "8                    10  \n",
       "5                     9  \n",
       "1                     5  \n",
       "12                    5  \n",
       "15                    3  \n",
       "41                    1  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "remapper = Remapper(provider_lut_df=pd.read_csv('../../_data/accdb/mors/csv/TISSUE.csv'),\n",
    "                    maris_lut_fn=bodyparts_lut_path,\n",
    "                    maris_col_id='bodypar_id',\n",
    "                    maris_col_name='bodypar',\n",
    "                    provider_col_to_match='TISSUE_DESCRIPTION',\n",
    "                    provider_col_key='TISSUE',\n",
    "                    fname_cache='tissues_helcom.pkl'\n",
    "                    )\n",
    "\n",
    "remapper.generate_lookup_table(as_df=True)\n",
    "remapper.select_match(match_score_threshold=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee1bb9",
   "metadata": {},
   "source": [
    "We address several entries that were not correctly matched by the Remapper object, as detailed below:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2b06f-5eb1-4708-8087-75c836f08112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "fixes_biota_tissues = {\n",
    "    'WHOLE FISH WITHOUT HEAD AND ENTRAILS': 'Whole animal eviscerated without head',\n",
    "    'ENTRAILS': 'Viscera',\n",
    "    'SKIN/EPIDERMIS': 'Skin'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fc4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  24%|██▍       | 7/29 [00:00<00:00, 59.01it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 29/29 [00:00<00:00, 61.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 entries matched the criteria, while 5 entries had a match score of 1 or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matched_maris_name</th>\n",
       "      <th>source_name</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flesh without bones</td>\n",
       "      <td>WHOLE FISH WITHOUT ENTRAILS</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Flesh without bones</td>\n",
       "      <td>FLESH WITHOUT BONES (FILETS)</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whole animal</td>\n",
       "      <td>WHOLE FISH</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stomach and intestine</td>\n",
       "      <td>STOMACH + INTESTINE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Whole animal</td>\n",
       "      <td>WHOLE ANIMALS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               matched_maris_name                   source_name  match_score\n",
       "source_key                                                                  \n",
       "2             Flesh without bones   WHOLE FISH WITHOUT ENTRAILS           13\n",
       "5             Flesh without bones  FLESH WITHOUT BONES (FILETS)            9\n",
       "1                    Whole animal                    WHOLE FISH            5\n",
       "15          Stomach and intestine           STOMACH + INTESTINE            3\n",
       "41                   Whole animal                 WHOLE ANIMALS            1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "remapper.generate_lookup_table(as_df=True, fixes=fixes_biota_tissues)\n",
    "remapper.select_match(match_score_threshold=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef75cb1",
   "metadata": {},
   "source": [
    "Visual inspection of the remaining unperfectly matched entries seem acceptable to proceed. \n",
    "\n",
    "We can now use the generic `RemapCB` callback to perform the remapping of the `TISSUE` column to the `body_part` column after having defined the lookup table `lut_tissues`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "lut_tissues = lambda: Remapper(provider_lut_df=pd.read_csv('../../_data/accdb/mors/csv/TISSUE.csv'),\n",
    "                               maris_lut_fn=bodyparts_lut_path,\n",
    "                               maris_col_id='bodypar_id',\n",
    "                               maris_col_name='bodypar',\n",
    "                               provider_col_to_match='TISSUE_DESCRIPTION',\n",
    "                               provider_col_key='TISSUE',\n",
    "                               fname_cache='tissues_helcom.pkl'\n",
    "                               ).generate_lookup_table(fixes=fixes_biota_tissues, as_df=False, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1887c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TISSUE  BODY_PART\n",
      "0       5         52\n",
      "1       5         52\n",
      "2       5         52\n",
      "3       5         52\n",
      "4       5         52\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "    RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='RUBIN', dest_grps='BIOTA'),\n",
    "    RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='TISSUE', dest_grps='BIOTA'),\n",
    "    ])\n",
    "\n",
    "print(tfm()['BIOTA'][['TISSUE', 'BODY_PART']][:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc596011",
   "metadata": {},
   "source": [
    "## Remap biogroup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42ebe6",
   "metadata": {},
   "source": [
    "`lut_biogroup` reads the file at `species_lut_path()` and from the contents of this file creates a dictionary linking `species_id` to `biogroup_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf290302",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "lut_biogroup = lambda: get_lut(src_dir=species_lut_path().parent, fname=species_lut_path().name, \n",
    "                               key='species_id', value='biogroup_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a37157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  2 14 11  8  3]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "    RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='RUBIN', dest_grps='BIOTA'),\n",
    "    RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='TISSUE', dest_grps='BIOTA'),\n",
    "    RemapCB(fn_lut=lut_biogroup, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA')\n",
    "    ])\n",
    "\n",
    "print(tfm()['BIOTA']['BIO_GROUP'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea8647",
   "metadata": {},
   "source": [
    "## Remap Taxon Information (REVIEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8f058",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK FOR NEXT VERSION**: \n",
    "The taxon information is not included in the NetCDF encoding. However, it is used for importing into the MARIS master database via OpenRefine. The `SPECIES` column is used to look up the taxon information. This section should be moved to the `open refine decoder notebook`. Will the `SPECIES` LUT be encoded in the NetCDF enums? \n",
    "\n",
    "For now I have removed the `Taxonname`, `TaxonRepName` and `Taxonrank` as they contain strings which will not be encoded in the NetCDF.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d52dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#| exports\\n# TODO: Include Commonname field after next MARIS data reconciling process.\\ndef get_taxon_info_lut(\\n    maris_lut:str # Path to the MARIS lookup table (Excel file)\\n) -> dict: # A dictionary mapping species_id to biogroup_id\\n    \"Retrieve a lookup table for Taxonname from a MARIS lookup table.\"\\n    species = pd.read_excel(maris_lut)\\n    return species[[\\'species_id\\', \\'Taxonname\\', \\'Taxonrank\\',\\'TaxonDB\\',\\'TaxonDBID\\',\\'TaxonDBURL\\']].set_index(\\'species_id\\').to_dict()\\n\\nlut_taxon = lambda: get_taxon_info_lut(species_lut_path())\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| exports\n",
    "# TODO: Include Commonname field after next MARIS data reconciling process.\n",
    "def get_taxon_info_lut(\n",
    "    maris_lut:str # Path to the MARIS lookup table (Excel file)\n",
    ") -> dict: # A dictionary mapping species_id to biogroup_id\n",
    "    \"Retrieve a lookup table for Taxonname from a MARIS lookup table.\"\n",
    "    species = pd.read_excel(maris_lut)\n",
    "    return species[['species_id', 'Taxonname', 'Taxonrank','TaxonDB','TaxonDBID','TaxonDBURL']].set_index('species_id').to_dict()\n",
    "\n",
    "lut_taxon = lambda: get_taxon_info_lut(species_lut_path())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04111c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# | exports\\nclass RemapTaxonInformationCB(Callback):\\n    \"Update taxon information based on MARIS species LUT.\"\\n    def __init__(self, fn_lut: Callable):\\n        self.fn_lut = fn_lut\\n\\n    def __call__(self, tfm: Transformer):\\n        lut = self.fn_lut()\\n        df = tfm.dfs[\\'BIOTA\\']\\n        \\n        df[\\'TaxonRepName\\'] = df.get(\\'RUBIN\\', \\'Unknown\\')\\n        \\n        taxon_columns = [\\'Taxonname\\', \\'Taxonrank\\', \\'TaxonDB\\', \\'TaxonDBID\\', \\'TaxonDBURL\\']\\n        for col in taxon_columns:\\n            df[col] = df[\\'SPECIES\\'].map(lut[col]).fillna(\\'Unknown\\')\\n        \\n        unmatched = df[df[\\'Taxonname\\'] == \\'Unknown\\'][\\'SPECIES\\'].unique()\\n        if len(unmatched) > 0:\\n            print(f\"Unmatched species IDs: {\\', \\'.join(unmatched)}\")\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# | exports\n",
    "class RemapTaxonInformationCB(Callback):\n",
    "    \"Update taxon information based on MARIS species LUT.\"\n",
    "    def __init__(self, fn_lut: Callable):\n",
    "        self.fn_lut = fn_lut\n",
    "\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        lut = self.fn_lut()\n",
    "        df = tfm.dfs['BIOTA']\n",
    "        \n",
    "        df['TaxonRepName'] = df.get('RUBIN', 'Unknown')\n",
    "        \n",
    "        taxon_columns = ['Taxonname', 'Taxonrank', 'TaxonDB', 'TaxonDBID', 'TaxonDBURL']\n",
    "        for col in taxon_columns:\n",
    "            df[col] = df['SPECIES'].map(lut[col]).fillna('Unknown')\n",
    "        \n",
    "        unmatched = df[df['Taxonname'] == 'Unknown']['SPECIES'].unique()\n",
    "        if len(unmatched) > 0:\n",
    "            print(f\"Unmatched species IDs: {', '.join(unmatched)}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7c54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#| eval: false\\ndfs = load_data(fname_in)\\ntfm = Transformer(dfs, cbs=[ \\n                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='RUBIN', dest_grps='BIOTA'),\\n                            RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='TISSUE', dest_grps='BIOTA'),\\n                            RemapCB(fn_lut=lut_biogroup, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA'),\\n                            RemapTaxonInformationCB(lut_taxon)\\n                            ])\\ntfm()\\nprint(tfm.dfs['biota'][['TaxonRepName', 'Taxonname', 'Taxonrank',\\n                        'TaxonDB','TaxonDBID','TaxonDBURL']].drop_duplicates().head())\\n\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[ \n",
    "                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='RUBIN', dest_grps='BIOTA'),\n",
    "                            RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='TISSUE', dest_grps='BIOTA'),\n",
    "                            RemapCB(fn_lut=lut_biogroup, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA'),\n",
    "                            RemapTaxonInformationCB(lut_taxon)\n",
    "                            ])\n",
    "tfm()\n",
    "print(tfm.dfs['biota'][['TaxonRepName', 'Taxonname', 'Taxonrank',\n",
    "                        'TaxonDB','TaxonDBID','TaxonDBURL']].drop_duplicates().head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf607d",
   "metadata": {},
   "source": [
    "## Remap Sediment types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f938d40",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: The `SEDI` values `56` and `73` are not found in the `SEDIMENT_TYPE.csv` lookup table provided. Note also there are many `nan` values in the `SEDIMENT_TYPE.csv` file.\n",
    "\n",
    "We reassign them to `-99` for now but should be clarified/fixed. This is demonstrated below.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb4547f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing SEDI values: {56.0, 73.0, nan}\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "df_sed_lut = pd.read_csv(Path(fname_in) / 'SEDIMENT_TYPE.csv')\n",
    "dfs = load_data(fname_in)\n",
    "\n",
    "sediment_sedi = set(dfs['SEDIMENT'].SEDI.unique())\n",
    "lookup_sedi = set(df_sed_lut['SEDI'])\n",
    "missing = sediment_sedi - lookup_sedi\n",
    "print(f\"Missing SEDI values: {missing if missing else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d1b5c5",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK FOR NEXT VERSION**: \n",
    " ``SedRepName`` is used by OpenRefine. ``SedRepName`` is not included in the NetCDF encoding. Description of the `SedRepName` from [MARIS Data Formats\n",
    "](https://github.com/franckalbinet/marisco/tree/main/install_configure_guide); 'Name of the sediment as reported by the data provider. The sediment name should be stored exactly as provided, without any modifications'. \n",
    "\n",
    "This information will be lost with the latest workflow (creating netcdf and decoding to csv) if we do not include strings in the netcdf encoding. What should we do? For now, I will remove the open refine requirement from the RemapSedimentCB callback. Note I have commented out `SedRepName` in the callback.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffefc5",
   "metadata": {},
   "source": [
    "Once again, we employ the **IMFA** (Inspect, Match, Fix, Apply) pattern to remap the HELCOM sediment types. Let's inspect the `SEDIMENT_TYPE.csv` file provided by HELCOM describing the sediment type nomenclature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6b82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEDI</th>\n",
       "      <th>SEDIMENT TYPE</th>\n",
       "      <th>RECOMMENDED TO BE USED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-99</td>\n",
       "      <td>NO DATA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>SILT AND GRAVEL</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>GRAVEL</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>SAND</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>FINE SAND</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEDI    SEDIMENT TYPE RECOMMENDED TO BE USED\n",
       "0   -99          NO DATA                    NaN\n",
       "1    30  SILT AND GRAVEL                    YES\n",
       "2     0           GRAVEL                    YES\n",
       "3     1             SAND                    YES\n",
       "4     2        FINE SAND                     NO"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "pd.read_csv(Path(fname_in) / 'SEDIMENT_TYPE.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e669f",
   "metadata": {},
   "source": [
    "Let's try to match as many as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 47/47 [00:00<00:00, 94.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 entries matched the criteria, while 3 entries had a match score of 1 or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matched_maris_name</th>\n",
       "      <th>source_name</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-99</th>\n",
       "      <td>Soft</td>\n",
       "      <td>NO DATA</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Mud and gravel</td>\n",
       "      <td>MUD AND GARVEL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Glacial clay</td>\n",
       "      <td>CLACIAL CLAY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           matched_maris_name     source_name  match_score\n",
       "source_key                                                \n",
       "-99                      Soft         NO DATA            5\n",
       " 50            Mud and gravel  MUD AND GARVEL            2\n",
       " 46              Glacial clay    CLACIAL CLAY            1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "remapper = Remapper(provider_lut_df=pd.read_csv(Path(fname_in)/'SEDIMENT_TYPE.csv'),\n",
    "                    maris_lut_fn=sediments_lut_path,\n",
    "                    maris_col_id='sedtype_id',\n",
    "                    maris_col_name='sedtype',\n",
    "                    provider_col_to_match='SEDIMENT TYPE',\n",
    "                    provider_col_key='SEDI',\n",
    "                    fname_cache='sediments_helcom.pkl'\n",
    "                    )\n",
    "\n",
    "remapper.generate_lookup_table(as_df=True)\n",
    "remapper.select_match(match_score_threshold=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a92e4",
   "metadata": {},
   "source": [
    "We address the remaining unmatched values by adding fixes_sediments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea46125",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "fixes_sediments = {\n",
    "    'NO DATA': '(Not available)'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05728a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 47/47 [00:00<00:00, 96.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 entries matched the criteria, while 2 entries had a match score of 1 or higher.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matched_maris_name</th>\n",
       "      <th>source_name</th>\n",
       "      <th>match_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Mud and gravel</td>\n",
       "      <td>MUD AND GARVEL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Glacial clay</td>\n",
       "      <td>CLACIAL CLAY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           matched_maris_name     source_name  match_score\n",
       "source_key                                                \n",
       "50             Mud and gravel  MUD AND GARVEL            2\n",
       "46               Glacial clay    CLACIAL CLAY            1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "remapper.generate_lookup_table(as_df=True, fixes=fixes_sediments)\n",
    "remapper.select_match(match_score_threshold=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152c8c7",
   "metadata": {},
   "source": [
    "A visual inspection of the remaining values shows that they are acceptable to proceed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16ca26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapSedimentCB(Callback):\n",
    "    \"Update sediment id based on MARIS species LUT (dbo_sedtype.xlsx).\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 fn_lut: Callable,  # Function that returns the lookup table dictionary\n",
    "                 sed_grp_name: str = 'SEDIMENT',  # The name of the sediment group\n",
    "                 replace_lut: dict = None  # Dictionary for replacing SEDI values\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"Remap sediment types in the DataFrame using the lookup table and handle specific replacements.\"\n",
    "        lut = self.fn_lut()\n",
    "        \n",
    "        # Fix inconsistent SEDI values\n",
    "        tfm.dfs[self.sed_grp_name] = self._fix_inconsistent_sedi(tfm.dfs[self.sed_grp_name], self.replace_lut)\n",
    "        \n",
    "        # Get unique SEDI values\n",
    "        unique_sedi = tfm.dfs[self.sed_grp_name]['SEDI'].unique()\n",
    "        \n",
    "        # Get sediment types for unique SEDI values\n",
    "        sediment_mapping = self._get_sediment_types(unique_sedi, lut)\n",
    "        \n",
    "        # Replace SEDI values in the DataFrame using the mapping\n",
    "        tfm.dfs[self.sed_grp_name]['SED_TYPE'] = tfm.dfs[self.sed_grp_name]['SEDI'].map(sediment_mapping)\n",
    "\n",
    "    def _fix_inconsistent_sedi(self, df: pd.DataFrame, replace_lut: dict) -> pd.DataFrame:\n",
    "        \"Temporary fix for inconsistent SEDI values. Data provider to confirm and clarify.\"\n",
    "        df['SEDI'] = df['SEDI'].replace(replace_lut)\n",
    "        return df\n",
    "\n",
    "    def _get_sediment_types(self, unique_sedi: np.ndarray, lut: dict) -> dict:\n",
    "        \"Get sediment types for unique SEDI values and return a mapping dictionary.\"\n",
    "        sediment_mapping = {}\n",
    "        \n",
    "        for sedi_value in unique_sedi:\n",
    "            match = lut.get(sedi_value, Match(0, None, None, None))\n",
    "            if match.matched_id == 0:\n",
    "                self._print_unmatched_sedi(sedi_value)\n",
    "            sediment_mapping[sedi_value] = match.matched_id\n",
    "        \n",
    "        return sediment_mapping\n",
    "\n",
    "    def _print_unmatched_sedi(self, \n",
    "                              sedi_value: int,  # The `SEDI` value from the DataFrame\n",
    "                             ) -> None:\n",
    "        \"Print the SEDI value if the matched_id is 0 (i.e. Not available).\"\n",
    "        print(f\"Unmatched SEDI: {sedi_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "lut_sediments = lambda: Remapper(provider_lut_df=pd.read_csv(Path(fname_in) / 'SEDIMENT_TYPE.csv'),\n",
    "                                 maris_lut_fn=sediments_lut_path,\n",
    "                                 maris_col_id='sedtype_id',\n",
    "                                 maris_col_name='sedtype',\n",
    "                                 provider_col_to_match='SEDIMENT TYPE',\n",
    "                                 provider_col_key='SEDI',\n",
    "                                 fname_cache='sediments_helcom.pkl'\n",
    "                                 ).generate_lookup_table(fixes=fixes_sediments, as_df=False, overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e7e02",
   "metadata": {},
   "source": [
    "Reassign the `SEDI` values of `56`, `73`, and `nan` to `-99`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63482233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "sed_replace_lut = {\n",
    "    56: -99,\n",
    "    73: -99,\n",
    "    np.nan: -99\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d334ac",
   "metadata": {},
   "source": [
    "Utilize the RemapSedimentCB callback to remap the SEDI values in the HELCOM dataset to the corresponding MARIS standard sediment type, referred to as SED_TYPE. After the remapping process, display the SEDI and SED_TYPE columns from the SEDIMENT DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25495b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched SEDI: -99.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  2, 58, 30, 59, 55, 56, 36, 29, 47,  4, 54, 33,  6, 44, 42, 48,\n",
       "       61, 57, 28, 49, 32, 45, 39, 46, 38, 31, 60, 62, 26, 53, 52,  1, 51,\n",
       "       37, 34, 50,  7, 10, 41, 43, 35])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[RemapSedimentCB(fn_lut=lut_sediments, replace_lut=sed_replace_lut)])\n",
    "\n",
    "tfm()\n",
    "\n",
    "tfm.dfs['SEDIMENT']['SED_TYPE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0add1",
   "metadata": {},
   "source": [
    "## Remap units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4064ed",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: The handling of unit types varies between `biota` and `sediment` sample types. For consistency and ease of use, it would be beneficial to have dedicated unit columns for all sample types.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a682ac",
   "metadata": {},
   "source": [
    "Given the inconsistent handling of units across sample types, we need to define custom mapping rules for standardizing the units. The units available in MARIS are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab93970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>unit</th>\n",
       "      <th>unit_sanitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NOT AVAILABLE</td>\n",
       "      <td>NOT AVAILABLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Bq/m3</td>\n",
       "      <td>Bq per m3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Bq/m2</td>\n",
       "      <td>Bq per m2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Bq/kg</td>\n",
       "      <td>Bq per kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>Bq/kgd</td>\n",
       "      <td>Bq per kgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Bq/kgw</td>\n",
       "      <td>Bq per kgw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>kg/kg</td>\n",
       "      <td>kg per kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>TU</td>\n",
       "      <td>TU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>DELTA/mill</td>\n",
       "      <td>DELTA per mill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>atom/kg</td>\n",
       "      <td>atom per kg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>atom/kgd</td>\n",
       "      <td>atom per kgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>atom/kgw</td>\n",
       "      <td>atom per kgw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>atom/l</td>\n",
       "      <td>atom per l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>Bq/kgC</td>\n",
       "      <td>Bq per kgC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    unit_id            unit  unit_sanitized\n",
       "0        -1  Not applicable  Not applicable\n",
       "1         0   NOT AVAILABLE   NOT AVAILABLE\n",
       "2         1           Bq/m3       Bq per m3\n",
       "3         2           Bq/m2       Bq per m2\n",
       "4         3           Bq/kg       Bq per kg\n",
       "5         4          Bq/kgd      Bq per kgd\n",
       "6         5          Bq/kgw      Bq per kgw\n",
       "7         6           kg/kg       kg per kg\n",
       "8         7              TU              TU\n",
       "9         8      DELTA/mill  DELTA per mill\n",
       "10        9         atom/kg     atom per kg\n",
       "11       10        atom/kgd    atom per kgd\n",
       "12       11        atom/kgw    atom per kgw\n",
       "13       12          atom/l      atom per l\n",
       "14       13          Bq/kgC      Bq per kgC"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "pd.read_excel(unit_lut_path())[['unit_id', 'unit', 'unit_sanitized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cbefe4",
   "metadata": {},
   "source": [
    "We define unit renaming rules for HELCOM in an **ad hoc** way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a86baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "lut_units = {\n",
    "    'SEAWATER': 1,  # 'Bq/m3'\n",
    "    'SEDIMENT': 4,  # 'Bq/kgd' for sediment\n",
    "    'BIOTA': {\n",
    "        'D': 4,  # 'Bq/kgd'\n",
    "        'W': 5,  # 'Bq/kgw'\n",
    "        'F': 5   # 'Bq/kgw' (assumed to be 'Fresh', so set to wet)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e404d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapUnitCB(Callback):\n",
    "    \"Set the `unit` id column in the DataFrames based on a lookup table.\"\n",
    "    def __init__(self, \n",
    "                 lut_units: dict=lut_units # Dictionary containing renaming rules for different unit categories\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for grp in tfm.dfs.keys():\n",
    "            if grp in ['SEAWATER', 'SEDIMENT']:\n",
    "                tfm.dfs[grp]['UNIT'] = self.lut_units[grp]\n",
    "            else:\n",
    "                tfm.dfs[grp]['UNIT'] = tfm.dfs[grp]['BASIS'].apply(lambda x: lut_units[grp].get(x, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a03fcc9",
   "metadata": {},
   "source": [
    "Apply the transformer for callback `RemapUnitCB()`. Then, print the unique `UNIT` for the `SEAWATER` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0f0abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIOTA: [5 0 4]\n",
      "SEDIMENT: [4]\n",
      "SEAWATER: [1]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[RemapUnitCB()])\n",
    "\n",
    "for grp in ['BIOTA', 'SEDIMENT', 'SEAWATER']:\n",
    "    print(f\"{grp}: {tfm()[grp]['UNIT'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d978c67",
   "metadata": {},
   "source": [
    "## Remap detection limit\n",
    "Detection limits are encoded as follows in MARIS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b07268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>name_sanitized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>=</td>\n",
       "      <td>Detected value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>Detection limit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>ND</td>\n",
       "      <td>Not detected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>DE</td>\n",
       "      <td>Derived</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            name   name_sanitized\n",
       "0  -1  Not applicable   Not applicable\n",
       "1   0   Not Available    Not available\n",
       "2   1               =   Detected value\n",
       "3   2               <  Detection limit\n",
       "4   3              ND     Not detected\n",
       "5   4              DE          Derived"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "pd.read_excel(detection_limit_lut_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7083b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "lut_dl = lambda: pd.read_excel(detection_limit_lut_path(), usecols=['name','id']).set_index('name').to_dict()['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023ddb4",
   "metadata": {},
   "source": [
    "Based on columns of interest for each sample type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc43c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "coi_dl = {'SEAWATER' : {'VALUE' : 'VALUE_Bq/m³',\n",
    "                       'UNCERTAINTY' : 'ERROR%_m³',\n",
    "                       'DL' : '< VALUE_Bq/m³'},\n",
    "          'BIOTA':  {'VALUE' : 'VALUE_Bq/kg',\n",
    "                     'UNCERTAINTY' : 'ERROR%',\n",
    "                     'DL' : '< VALUE_Bq/kg'},\n",
    "          'SEDIMENT': {\n",
    "              'VALUE' : 'VALUE_Bq/kg',\n",
    "              'UNCERTAINTY' : 'ERROR%_kg',\n",
    "              'DL' : '< VALUE_Bq/kg'}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8ac6a6",
   "metadata": {},
   "source": [
    "We follow the following business logic to encode the detection limit:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f4784b",
   "metadata": {},
   "source": [
    "`RemapDetectionLimitCB` creates a `detection_limit` column with values determined as follows:\n",
    "1. Perform a lookup with the appropriate columns value type (or DL) columns (`< VALUE_Bq/m³` or `< VALUE_Bq/kg`) against the table returned from the function `get_detectionlimit_lut`.\n",
    "2. If `< VALUE_Bq/m³` or `< VALUE_Bq/kg` is NaN but both activity values (`VALUE_Bq/m³` or `VALUE_Bq/kg`) and standard uncertainty (`ERROR%_m³`, `ERROR%`, or `ERROR%_kg`) are provided, then assign the ID of `1` (i.e. \"Detected value\").\n",
    "3. For other NaN values in the `detection_limit` column, set them to `0` (i.e. `Not Available`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "class RemapDetectionLimitCB(Callback):\n",
    "    \"Remap value type to MARIS format.\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 coi: dict,  # Configuration options for column names\n",
    "                 fn_lut: Callable  # Function that returns a lookup table\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"Remap detection limits in the DataFrames using the lookup table.\"\n",
    "        lut = self.fn_lut()\n",
    "        \n",
    "        for grp in tfm.dfs:\n",
    "            df = tfm.dfs[grp]\n",
    "            self._update_detection_limit(df, grp, lut)\n",
    "\n",
    "    def _update_detection_limit(self, \n",
    "                                df: pd.DataFrame,  # The DataFrame to modify\n",
    "                                grp: str,  # The group name to get the column configuration\n",
    "                                lut: dict  # The lookup table dictionary\n",
    "                               ) -> None:\n",
    "        \"Update detection limit column in the DataFrame based on lookup table and rules.\"\n",
    "        \n",
    "        # Check if the group exists in coi_dl\n",
    "        if grp not in coi_dl:\n",
    "            raise ValueError(f\"Group '{grp}' not found in coi_dl configuration.\")\n",
    "        \n",
    "        # Access column names from coi_dl\n",
    "        value_col = coi_dl[grp]['VALUE']\n",
    "        uncertainty_col = coi_dl[grp]['UNCERTAINTY']\n",
    "        detection_col = coi_dl[grp]['DL']\n",
    "\n",
    "        # Initialize detection limit column\n",
    "        df['DL'] = df[detection_col]\n",
    "        \n",
    "        # Set detection limits based on conditions\n",
    "        self._set_detection_limits(df, value_col, uncertainty_col, lut)\n",
    "\n",
    "    def _set_detection_limits(self, df: pd.DataFrame, value_col: str, uncertainty_col: str, lut: dict) -> None:\n",
    "        \"Set detection limits based on value and uncertainty columns.\"\n",
    "        \n",
    "        # Condition for setting '='\n",
    "        # 'DL' defaults to equal (i.e. '=') if there is a value and uncertainty and 'DL' value is not \n",
    "        # in the lookup table.\n",
    "        \n",
    "        condition_eq =(df[value_col].notna() & \n",
    "                       df[uncertainty_col].notna() & \n",
    "                       ~df['DL'].isin(lut.keys())\n",
    "        )\n",
    "        \n",
    "        df.loc[condition_eq, 'DL'] = '='\n",
    "\n",
    "        # Set 'Not Available' for unmatched detection limits\n",
    "        df.loc[~df['DL'].isin(lut.keys()), 'DL'] = 'Not Available'\n",
    "        \n",
    "        # Perform lookup to map detection limits\n",
    "        df['DL'] = df['DL'].map(lut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIOTA: [2 1 0]\n",
      "SEDIMENT: [1 2 0]\n",
      "SEAWATER: [1 2 0]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "                            NormalizeUncCB(),\n",
    "                            SanitizeValue(coi_val),                       \n",
    "                            RemapUnitCB(),\n",
    "                            RemapDetectionLimitCB(coi_dl, lut_dl)])\n",
    "\n",
    "\n",
    "for grp in ['BIOTA', 'SEDIMENT', 'SEAWATER']:\n",
    "    print(f\"{grp}: {tfm()[grp]['DL'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0026620e",
   "metadata": {},
   "source": [
    "## Remap filtering status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea63f3",
   "metadata": {},
   "source": [
    "HELCOM filtered status is encoded as follows in the `FILT` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eacd28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index value\n",
       "0      0     n\n",
       "1      1     F\n",
       "2      2     N\n",
       "3      3   NaN"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "get_unique_across_dfs(dfs, col_name='FILT', as_df=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ee067",
   "metadata": {},
   "source": [
    "MARIS uses a different encoding for filtered status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e737e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Not applicable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Not available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            name\n",
       "0  -1  Not applicable\n",
       "1   0   Not available\n",
       "2   1             Yes\n",
       "3   2              No"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "pd.read_excel(filtered_lut_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbf457",
   "metadata": {},
   "source": [
    "For only four categories to remap, the `Remapper` is an overkill. We can use a simple dictionary to map the values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b4bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "lut_filtered = {\n",
    "    'N': 2, # No\n",
    "    'n': 2, # No\n",
    "    'F': 1 # Yes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ea425",
   "metadata": {},
   "source": [
    "`RemapFiltCB` converts the HELCOM `FILT` format to the MARIS `FILT` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f58336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapFiltCB(Callback):\n",
    "    \"Lookup FILT value in dataframe using the lookup table.\"\n",
    "    def __init__(self,\n",
    "                 lut_filtered: dict=lut_filtered, # Dictionary mapping FILT codes to their corresponding names\n",
    "                ):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm):\n",
    "        for df in tfm.dfs.values():\n",
    "            if 'FILT' in df.columns:\n",
    "                df['FILT'] = df['FILT'].map(lambda x: self.lut_filtered.get(x, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719feb2c",
   "metadata": {},
   "source": [
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d13536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[RemapFiltCB(lut_filtered)])\n",
    "\n",
    "print(tfm()['SEAWATER']['FILT'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5ef74",
   "metadata": {},
   "source": [
    "## Add Sample Laboratory code (REVIEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3625631",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK FOR NEXT VERSION**: \n",
    "A sample laboratory code allows for entires in the netcdf or csv to be traced back to the samples in the datasource. \n",
    "A sample laboratory code is not included in the NetCDF output as it requries string values data types. We planned to include way to reference the entries using the SMP_ID (of data type integer). The SMP_ID works for some datasets (e.g. GEOTRACES) but not for others (e.g. HELCOM). The `KEY` column of the HELCOM dataset contains a unique code that comprises of a station code followed by a integer sequence. Below the uniqueness of the integer part of the `KEY` column is checked. It was found that the integer part of the `KEY` column is not unique.\n",
    "\n",
    "Possible solutions:\n",
    "- Use `SMP_ID` to generate unique integer IDs for each sample. Then link the `SMP_ID` to the `samplabcode` using a lookup table. How best to share the lookup table? \n",
    "- Use fixed length string IDs for each entry.\n",
    "- Use VLEN strings in the netcdf and pass the the `samplabcode` as is.\n",
    "\n",
    "Options will be explored in a separate notebook.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "def check_unique_key_int(tfm):\n",
    "    \"\"\"\n",
    "    Extracts unique 'KEY' values from specified DataFrames, separates them into string and integer components,\n",
    "    and groups keys by their integer components.\n",
    "\n",
    "    Parameters:\n",
    "    tfm (Transformer): The transformer object containing DataFrames.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with the unique keys, their string and integer components, and grouped keys by integer component.\n",
    "    \"\"\"\n",
    "    # Define the groups to extract keys from\n",
    "    groups = ['SEAWATER', 'BIOTA', 'SEDIMENT']\n",
    "    \n",
    "    # Initialize a set to store unique keys\n",
    "    unique_keys = set()\n",
    "    \n",
    "    # Collect unique keys from each DataFrame\n",
    "    for grp in groups:\n",
    "        unique_keys.update(tfm.dfs[grp]['KEY'].unique())\n",
    "    \n",
    "    # Initialize a dictionary to group keys by their integer components\n",
    "    int_key_map = {}\n",
    "    \n",
    "    for key in unique_keys:\n",
    "        # Assuming the integer part starts after the first 5 characters\n",
    "        int_part = int(key[5:]) if key[5:].isdigit() else None  # Remaining part as integer\n",
    "        \n",
    "        if int_part is not None:\n",
    "            if int_part not in int_key_map:\n",
    "                int_key_map[int_part] = []  # Initialize list for this integer part\n",
    "            int_key_map[int_part].append(key)  # Append the complete key to the list\n",
    "    \n",
    "    return {\n",
    "        'int_key_map': int_key_map  # Return the mapping of integer parts to complete keys\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b7d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KEYS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INT COMPONENT OF `KEY`</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007167</th>\n",
       "      <td>[SSTUK2007167, SDHIG2007167]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999018</th>\n",
       "      <td>[SKRIL1999018, SCLOR1999018, BBFFG1999018, BSSSI1999018, SSTUK1999018, SLVDC1999018, SERPC1999018, WKRIL1999018, WRISO1999018, WSTUK1999018, WIMGW1999018, BCLOR1999018, SSSSI1999018]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998025</th>\n",
       "      <td>[WKRIL1998025, BCLOR1998025, WRISO1998025, WSTUK1998025, SCLOR1998025, BBFFG1998025, WIMGW1998025, SSTUK1998025]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984001</th>\n",
       "      <td>[BNCRS1984001, BCLOR1984001, BSTUK1984001, BSAAS1984001, WSAAS1984001, SKRIL1984001, WSTUK1984001, SSAAS1984001, WCLOR1984001, SSTUK1984001, SCLOR1984001]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997007</th>\n",
       "      <td>[WSTUK1997007, BBFFG1997007, BSSSI1997007, WIMGW1997007, SSSSI1997007, SLEPA1997007, WLREB1997007, BCLOR1997007, WKRIL1997007, WDHIG1997007, SSTUK1997007, SLVDC1997007, BRISO1997007, WLVDC1997007, BSTUK1997007, WRISO1997007, SCLOR1997007]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                  KEYS\n",
       "INT COMPONENT OF `KEY`                                                                                                                                                                                                                                                \n",
       "2007167                                                                                                                                                                                                                                   [SSTUK2007167, SDHIG2007167]\n",
       "1999018                                                                         [SKRIL1999018, SCLOR1999018, BBFFG1999018, BSSSI1999018, SSTUK1999018, SLVDC1999018, SERPC1999018, WKRIL1999018, WRISO1999018, WSTUK1999018, WIMGW1999018, BCLOR1999018, SSSSI1999018]\n",
       "1998025                                                                                                                                               [WKRIL1998025, BCLOR1998025, WRISO1998025, WSTUK1998025, SCLOR1998025, BBFFG1998025, WIMGW1998025, SSTUK1998025]\n",
       "1984001                                                                                                     [BNCRS1984001, BCLOR1984001, BSTUK1984001, BSAAS1984001, WSAAS1984001, SKRIL1984001, WSTUK1984001, SSAAS1984001, WCLOR1984001, SSTUK1984001, SCLOR1984001]\n",
       "1997007                 [WSTUK1997007, BBFFG1997007, BSSSI1997007, WIMGW1997007, SSSSI1997007, SLEPA1997007, WLREB1997007, BCLOR1997007, WKRIL1997007, WDHIG1997007, SSTUK1997007, SLVDC1997007, BRISO1997007, WLVDC1997007, BSTUK1997007, WRISO1997007, SCLOR1997007]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Create DataFrame from dictionary and set index name and column name\n",
    "unique_key_df = pd.DataFrame.from_dict(check_unique_key_int(tfm)).rename_axis('INT COMPONENT OF `KEY`')\n",
    "unique_key_df=unique_key_df.rename(columns={unique_key_df.columns[0]: 'KEYS'})\n",
    "unique_key_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9a6766",
   "metadata": {},
   "source": [
    "The INT component of the `KEY` column is not unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c5e32",
   "metadata": {},
   "source": [
    "Sample Laboratory code is currently stored in MARIS master DB but not encoded as NetCDF variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a02de8",
   "metadata": {},
   "source": [
    "Remove the samplabcode column for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29d5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# | exports\\nclass AddSampleLabCodeCB(Callback):\\n    \"Remap `KEY` column to `samplabcode` in each DataFrame.\"\\n    def __call__(self, tfm: Transformer):\\n        for grp in tfm.dfs:\\n            self._remap_sample_id(tfm.dfs[grp])\\n    \\n    def _remap_sample_id(self, df: pd.DataFrame):\\n        df[\\'samplabcode\\'] = df[\\'KEY\\']\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# | exports\n",
    "class AddSampleLabCodeCB(Callback):\n",
    "    \"Remap `KEY` column to `samplabcode` in each DataFrame.\"\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        for grp in tfm.dfs:\n",
    "            self._remap_sample_id(tfm.dfs[grp])\n",
    "    \n",
    "    def _remap_sample_id(self, df: pd.DataFrame):\n",
    "        df['samplabcode'] = df['KEY']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ddf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AttributeError\\n#| eval: false\\ndfs = load_data(fname_in)\\ntfm = Transformer(dfs, cbs=[\\n                            AddSampleLabCodeCB(),\\n                            CompareDfsAndTfmCB(dfs)\\n                            ])\\n\\nprint(tfm()['seawater']['samplabcode'].unique())\\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\\n\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''AttributeError\n",
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "                            AddSampleLabCodeCB(),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "\n",
    "print(tfm()['seawater']['samplabcode'].unique())\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0fb210",
   "metadata": {},
   "source": [
    "## Add measurement note (REMOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5617ef",
   "metadata": {},
   "source": [
    "The `measurementnote` column is not included in the NetCDF output as it contains free text which is not suitable for the NetCDF format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c05383c",
   "metadata": {},
   "source": [
    "## Add Methods (FOR NEXT VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced7dcdb",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK FOR NEXT VERSION**: \n",
    "\n",
    "The HELCOM analysis method descriptions currently lack standardization, resulting in 63 unique descriptions for the methods used in the dataset. This variability complicates the integration of counting methods, sample methods, and preparation methods into the NetCDF output. The contents of the description field maps to MARIS NetCDF variables `COUNT_MET`, `SAMP_MET`, and `PREP_MET`. The HELCOM dataset includes 63 unique descriptions. \n",
    "Perhaps a LLM could help to standardize the methods?\n",
    "The MARIS counting method LUT includes methods that are similar. This might be challenging to identify by the LLM (or manually)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307c018",
   "metadata": {},
   "source": [
    "The HELCOM dataset includes a look-up table `ANALYSIS_METHOD.csv` which captures the methods used by HELCOM in a description field (free text). Lets review the ANALYSIS METHOD descriptions of HELCOM dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0985b9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>METHOD</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BFFG01</td>\n",
       "      <td>6</td>\n",
       "      <td>Gammaspectrometric analysis with Germanium detectors (p-type HGeLi's and HPGe's and 1 n-type HPGe), with efficiency 20-48% Energy resolution 1.8-2.3 keV at 1.33 MeV (not to in use any more)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BFFG02</td>\n",
       "      <td>6</td>\n",
       "      <td>Sr-90, a) Y-90 extraction method dried ash and added Y-90 + HCl, Ph adjustment and Y-90 extraction with HDEHP in n-heptane b) Modified version of classic nitric acid method (not to in use any more)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CLOR02</td>\n",
       "      <td>67</td>\n",
       "      <td>Radiochemical method Radiocaesium separation from seawater samples.134+137Cs was adsorbed on AMP mat,  dissolved with NaOH and after purification precipitated as chloroplatinate (Cs2PtCl6).Counting with low background anticoincidence beta counter.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   METHOD  COUNTRY  \\\n",
       "0  BFFG01        6   \n",
       "1  BFFG02        6   \n",
       "2  CLOR02       67   \n",
       "\n",
       "                                                                                                                                                                                                                                               DESCRIPTION  \n",
       "0                                                            Gammaspectrometric analysis with Germanium detectors (p-type HGeLi's and HPGe's and 1 n-type HPGe), with efficiency 20-48% Energy resolution 1.8-2.3 keV at 1.33 MeV (not to in use any more)  \n",
       "1                                                    Sr-90, a) Y-90 extraction method dried ash and added Y-90 + HCl, Ph adjustment and Y-90 extraction with HDEHP in n-heptane b) Modified version of classic nitric acid method (not to in use any more)  \n",
       "2  Radiochemical method Radiocaesium separation from seawater samples.134+137Cs was adsorbed on AMP mat,  dissolved with NaOH and after purification precipitated as chloroplatinate (Cs2PtCl6).Counting with low background anticoincidence beta counter.  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "analsis_method_df=pd.read_csv(Path(fname_in) / 'ANALYSIS_METHOD.csv')\n",
    "analsis_method_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39ead8",
   "metadata": {},
   "source": [
    "Number of unique ANALYSIS_METHOD DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28536ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "len(analsis_method_df['DESCRIPTION'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9976e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "lut_method = lambda: pd.read_csv(Path(fname_in) / 'ANALYSIS_METHOD.csv').set_index('METHOD').to_dict()['DESCRIPTION']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a7232",
   "metadata": {},
   "source": [
    "Review of MARIS METHOD LUTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef89d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "prepmet_lut = pd.read_excel(prepmet_lut_path())\n",
    "sampmet_lut = pd.read_excel(sampmet_lut_path())\n",
    "counmet_lut = pd.read_excel(counmet_lut_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d625717",
   "metadata": {},
   "source": [
    "**DISCUSS** repition of counting method in `counmet_lut`. When should we use each of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff266921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counmet_id</th>\n",
       "      <th>counmet</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Not available</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Atomic absorption</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Alpha</td>\n",
       "      <td>ALP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Alpha ionization chamber spectrometry</td>\n",
       "      <td>ALPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>Alpha liquid scintillation spectrometry</td>\n",
       "      <td>ALPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>Alpha semiconductor spectrometry</td>\n",
       "      <td>ALPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>Alpha total</td>\n",
       "      <td>ALPT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>Accelerator mass spectrometry</td>\n",
       "      <td>AMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>Beta</td>\n",
       "      <td>BET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   counmet_id                                  counmet  code\n",
       "0          -1                           Not applicable   NaN\n",
       "1           0                            Not available     0\n",
       "2           1                        Atomic absorption    AA\n",
       "3           2                                    Alpha   ALP\n",
       "4           3    Alpha ionization chamber spectrometry  ALPI\n",
       "5           4  Alpha liquid scintillation spectrometry  ALPL\n",
       "6           5         Alpha semiconductor spectrometry  ALPS\n",
       "7           6                              Alpha total  ALPT\n",
       "8           7            Accelerator mass spectrometry   AMS\n",
       "9           8                                     Beta   BET"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "counmet_lut.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90fa59a",
   "metadata": {},
   "source": [
    "## Add station (REMOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa0216",
   "metadata": {},
   "source": [
    "*For MARIS master DB import only (not included in the NetCDF output).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a663d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        SD24\n",
       "1        SD24\n",
       "2        SD24\n",
       "3        SD24\n",
       "4        SD24\n",
       "         ... \n",
       "14888     PL1\n",
       "14889     PL4\n",
       "14890     PL4\n",
       "14891     PL4\n",
       "14892    LZWI\n",
       "Name: STATION, Length: 14893, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "tfm.dfs['BIOTA']['STATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768db093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#| exports\\nclass RemapStationIdCB(Callback):\\n    \"Remap Station ID to MARIS format.\"\\n    def __init__(self):\\n        fc.store_attr()\\n\\n    def __call__(self, tfm: Transformer):\\n        \"Iterate through all DataFrames in the transformer object and remap `STATION` to `station_id`.\"\\n        for grp in tfm.dfs.keys(): \\n            tfm.dfs[grp][\\'station\\'] = tfm.dfs[grp][\\'STATION\\']\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| exports\n",
    "class RemapStationIdCB(Callback):\n",
    "    \"Remap Station ID to MARIS format.\"\n",
    "    def __init__(self):\n",
    "        fc.store_attr()\n",
    "\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"Iterate through all DataFrames in the transformer object and remap `STATION` to `station_id`.\"\n",
    "        for grp in tfm.dfs.keys(): \n",
    "            tfm.dfs[grp]['station'] = tfm.dfs[grp]['STATION']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb2604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#| eval: false\\ndfs = load_data(fname_in)\\ntfm = Transformer(dfs, cbs=[\\n                            RemapStationIdCB(),\\n                            CompareDfsAndTfmCB(dfs)\\n                            ])\\ntfm()\\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\\n\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "                            RemapStationIdCB(),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "tfm()\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff696fec",
   "metadata": {},
   "source": [
    "## Add slice position (TOP and BOTTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf398df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class RemapSedSliceTopBottomCB(Callback):\n",
    "    \"Remap Sediment slice top and bottom to MARIS format.\"\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"Iterate through all DataFrames in the transformer object and remap sediment slice top and bottom.\"\n",
    "        tfm.dfs['SEDIMENT']['TOP'] = tfm.dfs['SEDIMENT']['UPPSLI']\n",
    "        tfm.dfs['SEDIMENT']['BOTTOM'] = tfm.dfs['SEDIMENT']['LOWSLI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479e6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    TOP  BOTTOM\n",
      "0  15.0    20.0\n",
      "1  20.0    27.0\n",
      "2   0.0     2.0\n",
      "3   2.0     4.0\n",
      "4   4.0     6.0\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[RemapSedSliceTopBottomCB()])\n",
    "tfm()\n",
    "print(tfm.dfs['SEDIMENT'][['TOP','BOTTOM']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4bbf53",
   "metadata": {},
   "source": [
    "## Add dry weight, wet weight and percentage weight "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d2796",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: Entries for the ``BASIS`` value of the ``BIOTA`` dataset report a value of `F` which is not consistent with the HELCOM description provided in the metadata. The `GUIDELINES FOR MONITORING OF RADIOACTIVE SUBSTANCES` was obtained from [here](https://metadata.helcom.fi/geonetwork/srv/fin/catalog.search#/metadata/2fdd2d46-0329-40e3-bf96-cb08c7206a24).\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1d2b08",
   "metadata": {},
   "source": [
    "Lets take a look at the BIOTA BASIS values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dcfc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['W', nan, 'D', 'F'], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['BIOTA']['BASIS'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adafeff5",
   "metadata": {},
   "source": [
    "Number of entries for each ``BASIS`` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d37b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BASIS\n",
       "W    11167\n",
       "D     3634\n",
       "F       25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['BIOTA']['BASIS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc763755",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: Some entries for ``DW%`` (Dry weight as percentage (%) of fresh weight) are much higher than 100%. Additionally, ``DW%`` is repoted as 0% in some cases.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f3249",
   "metadata": {},
   "source": [
    "For BIOTA, the number of entries for ``DW%`` higher than 100%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc7826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['BIOTA']['DW%'][dfs['BIOTA']['DW%'] > 100].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b30b6b",
   "metadata": {},
   "source": [
    "For BIOTA, the number of entries for ``DW%`` equal to 0%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f386973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['BIOTA']['DW%'][dfs['BIOTA']['DW%'] == 0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b1201",
   "metadata": {},
   "source": [
    "For SEDIMENT, the number of entries for ``DW%`` higher than 100%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37493d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['SEDIMENT']['DW%'][dfs['SEDIMENT']['DW%'] > 100].count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f48f6",
   "metadata": {},
   "source": [
    "For SEDIMENT, the number of entries for ``DW%`` equal to 0%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44234014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['SEDIMENT']['DW%'][dfs['SEDIMENT']['DW%'] == 0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71231c2a",
   "metadata": {},
   "source": [
    "Lets take a look at the MARIS description of the `percentwt`, `drywt` and `wetwt` variables:\n",
    "\n",
    "- `percentwt`: Dry weight as ratio of fresh weight, expressed as a decimal .\n",
    "- `drywt`: Dry weight in grams.\n",
    "- `wetwt`: Fresh weight in grams.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4735dd22",
   "metadata": {},
   "source": [
    "HELCOM Description:\n",
    "\n",
    "**Sediment:**\n",
    "1. DW%: DRY WEIGHT AS PERCENTAGE (%) OF FRESH WEIGHT.\n",
    "2. VALUE_Bq/kg: Measured radioactivity concentration in Bq/kg dry wt. in scientific format(e.g. 123 = 1.23E+02, 0.076 = 7.6E-02).\n",
    "\n",
    "**Biota:**\n",
    "1. WEIGHT: Average weight (in g) of specimen in the sample\n",
    "2. DW%: DRY WEIGHT AS PERCENTAGE (%) OF FRESH WEIGHT.\n",
    "3. BASIS: Code for the basis the value is reported;\n",
    "        - W=WET WEIGHT\n",
    "        - D=DRY WEIGHT\n",
    "        - A= ASH WEIGHT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d92f75f",
   "metadata": {},
   "source": [
    "Lets take a look at the HELCOM dataset, the weight of the sample is not reported for ``SEDIMENT``. However, the percentage dry weight is reported as `DW%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa92c9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['KEY', 'NUCLIDE', 'METHOD', '< VALUE_Bq/kg', 'VALUE_Bq/kg', 'ERROR%_kg',\n",
       "       '< VALUE_Bq/m²', 'VALUE_Bq/m²', 'ERROR%_m²', 'DATE_OF_ENTRY_x',\n",
       "       'COUNTRY', 'LABORATORY', 'SEQUENCE', 'DATE', 'YEAR', 'MONTH', 'DAY',\n",
       "       'STATION', 'LATITUDE (ddmmmm)', 'LATITUDE (dddddd)',\n",
       "       'LONGITUDE (ddmmmm)', 'LONGITUDE (dddddd)', 'DEVICE', 'TDEPTH',\n",
       "       'UPPSLI', 'LOWSLI', 'AREA', 'SEDI', 'OXIC', 'DW%', 'LOI%',\n",
       "       'MORS_SUBBASIN', 'HELCOM_SUBBASIN', 'SUM_LINK', 'DATE_OF_ENTRY_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['SEDIMENT'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1037bff",
   "metadata": {},
   "source": [
    "The BIOTA dataset reports the weight of the sample as `WEIGHT` and the percentage dry weight as `DW%`. The `BASIS` column describes the basis the value reported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd4708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['KEY', 'NUCLIDE', 'METHOD', '< VALUE_Bq/kg', 'VALUE_Bq/kg', 'BASIS',\n",
       "       'ERROR%', 'NUMBER', 'DATE_OF_ENTRY_x', 'COUNTRY', 'LABORATORY',\n",
       "       'SEQUENCE', 'DATE', 'YEAR', 'MONTH', 'DAY', 'STATION',\n",
       "       'LATITUDE ddmmmm', 'LATITUDE dddddd', 'LONGITUDE ddmmmm',\n",
       "       'LONGITUDE dddddd', 'SDEPTH', 'RUBIN', 'BIOTATYPE', 'TISSUE', 'NO',\n",
       "       'LENGTH', 'WEIGHT', 'DW%', 'LOI%', 'MORS_SUBBASIN', 'HELCOM_SUBBASIN',\n",
       "       'DATE_OF_ENTRY_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['BIOTA'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef385c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class LookupDryWetPercentWeightCB(Callback):\n",
    "    \"Lookup dry-wet ratio and format for MARIS.\"\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"Iterate through all DataFrames in the transformer object and apply the dry-wet ratio lookup.\"\n",
    "        for grp in tfm.dfs.keys():\n",
    "            if 'DW%' in tfm.dfs[grp].columns:\n",
    "                self._apply_dry_wet_ratio(tfm.dfs[grp])\n",
    "            if 'WEIGHT' in tfm.dfs[grp].columns and 'BASIS' in tfm.dfs[grp].columns:\n",
    "                self._correct_basis(tfm.dfs[grp])\n",
    "                self._apply_weight(tfm.dfs[grp])\n",
    "\n",
    "    def _apply_dry_wet_ratio(self, df: pd.DataFrame) -> None:\n",
    "        \"Apply dry-wet ratio conversion and formatting to the given DataFrame.\"\n",
    "        df['PERCENTWT'] = df['DW%'] / 100  # Convert percentage to fraction\n",
    "        df.loc[df['PERCENTWT'] == 0, 'PERCENTWT'] = np.NaN  # Convert 0% to NaN\n",
    "\n",
    "    def _correct_basis(self, df: pd.DataFrame) -> None:\n",
    "        \"Correct BASIS values. Assuming F = Fresh weight, so F = W\"\n",
    "        df.loc[df['BASIS'] == 'F', 'BASIS'] = 'W'\n",
    "\n",
    "    def _apply_weight(self, df: pd.DataFrame) -> None:\n",
    "        \"Apply weight conversion and formatting to the given DataFrame.\"\n",
    "        dry_condition = df['BASIS'] == 'D'\n",
    "        wet_condition = df['BASIS'] == 'W'\n",
    "        \n",
    "        df.loc[dry_condition, 'DRYWT'] = df['WEIGHT']\n",
    "        df.loc[dry_condition & df['PERCENTWT'].notna(), 'WETWT'] = df['WEIGHT'] / df['PERCENTWT']\n",
    "        \n",
    "        df.loc[wet_condition, 'WETWT'] = df['WEIGHT']\n",
    "        df.loc[wet_condition & df['PERCENTWT'].notna(), 'DRYWT'] = df['WEIGHT'] * df['PERCENTWT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d714bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           BIOTA  SEAWATER  SEDIMENT\n",
      "Number of rows in dfs      14893     20318     37347\n",
      "Number of rows in tfm.dfs  14893     20318     37347\n",
      "Number of rows removed         0         0         0 \n",
      "\n",
      "BIOTA:    PERCENTWT      DRYWT  WETWT\n",
      "0    0.18453  174.93444  948.0\n",
      "1    0.18453  174.93444  948.0\n",
      "2    0.18453  174.93444  948.0\n",
      "3    0.18453  174.93444  948.0\n",
      "4    0.18458  177.93512  964.0\n",
      "SEDIMENT: 0   NaN\n",
      "1   NaN\n",
      "2   NaN\n",
      "3   NaN\n",
      "4   NaN\n",
      "Name: PERCENTWT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "                            LookupDryWetPercentWeightCB(),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "\n",
    "tfm()\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n",
    "print('BIOTA:', tfm.dfs['BIOTA'][['PERCENTWT','DRYWT','WETWT']].head())\n",
    "print('SEDIMENT:', tfm.dfs['SEDIMENT']['PERCENTWT'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68925d7c",
   "metadata": {},
   "source": [
    "Note that the dry weight is greater than the wet weight for some entries in the BIOTA dataset due to the DW% being greater than 100%, see above. Lets take a look at the number of entries where this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38bc359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DRYWT    20\n",
       "WETWT    20\n",
       "dtype: int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfm.dfs['BIOTA'][['DRYWT','WETWT']][tfm.dfs['BIOTA']['DRYWT'] > tfm.dfs['BIOTA']['WETWT']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b9aa0",
   "metadata": {},
   "source": [
    "## Standardize Coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3203cb3",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: Column names for geographical coordinates are inconsistent across sample types (biota, sediment, seawater). Sometimes using parentheses, sometimes not.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c04fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIOTA: ['LATITUDE ddmmmm', 'LATITUDE dddddd', 'LONGITUDE ddmmmm', 'LONGITUDE dddddd']\n",
      "SEAWATER: ['LATITUDE (ddmmmm)', 'LATITUDE (dddddd)', 'LONGITUDE (ddmmmm)', 'LONGITUDE (dddddd)']\n",
      "SEDIMENT: ['LATITUDE (ddmmmm)', 'LATITUDE (dddddd)', 'LONGITUDE (ddmmmm)', 'LONGITUDE (dddddd)']\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "for grp in dfs.keys():\n",
    "    print(f'{grp}: {[col for col in dfs[grp].columns if \"LON\" in col or \"LAT\" in col]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e83c2e1",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK TO DATA PROVIDER**: HELCOM SEAWATER datase includes values of 0 for both latitude and longitude. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEAWATER invalid coordinates for LATITUDE (dddddd)_LONGITUDE (dddddd):\n",
      "                KEY NUCLIDE  METHOD < VALUE_Bq/m³  VALUE_Bq/m³  ERROR%_m³  \\\n",
      "19238  WSTUK2015001      H3  STUK04             <        920.0        0.0   \n",
      "19239  WSTUK2015002      H3  STUK04             <        920.0        0.0   \n",
      "19240  WSTUK2015003      H3  STUK04             <        920.0        0.0   \n",
      "19241  WSTUK2015004      H3  STUK04             <        920.0        0.0   \n",
      "19242  WSTUK2015005      H3  STUK04             <        920.0        0.0   \n",
      "\n",
      "         DATE_OF_ENTRY_x  COUNTRY LABORATORY  SEQUENCE  ...  \\\n",
      "19238  12/07/16 00:00:00       34       STUK   2015001  ...   \n",
      "19239  12/07/16 00:00:00       34       STUK   2015002  ...   \n",
      "19240  12/07/16 00:00:00       34       STUK   2015003  ...   \n",
      "19241  12/07/16 00:00:00       34       STUK   2015004  ...   \n",
      "19242  12/07/16 00:00:00       34       STUK   2015005  ...   \n",
      "\n",
      "      LONGITUDE (ddmmmm)  LONGITUDE (dddddd)  TDEPTH  SDEPTH SALIN  TTEMP  \\\n",
      "19238            23.3761                 0.0    81.0     1.0   NaN    NaN   \n",
      "19239            23.3761                 0.0    81.0    80.0   NaN    NaN   \n",
      "19240            26.2080                 0.0    69.0     1.0   NaN    NaN   \n",
      "19241            26.2080                 0.0    69.0    68.0   NaN    NaN   \n",
      "19242            21.0477                 0.0   173.0     1.0   NaN    NaN   \n",
      "\n",
      "       FILT  MORS_SUBBASIN  HELCOM_SUBBASIN    DATE_OF_ENTRY_y  \n",
      "19238     N             11               11  12/07/16 00:00:00  \n",
      "19239     N             11               11  12/07/16 00:00:00  \n",
      "19240     N             11               11  12/07/16 00:00:00  \n",
      "19241     N             11               11  12/07/16 00:00:00  \n",
      "19242     N              3                3  12/07/16 00:00:00  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "SEDIMENT invalid coordinates for LATITUDE (ddmmmm)_LONGITUDE (ddmmmm):\n",
      "                KEY NUCLIDE  METHOD < VALUE_Bq/kg  VALUE_Bq/kg  ERROR%_kg  \\\n",
      "35783  SDHIG2016236   CS137  DHIG03           NaN       8.2952      2.351   \n",
      "\n",
      "      < VALUE_Bq/m²  VALUE_Bq/m²  ERROR%_m²    DATE_OF_ENTRY_x  ...  LOWSLI  \\\n",
      "35783           NaN   237.500899        NaN  05/13/19 00:00:00  ...     NaN   \n",
      "\n",
      "      AREA  SEDI OXIC  DW%  LOI%  MORS_SUBBASIN HELCOM_SUBBASIN  SUM_LINK  \\\n",
      "35783  NaN   NaN  NaN  NaN   NaN            NaN             NaN       NaN   \n",
      "\n",
      "       DATE_OF_ENTRY_y  \n",
      "35783              NaN  \n",
      "\n",
      "[1 rows x 35 columns]\n",
      "SEDIMENT invalid coordinates for LATITUDE (dddddd)_LONGITUDE (dddddd):\n",
      "                KEY NUCLIDE  METHOD < VALUE_Bq/kg  VALUE_Bq/kg  ERROR%_kg  \\\n",
      "35783  SDHIG2016236   CS137  DHIG03           NaN       8.2952      2.351   \n",
      "\n",
      "      < VALUE_Bq/m²  VALUE_Bq/m²  ERROR%_m²    DATE_OF_ENTRY_x  ...  LOWSLI  \\\n",
      "35783           NaN   237.500899        NaN  05/13/19 00:00:00  ...     NaN   \n",
      "\n",
      "      AREA  SEDI OXIC  DW%  LOI%  MORS_SUBBASIN HELCOM_SUBBASIN  SUM_LINK  \\\n",
      "35783  NaN   NaN  NaN  NaN   NaN            NaN             NaN       NaN   \n",
      "\n",
      "       DATE_OF_ENTRY_y  \n",
      "35783              NaN  \n",
      "\n",
      "[1 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "def get_invalid_coordinate_df(df, lat_cols, lon_cols):\n",
    "    invalid_dfs = {}\n",
    "    \n",
    "    for lat_col, lon_col in zip(lat_cols, lon_cols):\n",
    "        # Filter rows where latitude or longitude is NaN or zero\n",
    "        invalid_df = df[(df[lat_col].isna() | df[lon_col].isna()) | \n",
    "                        (df[lat_col] == 0) | (df[lon_col] == 0)]\n",
    "        \n",
    "        # Store the invalid DataFrame in the dictionary\n",
    "        if not invalid_df.empty:\n",
    "            invalid_dfs[f'{lat_col}_{lon_col}'] = invalid_df\n",
    "\n",
    "    return invalid_dfs\n",
    "\n",
    "def print_invalid_coordinates(invalid_dfs, dataset_name):\n",
    "    for key, invalid_df in invalid_dfs.items():\n",
    "        print(f'{dataset_name} invalid coordinates for {key}:')\n",
    "        print(invalid_df.head())\n",
    "\n",
    "# Define the columns for each dataset\n",
    "biota_lat_cols = ['LATITUDE ddmmmm', 'LATITUDE dddddd']\n",
    "biota_lon_cols = ['LONGITUDE ddmmmm', 'LONGITUDE dddddd']\n",
    "\n",
    "seawater_lat_cols = ['LATITUDE (ddmmmm)', 'LATITUDE (dddddd)']\n",
    "seawater_lon_cols = ['LONGITUDE (ddmmmm)', 'LONGITUDE (dddddd)']\n",
    "\n",
    "sediment_lat_cols = ['LATITUDE (ddmmmm)', 'LATITUDE (dddddd)']\n",
    "sediment_lon_cols = ['LONGITUDE (ddmmmm)', 'LONGITUDE (dddddd)']\n",
    "\n",
    "# Get invalid coordinate DataFrames for each dataset\n",
    "biota_invalid_dfs = get_invalid_coordinate_df(dfs['BIOTA'], biota_lat_cols, biota_lon_cols)\n",
    "seawater_invalid_dfs = get_invalid_coordinate_df(dfs['SEAWATER'], seawater_lat_cols, seawater_lon_cols)\n",
    "sediment_invalid_dfs = get_invalid_coordinate_df(dfs['SEDIMENT'], sediment_lat_cols, sediment_lon_cols)\n",
    "\n",
    "# Print only non-empty invalid DataFrames\n",
    "print_invalid_coordinates(biota_invalid_dfs, 'BIOTA')\n",
    "print_invalid_coordinates(seawater_invalid_dfs, 'SEAWATER')\n",
    "print_invalid_coordinates(sediment_invalid_dfs, 'SEDIMENT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61afcc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "class ParseCoordinates(Callback):\n",
    "    \"\"\"\n",
    "    Get geographical coordinates from columns expressed in degrees decimal format \n",
    "    or from columns in degrees/minutes decimal format where degrees decimal format is missing.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 fn_convert_cor: Callable # Function that converts coordinates from degree-minute to decimal degree format\n",
    "                 ):\n",
    "        self.fn_convert_cor = fn_convert_cor\n",
    "\n",
    "    def __call__(self, tfm:Transformer):\n",
    "        for df in tfm.dfs.values():\n",
    "            self._format_coordinates(df)\n",
    "\n",
    "    def _format_coordinates(self, df:pd.DataFrame) -> None:\n",
    "        coord_cols = self._get_coord_columns(df.columns)\n",
    "        \n",
    "        for coord in ['lat', 'lon']:\n",
    "            decimal_col, minute_col = coord_cols[f'{coord}_d'], coord_cols[f'{coord}_m']\n",
    "            \n",
    "            condition = df[decimal_col].isna() | (df[decimal_col] == 0)\n",
    "            df[coord.upper()] = np.where(condition,\n",
    "                                 df[minute_col].apply(self._safe_convert),\n",
    "                                 df[decimal_col])\n",
    "        \n",
    "        df.dropna(subset=['LAT', 'LON'], inplace=True)\n",
    "\n",
    "    def _get_coord_columns(self, columns) -> dict:\n",
    "        return {\n",
    "            'lon_d': self._find_coord_column(columns, 'LON', 'dddddd'),\n",
    "            'lat_d': self._find_coord_column(columns, 'LAT', 'dddddd'),\n",
    "            'lon_m': self._find_coord_column(columns, 'LON', 'ddmmmm'),\n",
    "            'lat_m': self._find_coord_column(columns, 'LAT', 'ddmmmm')\n",
    "        }\n",
    "\n",
    "    def _find_coord_column(self, columns, coord_type, coord_format) -> str:\n",
    "        pattern = re.compile(f'{coord_type}.*{coord_format}', re.IGNORECASE)\n",
    "        matching_columns = [col for col in columns if pattern.search(col)]\n",
    "        return matching_columns[0] if matching_columns else None\n",
    "\n",
    "    def _safe_convert(self, value) -> str:\n",
    "        if pd.isna(value):\n",
    "            return value\n",
    "        try:\n",
    "            return self.fn_convert_cor(value)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting value {value}: {e}\")\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf7136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           BIOTA  SEAWATER  SEDIMENT\n",
      "Number of rows in dfs      14893     20318     37347\n",
      "Number of rows in tfm.dfs  14893     20318     37346\n",
      "Number of rows removed         0         0         1 \n",
      "\n",
      "             LAT        LON\n",
      "0      54.283333  12.316667\n",
      "1      54.283333  12.316667\n",
      "2      54.283333  12.316667\n",
      "3      54.283333  12.316667\n",
      "4      54.283333  12.316667\n",
      "...          ...        ...\n",
      "14888  54.583300  19.000000\n",
      "14889  54.333300  15.500000\n",
      "14890  54.333300  15.500000\n",
      "14891  54.333300  15.500000\n",
      "14892  54.363900  19.433300\n",
      "\n",
      "[14893 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[                    \n",
    "                            ParseCoordinates(ddmm_to_dd),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "tfm()\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n",
    "print(tfm.dfs['BIOTA'][['LAT','LON']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a055628",
   "metadata": {},
   "source": [
    "Sanitize coordinates drops a row when both longitude & latitude equal 0 or data contains unrealistic longitude & latitude values. Converts longitude & latitude `,` separator to `.` separator.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a85059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           BIOTA  SEAWATER  SEDIMENT\n",
      "Number of rows in dfs      14893     20318     37347\n",
      "Number of rows in tfm.dfs  14893     20318     37346\n",
      "Number of rows removed         0         0         1 \n",
      "\n",
      "             LAT        LON\n",
      "0      54.283333  12.316667\n",
      "1      54.283333  12.316667\n",
      "2      54.283333  12.316667\n",
      "3      54.283333  12.316667\n",
      "4      54.283333  12.316667\n",
      "...          ...        ...\n",
      "14888  54.583300  19.000000\n",
      "14889  54.333300  15.500000\n",
      "14890  54.333300  15.500000\n",
      "14891  54.333300  15.500000\n",
      "14892  54.363900  19.433300\n",
      "\n",
      "[14893 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "                            ParseCoordinates(ddmm_to_dd),\n",
    "                            SanitizeLonLatCB(),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "\n",
    "tfm()\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n",
    "print(tfm.dfs['BIOTA'][['LAT','LON']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47716bff",
   "metadata": {},
   "source": [
    "## Review all callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a07959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1 missing time value(s) in SEDIMENT\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Match'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[431], line 24\u001b[0m\n\u001b[1;32m      2\u001b[0m dfs \u001b[38;5;241m=\u001b[39m load_data(fname_in)\n\u001b[1;32m      3\u001b[0m tfm \u001b[38;5;241m=\u001b[39m Transformer(dfs, cbs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      4\u001b[0m                             LowerStripNameCB(col_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUCLIDE\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m                             RemapNuclideNameCB(lut_nuclides),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m                             CompareDfsAndTfmCB(dfs)\n\u001b[1;32m     22\u001b[0m                             ])\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(tfm\u001b[38;5;241m.\u001b[39mcompare_stats) , \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/downloads/marisco/marisco/callbacks.py:79\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransform the dataframe(s) according to the specified callbacks.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbs: \u001b[43mrun_cbs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdfs\n",
      "File \u001b[0;32m~/downloads/marisco/marisco/callbacks.py:47\u001b[0m, in \u001b[0;36mrun_cbs\u001b[0;34m(cbs, obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(cbs, key\u001b[38;5;241m=\u001b[39mattrgetter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cb\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m: obj\u001b[38;5;241m.\u001b[39mlogs\u001b[38;5;241m.\u001b[39mappend(cb\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/downloads/marisco/marisco/callbacks.py:131\u001b[0m, in \u001b[0;36mRemapCB.__call__\u001b[0;34m(self, tfm)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdest_grps:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grp \u001b[38;5;129;01min\u001b[39;00m tfm\u001b[38;5;241m.\u001b[39mdfs:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remap_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the dataframes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/downloads/marisco/marisco/callbacks.py:136\u001b[0m, in \u001b[0;36mRemapCB._remap_group\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_remap_group\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m--> 136\u001b[0m     df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_remap] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol_src\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remap_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4780\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mariscoDev/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/downloads/marisco/marisco/callbacks.py:140\u001b[0m, in \u001b[0;36mRemapCB._remap_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_remap_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    139\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m--> 140\u001b[0m     match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(match, Match):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m match\u001b[38;5;241m.\u001b[39mmatched_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_value:\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Match'"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "                            LowerStripNameCB(col_src='NUCLIDE'),\n",
    "                            RemapNuclideNameCB(lut_nuclides),\n",
    "                            ParseTimeCB(),\n",
    "                            EncodeTimeCB(),\n",
    "                            SanitizeValue(coi_val),       \n",
    "                            NormalizeUncCB(),\n",
    "                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='RUBIN', dest_grps='BIOTA'),\n",
    "                            RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='TISSUE', dest_grps='BIOTA'),\n",
    "                            RemapCB(fn_lut=lut_biogroup, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA'),\n",
    "                            RemapSedimentCB(fn_lut=lut_sediments, replace_lut=sed_replace_lut),\n",
    "                            RemapUnitCB(),\n",
    "                            RemapDetectionLimitCB(coi_dl, lut_dl),\n",
    "                            RemapFiltCB(lut_filtered),\n",
    "                            RemapSedSliceTopBottomCB(),\n",
    "                            LookupDryWetPercentWeightCB(),\n",
    "                            ParseCoordinates(ddmm_to_dd),\n",
    "                            SanitizeLonLatCB(),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "\n",
    "tfm()\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIOTA columns:\n",
      "Index(['KEY', 'NUCLIDE', 'METHOD', '< VALUE_Bq/kg', 'VALUE_Bq/kg', 'BASIS',\n",
      "       'ERROR%', 'NUMBER', 'DATE_OF_ENTRY_x', 'COUNTRY', 'LABORATORY',\n",
      "       'SEQUENCE', 'DATE', 'YEAR', 'MONTH', 'DAY', 'STATION',\n",
      "       'LATITUDE ddmmmm', 'LATITUDE dddddd', 'LONGITUDE ddmmmm',\n",
      "       'LONGITUDE dddddd', 'SDEPTH', 'RUBIN', 'BIOTATYPE', 'TISSUE', 'NO',\n",
      "       'LENGTH', 'WEIGHT', 'DW%', 'LOI%', 'MORS_SUBBASIN', 'HELCOM_SUBBASIN',\n",
      "       'DATE_OF_ENTRY_y', 'TIME', 'VALUE', 'UNCERTAINTY', 'SPECIES',\n",
      "       'BODY_PART', 'BIO_GROUP', 'UNIT', 'DL', 'PERCENTWT', 'DRYWT', 'WETWT',\n",
      "       'LAT', 'LON'],\n",
      "      dtype='object')\n",
      "[31  4  9 33 12 21  6  8 22 10 24 77 17  2 37 41 47 23 11 13 25 16 14 36\n",
      " 35 29 34 67 63 46 43 42 94 55 50 40 53 87 92 86 15  7 93 85 91 90 51 59\n",
      " 76 72 54 57]\n"
     ]
    }
   ],
   "source": [
    "grp = 'BIOTA'\n",
    "print(f'{grp} columns:')\n",
    "print(tfm.dfs[grp].columns)\n",
    "print(tfm.dfs[grp].NUCLIDE.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764758f1",
   "metadata": {},
   "source": [
    "CHECK IF THE NUCLIDE IS TO BE INCLUDED AS VALUE OR ID?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13c7a2",
   "metadata": {},
   "source": [
    "For instance, lets inspect dropped rows for SEAWATER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29baf65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dropped rows: 76\n",
      "Example of dropped rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KEY</th>\n",
       "      <th>NUCLIDE</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>&lt; VALUE_Bq/m³</th>\n",
       "      <th>VALUE_Bq/m³</th>\n",
       "      <th>ERROR%_m³</th>\n",
       "      <th>DATE_OF_ENTRY_x</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>LABORATORY</th>\n",
       "      <th>SEQUENCE</th>\n",
       "      <th>...</th>\n",
       "      <th>LONGITUDE (ddmmmm)</th>\n",
       "      <th>LONGITUDE (dddddd)</th>\n",
       "      <th>TDEPTH</th>\n",
       "      <th>SDEPTH</th>\n",
       "      <th>SALIN</th>\n",
       "      <th>TTEMP</th>\n",
       "      <th>FILT</th>\n",
       "      <th>MORS_SUBBASIN</th>\n",
       "      <th>HELCOM_SUBBASIN</th>\n",
       "      <th>DATE_OF_ENTRY_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13439</th>\n",
       "      <td>WRISO2001025</td>\n",
       "      <td>CS137</td>\n",
       "      <td>RISO02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>RISO</td>\n",
       "      <td>2001025</td>\n",
       "      <td>...</td>\n",
       "      <td>10.500</td>\n",
       "      <td>10.833333</td>\n",
       "      <td>22.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14017</th>\n",
       "      <td>WLEPA2002001</td>\n",
       "      <td>CS134</td>\n",
       "      <td>LEPA02</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "      <td>LEPA</td>\n",
       "      <td>2002001</td>\n",
       "      <td>...</td>\n",
       "      <td>21.030</td>\n",
       "      <td>21.050000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.77</td>\n",
       "      <td>14.40</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14020</th>\n",
       "      <td>WLEPA2002002</td>\n",
       "      <td>CS134</td>\n",
       "      <td>LEPA02</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "      <td>LEPA</td>\n",
       "      <td>2002004</td>\n",
       "      <td>...</td>\n",
       "      <td>20.574</td>\n",
       "      <td>20.956667</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.57</td>\n",
       "      <td>11.95</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14023</th>\n",
       "      <td>WLEPA2002003</td>\n",
       "      <td>CS134</td>\n",
       "      <td>LEPA02</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "      <td>LEPA</td>\n",
       "      <td>2002007</td>\n",
       "      <td>...</td>\n",
       "      <td>19.236</td>\n",
       "      <td>19.393333</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>9.19</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14026</th>\n",
       "      <td>WLEPA2002004</td>\n",
       "      <td>CS134</td>\n",
       "      <td>LEPA02</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93</td>\n",
       "      <td>LEPA</td>\n",
       "      <td>2002010</td>\n",
       "      <td>...</td>\n",
       "      <td>20.205</td>\n",
       "      <td>20.341700</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.06</td>\n",
       "      <td>8.65</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                KEY NUCLIDE  METHOD < VALUE_Bq/m³  VALUE_Bq/m³  ERROR%_m³  \\\n",
       "13439  WRISO2001025   CS137  RISO02           NaN          NaN       10.0   \n",
       "14017  WLEPA2002001   CS134  LEPA02             <          NaN        NaN   \n",
       "14020  WLEPA2002002   CS134  LEPA02             <          NaN        NaN   \n",
       "14023  WLEPA2002003   CS134  LEPA02             <          NaN        NaN   \n",
       "14026  WLEPA2002004   CS134  LEPA02             <          NaN        NaN   \n",
       "\n",
       "      DATE_OF_ENTRY_x  COUNTRY LABORATORY  SEQUENCE  ... LONGITUDE (ddmmmm)  \\\n",
       "13439             NaN       26       RISO   2001025  ...             10.500   \n",
       "14017             NaN       93       LEPA   2002001  ...             21.030   \n",
       "14020             NaN       93       LEPA   2002004  ...             20.574   \n",
       "14023             NaN       93       LEPA   2002007  ...             19.236   \n",
       "14026             NaN       93       LEPA   2002010  ...             20.205   \n",
       "\n",
       "       LONGITUDE (dddddd)  TDEPTH  SDEPTH SALIN  TTEMP  FILT  MORS_SUBBASIN  \\\n",
       "13439           10.833333    22.0    20.0  0.00    NaN     N              5   \n",
       "14017           21.050000    16.0     0.0  3.77  14.40     N              4   \n",
       "14020           20.956667    14.0     0.0  6.57  11.95     N              4   \n",
       "14023           19.393333    73.0     0.0  7.00   9.19     N              4   \n",
       "14026           20.341700    47.0     0.0  7.06   8.65     N              4   \n",
       "\n",
       "       HELCOM_SUBBASIN  DATE_OF_ENTRY_y  \n",
       "13439                5              NaN  \n",
       "14017                9              NaN  \n",
       "14020                9              NaN  \n",
       "14023                9              NaN  \n",
       "14026                9              NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Number of dropped rows:', tfm.dfs_dropped['SEAWATER'].shape[0])\n",
    "print('Example of dropped rows:')\n",
    "tfm.dfs_dropped['SEAWATER'].head(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e17f6685",
   "metadata": {},
   "source": [
    "## Rename columns of interest for NetCDF (REMOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af441203",
   "metadata": {},
   "source": [
    "> Column names are standardized to MARIS NetCDF format (i.e. PEP8 ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7476af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#| exports\\nclass SelectAndRenameColumnCB(Callback):\\n    \"\"\"Select and rename columns in a DataFrame based on NC_VARS.\"\"\"\\n    def __init__(self, \\n                 renaming_rules: dict, # A dictionary of renaming rules\\n                 verbose: bool=False # Whether to print out renaming rules that were not applied\\n                 ):\\n        self.renaming_rules = renaming_rules\\n        self.verbose = verbose\\n\\n    def __call__(self, tfm: Transformer):\\n        \"\"\"Apply column selection and renaming to DataFrames in the transformer, and identify unused rules.\"\"\"\\n        for group in tfm.dfs.keys():\\n            df = tfm.dfs[group]\\n            df, not_found_keys = self._apply_renaming(df, self.renaming_rules)\\n            tfm.dfs[group] = df\\n            \\n            # Print any renaming rules that were not used\\n            if not_found_keys and self.verbose:\\n                print(f\"\\nGroup \\'{group}\\' has the following renaming rules not applied:\")\\n                for old_col in not_found_keys:\\n                    print(f\"Key \\'{old_col}\\' from renaming rules was not found in the DataFrame.\")\\n\\n    def _apply_renaming(self, \\n                        df: pd.DataFrame, # DataFrame to modify\\n                        rename_rules: dict # Renaming rules\\n                        ) -> tuple: # (Renamed and filtered df, Column names from renaming rules that were not found in the DataFrame)\\n        \"\"\"\\n        Select columns based on renaming rules and apply renaming, only for existing columns\\n        while maintaining the order of the dictionary columns.\\n        \"\"\"\\n        # Filter columns to only those in NC_VARS\\n        existing_columns = set(df.columns)\\n        valid_rules = {old_col: new_col for old_col, new_col in rename_rules.items() if old_col in existing_columns}\\n\\n        # Keep only the columns that are in the renaming rules\\n        df = df[list(valid_rules.keys())]\\n\\n        # Apply renaming\\n        df.rename(columns=valid_rules, inplace=True)\\n\\n        # Determine which keys were not found\\n        not_found_keys = set(rename_rules.keys()) - existing_columns\\n        return df, not_found_keys\\n'"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| exports\n",
    "class SelectAndRenameColumnCB(Callback):\n",
    "    \"\"\"Select and rename columns in a DataFrame based on NC_VARS.\"\"\"\n",
    "    def __init__(self, \n",
    "                 renaming_rules: dict, # A dictionary of renaming rules\n",
    "                 verbose: bool=False # Whether to print out renaming rules that were not applied\n",
    "                 ):\n",
    "        self.renaming_rules = renaming_rules\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def __call__(self, tfm: Transformer):\n",
    "        \"\"\"Apply column selection and renaming to DataFrames in the transformer, and identify unused rules.\"\"\"\n",
    "        for group in tfm.dfs.keys():\n",
    "            df = tfm.dfs[group]\n",
    "            df, not_found_keys = self._apply_renaming(df, self.renaming_rules)\n",
    "            tfm.dfs[group] = df\n",
    "            \n",
    "            # Print any renaming rules that were not used\n",
    "            if not_found_keys and self.verbose:\n",
    "                print(f\"\\nGroup '{group}' has the following renaming rules not applied:\")\n",
    "                for old_col in not_found_keys:\n",
    "                    print(f\"Key '{old_col}' from renaming rules was not found in the DataFrame.\")\n",
    "\n",
    "    def _apply_renaming(self, \n",
    "                        df: pd.DataFrame, # DataFrame to modify\n",
    "                        rename_rules: dict # Renaming rules\n",
    "                        ) -> tuple: # (Renamed and filtered df, Column names from renaming rules that were not found in the DataFrame)\n",
    "        \"\"\"\n",
    "        Select columns based on renaming rules and apply renaming, only for existing columns\n",
    "        while maintaining the order of the dictionary columns.\n",
    "        \"\"\"\n",
    "        # Filter columns to only those in NC_VARS\n",
    "        existing_columns = set(df.columns)\n",
    "        valid_rules = {old_col: new_col for old_col, new_col in rename_rules.items() if old_col in existing_columns}\n",
    "\n",
    "        # Keep only the columns that are in the renaming rules\n",
    "        df = df[list(valid_rules.keys())]\n",
    "\n",
    "        # Apply renaming\n",
    "        df.rename(columns=valid_rules, inplace=True)\n",
    "\n",
    "        # Determine which keys were not found\n",
    "        not_found_keys = set(rename_rules.keys()) - existing_columns\n",
    "        return df, not_found_keys\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a8682-672f-4188-9091-821b727b4764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#| eval: false\\ndfs = load_data(fname_in)\\ntfm = Transformer(dfs, cbs=[\\n                            LowerStripNameCB(col_src='NUCLIDE'),\\n                            RemapNuclideNameCB(lut_nuclides),\\n                            ParseTimeCB(),\\n                            EncodeTimeCB(),\\n                            SanitizeValue(coi_val),       \\n                            NormalizeUncCB(),\\n                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='RUBIN', dest_grps='BIOTA'),\\n                            RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='TISSUE', dest_grps='BIOTA'),\\n                            RemapCB(fn_lut=lut_biogroup, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA'),\\n                            RemapSedimentCB(fn_lut=lut_sediments, replace_lut=sed_replace_lut),\\n                            RemapUnitCB(),\\n                            RemapDetectionLimitCB(coi_dl, lut_dl),\\n                            RemapFiltCB(lut_filtered),\\n                            RemapSedSliceTopBottomCB(),\\n                            LookupDryWetPercentWeightCB(),\\n                            ParseCoordinates(ddmm_to_dd),\\n                            SanitizeLonLatCB(),\\n                            CompareDfsAndTfmCB(dfs),\\n                            #SelectAndRenameColumnCB(NC_VARS)\\n                            ])\\n\\ntfm()\\nfor grp in tfm.dfs.keys():\\n    print(f'{grp} columns:')\\n    print(tfm.dfs[grp].columns)\\n\""
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "                            LowerStripNameCB(col_src='NUCLIDE'),\n",
    "                            RemapNuclideNameCB(lut_nuclides),\n",
    "                            ParseTimeCB(),\n",
    "                            EncodeTimeCB(),\n",
    "                            SanitizeValue(coi_val),       \n",
    "                            NormalizeUncCB(),\n",
    "                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='RUBIN', dest_grps='BIOTA'),\n",
    "                            RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='TISSUE', dest_grps='BIOTA'),\n",
    "                            RemapCB(fn_lut=lut_biogroup, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA'),\n",
    "                            RemapSedimentCB(fn_lut=lut_sediments, replace_lut=sed_replace_lut),\n",
    "                            RemapUnitCB(),\n",
    "                            RemapDetectionLimitCB(coi_dl, lut_dl),\n",
    "                            RemapFiltCB(lut_filtered),\n",
    "                            RemapSedSliceTopBottomCB(),\n",
    "                            LookupDryWetPercentWeightCB(),\n",
    "                            ParseCoordinates(ddmm_to_dd),\n",
    "                            SanitizeLonLatCB(),\n",
    "                            CompareDfsAndTfmCB(dfs),\n",
    "                            #SelectAndRenameColumnCB(NC_VARS)\n",
    "                            ])\n",
    "\n",
    "tfm()\n",
    "for grp in tfm.dfs.keys():\n",
    "    print(f'{grp} columns:')\n",
    "    print(tfm.dfs[grp].columns)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a941172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#| eval: false\\ntfm.dfs['SEAWATER'].head()\\n\""
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| eval: false\n",
    "tfm.dfs['SEAWATER'].head()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21cb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#| eval: false\\ntfm.dfs['BIOTA'].head()\\n\""
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| eval: false\n",
    "tfm.dfs['BIOTA'].head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed316728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#| eval: false\\ntfm.dfs['SEDIMENT'].head()\\n\""
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| eval: false\n",
    "tfm.dfs['SEDIMENT'].head()\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ba0e40a",
   "metadata": {},
   "source": [
    "## NetCDF encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af7a47-0760-45bd-97f7-033bb7aa886e",
   "metadata": {},
   "source": [
    "### Example change logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1968d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1 missing time value(s) in SEDIMENT\n",
      "Unmatched SEDI: -99.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Convert values from 'NUCLIDE' to lowercase, strip spaces, and store in 'None'.\",\n",
       " 'Remap data provider nuclide names to MARIS nuclide names.',\n",
       " 'Parse and standardize time information in the dataframe.',\n",
       " 'Encode time as seconds since epoch.',\n",
       " 'Sanitize value/measurement by removing blank entries and populating `value` column.',\n",
       " 'Convert from relative error % to standard uncertainty.',\n",
       " \"Remap values from 'RUBIN' to 'SPECIES' for groups: B, I, O, T, A.\",\n",
       " \"Remap values from 'TISSUE' to 'BODY_PART' for groups: B, I, O, T, A.\",\n",
       " \"Remap values from 'SPECIES' to 'BIO_GROUP' for groups: B, I, O, T, A.\",\n",
       " 'Update sediment id based on MARIS species LUT (dbo_sedtype.xlsx).',\n",
       " 'Set the `unit` id column in the DataFrames based on a lookup table.',\n",
       " 'Remap value type to MARIS format.',\n",
       " 'Lookup FILT value in dataframe using the lookup table.',\n",
       " 'Remap Sediment slice top and bottom to MARIS format.',\n",
       " 'Lookup dry-wet ratio and format for MARIS.',\n",
       " '\\n    Get geographical coordinates from columns expressed in degrees decimal format \\n    or from columns in degrees/minutes decimal format where degrees decimal format is missing.\\n    ',\n",
       " 'Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.',\n",
       " 'Create a dataframe of dropped data. Data included in the `dfs` not in the `tfm`.']"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "                            LowerStripNameCB(col_src='NUCLIDE'),\n",
    "                            RemapNuclideNameCB(lut_nuclides),\n",
    "                            ParseTimeCB(),\n",
    "                            EncodeTimeCB(),\n",
    "                            SanitizeValue(coi_val),       \n",
    "                            NormalizeUncCB(),\n",
    "                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='RUBIN', dest_grps='BIOTA'),\n",
    "                            RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='TISSUE', dest_grps='BIOTA'),\n",
    "                            RemapCB(fn_lut=lut_biogroup, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA'),\n",
    "                            RemapSedimentCB(fn_lut=lut_sediments, replace_lut=sed_replace_lut),\n",
    "                            RemapUnitCB(),\n",
    "                            RemapDetectionLimitCB(coi_dl, lut_dl),\n",
    "                            RemapFiltCB(lut_filtered),\n",
    "                            RemapSedSliceTopBottomCB(),\n",
    "                            LookupDryWetPercentWeightCB(),\n",
    "                            ParseCoordinates(ddmm_to_dd),\n",
    "                            SanitizeLonLatCB(),\n",
    "                            CompareDfsAndTfmCB(dfs)\n",
    "                            ])\n",
    "\n",
    "tfm()\n",
    "tfm.logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b82526cc",
   "metadata": {},
   "source": [
    "### Feed global attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ba4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "kw = ['oceanography', 'Earth Science > Oceans > Ocean Chemistry> Radionuclides',\n",
    "      'Earth Science > Human Dimensions > Environmental Impacts > Nuclear Radiation Exposure',\n",
    "      'Earth Science > Oceans > Ocean Chemistry > Ocean Tracers, Earth Science > Oceans > Marine Sediments',\n",
    "      'Earth Science > Oceans > Ocean Chemistry, Earth Science > Oceans > Sea Ice > Isotopes',\n",
    "      'Earth Science > Oceans > Water Quality > Ocean Contaminants',\n",
    "      'Earth Science > Biological Classification > Animals/Vertebrates > Fish',\n",
    "      'Earth Science > Biosphere > Ecosystems > Marine Ecosystems',\n",
    "      'Earth Science > Biological Classification > Animals/Invertebrates > Mollusks',\n",
    "      'Earth Science > Biological Classification > Animals/Invertebrates > Arthropods > Crustaceans',\n",
    "      'Earth Science > Biological Classification > Plants > Macroalgae (Seaweeds)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def get_attrs(\n",
    "    tfm: Transformer, # Transformer object\n",
    "    zotero_key: str, # Zotero dataset record key\n",
    "    kw: list = kw # List of keywords\n",
    "    ) -> dict: # Global attributes\n",
    "    \"Retrieve all global attributes.\"\n",
    "    return GlobAttrsFeeder(tfm.dfs, cbs=[\n",
    "        BboxCB(),\n",
    "        DepthRangeCB(),\n",
    "        TimeRangeCB(),\n",
    "        ZoteroCB(zotero_key, cfg=cfg()),\n",
    "        KeyValuePairCB('keywords', ', '.join(kw)),\n",
    "        KeyValuePairCB('publisher_postprocess_logs', ', '.join(tfm.logs))\n",
    "        ])()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8aad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'geospatial_lat_min': '31.17',\n",
       " 'geospatial_lat_max': '65.75',\n",
       " 'geospatial_lon_min': '9.6333',\n",
       " 'geospatial_lon_max': '53.5',\n",
       " 'geospatial_bounds': 'POLYGON ((9.6333 53.5, 31.17 53.5, 31.17 65.75, 9.6333 65.75, 9.6333 53.5))',\n",
       " 'time_coverage_start': '1984-01-10T00:00:00',\n",
       " 'time_coverage_end': '2018-12-14T00:00:00',\n",
       " 'title': 'Environmental database - Helsinki Commission Monitoring of Radioactive Substances',\n",
       " 'summary': 'MORS Environment database has been used to collate data resulting from monitoring of environmental radioactivity in the Baltic Sea based on HELCOM Recommendation 26/3.\\n\\nThe database is structured according to HELCOM Guidelines on Monitoring of Radioactive Substances (https://www.helcom.fi/wp-content/uploads/2019/08/Guidelines-for-Monitoring-of-Radioactive-Substances.pdf), which specifies reporting format, database structure, data types and obligatory parameters used for reporting data under Recommendation 26/3.\\n\\nThe database is updated and quality assured annually by HELCOM MORS EG.',\n",
       " 'creator_name': '[{\"creatorType\": \"author\", \"name\": \"HELCOM MORS\"}]',\n",
       " 'keywords': 'oceanography, Earth Science > Oceans > Ocean Chemistry> Radionuclides, Earth Science > Human Dimensions > Environmental Impacts > Nuclear Radiation Exposure, Earth Science > Oceans > Ocean Chemistry > Ocean Tracers, Earth Science > Oceans > Marine Sediments, Earth Science > Oceans > Ocean Chemistry, Earth Science > Oceans > Sea Ice > Isotopes, Earth Science > Oceans > Water Quality > Ocean Contaminants, Earth Science > Biological Classification > Animals/Vertebrates > Fish, Earth Science > Biosphere > Ecosystems > Marine Ecosystems, Earth Science > Biological Classification > Animals/Invertebrates > Mollusks, Earth Science > Biological Classification > Animals/Invertebrates > Arthropods > Crustaceans, Earth Science > Biological Classification > Plants > Macroalgae (Seaweeds)',\n",
       " 'publisher_postprocess_logs': \"Convert values from 'NUCLIDE' to lowercase, strip spaces, and store in 'None'., Remap data provider nuclide names to MARIS nuclide names., Parse and standardize time information in the dataframe., Encode time as seconds since epoch., Sanitize value/measurement by removing blank entries and populating `value` column., Convert from relative error % to standard uncertainty., Remap values from 'RUBIN' to 'SPECIES' for groups: B, I, O, T, A., Remap values from 'TISSUE' to 'BODY_PART' for groups: B, I, O, T, A., Remap values from 'SPECIES' to 'BIO_GROUP' for groups: B, I, O, T, A., Update sediment id based on MARIS species LUT (dbo_sedtype.xlsx)., Set the `unit` id column in the DataFrames based on a lookup table., Remap value type to MARIS format., Lookup FILT value in dataframe using the lookup table., Remap Sediment slice top and bottom to MARIS format., Lookup dry-wet ratio and format for MARIS., \\n    Get geographical coordinates from columns expressed in degrees decimal format \\n    or from columns in degrees/minutes decimal format where degrees decimal format is missing.\\n    , Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator., Create a dataframe of dropped data. Data included in the `dfs` not in the `tfm`.\"}"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "get_attrs(tfm, zotero_key=zotero_key, kw=kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cec047",
   "metadata": {},
   "source": [
    ":::{.callout-tip}\n",
    "\n",
    "**FEEDBACK FOR NEXT VERSION**: I have removed enums_xtra as it no longer utilized in the NetCDF encoder. Please confirm if this change is appropriate? \n",
    "Please can we disucss the need for enums_xtra? Is the plan to keep it? To simplify, can we pass NC_DTYPES and get the unique values for the DTYPES from all tfm.dfs? Then filter the enums based on the unique values? \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ebcce-b8c8-4963-8c1c-f32e820f51d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#| exports\\ndef enums_xtra(\\n    tfm: Transformer, # Transformer object\\n    vars: list # List of variables to extract from the transformer. \\n    # The list of variales to extract are defined by the NC_VARS dictionary. \\n    ):\\n    \"Retrieve a subset of the lengthy enum as `species_t` for instance.\"\\n    enums = Enums(lut_src_dir=lut_path())\\n    xtras = {}\\n    \\n    \\n    for var in vars:\\n        print(var)\\n        unique_vals = tfm.unique(var)\\n        # update this as some vars do not enclude enums. Check the config file. \\n        print(tfm.unique(var))\\n        if unique_vals.any():\\n            xtras[f\\'{var}_t\\'] = enums.filter(f\\'{var}_t\\', unique_vals)\\n    return xtras\\n    '"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#| exports\n",
    "def enums_xtra(\n",
    "    tfm: Transformer, # Transformer object\n",
    "    vars: list # List of variables to extract from the transformer. \n",
    "    # The list of variales to extract are defined by the NC_VARS dictionary. \n",
    "    ):\n",
    "    \"Retrieve a subset of the lengthy enum as `species_t` for instance.\"\n",
    "    enums = Enums(lut_src_dir=lut_path())\n",
    "    xtras = {}\n",
    "    \n",
    "    \n",
    "    for var in vars:\n",
    "        print(var)\n",
    "        unique_vals = tfm.unique(var)\n",
    "        # update this as some vars do not enclude enums. Check the config file. \n",
    "        print(tfm.unique(var))\n",
    "        if unique_vals.any():\n",
    "            xtras[f'{var}_t'] = enums.filter(f'{var}_t', unique_vals)\n",
    "    return xtras\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edddb84",
   "metadata": {},
   "source": [
    "How are the enums attahced is it at the group level or at the primary level? Enums are created at the primary level. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e109f56",
   "metadata": {},
   "source": [
    "### <a name=\"encoding-netcdf\"></a>Encoding NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923236b-db58-4173-93ea-c416f5343eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def encode(\n",
    "    fname_in: str, # Input file name\n",
    "    fname_out_nc: str, # Output file name\n",
    "    **kwargs # Additional arguments\n",
    "    ) -> None:\n",
    "    \"Encode data to NetCDF.\"\n",
    "    dfs = load_data(fname_in)\n",
    "    tfm = Transformer(dfs, cbs=[\n",
    "                            LowerStripNameCB(col_src='NUCLIDE'),\n",
    "                            RemapNuclideNameCB(lut_nuclides),\n",
    "                            ParseTimeCB(),\n",
    "                            EncodeTimeCB(),\n",
    "                            SanitizeValue(coi_val),       \n",
    "                            NormalizeUncCB(),\n",
    "                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='RUBIN', dest_grps='BIOTA'),\n",
    "                            RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='TISSUE', dest_grps='BIOTA'),\n",
    "                            RemapCB(fn_lut=lut_biogroup, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA'),\n",
    "                            RemapSedimentCB(fn_lut=lut_sediments, replace_lut=sed_replace_lut),\n",
    "                            RemapUnitCB(),\n",
    "                            RemapDetectionLimitCB(coi_dl, lut_dl),\n",
    "                            RemapFiltCB(lut_filtered),\n",
    "                            RemapSedSliceTopBottomCB(),\n",
    "                            LookupDryWetPercentWeightCB(),\n",
    "                            ParseCoordinates(ddmm_to_dd),\n",
    "                            SanitizeLonLatCB(),\n",
    "                            ])\n",
    "    tfm()\n",
    "    encoder = NetCDFEncoder(tfm.dfs, \n",
    "                            dest_fname=fname_out_nc, \n",
    "                            global_attrs=get_attrs(tfm, zotero_key=zotero_key, kw=kw),\n",
    "                            verbose=kwargs.get('verbose', False),\n",
    "                           )\n",
    "    encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd973e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 1 missing time value(s) in SEDIMENT\n",
      "Unmatched SEDI: -99.0\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: lon\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: lat\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: time\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: nuclide\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: value\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: unit\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: dl\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: bio_group\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: species\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: body_part\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: drywt\n",
      "--------------------------------------------------------------------------------\n",
      "Group: biota, Variable: wetwt\n",
      "--------------------------------------------------------------------------------\n",
      "Group: seawater, Variable: lon\n",
      "--------------------------------------------------------------------------------\n",
      "Group: seawater, Variable: lat\n",
      "--------------------------------------------------------------------------------\n",
      "Group: seawater, Variable: time\n",
      "--------------------------------------------------------------------------------\n",
      "Group: seawater, Variable: nuclide\n",
      "--------------------------------------------------------------------------------\n",
      "Group: seawater, Variable: value\n",
      "--------------------------------------------------------------------------------\n",
      "Group: seawater, Variable: unit\n",
      "--------------------------------------------------------------------------------\n",
      "Group: seawater, Variable: dl\n",
      "--------------------------------------------------------------------------------\n",
      "Group: seawater, Variable: filt\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: lon\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: lat\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: time\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: area\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: nuclide\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: value\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: unit\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: dl\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: sed_type\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: top\n",
      "--------------------------------------------------------------------------------\n",
      "Group: sediment, Variable: bottom\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "encode(fname_in, fname_out_nc, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05beed7f",
   "metadata": {},
   "source": [
    "## Open Refine Pipeline (WIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f45970",
   "metadata": {},
   "source": [
    "### Rename columns for Open Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468d6dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RemapTaxonInformationCB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[327], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| eval: false\u001b[39;00m\n\u001b[1;32m      2\u001b[0m dfs \u001b[38;5;241m=\u001b[39m load_data(fname_in)\n\u001b[1;32m      3\u001b[0m tfm \u001b[38;5;241m=\u001b[39m Transformer(dfs, cbs\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      4\u001b[0m     AddSampleTypeIdColumnCB(),\n\u001b[1;32m      5\u001b[0m     LowerStripNameCB(col_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUCLIDE\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      6\u001b[0m     RemapNuclideNameCB(lut_nuclides),\n\u001b[1;32m      7\u001b[0m     AddNuclideIdColumnCB(col_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUCLIDE\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      8\u001b[0m     ParseTimeCB(),\n\u001b[1;32m      9\u001b[0m     EncodeTimeCB(cfg()),        \n\u001b[1;32m     10\u001b[0m     SanitizeValue(coi_val),                       \n\u001b[1;32m     11\u001b[0m     NormalizeUncCB(),\n\u001b[1;32m     12\u001b[0m     RemapCB(fn_lut\u001b[38;5;241m=\u001b[39mlut_biota, col_remap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m'\u001b[39m, col_src\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRUBIN\u001b[39m\u001b[38;5;124m'\u001b[39m, dest_grps\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiota\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     13\u001b[0m     RemapCB(lut_tissues, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody_part\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTISSUE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiota\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     14\u001b[0m     RemapCB(lut_biogroup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbio_group\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiota\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mRemapTaxonInformationCB\u001b[49m(lut_taxon),\n\u001b[1;32m     16\u001b[0m     RemapSedimentCB(lut_sediments),\n\u001b[1;32m     17\u001b[0m     RemapUnitCB(),\n\u001b[1;32m     18\u001b[0m     RemapDetectionLimitCB(coi_dl, lut_dl),\n\u001b[1;32m     19\u001b[0m     RemapFiltCB(lut_filtered),\n\u001b[1;32m     20\u001b[0m     AddSampleLabCodeCB(),\n\u001b[1;32m     21\u001b[0m     AddMeasurementNoteCB(lut_method),\n\u001b[1;32m     22\u001b[0m     RemapStationIdCB(),\n\u001b[1;32m     23\u001b[0m     RemapSedSliceTopBottomCB(),\n\u001b[1;32m     24\u001b[0m     LookupDryWetRatio(),\n\u001b[1;32m     25\u001b[0m     ParseCoordinates(ddmm_to_dd),\n\u001b[1;32m     26\u001b[0m     SanitizeLonLatCB(),\n\u001b[1;32m     27\u001b[0m     SelectAndRenameColumnCB(get_renaming_rules, encoding_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenrefine\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     28\u001b[0m     CompareDfsAndTfmCB(dfs)\n\u001b[1;32m     29\u001b[0m     ])\n\u001b[1;32m     31\u001b[0m tfm()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(tfm\u001b[38;5;241m.\u001b[39mcompare_stats) , \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RemapTaxonInformationCB' is not defined"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[\n",
    "    AddSampleTypeIdColumnCB(),\n",
    "    LowerStripNameCB(col_src='NUCLIDE'),\n",
    "    RemapNuclideNameCB(lut_nuclides),\n",
    "    AddNuclideIdColumnCB(col_value='NUCLIDE'),\n",
    "    ParseTimeCB(),\n",
    "    EncodeTimeCB(cfg()),        \n",
    "    SanitizeValue(coi_val),                       \n",
    "    NormalizeUncCB(),\n",
    "    RemapCB(fn_lut=lut_biota, col_remap='species', col_src='RUBIN', dest_grps='biota'),\n",
    "    RemapCB(lut_tissues, 'body_part', 'TISSUE', 'biota'),\n",
    "    RemapCB(lut_biogroup, 'bio_group', 'species', 'biota'),\n",
    "    RemapTaxonInformationCB(lut_taxon),\n",
    "    RemapSedimentCB(lut_sediments),\n",
    "    RemapUnitCB(),\n",
    "    RemapDetectionLimitCB(coi_dl, lut_dl),\n",
    "    RemapFiltCB(lut_filtered),\n",
    "    AddSampleLabCodeCB(),\n",
    "    AddMeasurementNoteCB(lut_method),\n",
    "    RemapStationIdCB(),\n",
    "    RemapSedSliceTopBottomCB(),\n",
    "    LookupDryWetRatio(),\n",
    "    ParseCoordinates(ddmm_to_dd),\n",
    "    SanitizeLonLatCB(),\n",
    "    SelectAndRenameColumnCB(get_renaming_rules, encoding_type='openrefine', verbose=True),\n",
    "    CompareDfsAndTfmCB(dfs)\n",
    "    ])\n",
    "\n",
    "tfm()\n",
    "print(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e68ad8",
   "metadata": {},
   "source": [
    "**Example of data included in dfs_dropped.**\n",
    "\n",
    "Main reasons for data to be dropped from dfs:\n",
    "- No activity value reported (e.g. VALUE_Bq/kg)\n",
    "- No time value reported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe229c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KEY</th>\n",
       "      <th>NUCLIDE</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>&lt; VALUE_Bq/kg</th>\n",
       "      <th>VALUE_Bq/kg</th>\n",
       "      <th>ERROR%_kg</th>\n",
       "      <th>&lt; VALUE_Bq/m²</th>\n",
       "      <th>VALUE_Bq/m²</th>\n",
       "      <th>ERROR%_m²</th>\n",
       "      <th>DATE_OF_ENTRY_x</th>\n",
       "      <th>...</th>\n",
       "      <th>LOWSLI</th>\n",
       "      <th>AREA</th>\n",
       "      <th>SEDI</th>\n",
       "      <th>OXIC</th>\n",
       "      <th>DW%</th>\n",
       "      <th>LOI%</th>\n",
       "      <th>MORS_SUBBASIN</th>\n",
       "      <th>HELCOM_SUBBASIN</th>\n",
       "      <th>SUM_LINK</th>\n",
       "      <th>DATE_OF_ENTRY_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11784</th>\n",
       "      <td>SLREB1998021</td>\n",
       "      <td>SR90</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>55.0</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11824</th>\n",
       "      <td>SLVDC1997023</td>\n",
       "      <td>CS137</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>55.0</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11832</th>\n",
       "      <td>SLVDC1997031</td>\n",
       "      <td>CS137</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>55.0</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>SLVDC1997040</td>\n",
       "      <td>CS137</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>55.0</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11849</th>\n",
       "      <td>SLVDC1998011</td>\n",
       "      <td>CS137</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.02100</td>\n",
       "      <td>55.0</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39769</th>\n",
       "      <td>SSSSM2021030</td>\n",
       "      <td>CO60</td>\n",
       "      <td>SSSM43</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/06/22 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.01608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/06/22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39774</th>\n",
       "      <td>SSSSM2021030</td>\n",
       "      <td>RA226</td>\n",
       "      <td>SSSM43</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/06/22 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.01608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/06/22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39775</th>\n",
       "      <td>SSSSM2021030</td>\n",
       "      <td>RA223</td>\n",
       "      <td>SSSM43</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/06/22 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.01608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/06/22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39777</th>\n",
       "      <td>SSSSM2021031</td>\n",
       "      <td>CS137</td>\n",
       "      <td>SSSM43</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/06/22 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.01608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.993243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/06/22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39779</th>\n",
       "      <td>SSSSM2021031</td>\n",
       "      <td>CO60</td>\n",
       "      <td>SSSM43</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/06/22 00:00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.01608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.993243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/06/22 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                KEY NUCLIDE  METHOD < VALUE_Bq/kg  VALUE_Bq/kg  ERROR%_kg  \\\n",
       "11784  SLREB1998021    SR90       2           NaN          NaN        NaN   \n",
       "11824  SLVDC1997023   CS137       1           NaN          NaN        NaN   \n",
       "11832  SLVDC1997031   CS137       1           NaN          NaN        NaN   \n",
       "11841  SLVDC1997040   CS137       1           NaN          NaN        NaN   \n",
       "11849  SLVDC1998011   CS137       1           NaN          NaN        NaN   \n",
       "...             ...     ...     ...           ...          ...        ...   \n",
       "39769  SSSSM2021030    CO60  SSSM43             <          NaN        NaN   \n",
       "39774  SSSSM2021030   RA226  SSSM43             <          NaN        NaN   \n",
       "39775  SSSSM2021030   RA223  SSSM43             <          NaN        NaN   \n",
       "39777  SSSSM2021031   CS137  SSSM43             <          NaN        NaN   \n",
       "39779  SSSSM2021031    CO60  SSSM43             <          NaN        NaN   \n",
       "\n",
       "      < VALUE_Bq/m²  VALUE_Bq/m²  ERROR%_m²    DATE_OF_ENTRY_x  ...  LOWSLI  \\\n",
       "11784           NaN          NaN        NaN                NaN  ...    12.0   \n",
       "11824           NaN          NaN        NaN                NaN  ...    14.0   \n",
       "11832           NaN          NaN        NaN                NaN  ...    14.0   \n",
       "11841           NaN          NaN        NaN                NaN  ...    16.0   \n",
       "11849           NaN          NaN        NaN                NaN  ...    16.0   \n",
       "...             ...          ...        ...                ...  ...     ...   \n",
       "39769             <          NaN        NaN  09/06/22 00:00:00  ...     2.0   \n",
       "39774             <          NaN        NaN  09/06/22 00:00:00  ...     2.0   \n",
       "39775             <          NaN        NaN  09/06/22 00:00:00  ...     2.0   \n",
       "39777             <          0.0        NaN  09/06/22 00:00:00  ...     2.0   \n",
       "39779             <          NaN        NaN  09/06/22 00:00:00  ...     2.0   \n",
       "\n",
       "          AREA  SEDI OXIC        DW%  LOI%  MORS_SUBBASIN HELCOM_SUBBASIN  \\\n",
       "11784  0.02100  55.0    O        NaN   NaN           14.0            14.0   \n",
       "11824  0.02100  55.0    O        NaN   NaN            9.0             9.0   \n",
       "11832  0.02100  55.0    O        NaN   NaN            9.0             9.0   \n",
       "11841  0.02100  55.0    O        NaN   NaN            9.0             9.0   \n",
       "11849  0.02100  55.0    O        NaN   NaN           14.0            14.0   \n",
       "...        ...   ...  ...        ...   ...            ...             ...   \n",
       "39769  0.01608   NaN  NaN  28.200000  15.0           12.0            12.0   \n",
       "39774  0.01608   NaN  NaN  28.200000  15.0           12.0            12.0   \n",
       "39775  0.01608   NaN  NaN  28.200000  15.0           12.0            12.0   \n",
       "39777  0.01608   NaN  NaN  31.993243   NaN           13.0            13.0   \n",
       "39779  0.01608   NaN  NaN  31.993243   NaN           13.0            13.0   \n",
       "\n",
       "       SUM_LINK    DATE_OF_ENTRY_y  \n",
       "11784         a                NaN  \n",
       "11824         a                NaN  \n",
       "11832         a                NaN  \n",
       "11841         a                NaN  \n",
       "11849         a                NaN  \n",
       "...         ...                ...  \n",
       "39769       NaN  09/06/22 00:00:00  \n",
       "39774       NaN  09/06/22 00:00:00  \n",
       "39775       NaN  09/06/22 00:00:00  \n",
       "39777       NaN  09/06/22 00:00:00  \n",
       "39779       NaN  09/06/22 00:00:00  \n",
       "\n",
       "[286 rows x 35 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "grp='sediment'\n",
    "#grp='seawater'\n",
    "#grp='biota'\n",
    "\n",
    "tfm.dfs_dropped[grp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6241c",
   "metadata": {},
   "source": [
    "## Open Refine encoder (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd81eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "def encode_or(\n",
    "    fname_in: str, # Input file name\n",
    "    fname_out_csv: str, # Output file name\n",
    "    ref_id: str, # Reference ID as defined in MARIS master DB\n",
    "    **kwargs # Additional arguments\n",
    "    ) -> None:\n",
    "    \"Encode data to Open Refine CSV.\"\n",
    "    dfs = load_data(fname_in)\n",
    "    tfm = Transformer(dfs, cbs=[\n",
    "        AddSampleTypeIdColumnCB(),\n",
    "        LowerStripNameCB(col_src='NUCLIDE'),\n",
    "        RemapNuclideNameCB(lut_nuclides),\n",
    "        AddNuclideIdColumnCB(col_value='NUCLIDE'),\n",
    "        ParseTimeCB(),\n",
    "        EncodeTimeCB(cfg()),        \n",
    "        SanitizeValue(coi_val),                       \n",
    "        NormalizeUncCB(),\n",
    "        RemapCB(fn_lut=lut_biota, col_remap='species', col_src='RUBIN', dest_grps='biota'),\n",
    "        RemapCB(lut_tissues, 'body_part', 'TISSUE', 'biota'),\n",
    "        RemapCB(lut_biogroup, 'bio_group', 'species', 'biota'),\n",
    "        RemapTaxonInformationCB(lut_taxon),\n",
    "        RemapSedimentCB(lut_sediments),\n",
    "        RemapUnitCB(),\n",
    "        RemapDetectionLimitCB(coi_dl, lut_dl),\n",
    "        RemapFiltCB(lut_filtered),\n",
    "        AddSampleLabCodeCB(),\n",
    "        AddMeasurementNoteCB(lut_method),\n",
    "        RemapStationIdCB(),\n",
    "        RemapSedSliceTopBottomCB(),\n",
    "        LookupDryWetRatio(),\n",
    "        ParseCoordinates(ddmm_to_dd),\n",
    "        SanitizeLonLatCB(),\n",
    "        SelectAndRenameColumnCB(get_renaming_rules, encoding_type='openrefine', verbose=True),\n",
    "        CompareDfsAndTfmCB(dfs)\n",
    "        ])\n",
    "    \n",
    "    tfm()\n",
    "\n",
    "    encoder = OpenRefineCsvEncoder(tfm.dfs, \n",
    "                                    dest_fname=fname_out_csv, \n",
    "                                    ref_id = ref_id,\n",
    "                                    verbose = True\n",
    "                                )\n",
    "    encoder.encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "encode_or(fname_in, fname_out_csv, ref_id, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ecc9e8",
   "metadata": {},
   "source": [
    "###  Open Refine Variables not included in Helcom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca95cc",
   "metadata": {},
   "source": [
    "| Field name      | Full name                | HELCOM     |\n",
    "|-----------------|--------------------------|------------|\n",
    "| sampquality     | Sample quality           | N          |\n",
    "| lab_id          | Laboratory ID            | N          |\n",
    "| profile_id      | Profile ID               | N          |\n",
    "| transect_id     | Transect ID              | N          |\n",
    "| endperiod       | End period               | N          |\n",
    "| vartype         | Variable type            | N          |\n",
    "| freq            | Frequency                | N          |\n",
    "| rl_detection    | Range low detection      | N          |\n",
    "| rangelow        | Range low                | N          |\n",
    "| rangeupp        | Range upper              | N          |\n",
    "| Commonname      | Common name              | N          |\n",
    "| volume          | Volume                   | N          |\n",
    "| filtpore        | Filter pore              | N          |\n",
    "| acid            | Acidified                | N          |\n",
    "| oxygen          | Oxygen                   | N          |\n",
    "| samparea        | Sample area              | N          |\n",
    "| drywt           | Dry weight               | N          |\n",
    "| wetwt           | Wet weight               | N          |\n",
    "| sampmet_id      | Sampling method ID       | N          |\n",
    "| drymet_id       | Drying method ID         | N          |\n",
    "| prepmet_id      | Preparation method ID    | N          |\n",
    "| counmet_id      | Counting method ID       | N          |\n",
    "| refnote         | Reference note           | N          |\n",
    "| sampnote        | Sample note              | N          |\n",
    "| gfe             | Good for export          | ?          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d9df4",
   "metadata": {},
   "source": [
    "**TODO**:\n",
    "\n",
    "- Should we use a single encoder for both NetCDF and OpenRefine? If so, should we have a single encode function that accepts a variable 'encoding_type'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a206afa",
   "metadata": {},
   "source": [
    "TODO: Include FILT for NetCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc44bf97",
   "metadata": {},
   "source": [
    "TODO: Check sediment 'DW%' data that is less than 1%. Is this realistic? Check the 'DW%' data that is 0%. Run below before SelectAndRenameColumnCB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002712da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seawater':                 KEY NUCLIDE METHOD < VALUE_Bq/m³  VALUE_Bq/m³  ERROR%_m³  \\\n",
       " 0      WKRIL2012003   cs137    NaN           NaN          5.3  32.000000   \n",
       " 1      WKRIL2012004   cs137    NaN           NaN         19.9  20.000000   \n",
       " 2      WKRIL2012005   cs137    NaN           NaN         25.5  20.000000   \n",
       " 3      WKRIL2012006   cs137    NaN           NaN         17.0  29.000000   \n",
       " 4      WKRIL2012007   cs137    NaN           NaN         22.2  18.000000   \n",
       " ...             ...     ...    ...           ...          ...        ...   \n",
       " 21211  WSSSM2021005      h3  SSM45           NaN       1030.0  93.203883   \n",
       " 21212  WSSSM2021006      h3  SSM45           NaN       2240.0  43.303571   \n",
       " 21213  WSSSM2021007      h3  SSM45           NaN       2060.0  47.087379   \n",
       " 21214  WSSSM2021008      h3  SSM45           NaN       2300.0  43.478261   \n",
       " 21215  WSSSM2021004      h3  SSM45             <          NaN        NaN   \n",
       " \n",
       "          DATE_OF_ENTRY_x  COUNTRY LABORATORY   SEQUENCE  ...  \\\n",
       " 0      08/20/14 00:00:00     90.0       KRIL  2012003.0  ...   \n",
       " 1      08/20/14 00:00:00     90.0       KRIL  2012004.0  ...   \n",
       " 2      08/20/14 00:00:00     90.0       KRIL  2012005.0  ...   \n",
       " 3      08/20/14 00:00:00     90.0       KRIL  2012006.0  ...   \n",
       " 4      08/20/14 00:00:00     90.0       KRIL  2012007.0  ...   \n",
       " ...                  ...      ...        ...        ...  ...   \n",
       " 21211  09/06/22 00:00:00     77.0       SSSM   202105.0  ...   \n",
       " 21212  09/06/22 00:00:00     77.0       SSSM   202106.0  ...   \n",
       " 21213  09/06/22 00:00:00     77.0       SSSM   202107.0  ...   \n",
       " 21214  09/06/22 00:00:00     77.0       SSSM   202108.0  ...   \n",
       " 21215  09/06/22 00:00:00     77.0       SSSM   202104.0  ...   \n",
       " \n",
       "       LONGITUDE (ddmmmm)  LONGITUDE (dddddd)  TDEPTH  SDEPTH SALIN  TTEMP  \\\n",
       " 0                29.2000             29.3333     NaN     0.0   NaN    NaN   \n",
       " 1                29.2000             29.3333     NaN    29.0   NaN    NaN   \n",
       " 2                23.0900             23.1500     NaN     0.0   NaN    NaN   \n",
       " 3                27.5900             27.9833     NaN     0.0   NaN    NaN   \n",
       " 4                27.5900             27.9833     NaN    39.0   NaN    NaN   \n",
       " ...                  ...                 ...     ...     ...   ...    ...   \n",
       " 21211            18.2143             18.3572     NaN     1.0   NaN    NaN   \n",
       " 21212            17.0000             17.0000     NaN     1.0   NaN    NaN   \n",
       " 21213            11.5671             11.9452     NaN     1.0   NaN    NaN   \n",
       " 21214            11.5671             11.9452     NaN     1.0   NaN    NaN   \n",
       " 21215            11.1470             11.2450     NaN     1.0   NaN    NaN   \n",
       " \n",
       "        FILT  MORS_SUBBASIN  HELCOM_SUBBASIN    DATE_OF_ENTRY_y  \n",
       " 0       NaN           11.0             11.0  08/20/14 00:00:00  \n",
       " 1       NaN           11.0             11.0  08/20/14 00:00:00  \n",
       " 2       NaN           11.0              3.0  08/20/14 00:00:00  \n",
       " 3       NaN           11.0             11.0  08/20/14 00:00:00  \n",
       " 4       NaN           11.0             11.0  08/20/14 00:00:00  \n",
       " ...     ...            ...              ...                ...  \n",
       " 21211     N            1.0              8.0  09/06/22 00:00:00  \n",
       " 21212     N           10.0             10.0  09/06/22 00:00:00  \n",
       " 21213     N           12.0             12.0  09/06/22 00:00:00  \n",
       " 21214     N           12.0             12.0  09/06/22 00:00:00  \n",
       " 21215     N           15.0             18.0  09/06/22 00:00:00  \n",
       " \n",
       " [21216 rows x 27 columns],\n",
       " 'sediment':                 KEY NUCLIDE  METHOD < VALUE_Bq/kg  VALUE_Bq/kg  ERROR%_kg  \\\n",
       " 0      SKRIL2012048   ra226     NaN           NaN         35.0      26.00   \n",
       " 1      SKRIL2012049   ra226     NaN           NaN         36.0      22.00   \n",
       " 2      SKRIL2012050   ra226     NaN           NaN         38.0      24.00   \n",
       " 3      SKRIL2012051   ra226     NaN           NaN         36.0      25.00   \n",
       " 4      SKRIL2012052   ra226     NaN           NaN         30.0      23.00   \n",
       " ...             ...     ...     ...           ...          ...        ...   \n",
       " 39812  SSSSM2020029   ac228  SSSM43           NaN         37.5       5.00   \n",
       " 39813  SSSSM2020030     k40  SSSM43           NaN        526.0       1.72   \n",
       " 39814  SSSSM2020030   cs137  SSSM43           NaN         17.2       2.21   \n",
       " 39815  SSSSM2020031     k40  SSSM43           NaN       1000.0       1.80   \n",
       " 39816  SSSSM2020031   cs137  SSSM43           NaN         64.0       1.20   \n",
       " \n",
       "       < VALUE_Bq/m²  VALUE_Bq/m²  ERROR%_m²    DATE_OF_ENTRY_x  ...  LOWSLI  \\\n",
       " 0               NaN          NaN        NaN  08/20/14 00:00:00  ...    20.0   \n",
       " 1               NaN          NaN        NaN  08/20/14 00:00:00  ...    27.0   \n",
       " 2               NaN          NaN        NaN  08/20/14 00:00:00  ...     2.0   \n",
       " 3               NaN          NaN        NaN  08/20/14 00:00:00  ...     4.0   \n",
       " 4               NaN          NaN        NaN  08/20/14 00:00:00  ...     6.0   \n",
       " ...             ...          ...        ...                ...  ...     ...   \n",
       " 39812           NaN        255.0       28.0  04/22/22 00:00:00  ...     2.0   \n",
       " 39813           NaN       5690.0        2.0  04/22/22 00:00:00  ...     2.0   \n",
       " 39814           NaN        186.0        2.0  04/22/22 00:00:00  ...     2.0   \n",
       " 39815           NaN      16000.0        2.0  04/22/22 00:00:00  ...     2.0   \n",
       " 39816           NaN       1020.0        1.0  04/22/22 00:00:00  ...     2.0   \n",
       " \n",
       "         AREA  SEDI OXIC    DW%  LOI%  MORS_SUBBASIN HELCOM_SUBBASIN  SUM_LINK  \\\n",
       " 0      0.006   NaN  NaN    NaN   NaN           11.0            11.0       NaN   \n",
       " 1      0.006   NaN  NaN    NaN   NaN           11.0            11.0       NaN   \n",
       " 2      0.006   NaN  NaN    NaN   NaN           11.0            11.0       NaN   \n",
       " 3      0.006   NaN  NaN    NaN   NaN           11.0            11.0       NaN   \n",
       " 4      0.006   NaN  NaN    NaN   NaN           11.0            11.0       NaN   \n",
       " ...      ...   ...  ...    ...   ...            ...             ...       ...   \n",
       " 39812  0.019   0.0    O  28.73  14.0           13.0            13.0       NaN   \n",
       " 39813  0.019   0.0    O  32.03   NaN           12.0            12.0       NaN   \n",
       " 39814  0.019   0.0    O  32.03   NaN           12.0            12.0       NaN   \n",
       " 39815  0.017   0.0    O  48.77   NaN            1.0             8.0       NaN   \n",
       " 39816  0.017   0.0    O  48.77   NaN            1.0             8.0       NaN   \n",
       " \n",
       "          DATE_OF_ENTRY_y  \n",
       " 0      08/20/14 00:00:00  \n",
       " 1      08/20/14 00:00:00  \n",
       " 2      08/20/14 00:00:00  \n",
       " 3      08/20/14 00:00:00  \n",
       " 4      08/20/14 00:00:00  \n",
       " ...                  ...  \n",
       " 39812  04/22/22 00:00:00  \n",
       " 39813  04/22/22 00:00:00  \n",
       " 39814  04/22/22 00:00:00  \n",
       " 39815  04/22/22 00:00:00  \n",
       " 39816  04/22/22 00:00:00  \n",
       " \n",
       " [39817 rows x 35 columns],\n",
       " 'biota':                 KEY NUCLIDE  METHOD < VALUE_Bq/kg  VALUE_Bq/kg BASIS  ERROR%  \\\n",
       " 0      BVTIG2012041   cs134  VTIG01             <     0.010140     W     NaN   \n",
       " 1      BVTIG2012041     k40  VTIG01                 135.300000     W    3.57   \n",
       " 2      BVTIG2012041    co60  VTIG01             <     0.013980     W     NaN   \n",
       " 3      BVTIG2012041   cs137  VTIG01                   4.338000     W    3.48   \n",
       " 4      BVTIG2012040   cs134  VTIG01             <     0.009614     W     NaN   \n",
       " ...             ...     ...     ...           ...          ...   ...     ...   \n",
       " 15822  BSSSM2020016     k40  SSSM42           NaN    65.000000     D   10.20   \n",
       " 15823  BSSSM2020016   cs137  SSSM42           NaN     4.500000     D    6.20   \n",
       " 15824  BSSSM2020017     be7  SSSM42           NaN    94.000000     D    3.40   \n",
       " 15825  BSSSM2020017     k40  SSSM42           NaN  1100.000000     D    1.60   \n",
       " 15826  BSSSM2020017   cs137  SSSM42           NaN    13.000000     D    2.50   \n",
       " \n",
       "        NUMBER    DATE_OF_ENTRY_x  COUNTRY  ... BIOTATYPE  TISSUE     NO  \\\n",
       " 0         NaN  02/27/14 00:00:00      6.0  ...         F       5   16.0   \n",
       " 1         NaN  02/27/14 00:00:00      6.0  ...         F       5   16.0   \n",
       " 2         NaN  02/27/14 00:00:00      6.0  ...         F       5   16.0   \n",
       " 3         NaN  02/27/14 00:00:00      6.0  ...         F       5   16.0   \n",
       " 4         NaN  02/27/14 00:00:00      6.0  ...         F       5   17.0   \n",
       " ...       ...                ...      ...  ...       ...     ...    ...   \n",
       " 15822     NaN  04/22/22 00:00:00     77.0  ...         B      41  319.0   \n",
       " 15823     NaN  04/22/22 00:00:00     77.0  ...         B      41  319.0   \n",
       " 15824     NaN  04/22/22 00:00:00     77.0  ...         P      51    NaN   \n",
       " 15825     NaN  04/22/22 00:00:00     77.0  ...         P      51    NaN   \n",
       " 15826     NaN  04/22/22 00:00:00     77.0  ...         P      51    NaN   \n",
       " \n",
       "        LENGTH  WEIGHT     DW%  LOI%  MORS_SUBBASIN  HELCOM_SUBBASIN  \\\n",
       " 0        45.7   948.0  18.453  92.9            2.0               16   \n",
       " 1        45.7   948.0  18.453  92.9            2.0               16   \n",
       " 2        45.7   948.0  18.453  92.9            2.0               16   \n",
       " 3        45.7   948.0  18.453  92.9            2.0               16   \n",
       " 4        45.9   964.0  18.458  92.9            2.0               16   \n",
       " ...       ...     ...     ...   ...            ...              ...   \n",
       " 15822     NaN     NaN  41.000   0.0            1.0                8   \n",
       " 15823     NaN     NaN  41.000   0.0            1.0                8   \n",
       " 15824     NaN     NaN  21.000   0.0            1.0                8   \n",
       " 15825     NaN     NaN  21.000   0.0            1.0                8   \n",
       " 15826     NaN     NaN  21.000   0.0            1.0                8   \n",
       " \n",
       "          DATE_OF_ENTRY_y  \n",
       " 0      02/27/14 00:00:00  \n",
       " 1      02/27/14 00:00:00  \n",
       " 2      02/27/14 00:00:00  \n",
       " 3      02/27/14 00:00:00  \n",
       " 4      02/27/14 00:00:00  \n",
       " ...                  ...  \n",
       " 15822  04/22/22 00:00:00  \n",
       " 15823  04/22/22 00:00:00  \n",
       " 15824  04/22/22 00:00:00  \n",
       " 15825  04/22/22 00:00:00  \n",
       " 15826  04/22/22 00:00:00  \n",
       " \n",
       " [15827 rows x 33 columns]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "dfs = load_data(fname_in)\n",
    "tfm = Transformer(dfs, cbs=[LowerStripRdnNameCB(col_src='NUCLIDE'),\n",
    "                            ])\n",
    "tfm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de551778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KEY</th>\n",
       "      <th>NUCLIDE</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>&lt; VALUE_Bq/kg</th>\n",
       "      <th>VALUE_Bq/kg</th>\n",
       "      <th>ERROR%_kg</th>\n",
       "      <th>&lt; VALUE_Bq/m²</th>\n",
       "      <th>VALUE_Bq/m²</th>\n",
       "      <th>ERROR%_m²</th>\n",
       "      <th>DATE_OF_ENTRY_x</th>\n",
       "      <th>...</th>\n",
       "      <th>LOWSLI</th>\n",
       "      <th>AREA</th>\n",
       "      <th>SEDI</th>\n",
       "      <th>OXIC</th>\n",
       "      <th>DW%</th>\n",
       "      <th>LOI%</th>\n",
       "      <th>MORS_SUBBASIN</th>\n",
       "      <th>HELCOM_SUBBASIN</th>\n",
       "      <th>SUM_LINK</th>\n",
       "      <th>DATE_OF_ENTRY_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30938</th>\n",
       "      <td>SLVEA2010001</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>334.25</td>\n",
       "      <td>1.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.886</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>O</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30939</th>\n",
       "      <td>SLVEA2010002</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>343.58</td>\n",
       "      <td>1.49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.092</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30940</th>\n",
       "      <td>SLVEA2010003</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>334.69</td>\n",
       "      <td>1.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.390</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30941</th>\n",
       "      <td>SLVEA2010004</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>348.50</td>\n",
       "      <td>1.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136.699</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30942</th>\n",
       "      <td>SLVEA2010005</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258.67</td>\n",
       "      <td>1.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.894</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30943</th>\n",
       "      <td>SLVEA2010006</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182.02</td>\n",
       "      <td>2.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.523</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30944</th>\n",
       "      <td>SLVEA2010007</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.34</td>\n",
       "      <td>2.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.946</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30945</th>\n",
       "      <td>SLVEA2010008</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.07</td>\n",
       "      <td>2.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.162</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30946</th>\n",
       "      <td>SLVEA2010009</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.70</td>\n",
       "      <td>3.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.444</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30947</th>\n",
       "      <td>SLVEA2010010</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.63</td>\n",
       "      <td>3.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.220</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30948</th>\n",
       "      <td>SLVEA2010011</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>12.24</td>\n",
       "      <td>3.88</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>5.035</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.7</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30949</th>\n",
       "      <td>SLVEA2010012</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;</td>\n",
       "      <td>0.330</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30950</th>\n",
       "      <td>SLVEA2010013</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>331.61</td>\n",
       "      <td>1.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125.566</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>O</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.9</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30951</th>\n",
       "      <td>SLVEA2010014</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>352.06</td>\n",
       "      <td>1.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144.516</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30952</th>\n",
       "      <td>SLVEA2010015</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>367.11</td>\n",
       "      <td>1.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.434</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30953</th>\n",
       "      <td>SLVEA2010016</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>328.97</td>\n",
       "      <td>1.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.348</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30954</th>\n",
       "      <td>SLVEA2010017</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>356.30</td>\n",
       "      <td>1.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.447</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30955</th>\n",
       "      <td>SLVEA2010018</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314.75</td>\n",
       "      <td>1.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118.765</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30956</th>\n",
       "      <td>SLVEA2010019</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>261.64</td>\n",
       "      <td>1.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.580</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30957</th>\n",
       "      <td>SLVEA2010020</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.00</td>\n",
       "      <td>1.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.058</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30958</th>\n",
       "      <td>SLVEA2010021</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143.65</td>\n",
       "      <td>2.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.680</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30959</th>\n",
       "      <td>SLVEA2010022</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109.36</td>\n",
       "      <td>2.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.153</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30960</th>\n",
       "      <td>SLVEA2010023</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.12</td>\n",
       "      <td>1.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.873</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30961</th>\n",
       "      <td>SLVEA2010024</td>\n",
       "      <td>cs137</td>\n",
       "      <td>LVEA01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.63</td>\n",
       "      <td>1.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.864</td>\n",
       "      <td>41179.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/11 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                KEY NUCLIDE  METHOD < VALUE_Bq/kg  VALUE_Bq/kg  ERROR%_kg  \\\n",
       "30938  SLVEA2010001   cs137  LVEA01           NaN       334.25       1.57   \n",
       "30939  SLVEA2010002   cs137  LVEA01           NaN       343.58       1.49   \n",
       "30940  SLVEA2010003   cs137  LVEA01           NaN       334.69       1.56   \n",
       "30941  SLVEA2010004   cs137  LVEA01           NaN       348.50       1.56   \n",
       "30942  SLVEA2010005   cs137  LVEA01           NaN       258.67       1.73   \n",
       "30943  SLVEA2010006   cs137  LVEA01           NaN       182.02       2.05   \n",
       "30944  SLVEA2010007   cs137  LVEA01           NaN       116.34       2.79   \n",
       "30945  SLVEA2010008   cs137  LVEA01           NaN        94.07       2.61   \n",
       "30946  SLVEA2010009   cs137  LVEA01           NaN        69.70       3.12   \n",
       "30947  SLVEA2010010   cs137  LVEA01           NaN        59.63       3.40   \n",
       "30948  SLVEA2010011   cs137  LVEA01             <        12.24       3.88   \n",
       "30949  SLVEA2010012   cs137  LVEA01             <         0.83        NaN   \n",
       "30950  SLVEA2010013   cs137  LVEA01           NaN       331.61       1.40   \n",
       "30951  SLVEA2010014   cs137  LVEA01           NaN       352.06       1.33   \n",
       "30952  SLVEA2010015   cs137  LVEA01           NaN       367.11       1.36   \n",
       "30953  SLVEA2010016   cs137  LVEA01           NaN       328.97       1.42   \n",
       "30954  SLVEA2010017   cs137  LVEA01           NaN       356.30       1.37   \n",
       "30955  SLVEA2010018   cs137  LVEA01           NaN       314.75       1.42   \n",
       "30956  SLVEA2010019   cs137  LVEA01           NaN       261.64       1.52   \n",
       "30957  SLVEA2010020   cs137  LVEA01           NaN       181.00       1.76   \n",
       "30958  SLVEA2010021   cs137  LVEA01           NaN       143.65       2.02   \n",
       "30959  SLVEA2010022   cs137  LVEA01           NaN       109.36       2.15   \n",
       "30960  SLVEA2010023   cs137  LVEA01           NaN        94.12       1.39   \n",
       "30961  SLVEA2010024   cs137  LVEA01           NaN        96.63       1.35   \n",
       "\n",
       "      < VALUE_Bq/m²  VALUE_Bq/m²  ERROR%_m² DATE_OF_ENTRY_x  ...  LOWSLI  \\\n",
       "30938           NaN      131.886    41179.0             NaN  ...     2.0   \n",
       "30939           NaN      132.092    41179.0             NaN  ...     4.0   \n",
       "30940           NaN      134.390    41179.0             NaN  ...     6.0   \n",
       "30941           NaN      136.699    41179.0             NaN  ...     8.0   \n",
       "30942           NaN      104.894    41179.0             NaN  ...    10.0   \n",
       "30943           NaN       77.523    41179.0             NaN  ...    12.0   \n",
       "30944           NaN       46.946    41179.0             NaN  ...    14.0   \n",
       "30945           NaN       38.162    41179.0             NaN  ...    16.0   \n",
       "30946           NaN       27.444    41179.0             NaN  ...    18.0   \n",
       "30947           NaN       24.220    41179.0             NaN  ...    20.0   \n",
       "30948             <        5.035    41179.0             NaN  ...    22.0   \n",
       "30949             <        0.330    41179.0             NaN  ...    24.0   \n",
       "30950           NaN      125.566    41179.0             NaN  ...     2.0   \n",
       "30951           NaN      144.516    41179.0             NaN  ...     4.0   \n",
       "30952           NaN      139.434    41179.0             NaN  ...     6.0   \n",
       "30953           NaN      124.348    41179.0             NaN  ...     8.0   \n",
       "30954           NaN      135.447    41179.0             NaN  ...    10.0   \n",
       "30955           NaN      118.765    41179.0             NaN  ...    12.0   \n",
       "30956           NaN      104.580    41179.0             NaN  ...    14.0   \n",
       "30957           NaN       74.058    41179.0             NaN  ...    16.0   \n",
       "30958           NaN       57.680    41179.0             NaN  ...    18.0   \n",
       "30959           NaN       42.153    41179.0             NaN  ...    20.0   \n",
       "30960           NaN       35.873    41179.0             NaN  ...    22.0   \n",
       "30961           NaN       38.864    41179.0             NaN  ...    24.0   \n",
       "\n",
       "         AREA  SEDI OXIC    DW%  LOI%  MORS_SUBBASIN HELCOM_SUBBASIN  \\\n",
       "30938  0.0151   5.0    O  0.115   0.9           14.0            14.0   \n",
       "30939  0.0151   5.0    A  0.159   0.8           14.0            14.0   \n",
       "30940  0.0151   5.0    A  0.189   0.8           14.0            14.0   \n",
       "30941  0.0151   5.0    A  0.194   0.8           14.0            14.0   \n",
       "30942  0.0151   5.0    A  0.195   0.8           14.0            14.0   \n",
       "30943  0.0151   5.0    A  0.221   0.8           14.0            14.0   \n",
       "30944  0.0151   5.0    A  0.238   0.8           14.0            14.0   \n",
       "30945  0.0151   5.0    A  0.234   0.8           14.0            14.0   \n",
       "30946  0.0151   5.0    A  0.242   0.8           14.0            14.0   \n",
       "30947  0.0151   5.0    A  0.257   0.7           14.0            14.0   \n",
       "30948  0.0151   5.0    A  0.264   0.7           14.0            14.0   \n",
       "30949  0.0151   5.0    A  0.244   0.8           14.0            14.0   \n",
       "30950  0.0151   5.0    O  0.115   0.9           14.0            14.0   \n",
       "30951  0.0151   5.0    A  0.164   0.8           14.0            14.0   \n",
       "30952  0.0151   5.0    A  0.191   0.8           14.0            14.0   \n",
       "30953  0.0151   5.0    A  0.188   0.8           14.0            14.0   \n",
       "30954  0.0151   5.0    A  0.179   0.8           14.0            14.0   \n",
       "30955  0.0151   5.0    A  0.186   0.8           14.0            14.0   \n",
       "30956  0.0151   5.0    A  0.194   0.8           14.0            14.0   \n",
       "30957  0.0151   5.0    A  0.209   0.8           14.0            14.0   \n",
       "30958  0.0151   5.0    A  0.214   0.8           14.0            14.0   \n",
       "30959  0.0151   5.0    A  0.218   0.8           14.0            14.0   \n",
       "30960  0.0151   5.0    A  0.212   0.8           14.0            14.0   \n",
       "30961  0.0151   5.0    A  0.217   0.8           14.0            14.0   \n",
       "\n",
       "       SUM_LINK    DATE_OF_ENTRY_y  \n",
       "30938       NaN  11/11/11 00:00:00  \n",
       "30939       NaN  11/11/11 00:00:00  \n",
       "30940       NaN  11/11/11 00:00:00  \n",
       "30941       NaN  11/11/11 00:00:00  \n",
       "30942       NaN  11/11/11 00:00:00  \n",
       "30943       NaN  11/11/11 00:00:00  \n",
       "30944       NaN  11/11/11 00:00:00  \n",
       "30945       NaN  11/11/11 00:00:00  \n",
       "30946       NaN  11/11/11 00:00:00  \n",
       "30947       NaN  11/11/11 00:00:00  \n",
       "30948       NaN  11/11/11 00:00:00  \n",
       "30949       NaN  11/11/11 00:00:00  \n",
       "30950       NaN  11/11/11 00:00:00  \n",
       "30951       NaN  11/11/11 00:00:00  \n",
       "30952       NaN  11/11/11 00:00:00  \n",
       "30953       NaN  11/11/11 00:00:00  \n",
       "30954       NaN  11/11/11 00:00:00  \n",
       "30955       NaN  11/11/11 00:00:00  \n",
       "30956       NaN  11/11/11 00:00:00  \n",
       "30957       NaN  11/11/11 00:00:00  \n",
       "30958       NaN  11/11/11 00:00:00  \n",
       "30959       NaN  11/11/11 00:00:00  \n",
       "30960       NaN  11/11/11 00:00:00  \n",
       "30961       NaN  11/11/11 00:00:00  \n",
       "\n",
       "[24 rows x 35 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "grp='sediment'\n",
    "check_data_sediment=tfm.dfs[grp][(tfm.dfs[grp]['DW%'] < 1) & (tfm.dfs[grp]['DW%'] > 0.001) ]\n",
    "check_data_sediment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe533d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KEY</th>\n",
       "      <th>NUCLIDE</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>&lt; VALUE_Bq/kg</th>\n",
       "      <th>VALUE_Bq/kg</th>\n",
       "      <th>ERROR%_kg</th>\n",
       "      <th>&lt; VALUE_Bq/m²</th>\n",
       "      <th>VALUE_Bq/m²</th>\n",
       "      <th>ERROR%_m²</th>\n",
       "      <th>DATE_OF_ENTRY_x</th>\n",
       "      <th>...</th>\n",
       "      <th>LOWSLI</th>\n",
       "      <th>AREA</th>\n",
       "      <th>SEDI</th>\n",
       "      <th>OXIC</th>\n",
       "      <th>DW%</th>\n",
       "      <th>LOI%</th>\n",
       "      <th>MORS_SUBBASIN</th>\n",
       "      <th>HELCOM_SUBBASIN</th>\n",
       "      <th>SUM_LINK</th>\n",
       "      <th>DATE_OF_ENTRY_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9824</th>\n",
       "      <td>SERPC1997001</td>\n",
       "      <td>cs134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.80</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9825</th>\n",
       "      <td>SERPC1997001</td>\n",
       "      <td>cs137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>389.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>589.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9826</th>\n",
       "      <td>SERPC1997002</td>\n",
       "      <td>cs134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.78</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9827</th>\n",
       "      <td>SERPC1997002</td>\n",
       "      <td>cs137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>420.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1060.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9828</th>\n",
       "      <td>SERPC1997003</td>\n",
       "      <td>cs134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.12</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15257</th>\n",
       "      <td>SKRIL1999062</td>\n",
       "      <td>th228</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15258</th>\n",
       "      <td>SKRIL1999063</td>\n",
       "      <td>k40</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1210.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15259</th>\n",
       "      <td>SKRIL1999063</td>\n",
       "      <td>ra226</td>\n",
       "      <td>KRIL01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15260</th>\n",
       "      <td>SKRIL1999063</td>\n",
       "      <td>ra228</td>\n",
       "      <td>KRIL01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15261</th>\n",
       "      <td>SKRIL1999063</td>\n",
       "      <td>th228</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                KEY NUCLIDE  METHOD < VALUE_Bq/kg  VALUE_Bq/kg  ERROR%_kg  \\\n",
       "9824   SERPC1997001   cs134     NaN           NaN         3.80       20.0   \n",
       "9825   SERPC1997001   cs137     NaN           NaN       389.00        4.0   \n",
       "9826   SERPC1997002   cs134     NaN           NaN         4.78       13.0   \n",
       "9827   SERPC1997002   cs137     NaN           NaN       420.00        4.0   \n",
       "9828   SERPC1997003   cs134     NaN           NaN         3.12       17.0   \n",
       "...             ...     ...     ...           ...          ...        ...   \n",
       "15257  SKRIL1999062   th228       1           NaN        68.00        NaN   \n",
       "15258  SKRIL1999063     k40       1           NaN      1210.00        NaN   \n",
       "15259  SKRIL1999063   ra226  KRIL01           NaN        56.50        NaN   \n",
       "15260  SKRIL1999063   ra228  KRIL01           NaN        72.20        NaN   \n",
       "15261  SKRIL1999063   th228       1           NaN        74.20        NaN   \n",
       "\n",
       "      < VALUE_Bq/m²  VALUE_Bq/m²  ERROR%_m² DATE_OF_ENTRY_x  ...  LOWSLI  \\\n",
       "9824            NaN         5.75        NaN             NaN  ...     2.0   \n",
       "9825            NaN       589.00        NaN             NaN  ...     2.0   \n",
       "9826            NaN        12.00        NaN             NaN  ...     4.0   \n",
       "9827            NaN      1060.00        NaN             NaN  ...     4.0   \n",
       "9828            NaN        12.00        NaN             NaN  ...     6.0   \n",
       "...             ...          ...        ...             ...  ...     ...   \n",
       "15257           NaN          NaN        NaN             NaN  ...    15.0   \n",
       "15258           NaN          NaN        NaN             NaN  ...    21.5   \n",
       "15259           NaN          NaN        NaN             NaN  ...    21.5   \n",
       "15260           NaN          NaN        NaN             NaN  ...    21.5   \n",
       "15261           NaN          NaN        NaN             NaN  ...    21.5   \n",
       "\n",
       "        AREA  SEDI OXIC  DW%  LOI%  MORS_SUBBASIN HELCOM_SUBBASIN  SUM_LINK  \\\n",
       "9824   0.008   5.0    A  0.0   0.0           11.0            11.0         a   \n",
       "9825   0.008   5.0    A  0.0   0.0           11.0            11.0         a   \n",
       "9826   0.008   5.0    A  0.0   0.0           11.0            11.0         a   \n",
       "9827   0.008   5.0    A  0.0   0.0           11.0            11.0         a   \n",
       "9828   0.008   5.0    A  0.0   0.0           11.0            11.0         a   \n",
       "...      ...   ...  ...  ...   ...            ...             ...       ...   \n",
       "15257  0.006   0.0    O  0.0   0.0           11.0            11.0         a   \n",
       "15258  0.006   0.0    O  0.0   0.0           11.0            11.0         a   \n",
       "15259  0.006   0.0    O  0.0   0.0           11.0            11.0         a   \n",
       "15260  0.006   0.0    O  0.0   0.0           11.0            11.0         a   \n",
       "15261  0.006   0.0    O  0.0   0.0           11.0            11.0         a   \n",
       "\n",
       "      DATE_OF_ENTRY_y  \n",
       "9824              NaN  \n",
       "9825              NaN  \n",
       "9826              NaN  \n",
       "9827              NaN  \n",
       "9828              NaN  \n",
       "...               ...  \n",
       "15257             NaN  \n",
       "15258             NaN  \n",
       "15259             NaN  \n",
       "15260             NaN  \n",
       "15261             NaN  \n",
       "\n",
       "[302 rows x 35 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "grp='sediment'\n",
    "check_data_sediment=tfm.dfs[grp][(tfm.dfs[grp]['DW%'] == 0) ]\n",
    "check_data_sediment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357222d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KEY</th>\n",
       "      <th>NUCLIDE</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>&lt; VALUE_Bq/kg</th>\n",
       "      <th>VALUE_Bq/kg</th>\n",
       "      <th>BASIS</th>\n",
       "      <th>ERROR%</th>\n",
       "      <th>NUMBER</th>\n",
       "      <th>DATE_OF_ENTRY_x</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>...</th>\n",
       "      <th>BIOTATYPE</th>\n",
       "      <th>TISSUE</th>\n",
       "      <th>NO</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>WEIGHT</th>\n",
       "      <th>DW%</th>\n",
       "      <th>LOI%</th>\n",
       "      <th>MORS_SUBBASIN</th>\n",
       "      <th>HELCOM_SUBBASIN</th>\n",
       "      <th>DATE_OF_ENTRY_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>BERPC1997002</td>\n",
       "      <td>k40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.00</td>\n",
       "      <td>W</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5972</th>\n",
       "      <td>BERPC1997002</td>\n",
       "      <td>cs137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.60</td>\n",
       "      <td>W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5973</th>\n",
       "      <td>BERPC1997002</td>\n",
       "      <td>cs134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.14</td>\n",
       "      <td>W</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5974</th>\n",
       "      <td>BERPC1997001</td>\n",
       "      <td>k40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5975</th>\n",
       "      <td>BERPC1997001</td>\n",
       "      <td>cs137</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5976</th>\n",
       "      <td>BERPC1997001</td>\n",
       "      <td>cs134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.21</td>\n",
       "      <td>W</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>F</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               KEY NUCLIDE METHOD < VALUE_Bq/kg  VALUE_Bq/kg BASIS  ERROR%  \\\n",
       "5971  BERPC1997002     k40    NaN           NaN       116.00     W     3.0   \n",
       "5972  BERPC1997002   cs137    NaN           NaN        12.60     W     4.0   \n",
       "5973  BERPC1997002   cs134    NaN           NaN         0.14     W    18.0   \n",
       "5974  BERPC1997001     k40    NaN           NaN       116.00     W     4.0   \n",
       "5975  BERPC1997001   cs137    NaN           NaN        12.00     W     4.0   \n",
       "5976  BERPC1997001   cs134    NaN           NaN         0.21     W    24.0   \n",
       "\n",
       "      NUMBER DATE_OF_ENTRY_x  COUNTRY  ... BIOTATYPE  TISSUE   NO  LENGTH  \\\n",
       "5971     NaN             NaN     91.0  ...         F       5  0.0     0.0   \n",
       "5972     NaN             NaN     91.0  ...         F       5  0.0     0.0   \n",
       "5973     NaN             NaN     91.0  ...         F       5  0.0     0.0   \n",
       "5974     NaN             NaN     91.0  ...         F       5  0.0     0.0   \n",
       "5975     NaN             NaN     91.0  ...         F       5  0.0     0.0   \n",
       "5976     NaN             NaN     91.0  ...         F       5  0.0     0.0   \n",
       "\n",
       "      WEIGHT  DW% LOI%  MORS_SUBBASIN  HELCOM_SUBBASIN  DATE_OF_ENTRY_y  \n",
       "5971     0.0  0.0  0.0           11.0               11              NaN  \n",
       "5972     0.0  0.0  0.0           11.0               11              NaN  \n",
       "5973     0.0  0.0  0.0           11.0               11              NaN  \n",
       "5974     0.0  0.0  0.0           11.0               11              NaN  \n",
       "5975     0.0  0.0  0.0           11.0               11              NaN  \n",
       "5976     0.0  0.0  0.0           11.0               11              NaN  \n",
       "\n",
       "[6 rows x 33 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "grp='biota'\n",
    "check_data_sediment=tfm.dfs[grp][(tfm.dfs[grp]['DW%'] == 0) ]\n",
    "check_data_sediment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

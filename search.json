[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MARISCO",
    "section": "",
    "text": "The IAEA Marine Radioactivity Information System (MARIS) provides open access to radioactivity measurements in marine environments. Developed by the IAEA Marine Environmental Laboratories in Monaco, MARIS offers data on seawater, biota, sediment, and suspended matter.\nThis Python package includes command-line tools to convert MARIS datasets into NetCDF or .csv formats, enhancing compatibility with various scientific and data analysis software.",
    "crumbs": [
      "MARISCO"
    ]
  },
  {
    "objectID": "index.html#core-concept-handlers",
    "href": "index.html#core-concept-handlers",
    "title": "MARISCO",
    "section": "Core Concept: Handlers",
    "text": "Core Concept: Handlers\nmarisco is built around the concept of handlers - specialized modules designed to convert MARIS datasets into NetCDF format. Each handler is tailored to a specific data provider and implemented as a dedicated Jupyter notebook.\n\nLiterate Programming Approach\nWe’ve adopted a Literate Programming approach, which means:\n\nDocumentation: Each handler serves as comprehensive documentation.\nCode Reference: The notebooks contain the actual implementation code.\nCommunication Tool: They facilitate discussions with data providers about discrepancies or inconsistencies.\n\n\n\nPowered by nbdev\nTo achieve this, we leverage nbdev, a powerful tool that allows us to:\n\nWrite code within Jupyter notebooks\nAutomatically export relevant parts as dedicated Python modules\n\nThis approach bridges the gap between documentation and implementation, ensuring they remain in sync.\n\n\nSee It in Action\nFor a concrete example of this approach, check out our OSPAR dataset handler implementation.\n\n\nList of currently available handlers\nMARISCO includes a suite of specialized data handlers designed to:\n\nConvert provider-specific data formats into standardized MARIS NetCDF files\nEnsure data quality and consistency across providers\nFacilitate integration with the MARIS marine radioactivity database\nSupport automated data processing workflows\n\nThe following handlers are currently implemented:\n\n\n\nHandler\nDescription\nLink to Data Source\n\n\n\n\nMARIS Legacy\nAll legacy MARIS datasets from the MARIS Master Database\n-\n\n\nHELCOM\nHELCOM marine environment protection datasets\nHELCOM\n\n\nOSPAR\nOSPAR marine environment datasets\nODIMS OSPAR\n\n\nTEPCO\nTEPCO Fukushima monitoring data\nTEPCO Monitoring\n\n\nGEOTRACES\nBODC GEOTRACES oceanographic data\nGEOTRACES IDP2021",
    "crumbs": [
      "MARISCO"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "MARISCO",
    "section": "Install",
    "text": "Install\nNow, to install marisco simply run\npip install marisco\nOnce successfully installed, run the following command:\nmaris_init\nThis command:\n\ncreates a .marisco/ directory containing various configuration/configurable files ((below)) in your /home directory;\ncreates a configs.toml file containing default but configurable settings (default paths, …);\ndownloads several MARIS DB nomenclature/lookup table into .marisco/lut/ directory;\ndownloads maris-template.nc, the MARIS NetCDF4 template.\n\n\nZotero API key\nUpon conversion, marisco will automatically retrieve the bibliographic metadata of each MARIS dataset from Zotero. To do so, you need to define the following environment variable ZOTERO_API_KEY containing the MARIS Zotero API key. Please contact the MARIS team to get your API key.",
    "crumbs": [
      "MARISCO"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "MARISCO",
    "section": "Getting started",
    "text": "Getting started\n\nCommand line utilities\nAll commands accept a -h argument to get access to its documentation.\n\nmaris_init\nDownload configuration file, NetCDF MARIS template and required lookup tables (nomenclatures).\n\n\nmaris_to_nc\nConvert helcom, geotraces, tepco or ospar marine radioactivity datasets to MARIS NetCDF4 format.\nusage: maris_to_nc [-h] [--src SRC] ds dest\n\npositional arguments:\n  ds          Name of the dataset to encode as NetCDF4\n  dest        Output path for NetCDF file\n\noptions:\n  -h, --help  show this help message and exit\n  --src SRC   Optional input data path only required for the 'GEOTRACES' dataset\nFor instance: maris_to_nc ospar 191-OSPAR-2024.nc\n\n\nmaris_db_to_nc\nThe MARIS Master Database integrates two types of datasets:\n\nHistorical datasets retrieved from published scientific papers\nOngoing monitoring data from international programs like HELCOM, OSPAR, TEPCO, and GEOTRACES\n\nThis command-line utility converts MARIS datasets from their legacy format to NetCDF4, making them more accessible for modern data analysis workflows. Users can either convert the entire database or specify particular datasets by their reference IDs for selective conversion.\nusage: maris_db_to_nc [-h] [--ref_ids REF_IDS] src dest\n\nConvert MARIS legacy database to NetCDF4 format. If ref_ids is provided as comma-separated values, only encodes those subsets.\n\npositional arguments:\n  src                Path to MARIS database dump as `.txt` file\n  dest               Output path for NetCDF file(s)\n\noptions:\n  -h, --help         show this help message and exit\n  --ref_ids REF_IDS  Optional comma-separated reference IDs (e.g., \"123,456,789\") (default: )\nFor instance:\n\nmaris_db_to_nc \"~/pro/data/maris/2024-11-20 MARIS_QA_shapetype_id=1.txt\" ~/pro/tmp/output\n\nor maris_db_to_nc \"~/pro/data/maris/2024-11-20 MARIS_QA_shapetype_id=1.txt\" ~/pro/tmp/output --ref_ids=\"16,30\" for a subset of the MARIS Master Database.\n\n\n\nmaris_nc_to_csv\nThis utility converts NetCDF files to CSV files that conform to the MARIS Standard format, originally designed for OpenRefine workflows.\nAlthough MARISCO has now superseded OpenRefine in the data preparation pipeline, the MARIS master database continues to require CSV inputs in this legacy format. This command-line utility, built with the MARISCO library, handles the conversion process.\nusage: maris_nc_to_csv [-h] src dest\n\nConverts NetCDF files into CSV files that follow the MARIS Standard format.\n\npositional arguments:\n  src         Input path and filename for NetCDF file\n  dest        Output path and filename (without extension) for CSV file\n\noptions:\n  -h, --help  show this help message and exit\nFor instance: maris_nc_to_csv ~/pro/tmp/output/191-OSPAR-2024.nc ~/pro/tmp/output/191-OSPAR-2024\n\n\n\n\n\n\nTipNote\n\n\n\nWhen specifying the destination path (e.g., ~/pro/tmp/output/191-OSPAR-2024), the utility automatically appends the MARIS sample type to the filename. For example:\n\n191-OSPAR-2024_BIOTA.csv for biological samples\n\nWhile this specific example produces only a BIOTA file, the utility can generate multiple files (one per sample type) depending on the content of the source dataset. This reflects the NetCDF4 file structure, where each MARIS sample type is stored as a separate group within the file.",
    "crumbs": [
      "MARISCO"
    ]
  },
  {
    "objectID": "index.html#development",
    "href": "index.html#development",
    "title": "MARISCO",
    "section": "Development",
    "text": "Development\nAs already explained the maris_init command will download the maris-template.nc file from MARISCO GitHub repository from the following path: nbs/api/files/nc/maris-template.nc to your home directory under .marisco/ folder.\nYou have to know that the MARIS NetCDF template is generated from nbs/api/files/cdl/maris.cdl Common Data Language (CDL) file as defined by Unidata. During development, to generate the MARIS NetCDF template nbs/api/files/nc/maris-template.nc:\n\ninstall the NetCDF-C utilities\nonce in Marisco home directory, run:\n\nncgen -4 -o nbs/files/nc/maris-template.nc nbs/files/cdl/maris.cdl\ncp nbs/files/nc/maris-template.nc ~/.marisco/",
    "crumbs": [
      "MARISCO"
    ]
  },
  {
    "objectID": "metadata/field-definition.html",
    "href": "metadata/field-definition.html",
    "title": "Field definitions",
    "section": "",
    "text": "The MARIS data is converted to a standardized CSV format for importing into the MARIS database using Open Refine. The standardized variable names for Open Refine are provided in Table 1, and a detailed description of each variable is given below. Additionally, MARIS data is available in NetCDF4 format. The standardized variable names for NetCDF4 are also provided in Table 1, with descriptions for each variable included below.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#table-1-standardized-variable-names-open-refine-and-netcdf",
    "href": "metadata/field-definition.html#table-1-standardized-variable-names-open-refine-and-netcdf",
    "title": "Field definitions",
    "section": "Table 1 Standardized Variable Names (Open Refine and NetCDF)",
    "text": "Table 1 Standardized Variable Names (Open Refine and NetCDF)\n\n\n\n\n\n\n\n\n\nFriendly Name\nMARISCO Name\nOpen Refine Name\nNetCDF Name\n\n\n\n\nSample quality\n_\nsampquality\n_\n\n\nSample type ID\nSAMPLE_TYPE\nsamptype_id\nSample type included as netcdf.group\n\n\nLaboratory ID\nLAB\nlab_id\nlab\n\n\nLatitude\nLAT\nlatitude\nlat\n\n\nLongitude\nLON\nlongitude\nlon\n\n\nStation\n_\nstation\n_\n\n\nSample lab code\nSMP_ID\nsamplabcode\nsmp_id\n\n\nProfile ID\nPROFILE_ID\nprofile_id\nprofile_id\n\n\nTransect ID\n_\ntransect_id\n_\n\n\nSampling depth\nSMP_DEPTH\nsampdepth\nsmp_depth\n\n\nTotal depth\nTOT_DEPTH\ntotdepth\ntot_depth\n\n\nBegin period\nTIME\nbegperiod\ntime\n\n\nEnd period\n_\nendperiod\n_\n\n\nNuclide ID\nNUCLIDE\nnuclide_id\nnuclide\n\n\nDetection Limit\nDL\ndetection\ndl\n\n\nActivity\nVALUE\nactivity\nvalue\n\n\nUncertainty\nUNC\nuncertaint\nunc\n\n\nUnit ID\nUNIT\nunit_id\nunit\n\n\nVariable type\n_\nvartype\n_\n\n\nFrequency\n_\nfreq\n_\n\n\nRange low detection\n_\nrl_detection\n_\n\n\nRange low\n_\nrangelow\n_\n\n\nRange upper\n_\nrangeupp\n_\n\n\nSpecies ID\nSPECIES\nspecies_id\nspecies\n\n\nBiological group\nBIO_GROUP\n_\nbio_group\n\n\nTaxon name\nTAXONNAME\nTaxonname\n_\n\n\nTaxon reported name\nTAXONREPNAME\nTaxonRepName\n_\n\n\nCommon name\n_\nCommonname\n_\n\n\nTaxon rank\nTAXONRANK\nTaxonrank\n_\n\n\nTaxon database\nTAXONDB\nTaxonDB\n_\n\n\nTaxon database ID\nTAXONDBID\nTaxonDBID\n_\n\n\nTaxon database URL\nTAXONDBURL\nTaxonDBURL\n_\n\n\nBody part ID\nBODY_PART\nbodypar_id\nbody_part\n\n\nSlice up\nTOP\nsliceup\ntop\n\n\nSlice down\nBOTTOM\nslicedown\nbottom\n\n\nSediment type ID\nSED_TYPE\nsedtype_id\nsed_type\n\n\nSediment reported name\n_\nSedRepName\n_\n\n\nVolume\nVOL\nvolume\n_\n\n\nSalinity\nSAL\nsalinity\nsalinity\n\n\nTemperature\nTEMP\ntemperatur\ntemperature\n\n\nFiltered\nFILT\nfiltered\nfiltered\n\n\nFilter pore\n_\nfiltpore\n_\n\n\nAcidified\n_\nacid\n_\n\n\nOxygen\n_\noxygen\n_\n\n\nSample area\nAREA\nsamparea\narea\n\n\nDry weight\nDRYWT\ndrywt\ndrywt\n\n\nWet weight\nWETWT\nwetwt\nwetwt\n\n\nPercent weight\nPERCENTWT\npercentwt\n_\n\n\nSampling method\nSAMP_MET\nsampmet_id\nsamp_met\n\n\nDrying method\n_\ndrymet_id\n_\n\n\nPreparation method\nPREP_MET\nprepmet_id\nprep_met\n\n\nCounting method\nCOUNT_MET\ncounmet_id\ncount_met\n\n\nReference\nREF_ID\nref_id\n_\n\n\nReference note\n_\nrefnote\n_\n\n\nSample note\n_\nsampnote\n_\n\n\nMeasurement note\n_\nmeasurenote\n_\n\n\nGood for export\n_\ngfe\n_\n\n\nArea\nAREA\narea\narea",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#variable-descriptions",
    "href": "metadata/field-definition.html#variable-descriptions",
    "title": "Field definitions",
    "section": "Variable Descriptions",
    "text": "Variable Descriptions",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#sample-quality",
    "href": "metadata/field-definition.html#sample-quality",
    "title": "Field definitions",
    "section": "Sample Quality",
    "text": "Sample Quality\n\nDescription:\nDefines the quality of the sample. Examples include: Good (G), Caution (C), Fail (F).\n\n\nLookup Table (LUT) in use:\nNo. \n\n\nOpen Refine Variable Name:\nsampquality\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF\n\n\nNetCDF Data Type:\nNot included in NetCDF",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#sample-type-id",
    "href": "metadata/field-definition.html#sample-type-id",
    "title": "Field definitions",
    "section": "Sample Type ID",
    "text": "Sample Type ID\n\nDescription:\nIn MARIS, samples are categorized by type into WATER, BIOTA, SEDIMENT, and SUSPENDED types. The NetCDF data format separates MARIS data into ‘NetCDF groups’ by the sample type. Open Refine formats MARIS data into separate CSV files by sample type.\n\nSEAWATER includes seawater and brackish water.\nBIOTA includes various types of biota.\nSEDIMENT includes various types of sediments.\nSUSPENDED includes various types of suspended matter.\n\n\n\nLookup Table (LUT) in use:\nNo.\n\n\nOpen Refine Variable Name:\nsamptype_id\n\n\nOpen Refine Data Type:\nAn integer value : - 1 : SEAWATER - 2 : BIOTA - 3 : SEDIMENT - 4 : SUSPENDED\n\n\nNetCDF Variable Name:\nSample type is included as netcdf.group\n\n\nNetCDF Data Type:\nstring",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#laboratory-id",
    "href": "metadata/field-definition.html#laboratory-id",
    "title": "Field definitions",
    "section": "Laboratory ID",
    "text": "Laboratory ID\n\nDescription:\nThe Laboratory ID identifies the laboratory that processed the sample.\n\n\nLookup Table (LUT) in use:\nYes, dbo_lab.xlsx\n\n\nOpen Refine Variable Name:\nlab_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nlab\n\n\nNetCDF Data Type:\nAn integer value (the ‘id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#latitude-decimal",
    "href": "metadata/field-definition.html#latitude-decimal",
    "title": "Field definitions",
    "section": "Latitude Decimal",
    "text": "Latitude Decimal\n\nDescription:\nLatitude in decimal format (DDD.DDDDD°) with ranges from -90° to 90°.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nlatitude\n\n\nOpen Refine Data Type:\nFloat with values between -90° to 90°.\n\n\nNetCDF Variable Name:\nlat\n\n\nNetCDF Data Type:\nFloat with values between -90° to 90°.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#longitude-decimal",
    "href": "metadata/field-definition.html#longitude-decimal",
    "title": "Field definitions",
    "section": "Longitude Decimal",
    "text": "Longitude Decimal\n\nDescription:\nLongitude in decimal format (DDD.DDDDD°) with ranges from -180° to 180°.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nlongitude\n\n\nOpen Refine Data Type:\nFloat with values between -180° to 180°.\n\n\nNetCDF Variable Name:\nlon\n\n\nNetCDF Data Type:\nFloat with values between -180° to 180°.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#station",
    "href": "metadata/field-definition.html#station",
    "title": "Field definitions",
    "section": "Station",
    "text": "Station\n\nDescription:\nThe name of the station where the sample was collected.\n\n\nLookup Table (LUT) in use:\nNo.\n\n\nOpen Refine Variable Name:\nstation\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#sample-lab-code",
    "href": "metadata/field-definition.html#sample-lab-code",
    "title": "Field definitions",
    "section": "Sample Lab Code",
    "text": "Sample Lab Code\n\nDescription:\nThe data provider’s sample laboratory code should be stored exactly as provided, without any modifications.\n\n\nLookup Table (LUT) in use:\nNo.\n\n\nOpen Refine Variable Name:\nsamplabcode\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#profile-id",
    "href": "metadata/field-definition.html#profile-id",
    "title": "Field definitions",
    "section": "Profile ID",
    "text": "Profile ID\n\nDescription:\nProfile ID is provided as is by the data provider and is an identifier for linking data which are part of a sequence, i.e., a vertical profile.\n\n\nLookup Table (LUT) in use:\nNo.\n\n\nOpen Refine Variable Name:\nprofile_id\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot defined in NetCDF.\n\n\nNetCDF Data Type:\nNot defined in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#transect-id",
    "href": "metadata/field-definition.html#transect-id",
    "title": "Field definitions",
    "section": "Transect ID",
    "text": "Transect ID\n\nDescription:\nTransect ID is provided as is by the data provider and is an identifier for linking data which are part of a sequence, i.e., a horizontal transect.\n\n\nLookup Table (LUT) in use:\nNo.\n\n\nOpen Refine Variable Name:\ntransect_id\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot defined in NetCDF.\n\n\nNetCDF Data Type:\nNot defined in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#sampling-depth",
    "href": "metadata/field-definition.html#sampling-depth",
    "title": "Field definitions",
    "section": "Sampling Depth",
    "text": "Sampling Depth\n\nDescription:\nDepth from the water surface in meters at which the sample was taken. A value of “0” indicates that the sample was collected at the surface. A value of “-1” indicates that sample depth information is not available.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nsampdepth\n\n\nOpen Refine Data Type:\nFloat\n\n\nNetCDF Variable Name:\nsmp_depth\n\n\nNetCDF Data Type:\nFloat",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#total-depth",
    "href": "metadata/field-definition.html#total-depth",
    "title": "Field definitions",
    "section": "Total Depth",
    "text": "Total Depth\n\nDescription:\nTotal water column depth in meters (m).\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\ntotdepth\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\ntot_depth\n\n\nNetCDF Data Type:\nfloat",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#begin-period",
    "href": "metadata/field-definition.html#begin-period",
    "title": "Field definitions",
    "section": "Begin Period",
    "text": "Begin Period\n\nDescription:\n‘Begin Period’ refers to the date when the collection of sample(s) began. If only a year is provided in the dataset, set the date to January 1st of that year (e.g., 2024 becomes 2024-01-01). If both a year and month are provided, set the date to the first day of that month (e.g., May 2024 becomes 2024-05-01). Date format of yyyy-mm-dd.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nbegperiod\n\n\nOpen Refine Data Type:\nDATETIME string in the format (yyyy-mm-dd hh:mm:ss)\n\n\nNetCDF Variable Name:\ntime\n\n\nNetCDF Data Type:\nDATETIME string in the format (yyyy-mm-dd hh:mm:ss)",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#end-period",
    "href": "metadata/field-definition.html#end-period",
    "title": "Field definitions",
    "section": "End Period",
    "text": "End Period\n\nDescription:\n‘End Period’ refers to the date when the collection of sample(s) ended. If only a year is provided in the dataset, set the date to January 1st of that year (e.g., 2024 becomes 2024-01-01). If both a year and month are provided, set the date to the first day of that month (e.g., May 2024 becomes 2024-05-01). Date format of yyyy-mm-dd.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nendperiod\n\n\nOpen Refine Data Type:\nDATETIME string in the format (yyyy-mm-dd hh:mm:ss)\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#nuclide-id",
    "href": "metadata/field-definition.html#nuclide-id",
    "title": "Field definitions",
    "section": "Nuclide ID",
    "text": "Nuclide ID\n\nDescription:\nIdentifier for a specific nuclide (isotope) within the MARIS database.\n\n\nLookup Table (LUT) in use:\nYes, dbo_nuclide.xlsx\n\n\nOpen Refine Variable Name:\nnuclide_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nnuclide\n\n\nNetCDF Data Type:\nstring",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#detection-limit",
    "href": "metadata/field-definition.html#detection-limit",
    "title": "Field definitions",
    "section": "Detection Limit",
    "text": "Detection Limit\n\nDescription:\nThe detection limit variable describes the Activity variable as follows:\n‘&lt;’: The reported value for the Activity variable is the Minimum Detectable Activity (MDA) or ISO11029 detection limit. ‘=’: The Activity variable represents the measured value, and an associated uncertainty should be provided. ‘ND’: Indicates that neither an activity value nor an MDA (or detection limit) is reported. ‘DE’: When the reported Activity variable is an aggregation of multiple samples, the detection limit variable is defined as Derived (DE), see Variable Type for more information related to aggregation of activity reported.\n\n\nLookup Table (LUT) in use:\nYes, dbo_detectlimit.xlsx.\n\n\nOpen Refine Variable Name:\ndetection\n\n\nOpen Refine Data Type:\nAn integer value (the ‘id’ defined in the LUT).\n\n\nNetCDF Variable Name:\ndl\n\n\nNetCDF Data Type:\nAn integer value (the ‘id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#activity",
    "href": "metadata/field-definition.html#activity",
    "title": "Field definitions",
    "section": "Activity",
    "text": "Activity\n\nDescription:\nThe measured activity value or MDA for the nuclide reported. Several variables are used to describe the Activity variable, including:\nNuclide ID: Describes the nuclide for which the activity is reported. Detection Limit: Indicates whether the reported Activity value is a measured activity value or below the detection limit. Unit ID: Describes the unit associated with the reported Activity variable. Uncertainty: The associated uncertainty of the Activity variable. Variable Type: Describes whether the reported Activity variable is an aggregate, sum, mean, median, etc.\n\n\nLookup Table (LUT) in use:\nNo.\n\n\nOpen Refine Variable Name:\nactivity\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nvalue\n\n\nNetCDF Data Type:\nfloat",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#uncertainty",
    "href": "metadata/field-definition.html#uncertainty",
    "title": "Field definitions",
    "section": "Uncertainty",
    "text": "Uncertainty\n\nDescription:\nThe uncertainty associated with the measurement of the activity must be reported as a 1 sigma (k=1) measurement uncertainty. This uncertainty should be expressed in the same units as the activity variable.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nuncertaint\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nunc\n\n\nNetCDF Data Type:\nfloat",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#unit-id",
    "href": "metadata/field-definition.html#unit-id",
    "title": "Field definitions",
    "section": "Unit ID",
    "text": "Unit ID\n\nDescription:\nRepresents the ID value from the Lookup Table (LUT) corresponding to the unit of measurement for both the activity variable and the uncertainty variable (if applicable). For seawater measurements, ensure that the unit is converted from ‘Bq L⁻¹’ to ‘Bq m⁻³’, and use the corresponding LUT value for ‘Bq m⁻³’.\n\n\nLookup Table (LUT) in use:\nYes, dbo_unit.xlsx.\n\n\nOpen Refine Variable Name:\nunit_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nunit\n\n\nNetCDF Data Type:\nAn integer value (the ‘id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#variable-type",
    "href": "metadata/field-definition.html#variable-type",
    "title": "Field definitions",
    "section": "Variable Type",
    "text": "Variable Type\n\nDescription:\nDescribes the type of aggregation applied to the measurements if they are not reported as individual values. Possible values include:\nAM: Arithmetic Mean GM: Geometric Mean MED: Median MAX: Maximum MIN: Minimum\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nvartype\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#frequency",
    "href": "metadata/field-definition.html#frequency",
    "title": "Field definitions",
    "section": "Frequency",
    "text": "Frequency\n\nDescription:\nIndicates how often the sample is taken or the measurement is recorded. This variable helps to understand the regularity of data collection and can include details such as daily, weekly, monthly, or any other specified time interval.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nfreq\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#range-low-detection",
    "href": "metadata/field-definition.html#range-low-detection",
    "title": "Field definitions",
    "section": "Range Low Detection",
    "text": "Range Low Detection\n\nDescription:\nIf aggregation occurred when evaluating the Activity variable, the ‘Range Low Detection’ value describes the ‘Range Low’ variable as follows:\n\n‘&lt;’: The reported value for the Activity variable is the Minimum Detectable Activity (MDA) or ISO11029 detection limit.\n‘=’: The Activity variable represents the measured value, with an associated uncertainty provided.\n‘ND’: Indicates that neither an activity value nor an MDA (or detection limit) is reported.\n‘DE’: When the reported Activity variable is an aggregation of multiple samples, the detection limit variable is defined as Derived (DE). See the Variable Type section for more details on aggregation of reported activity.\n\n\n\nLookup Table (LUT) in use:\nYes, dbo_detectlimit.xlsx.\n\n\nOpen Refine Variable Name:\nrl_detection\n\n\nOpen Refine Data Type:\nAn integer value (the ‘id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#range-low",
    "href": "metadata/field-definition.html#range-low",
    "title": "Field definitions",
    "section": "Range Low",
    "text": "Range Low\n\nDescription:\nIf aggregation is applied to the measurements, the ‘Range Low’ variable represents the smallest activity measured within the aggregated dataset. This value should be reported in a format consistent with the unit specified by the Unit ID variable.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nrangelow\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#range-upper",
    "href": "metadata/field-definition.html#range-upper",
    "title": "Field definitions",
    "section": "Range Upper",
    "text": "Range Upper\n\nDescription:\nIf aggregation is applied to the measurements, the ‘Range Upper’ variable represents the largest activity measured within the aggregated dataset. This value should be reported in a format consistent with the unit specified by the Unit ID variable.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nrangeupp\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#species-id",
    "href": "metadata/field-definition.html#species-id",
    "title": "Field definitions",
    "section": "Species ID",
    "text": "Species ID\n\nDescription:\nRepresents the identifier for the species included in the sample. If a specific species is not provided but a biological group (e.g., ‘Fish’) is specified, use this group information to define the species.\n\n\nLookup Table (LUT) in use:\nYes, dbo_species.xlsx.\n\n\nOpen Refine Variable Name:\nspecies_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘species_id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nspecies\n\n\nNetCDF Data Type:\nAn integer value (the ‘species_id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#biological-group",
    "href": "metadata/field-definition.html#biological-group",
    "title": "Field definitions",
    "section": "Biological Group",
    "text": "Biological Group\n\nDescription:\nThe biological group of the sample, if applicable. Grouping of related species (e.g. crustaceans, molluscs, fish etc.).\n\n\nLookup Table (LUT) in use:\nYes, dbo_species.xlsx includes a “biogroup_id” for all species which links to the dbo_biogroup.xlsx look-up.\n\n\nOpen Refine Variable Name:\nNone\n\n\nOpen Refine Data Type:\nNone\n\n\nNetCDF Variable Name:\nbio_group\n\n\nNetCDF Data Type:\nAn integer value (the ‘biogroup_id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#taxon-name",
    "href": "metadata/field-definition.html#taxon-name",
    "title": "Field definitions",
    "section": "Taxon Name",
    "text": "Taxon Name\n\nDescription:\nScientific name of the taxon.\n\n\nLookup Table (LUT) in use:\nNo, ‘Taxon Name’ is defined for each species in the dbo_species.xlsx look-up.\n\n\nOpen Refine Variable Name:\nTaxonname\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#taxon-reported-name",
    "href": "metadata/field-definition.html#taxon-reported-name",
    "title": "Field definitions",
    "section": "Taxon Reported Name",
    "text": "Taxon Reported Name\n\nDescription:\nTaxon name reported by the data provider.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nTaxonRepName\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#common-name",
    "href": "metadata/field-definition.html#common-name",
    "title": "Field definitions",
    "section": "Common Name",
    "text": "Common Name\n\nDescription:\nCommon name of the species or organism.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nCommonname\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#taxon-rank",
    "href": "metadata/field-definition.html#taxon-rank",
    "title": "Field definitions",
    "section": "Taxon Rank",
    "text": "Taxon Rank\n\nDescription:\nRank of the taxon in the biological classification system.\n\n\nLookup Table (LUT) in use:\nNo, ‘Taxon Rank’ is defined for each species in the dbo_species.xlsx look-up.\n\n\nOpen Refine Variable Name:\nTaxonrank\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#taxon-database",
    "href": "metadata/field-definition.html#taxon-database",
    "title": "Field definitions",
    "section": "Taxon Database",
    "text": "Taxon Database\n\nDescription:\nDatabase or repository where taxon information is stored.\n\n\nLookup Table (LUT) in use:\nNo, ‘Taxon Database’ is defined for each species in the dbo_species.xlsx look-up.\n\n\nOpen Refine Variable Name:\nTaxonDB\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#taxon-database-id",
    "href": "metadata/field-definition.html#taxon-database-id",
    "title": "Field definitions",
    "section": "Taxon Database ID",
    "text": "Taxon Database ID\n\nDescription:\nIdentifier for the taxon in the taxon database.\n\n\nLookup Table (LUT) in use:\nNo, ‘Taxon Database ID’ is defined for each species in the dbo_species.xlsx look-up.\n\n\nOpen Refine Variable Name:\nTaxonDBID\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#taxon-database-url",
    "href": "metadata/field-definition.html#taxon-database-url",
    "title": "Field definitions",
    "section": "Taxon Database URL",
    "text": "Taxon Database URL\n\nDescription:\nURL the taxon database.\n\n\nLookup Table (LUT) in use:\nNo, ‘Taxon Database URL’ is defined for each species in the dbo_species.xlsx look-up.\n\n\nOpen Refine Variable Name:\nTaxonDBURL\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#body-part-id",
    "href": "metadata/field-definition.html#body-part-id",
    "title": "Field definitions",
    "section": "Body Part ID",
    "text": "Body Part ID\n\nDescription:\nRepresents the identifier for the specific body part of the sample.\n\n\nLookup Table (LUT) in use:\nYes, dbo_bodypar.xlsx.\n\n\nOpen Refine Variable Name:\nbodypar_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘bodypar_id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nbody_part\n\n\nNetCDF Data Type:\nAn integer value (the ‘bodypar_id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#slice-up",
    "href": "metadata/field-definition.html#slice-up",
    "title": "Field definitions",
    "section": "Slice Up",
    "text": "Slice Up\n\nDescription:\nTop of sediment core interval relative to the water-sediment interface (cm).\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nsliceup\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\ntop\n\n\nNetCDF Data Type:\nfloat",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#slice-down",
    "href": "metadata/field-definition.html#slice-down",
    "title": "Field definitions",
    "section": "Slice Down",
    "text": "Slice Down\n\nDescription:\nBottom of sediment core interval relative to the water-sediment interface (cm).\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nslicedown\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nbottom\n\n\nNetCDF Data Type:\nfloat",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#sediment-type-id",
    "href": "metadata/field-definition.html#sediment-type-id",
    "title": "Field definitions",
    "section": "Sediment Type ID",
    "text": "Sediment Type ID\n\nDescription:\nRepresents the classification of sediment according to the Udden-Wentworth scale.\n\n\nLookup Table (LUT) in use:\nYes, dbo_sedtype.xlsx.\n\n\nOpen Refine Variable Name:\nsedtype_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘sedtype_id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nsed_type\n\n\nNetCDF Data Type:\nAn integer value (the ‘sedtype_id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#sediment-reported-name",
    "href": "metadata/field-definition.html#sediment-reported-name",
    "title": "Field definitions",
    "section": "Sediment Reported Name",
    "text": "Sediment Reported Name\n\nDescription:\nName of the sediment as reported by the data provider. The sediment name should be stored exactly as provided, without any modifications.\n\n\nLookup Table (LUT) in use:\nNo.\n\n\nOpen Refine Variable Name:\nSedRepName\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#volume",
    "href": "metadata/field-definition.html#volume",
    "title": "Field definitions",
    "section": "Volume",
    "text": "Volume\n\nDescription:\nVolume of the sample.\n\n\nLookup Table (LUT) in use:\nNo.\n\n\nOpen Refine Variable Name:\nvolume\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#salinity",
    "href": "metadata/field-definition.html#salinity",
    "title": "Field definitions",
    "section": "Salinity",
    "text": "Salinity\n\nDescription:\nSalinity of the sample, expressed in practical salinity units (PSU).If required, consult TEOS-10 guidelines (www.teos-10.org/) for converting from Absolute Salinity (g/kg) to Practical Salinity.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nsalinity\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nsalinity\n\n\nNetCDF Data Type:\nfloat",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#temperature",
    "href": "metadata/field-definition.html#temperature",
    "title": "Field definitions",
    "section": "Temperature",
    "text": "Temperature\n\nDescription:\nTemperature of the sample (°C).\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\ntemperatur\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\ntemperature\n\n\nNetCDF Data Type:\nfloat",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#filtered",
    "href": "metadata/field-definition.html#filtered",
    "title": "Field definitions",
    "section": "Filtered",
    "text": "Filtered\n\nDescription:\nIndicates whether the sample was filtered: - Y : Sample was filtered. - N : Sample was not filtered. - NA : Not applicable or information not available.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nfiltered\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#filter-pore-mesh-size",
    "href": "metadata/field-definition.html#filter-pore-mesh-size",
    "title": "Field definitions",
    "section": "Filter Pore Mesh size",
    "text": "Filter Pore Mesh size\n\nDescription:\nThe pore size of the filter used, if applicable, expressed in micrometers (µm).\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nfiltpore\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#acidified",
    "href": "metadata/field-definition.html#acidified",
    "title": "Field definitions",
    "section": "Acidified",
    "text": "Acidified\n\nDescription:\nIndicates if the sample was acidified. - A: Sample acidified - NA: Sample not acidified\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nacid\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#oxygen",
    "href": "metadata/field-definition.html#oxygen",
    "title": "Field definitions",
    "section": "Oxygen",
    "text": "Oxygen\n\nDescription:\nDissolved oxygen concentration.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\noxygen\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#sample-area",
    "href": "metadata/field-definition.html#sample-area",
    "title": "Field definitions",
    "section": "Sample Area",
    "text": "Sample Area\n\nDescription:\nSample surface area of sediment (cm2).\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nsamparea\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#dry-weight",
    "href": "metadata/field-definition.html#dry-weight",
    "title": "Field definitions",
    "section": "Dry Weight",
    "text": "Dry Weight\n\nDescription:\nDry weight of the sample, expressed in grams (g).\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\ndrywt\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\ndrywt\n\n\nNetCDF Data Type:\nfloat",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#wet-weight",
    "href": "metadata/field-definition.html#wet-weight",
    "title": "Field definitions",
    "section": "Wet Weight",
    "text": "Wet Weight\n\nDescription:\nWet weight of the sample, expressed in grams (g).\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nwetwt\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nwetwt\n\n\nNetCDF Data Type:\nfloat",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#percent-weight",
    "href": "metadata/field-definition.html#percent-weight",
    "title": "Field definitions",
    "section": "Percent Weight",
    "text": "Percent Weight\n\nDescription:\nExpressed as a percentage. This is calculated by dividing the dry weight by the wet weight and then multiplying by 100. The reported value should be greater than 0 and less than 100.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\npercentwt\n\n\nOpen Refine Data Type:\nfloat\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#sampling-method-id",
    "href": "metadata/field-definition.html#sampling-method-id",
    "title": "Field definitions",
    "section": "Sampling Method ID",
    "text": "Sampling Method ID\n\nDescription:\nIdentifier for the method used to collect the sample.\n\n\nLookup Table (LUT) in use:\nYes, dbo_sampmet.xlsx.\n\n\nOpen Refine Variable Name:\nsampmet_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘sampmet_id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nsamp_met\n\n\nNetCDF Data Type:\nAn integer value (the ‘sampmet_id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#drying-method-id",
    "href": "metadata/field-definition.html#drying-method-id",
    "title": "Field definitions",
    "section": "Drying Method ID",
    "text": "Drying Method ID\n\nDescription:\nIdentifier for the method used to dry the sample.\n\n\nLookup Table (LUT) in use:\nYes, [dbo_sampmet.xlsx]\n\n\nOpen Refine Variable Name:\ndrymet_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#preparation-method-id",
    "href": "metadata/field-definition.html#preparation-method-id",
    "title": "Field definitions",
    "section": "Preparation Method ID",
    "text": "Preparation Method ID\n\nDescription:\nIdentifier for the method used to prepare the sample.\n\n\nLookup Table (LUT) in use:\nYes, dbo_prepmet.xlsx. ### Open Refine Variable Name: prepmet_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘prepmet_id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nprep_met\n\n\nNetCDF Data Type:\nAn integer value (the ‘prepmet_id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#counting-method-id",
    "href": "metadata/field-definition.html#counting-method-id",
    "title": "Field definitions",
    "section": "Counting Method ID",
    "text": "Counting Method ID\n\nDescription:\nIdentifier for the method used to count the sample.\n\n\nLookup Table (LUT) in use:\nYes, dbo_counmet.xlsx.\n\n\nOpen Refine Variable Name:\ncounmet_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘counmet_id’ defined in the LUT).\n\n\nNetCDF Variable Name:\ncount_met\n\n\nNetCDF Data Type:\nAn integer value (the ‘counmet_id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#reference-id",
    "href": "metadata/field-definition.html#reference-id",
    "title": "Field definitions",
    "section": "Reference ID",
    "text": "Reference ID\n\nDescription:\nIdentifier which identifies the source provider of the data.\n\n\nLookup Table (LUT) in use:\nYes, [dbo_ref.xlsx]\n\n\nOpen Refine Variable Name:\nref_id\n\n\nOpen Refine Data Type:\nAn integer value (the ‘id’ defined in the LUT).\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#reference-note",
    "href": "metadata/field-definition.html#reference-note",
    "title": "Field definitions",
    "section": "Reference Note",
    "text": "Reference Note\n\nDescription:\nNotes or comments related to the reference or source.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nrefnote\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#sample-note",
    "href": "metadata/field-definition.html#sample-note",
    "title": "Field definitions",
    "section": "Sample Note",
    "text": "Sample Note\n\nDescription:\nNotes or comments related to the sample.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nsampnote\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#measurement-note",
    "href": "metadata/field-definition.html#measurement-note",
    "title": "Field definitions",
    "section": "Measurement Note",
    "text": "Measurement Note\n\nDescription:\nNotes or comments related to the measurement process.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\nmeasurenote\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#good-for-export",
    "href": "metadata/field-definition.html#good-for-export",
    "title": "Field definitions",
    "section": "Good for Export",
    "text": "Good for Export\n\nDescription:\nIndicates if the sample data is deemed good for export.\n\n\nLookup Table (LUT) in use:\nNo\n\n\nOpen Refine Variable Name:\ngfe\n\n\nOpen Refine Data Type:\nstring\n\n\nNetCDF Variable Name:\nNot included in NetCDF.\n\n\nNetCDF Data Type:\nNot included in NetCDF.",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/field-definition.html#area",
    "href": "metadata/field-definition.html#area",
    "title": "Field definitions",
    "section": "Area",
    "text": "Area\n\nDescription:\nProvides the name of the area region where the sample was collected.\n\n\nLookup Table (LUT) in use:\nYes, dbo_area.xlsx.\n\n\nOpen Refine Variable Name:\narea\n\n\nOpen Refine Data Type:\nAn integer value (the ‘area_id’ defined in the LUT).\n\n\nNetCDF Variable Name:\narea\n\n\nNetCDF Data Type:\nAn integer value (the ‘area_id’ defined in the LUT).",
    "crumbs": [
      "Metadata",
      "Field definitions"
    ]
  },
  {
    "objectID": "metadata/sample-uniqueness.html",
    "href": "metadata/sample-uniqueness.html",
    "title": "Sample uniqueness",
    "section": "",
    "text": "What constitutes a single sample in the context of MARIS database?\nAs defined in OpenRefine MARIS currently, unique sample IDs are defined as the concatenation of the following columns:"
  },
  {
    "objectID": "metadata/sample-uniqueness.html#rule-1",
    "href": "metadata/sample-uniqueness.html#rule-1",
    "title": "Sample uniqueness",
    "section": "Rule 1",
    "text": "Rule 1\nWe also use station and samplabcode, when available. Station is a name given to a sampling location, samplabcode is the data provider’s unique ID. TO BE CLARIFIED\nSeawater\nAs you can see in most cases for seawater we can use the required information – lat, lon, time, sample depth – to define a unique sample ID to which we can link measurements. If sample depth is not provided we assume surface and indicate this using a value = -1 beforehand.\nQuestions: - cases where sample depth is not provided several times at the same location and time?\nSediment\nFor sediment we extend this a bit using top, bottom (sliceup and slicedown), SedRepName (mud, clay, …?).\nIf top and bottom are missing for a sediment sample we assume that it is a grab sample from the surface. In this case I think we set top: -1 to indicate this (I will check for sure).\nQuestions: - so in essence same as above\nBiota\nFor biota it is extended using the taxon and tissue IDs (species_id, bodypar_id).\nFor biota it is extended using the taxon and tissue IDs (species_id, bodypar_id).\nItem 1 examples:\n\nin geotraces when we have the same (e.g for seawater) lon, lat, time, smp_depth for several nuclides measurement in a given rosette (at least that’s what I understand);\n\nWe ignore rosette (cast?) and bottle IDs. We assume that all measurements with the same lon, lat, time, depth are the same sample. Salinity is used to provide further confidence. Geotraces is more or less a big edge case. Normally data is not provided with such detail.\nQuestions: - so it means that we have to include nuclide type to get unicity\n\nor in OSPAR sediment when we have records where top, bottom is NaN for a given lon, lat, time. In that case our compound index would be (lon, lat, time, top, bottom);\n\nSee above, if missing top = -1. There should not be multiple grab sediment samples for the same lat, lon and time. If there are then it probably indicates that a core was taken but slice top and bottom is missing. In this case the records should be ignored until this information is provided.\nIn the HELCOM example where top and bottom are NaN and actual values for the same lat, lon, time then I would assume a grab sample and a core were taken simultaneously so they are all different samples. If there are multiple records with NaN for top and bottom then it could be multiple grab samples at the same location and time but this would be unusual and should be queried with the data provider.\n\nor (not sure it happens sometimes but that’s a point Niall mentioned) when we have replicates at the same location, time, depth, …\n\nSometimes there are replicates, yes. E.g. sometimes TEPCO report quick results for certain samples and then reanalyse them for a longer time for more precision – they report both. So in this case replicates are valid.\nQuestion: - what do we do in such case?"
  },
  {
    "objectID": "metadata/sample-uniqueness.html#rule-2-location-must-be-inferred.",
    "href": "metadata/sample-uniqueness.html#rule-2-location-must-be-inferred.",
    "title": "Sample uniqueness",
    "section": "Rule 2: Location must be inferred.",
    "text": "Rule 2: Location must be inferred.\nAnother example is when we do not have detailed information about sampling location or time and are forced to make general assumptions (e.g. the location of a port where multiple samples of the same species are landed and/or the sampling date for such samples is reported simply as a year or a quarter and we are forced to assume the mid-point) then, unless samplabcode is provided, there can be replicates. Currently we can live with this (though if we spot it we can force unique sample IDs by temporarily injecting dummy values for samplabcode which are removed after the sample IDs are generated)."
  },
  {
    "objectID": "metadata/sample-uniqueness.html#nialls-situation",
    "href": "metadata/sample-uniqueness.html#nialls-situation",
    "title": "Sample uniqueness",
    "section": "Niall’s situation",
    "text": "Niall’s situation\n\nin geotraces when we have the same (e.g for seawater) lon, lat, time, smp_depth for several nuclides measurement in a given rosette (at least that’s what I understand);\nin OSPAR sediment when we have records where top, bottom is NaN for a given lon, lat, time. In that case our compound index would be (lon, lat, time, top, bottom);\nIn situations where a nuclide is measured for a sample using more than one method\nIn situations where rapid analysis and detailed analysis is reported (rapid -while arriving at lab- vs detailed measurement afterwards);\nIn a situation where a sample is collected and split into two or more sub-samples. For this sample the compound index would be the same. Sometimes this type of sample is sent to several laboratories (ring trial/inter-lab comparison);\nIn situations where a nuclide is measured for a sample using more than one method (e.g. Am241 normally measured by alpha and gamma spectrometry);"
  },
  {
    "objectID": "cli/nc_to_csv.html",
    "href": "cli/nc_to_csv.html",
    "title": "marisco",
    "section": "",
    "text": "source\n\nmain\n\n main (src:str, dest:str)\n\nConverts NetCDF files into CSV files that follow the MARIS Standard format.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nsrc\nstr\nInput path and filename for NetCDF file\n\n\ndest\nstr\nOutput path and filename (without extension) for CSV file\n\n\nReturns\nNone"
  },
  {
    "objectID": "cli/to_nc.html",
    "href": "cli/to_nc.html",
    "title": "marisco",
    "section": "",
    "text": "source\n\nimport_handler\n\n import_handler (handler_name, fn_name='encode')\n\n\nsource\n\n\nmain\n\n main (ds:str, dest:str, src:Optional[str]=None)\n\nConvert ‘helcom’, ‘geotraces’, ‘tepco’ or ‘ospar’ marine radioactivity datasets to MARIS NetCDF4 format.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nds\nstr\n\nName of the dataset to encode as NetCDF4\n\n\ndest\nstr\n\nOutput path and filename for NetCDF file\n\n\nsrc\nOptional\nNone\nOptional input data path only required for the ‘GEOTRACES’ dataset\n\n\nReturns\nNone"
  },
  {
    "objectID": "handlers/tepco.html",
    "href": "handlers/tepco.html",
    "title": "TEPCO",
    "section": "",
    "text": "Exported source\nfname_coastal_water = 'https://radioactivity.nra.go.jp/cont/en/results/sea/coastal_water.csv'\nfname_clos1F = 'https://radioactivity.nra.go.jp/cont/en/results/sea/close1F_water.xlsx'\nfname_iaea_orbs = 'https://raw.githubusercontent.com/RML-IAEA/iaea.orbs/refs/heads/main/src/iaea/orbs/stations/station_points.csv'\n\nfname_out = '../../_data/output/tepco.nc'",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#configuration-file-paths",
    "href": "handlers/tepco.html#configuration-file-paths",
    "title": "TEPCO",
    "section": "",
    "text": "Exported source\nfname_coastal_water = 'https://radioactivity.nra.go.jp/cont/en/results/sea/coastal_water.csv'\nfname_clos1F = 'https://radioactivity.nra.go.jp/cont/en/results/sea/close1F_water.xlsx'\nfname_iaea_orbs = 'https://raw.githubusercontent.com/RML-IAEA/iaea.orbs/refs/heads/main/src/iaea/orbs/stations/station_points.csv'\n\nfname_out = '../../_data/output/tepco.nc'",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#load-data",
    "href": "handlers/tepco.html#load-data",
    "title": "TEPCO",
    "section": "Load data",
    "text": "Load data\nWe here load the data from the NRA (Nuclear Regulatory Authority) website. For the moment, we only process radioactivity concentration data in the seawater around Fukushima Dai-ichi NPP [TEPCO] (coastal_water.csv) and in the close1F_water.xlsx file.\nIn near future, MARIS will provide a dedicated handler for all related ALPS data including measurements not only provided by TEPCO but also MOE, NRA, MLITT and Fukushima Prefecture.\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nThe coastal_water.csv file contains two sections: the measurements and the locations. We identify below the line number where the locations begin. A single point of truth for the location of the stations would ease the processing in future.\n\n\n\nsource\n\nfind_location_section\n\n find_location_section (df, col_idx=0, pattern='Sampling point number')\n\nFind the line number where location data begins.\n\n\nExported source\ndef find_location_section(df, \n                          col_idx=0,\n                          pattern='Sampling point number'\n                          ):\n    \"Find the line number where location data begins.\"\n    mask = df.iloc[:, col_idx] == pattern\n    indices = df[mask].index\n    return indices[0] if len(indices) &gt; 0 else -1\n\n\n\nfind_location_section(pd.read_csv(fname_coastal_water, low_memory=False))\n\nnp.int64(29252)\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nDistinct parsing of the time from coastal_water.csv and close1F_water.xlsx files are required. Indeed:\n\ncoastal_water.csv uses the format YYYY/MM/DD in the Sampling  HH:MM and\nclose1F_water.xlsx uses the format YYYY-MM-DD HH:MM:SS.\n\n\n\n\nsource\n\n\nfix_sampling_time\n\n fix_sampling_time (x)\n\n\n\nExported source\ndef fix_sampling_time(x):\n    if pd.isna(x): \n        return '00:00:00'\n    else:\n        hour, min =  x.split(':')[:2]\n        return f\"{hour if len(hour) == 2 else '0' + hour}:{min}:00\"\n\n\n\nsource\n\n\nget_coastal_water_df\n\n get_coastal_water_df (fname_coastal_water)\n\nGet the measurements dataframe from the coastal_water.csv file.\n\n\nExported source\ndef get_coastal_water_df(fname_coastal_water):\n    \"Get the measurements dataframe from the `coastal_water.csv` file.\"\n    \n    locs_idx = find_location_section(pd.read_csv(fname_coastal_water, \n                                      skiprows=0, low_memory=False))\n    \n    df = pd.read_csv(fname_coastal_water, skiprows=1, \n                     nrows=locs_idx - 1,\n                     low_memory=False)\n    df.dropna(subset=['Sampling point number'], inplace=True)\n    df['Sampling time'] = df['Sampling time'].map(fix_sampling_time)\n    \n    df['TIME'] = df['Sampling date'].replace('-', '/') + ' ' + df['Sampling time']\n    \n    df = df.drop(columns=['Sampling date', 'Sampling time'])\n    return df\n\n\n\ndf_coastal_water = get_coastal_water_df(fname_coastal_water)\ndf_coastal_water.tail()\n\n\n\n\n\n\n\n\nSampling point number\nCollection layer of seawater\n131I radioactivity concentration (Bq/L)\n131I detection limit (Bq/L)\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n137Cs radioactivity concentration (Bq/L)\n137Cs detection limit (Bq/L)\n132I radioactivity concentration (Bq/L)\n132I detection limit (Bq/L)\n...\n54Mn radioactivity concentration (Bq/L)\n54Mn detection limit (Bq/L)\n3H radioactivity concentration (Bq/L)\n3H detection limit (Bq/L)\n125Sb radioactivity concentration (Bq/L)\n125Sb detection limit (Bq/L)\n105Ru radioactivity concentration (Bq/L)\n105Ru detection limit (Bq/L)\nUnnamed: 49\nTIME\n\n\n\n\n29219\nT-D5\n上層\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nND\n6.2E+00\nNaN\nNaN\nNaN\nNaN\nNaN\n2025/7/17 07:56:00\n\n\n29220\nT-S8\n上層\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nND\n6.8E+00\nNaN\nNaN\nNaN\nNaN\nNaN\n2025/7/18 05:34:00\n\n\n29221\nT-D5\n上層\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nND\n7.9E+00\nNaN\nNaN\nNaN\nNaN\nNaN\n2025/7/21 08:05:00\n\n\n29222\nT-S3\n上層\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nND\n7.3E+00\nNaN\nNaN\nNaN\nNaN\nNaN\n2025/7/22 05:54:00\n\n\n29223\nT-S4\n上層\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nND\n7.4E+00\nNaN\nNaN\nNaN\nNaN\nNaN\n2025/7/22 06:17:00\n\n\n\n\n5 rows × 49 columns\n\n\n\n\ncoi = [o for o in df_coastal_water.columns if \"134Cs\" in o]\n\ndf_coastal_water[coi + ['Sampling point number', 'TIME']].head(30)\n\n\n\n\n\n\n\n\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\nSampling point number\nTIME\n\n\n\n\n0\n4.8E+01\n9.2E+00\nT-3\n2011/3/21 23:15:00\n\n\n1\n3.1E+01\n8.7E+00\nT-4\n2011/3/21 23:45:00\n\n\n2\n4.6E+01\n1.4E+01\nT-3\n2011/3/22 14:28:00\n\n\n3\n3.9E+01\n1.1E+01\nT-4\n2011/3/22 15:06:00\n\n\n4\n5.1E+01\n2.0E+01\nT-3\n2011/3/23 13:51:00\n\n\n5\n3.3E+01\n2.1E+01\nT-4\n2011/3/23 14:25:00\n\n\n6\n9.9E+01\n3.8E+01\nT-3\n2011/3/24 09:30:00\n\n\n7\n3.5E+01\n7.0E+00\nT-4\n2011/3/24 08:45:00\n\n\n8\n2.6E+01\n7.4E+00\nT-3\n2011/3/25 10:00:00\n\n\n9\n2.0E+01\n6.7E+00\nT-4\n2011/3/25 09:10:00\n\n\n10\n2.6E+01\n1.8E+01\nT-3\n2011/3/26 15:15:00\n\n\n11\n1.3E+01\n7.1E+00\nT-4\n2011/3/26 15:50:00\n\n\n12\n5.4E+02\n1.2E+01\nT-3\n2011/3/27 14:30:00\n\n\n13\n2.0E+01\n6.0E+00\nT-4\n2011/3/27 08:45:00\n\n\n14\n6.1E+02\n2.3E+01\nT-3\n2011/3/28 09:35:00\n\n\n15\n3.3E+02\n2.1E+01\nT-4\n2011/3/28 08:45:00\n\n\n16\n3.2E+02\n1.3E+01\nT-3\n2011/3/29 10:15:00\n\n\n17\n2.3E+02\n1.2E+01\nT-4\n2011/3/29 09:20:00\n\n\n18\n3.6E+02\n2.0E+01\nT-3\n2011/3/30 10:00:00\n\n\n19\n1.8E+02\n2.0E+01\nT-4\n2011/3/30 09:05:00\n\n\n20\n3.6E+02\n2.1E+01\nT-3\n2011/3/31 10:00:00\n\n\n21\n1.6E+02\n2.0E+01\nT-4\n2011/3/31 09:15:00\n\n\n22\n3.0E+02\n1.8E+01\nT-3\n2011/4/1 09:50:00\n\n\n23\n2.0E+02\n1.8E+01\nT-4\n2011/4/1 09:00:00\n\n\n24\n1.9E+01\n1.5E+01\n8\n2011/4/2 13:35:00\n\n\n25\n1.7E+02\n1.7E+01\nT-3\n2011/4/2 09:55:00\n\n\n26\n5.1E+01\n1.7E+01\nT-4\n2011/4/2 09:00:00\n\n\n27\n2.3E+01\n4.9E+00\nT-5\n2011/4/2 14:03:00\n\n\n28\nNaN\nNaN\nT-7\n2011/4/2 13:12:00\n\n\n29\nNaN\nNaN\n8\n2011/4/3 12:20:00\n\n\n\n\n\n\n\n\ncoi\n\n['134Cs radioactivity concentration (Bq/L)', '134Cs detection limit (Bq/L)']\n\n\n\ndf_coastal_water.dropna(subset=coi, how='any')[coi + ['Sampling point number', 'TIME']]\n\n\n\n\n\n\n\n\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\nSampling point number\nTIME\n\n\n\n\n0\n4.8E+01\n9.2E+00\nT-3\n2011/3/21 23:15:00\n\n\n1\n3.1E+01\n8.7E+00\nT-4\n2011/3/21 23:45:00\n\n\n2\n4.6E+01\n1.4E+01\nT-3\n2011/3/22 14:28:00\n\n\n3\n3.9E+01\n1.1E+01\nT-4\n2011/3/22 15:06:00\n\n\n4\n5.1E+01\n2.0E+01\nT-3\n2011/3/23 13:51:00\n\n\n...\n...\n...\n...\n...\n\n\n29209\nND\n1.1E-03\nT-11\n2025/6/27 09:41:00\n\n\n29210\nND\n1.3E-03\nT-5\n2025/6/27 08:09:00\n\n\n29211\nND\n1.2E-03\nT-5\n2025/6/27 08:09:00\n\n\n29212\nND\n1.1E-03\nT-D9\n2025/6/27 09:03:00\n\n\n29213\nND\n1.0E-03\nT-D9\n2025/6/27 09:03:00\n\n\n\n\n19128 rows × 4 columns\n\n\n\n\nmask = df_coastal_water['134Cs radioactivity concentration (Bq/L)'] == 'ND'\ndf_coastal_water[mask][coi + ['Sampling point number', 'TIME']]\n\n\n\n\n\n\n\n\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\nSampling point number\nTIME\n\n\n\n\n53\nND\nNaN\n5\n2011/4/6 11:30:00\n\n\n57\nND\nNaN\n8\n2011/4/6 12:52:00\n\n\n59\nND\nNaN\n10\n2011/4/6 13:37:00\n\n\n64\nND\nNaN\nT-7\n2011/4/6 12:44:00\n\n\n65\nND\nNaN\nT-7\n2011/4/6 13:15:00\n\n\n...\n...\n...\n...\n...\n\n\n29209\nND\n1.1E-03\nT-11\n2025/6/27 09:41:00\n\n\n29210\nND\n1.3E-03\nT-5\n2025/6/27 08:09:00\n\n\n29211\nND\n1.2E-03\nT-5\n2025/6/27 08:09:00\n\n\n29212\nND\n1.1E-03\nT-D9\n2025/6/27 09:03:00\n\n\n29213\nND\n1.0E-03\nT-D9\n2025/6/27 09:03:00\n\n\n\n\n19215 rows × 4 columns\n\n\n\n\nlen(df_coastal_water)\n\n29224\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nIdentification of the stations location requires three distinct files:\n\nthe second section of the coastal_water.csv file\nthe R6zahyo.pdf file further processed by https://github.com/RML-IAEA/iaea.orbs\nthe second sections of all sheets of close1F_water.xlsx file\n\nAll files and sheets required to look up the location of the stations.\n\n\n\nsource\n\n\nget_locs_coastal_water\n\n get_locs_coastal_water (fname_coastal_water)\n\n\n\nExported source\ndef get_locs_coastal_water(fname_coastal_water):\n    locs_idx = find_location_section(pd.read_csv(fname_coastal_water, \n                                      skiprows=0, low_memory=False))\n    \n    df = pd.read_csv(fname_coastal_water, skiprows=locs_idx+1, \n                     low_memory=False).iloc[:, :3]\n    \n    df.columns = ['STATION', 'LON', 'LAT']\n    df.dropna(subset=['LAT'], inplace=True)\n    df['org'] = 'coastal_seawater.csv'\n    return df\n\n\n\ndf_locs_coastal_water = get_locs_coastal_water(fname_coastal_water)\nprint(f'Nb. of stations: {len(df_locs_coastal_water)}')\ndf_locs_coastal_water.head()\n\nNb. of stations: 48\n\n\n\n\n\n\n\n\n\nSTATION\nLON\nLAT\norg\n\n\n\n\n0\nT-0\n37.42\n141.04\ncoastal_seawater.csv\n\n\n1\nT-11\n37.24\n141.05\ncoastal_seawater.csv\n\n\n2\nT-12\n37.15\n141.04\ncoastal_seawater.csv\n\n\n3\nT-13-1\n37.64\n141.04\ncoastal_seawater.csv\n\n\n4\nT-14\n37.55\n141.06\ncoastal_seawater.csv\n\n\n\n\n\n\n\n\ndf_locs_coastal_water.STATION.unique()\n\narray(['T-0', 'T-11', 'T-12', 'T-13-1', 'T-14', 'T-17-1', 'T-18', 'T-20',\n       'T-22', 'T-3', 'T-4', 'T-4-1', 'T-4-2', 'T-5', 'T-6', 'T-7', 'T-A',\n       'T-B', 'T-B1', 'T-B2', 'T-B3', 'T-B4', 'T-C', 'T-D', 'T-D1',\n       'T-D5', 'T-D9', 'T-E', 'T-E1', 'T-Z', 'T-MG6', 'T-S1', 'T-S7',\n       'T-H1', 'T-S2', 'T-S6', 'T-M10', 'T-MA', 'T-S3', 'T-S4', 'T-S8',\n       'T-MG4', 'T-G4', 'T-MG5', 'T-MG1', 'T-MG0', 'T-MG3', 'T-MG2'],\n      dtype=object)\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nData contained in the close1F_water.xlsx file are spread in several sheets (one per station). Each sheet further contains two sections: the measurements and the locations.\nFor each sheet, we have to identify the line number where to split both measurements and the location. We then need to further iterate over all sheets to concatenate the results.\n\n\n\nsource\n\n\nget_clos1F_df\n\n get_clos1F_df (fname_clos1F)\n\nGet measurements dataframe from close1F_water.xlsx file and parse datetime.\n\n\nExported source\ndef get_clos1F_df(fname_clos1F):\n    \"Get measurements dataframe from close1F_water.xlsx file and parse datetime.\"\n    excel_file = pd.ExcelFile(fname_clos1F)\n    dfs = {}\n    \n    for sheet_name in tqdm(excel_file.sheet_names):\n        locs_idx = find_location_section(pd.read_excel(excel_file, \n                                                       sheet_name=sheet_name,\n                                                       skiprows=1))\n        df = pd.read_excel(excel_file, \n                   sheet_name=sheet_name, \n                   skiprows=1,\n                   nrows=locs_idx-1)\n        \n        df.dropna(subset=['Sampling point number'], inplace=True)\n        df['Sampling date'] = df['Sampling date']\\\n            .astype(str)\\\n            .apply(lambda x: x.split(' ')[0]\\\n            .replace('-', '/'))\n            \n        dfs[sheet_name] = df\n    \n    df = pd.concat(dfs.values(), ignore_index=True)\n    df.dropna(subset=['Sampling date'], inplace=True)\n    df['TIME'] = df['Sampling date'] + ' ' + df['Sampling time'].astype(str)\n    df = df.drop(columns=['Sampling date', 'Sampling time'])\n    return df\n\n\n\ndf_clos1F = get_clos1F_df(fname_clos1F)\ndf_clos1F.head()\n\n100%|██████████| 11/11 [00:05&lt;00:00,  2.17it/s]\n\n\n\n\n\n\n\n\n\nSampling point number\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n137Cs radioactivity concentration (Bq/L)\n137Cs detection limit (Bq/L)\nTotal beta radioactivity concentration (Bq/L)\nTotal beta detection limit (Bq/L)\n3H radioactivity concentration (Bq/L)\n3H detection limit (Bq/L)\nCollection layer of seawater\n...\n106Ru detection limit (Bq/L)\n60Co radioactivity concentration (Bq/L)\n60Co detection limit (Bq/L)\n95Zr radioactivity concentration (Bq/L)\n95Zr detection limit (Bq/L)\n99Mo radioactivity concentration (Bq/L)\n99Mo detection limit (Bq/L)\n105Ru radioactivity concentration (Bq/L)\n105Ru detection limit (Bq/L)\nTIME\n\n\n\n\n0\nT-0-1\nND\n1.5\nND\n1.4\nND\n18.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2013/08/14 08:17:00\n\n\n1\nT-0-1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n4.7\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2013/08/14 08:17:00\n\n\n2\nT-0-1\nND\n1.1\nND\n1.4\nND\n20.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2013/08/21 08:09:00\n\n\n3\nT-0-1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nND\n2.9\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2013/08/21 08:09:00\n\n\n4\nT-0-1\nND\n0.66\nND\n0.49\nND\n17.0\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2013/08/27 08:14:00\n\n\n\n\n5 rows × 57 columns\n\n\n\n\ndf_clos1F['Sampling point number'].unique()\n\narray(['T-0-1', 'T-0-1A', 'T-0-2', 'T-0-3', 'T-0-3A', 'T-1', 'T-2',\n       'T-2-1', 'T-A1', 'T-A2', 'T-A3'], dtype=object)\n\n\n\nsource\n\n\nget_locs_clos1F\n\n get_locs_clos1F (fname_clos1F)\n\nGet locations dataframe from close1F_water.xlsx file from each sheets.\n\n\nExported source\ndef get_locs_clos1F(fname_clos1F):\n    \"Get locations dataframe from close1F_water.xlsx file from each sheets.\"\n    excel_file = pd.ExcelFile(fname_clos1F)\n    dfs = {}\n    \n    for sheet_name in tqdm(excel_file.sheet_names):\n        locs_idx = find_location_section(pd.read_excel(excel_file, \n                                                       sheet_name=sheet_name,\n                                                       skiprows=1))\n        df = pd.read_excel(excel_file, \n                           sheet_name=sheet_name, \n                           skiprows=locs_idx+2)\n            \n        dfs[sheet_name] = df\n    \n    df = pd.concat(dfs.values(), ignore_index=True).iloc[:, :3]\n    df.dropna(subset=['Sampling coordinate North latitude (Decimal)'], inplace=True)    \n    df.columns = ['STATION', 'LON', 'LAT']\n    df['org'] = 'close1F.csv'\n    return df\n\n\n\ndf_locs_clos1F = get_locs_clos1F(fname_clos1F)\nprint(f'Nb. of stations: {len(df_locs_clos1F)}')\ndf_locs_clos1F.head()\n\n100%|██████████| 11/11 [00:05&lt;00:00,  1.97it/s]\n\n\nNb. of stations: 11\n\n\n\n\n\n\n\n\n\n\n\n\nSTATION\nLON\nLAT\norg\n\n\n\n\n0\nT-0-1\n37.43\n141.04\nclose1F.csv\n\n\n11\nT-0-1A\n37.43\n141.05\nclose1F.csv\n\n\n22\nT-0-2\n37.42\n141.05\nclose1F.csv\n\n\n33\nT-0-3\n37.42\n141.04\nclose1F.csv\n\n\n44\nT-0-3A\n37.42\n141.05\nclose1F.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nThe close1F_water.xlsx file contains station locations that are not present in the coastal_water.csv dataset, as demonstrated in the comparison below:\n\n\n\nset(df_locs_clos1F.STATION) - set(df_locs_coastal_water.STATION)\n\n{'T-0-1',\n 'T-0-1A',\n 'T-0-2',\n 'T-0-3',\n 'T-0-3A',\n 'T-1',\n 'T-2',\n 'T-2-1',\n 'T-A1',\n 'T-A2',\n 'T-A3'}\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nIn theory all locations are supposed to be provided in the R6zahyo.pdf file. This file is further processed by https://github.com/RML-IAEA/iaea.orbs and the result is provided in the station_points.csv file.\nHowever, this file lacks complete coverage of locations referenced in both coastal_water.csv and close1F_water.xlsx files, while simultaneously containing additional locations not present in either (see below). A more standardized and comprehensive location reference system would significantly improve the efficiency and reliability of the data ingestion process.\n\n\n\nsource\n\n\nget_locs_orbs\n\n get_locs_orbs (fname_iaea_orbs)\n\n\n\nExported source\ndef get_locs_orbs(fname_iaea_orbs):\n    df = pd.read_csv(fname_iaea_orbs)\n    df.columns = ['org', 'STATION', 'LON', 'LAT']\n    return df\n\n\n\ndf_locs_orbs = get_locs_orbs(fname_iaea_orbs)\ndf_locs_orbs.head()\n\n\n\n\n\n\n\n\norg\nSTATION\nLON\nLAT\n\n\n\n\n0\nMOE\nE-31\n141.727667\n39.059167\n\n\n1\nMOE\nE-32\n141.635667\n38.996000\n\n\n2\nMOE\nE-37\n141.948611\n39.259167\n\n\n3\nMOE\nE-38\n141.755000\n39.008333\n\n\n4\nMOE\nE-39\n141.766667\n38.991667\n\n\n\n\n\n\n\n\nset(df_locs_orbs.STATION) - (set(df_locs_clos1F.STATION) | set(df_locs_coastal_water.STATION))\n\n{'C-P1',\n 'C-P2',\n 'C-P3',\n 'C-P4',\n 'C-P5',\n 'C-P8',\n 'E-31',\n 'E-32',\n 'E-37',\n 'E-38',\n 'E-39',\n 'E-3A',\n 'E-41',\n 'E-42',\n 'E-43',\n 'E-44',\n 'E-45',\n 'E-46',\n 'E-47',\n 'E-48',\n 'E-49',\n 'E-4A',\n 'E-4B',\n 'E-4C',\n 'E-4F',\n 'E-4G',\n 'E-4H',\n 'E-4J',\n 'E-4K',\n 'E-4L',\n 'E-4M',\n 'E-71',\n 'E-72',\n 'E-73',\n 'E-74',\n 'E-75',\n 'E-76',\n 'E-77',\n 'E-78',\n 'E-79',\n 'E-7A',\n 'E-7B',\n 'E-7C',\n 'E-7D',\n 'E-7F',\n 'E-7G',\n 'E-7H',\n 'E-7I',\n 'E-7J',\n 'E-7K',\n 'E-7L',\n 'E-81',\n 'E-82',\n 'E-83',\n 'E-84',\n 'E-85',\n 'E-S1',\n 'E-S10',\n 'E-S13',\n 'E-S14',\n 'E-S15',\n 'E-S17',\n 'E-S18',\n 'E-S19',\n 'E-S20',\n 'E-S21',\n 'E-S22',\n 'E-S23',\n 'E-S24',\n 'E-S25',\n 'E-S26',\n 'E-S27',\n 'E-S28',\n 'E-S29',\n 'E-S3',\n 'E-S30',\n 'E-S31',\n 'E-S32',\n 'E-S33',\n 'E-S34',\n 'E-S35',\n 'E-S36',\n 'E-S4',\n 'E-S5',\n 'E-T1',\n 'E-T2',\n 'E-T3',\n 'E-T4',\n 'E-T5',\n 'E-T6',\n 'E-T7',\n 'E-T8',\n 'F-P01',\n 'F-P02',\n 'F-P03',\n 'F-P04',\n 'F-P05',\n 'F-P06',\n 'F-P07',\n 'F-P08',\n 'F-P09',\n 'F-P10',\n 'F-P11',\n 'F-P12',\n 'F-P13',\n 'F-P14',\n 'F-P15',\n 'F-P29',\n 'F-P30',\n 'F-P31',\n 'F-P32',\n 'F-P33',\n 'F-P34',\n 'F-P35',\n 'F-P37',\n 'F-P38',\n 'F-P39',\n 'F-P40',\n 'F-P41',\n 'F-P42',\n 'F-P43',\n 'F-P45',\n 'F-P46',\n 'F-P47',\n 'F-P48',\n 'F-P49',\n 'F-P50',\n 'F-P51',\n 'F-P52',\n 'F-P53',\n 'F-P54',\n 'F-P55',\n 'F-P56',\n 'F-P57',\n 'F-P58',\n 'F-P59',\n 'F-P60',\n 'F-P61',\n 'F-P62',\n 'F-P63',\n 'F-P64',\n 'F-P65',\n 'F-P66',\n 'F-P67',\n 'F-P68',\n 'F-P69',\n 'F-P70',\n 'F-P71',\n 'F-P72',\n 'F-P73',\n 'F-P74',\n 'F-P75',\n 'F-P76',\n 'F-P77',\n 'F-P78',\n 'F-P79',\n 'F-P80',\n 'F-P81',\n 'F-P82',\n 'F-P83',\n 'K-T1',\n 'K-T2',\n 'KK-U1',\n 'M-10',\n 'M-101',\n 'M-102',\n 'M-103',\n 'M-104',\n 'M-11',\n 'M-14',\n 'M-15',\n 'M-19',\n 'M-20',\n 'M-21',\n 'M-25',\n 'M-26',\n 'M-27',\n 'M-A1',\n 'M-A3',\n 'M-B1',\n 'M-B5',\n 'M-C1',\n 'M-C10',\n 'M-C2',\n 'M-C3',\n 'M-C4',\n 'M-C6',\n 'M-C7',\n 'M-C8',\n 'M-C9',\n 'M-D1',\n 'M-D3',\n 'M-E1',\n 'M-E3',\n 'M-E5',\n 'M-F1',\n 'M-F3',\n 'M-G0',\n 'M-G1',\n 'M-G3',\n 'M-G4',\n 'M-H1',\n 'M-H3',\n 'M-I0',\n 'M-I1',\n 'M-I3',\n 'M-IB2',\n 'M-IB4',\n 'M-J1',\n 'M-J3',\n 'M-K1',\n 'M-L1',\n 'M-L3',\n 'M-M1',\n 'M-MI4',\n 'T-S5',\n 'T-①',\n 'T-②',\n 'T-③',\n 'T-④',\n 'T-⑤',\n 'T-⑥',\n 'T-⑦',\n 'T-⑧',\n 'T-⑨',\n 'T-⑩',\n 'T-⑪',\n 'T-⑫',\n 'T-⑬'}\n\n\n\nsource\n\n\nconcat_locs\n\n concat_locs (dfs)\n\nConcatenate and drop duplicates from coastal_seawater.csv and iaea_orbs.csv (kept)\n\n\nExported source\ndef concat_locs(dfs):\n    \"Concatenate and drop duplicates from coastal_seawater.csv and iaea_orbs.csv (kept)\"\n    df = pd.concat(dfs)\n    # Group by org to be used for sorting\n    df['org_grp'] = df['org'].apply(\n        lambda x: 1 if x == 'coastal_seawater.csv' else 2 if x == 'close1F.csv' else 0)\n    df.sort_values('org_grp', ascending=True, inplace=True)\n    # Drop duplicates and keep orbs data first\n    df.drop_duplicates(subset='STATION', keep='first', inplace=True)\n    df.drop(columns=['org_grp'], inplace=True)\n    df.sort_values('STATION', ascending=True, inplace=True)\n    return df\n\n\n\ndf_locs = concat_locs([df_locs_clos1F, df_locs_coastal_water, df_locs_orbs])\ndf_locs.head()\n\n\n\n\n\n\n\n\nSTATION\nLON\nLAT\norg\n\n\n\n\n214\nC-P1\n139.863333\n35.425000\nNRA\n\n\n215\nC-P2\n139.863333\n35.401667\nNRA\n\n\n216\nC-P3\n139.881667\n35.370000\nNRA\n\n\n217\nC-P4\n139.846667\n35.356667\nNRA\n\n\n218\nC-P5\n139.800000\n35.343333\nNRA\n\n\n\n\n\n\n\n\nsource\n\n\nalign_dfs\n\n align_dfs (df_from, df_to)\n\nAlign columns structure of df_from to df_to.\n\n\nExported source\ndef align_dfs(df_from, df_to):\n    \"Align columns structure of df_from to df_to.\"\n    df = defaultdict()    \n    for c in df_to.columns:\n        df[c] = df_from[c].values if c in df_from.columns else np.nan\n    return pd.DataFrame(df)\n\n\n\nalign_dfs(df_clos1F, df_coastal_water).head()\n\n\n\n\n\n\n\n\nSampling point number\nCollection layer of seawater\n131I radioactivity concentration (Bq/L)\n131I detection limit (Bq/L)\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n137Cs radioactivity concentration (Bq/L)\n137Cs detection limit (Bq/L)\n132I radioactivity concentration (Bq/L)\n132I detection limit (Bq/L)\n...\n54Mn radioactivity concentration (Bq/L)\n54Mn detection limit (Bq/L)\n3H radioactivity concentration (Bq/L)\n3H detection limit (Bq/L)\n125Sb radioactivity concentration (Bq/L)\n125Sb detection limit (Bq/L)\n105Ru radioactivity concentration (Bq/L)\n105Ru detection limit (Bq/L)\nUnnamed: 49\nTIME\n\n\n\n\n0\nT-0-1\nNaN\nNaN\nNaN\nND\n1.5\nND\n1.4\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2013/08/14 08:17:00\n\n\n1\nT-0-1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\n4.7\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2013/08/14 08:17:00\n\n\n2\nT-0-1\nNaN\nNaN\nNaN\nND\n1.1\nND\n1.4\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2013/08/21 08:09:00\n\n\n3\nT-0-1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nND\n2.9\nNaN\nNaN\nNaN\nNaN\nNaN\n2013/08/21 08:09:00\n\n\n4\nT-0-1\nNaN\nNaN\nNaN\nND\n0.66\nND\n0.49\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2013/08/27 08:14:00\n\n\n\n\n5 rows × 49 columns\n\n\n\n\nsource\n\n\nconcat_dfs\n\n concat_dfs (df_coastal_water, df_clos1F)\n\nConcatenate and drop duplicates from coastal_seawater.csv and close1F_water.xlsx (kept)\n\n\nExported source\ndef concat_dfs(df_coastal_water, df_clos1F):\n    \"Concatenate and drop duplicates from coastal_seawater.csv and close1F_water.xlsx (kept)\"\n    df_clos1F = align_dfs(df_clos1F, df_coastal_water)\n    df = pd.concat([df_coastal_water, df_clos1F])\n    return df\n\n\n\ndf_meas = concat_dfs(df_coastal_water, df_clos1F)\ndf_meas.head()\n\n\n\n\n\n\n\n\nSampling point number\nCollection layer of seawater\n131I radioactivity concentration (Bq/L)\n131I detection limit (Bq/L)\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n137Cs radioactivity concentration (Bq/L)\n137Cs detection limit (Bq/L)\n132I radioactivity concentration (Bq/L)\n132I detection limit (Bq/L)\n...\n54Mn radioactivity concentration (Bq/L)\n54Mn detection limit (Bq/L)\n3H radioactivity concentration (Bq/L)\n3H detection limit (Bq/L)\n125Sb radioactivity concentration (Bq/L)\n125Sb detection limit (Bq/L)\n105Ru radioactivity concentration (Bq/L)\n105Ru detection limit (Bq/L)\nUnnamed: 49\nTIME\n\n\n\n\n0\nT-3\nNaN\n1.1E+03\n1.3E+01\n4.8E+01\n9.2E+00\n5.3E+01\n8.8E+00\n1.6E+02\n44.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/21 23:15:00\n\n\n1\nT-4\nNaN\n6.6E+02\n1.2E+01\n3.1E+01\n8.7E+00\n3.3E+01\n8.3E+00\n1.2E+02\n41.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/21 23:45:00\n\n\n2\nT-3\nNaN\n1.1E+03\n2.0E+01\n4.6E+01\n1.4E+01\n4.0E+01\n1.4E+01\nND\n88.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/22 14:28:00\n\n\n3\nT-4\nNaN\n6.7E+02\n1.9E+01\n3.9E+01\n1.1E+01\n4.4E+01\n1.1E+01\nND\n79.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/22 15:06:00\n\n\n4\nT-3\nNaN\n7.4E+02\n2.7E+01\n5.1E+01\n2.0E+01\n5.5E+01\n2.0E+01\n2.0E+02\n58.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n34.0\n25.0\nNaN\n2011/3/23 13:51:00\n\n\n\n\n5 rows × 49 columns\n\n\n\n\nsource\n\n\ngeoref_data\n\n georef_data (df_meas, df_locs)\n\nGeoreference measurements dataframe using locations dataframe.\n\n\nExported source\ndef georef_data(df_meas, df_locs):\n    \"Georeference measurements dataframe using locations dataframe.\"\n    assert \"Sampling point number\" in df_meas.columns and \"STATION\" in df_locs.columns\n    return pd.merge(df_meas, df_locs, how=\"inner\", \n                    left_on='Sampling point number', right_on='STATION')\n\n\n\ndf_meas_georef = georef_data(df_meas, df_locs)\ndf_meas_georef.head()\n\n\n\n\n\n\n\n\nSampling point number\nCollection layer of seawater\n131I radioactivity concentration (Bq/L)\n131I detection limit (Bq/L)\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n137Cs radioactivity concentration (Bq/L)\n137Cs detection limit (Bq/L)\n132I radioactivity concentration (Bq/L)\n132I detection limit (Bq/L)\n...\n125Sb radioactivity concentration (Bq/L)\n125Sb detection limit (Bq/L)\n105Ru radioactivity concentration (Bq/L)\n105Ru detection limit (Bq/L)\nUnnamed: 49\nTIME\nSTATION\nLON\nLAT\norg\n\n\n\n\n0\nT-3\nNaN\n1.1E+03\n1.3E+01\n4.8E+01\n9.2E+00\n5.3E+01\n8.8E+00\n1.6E+02\n44.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/21 23:15:00\nT-3\n141.026389\n37.322222\nTEPCO\n\n\n1\nT-4\nNaN\n6.6E+02\n1.2E+01\n3.1E+01\n8.7E+00\n3.3E+01\n8.3E+00\n1.2E+02\n41.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/21 23:45:00\nT-4\n141.013889\n37.241667\nTEPCO\n\n\n2\nT-3\nNaN\n1.1E+03\n2.0E+01\n4.6E+01\n1.4E+01\n4.0E+01\n1.4E+01\nND\n88.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/22 14:28:00\nT-3\n141.026389\n37.322222\nTEPCO\n\n\n3\nT-4\nNaN\n6.7E+02\n1.9E+01\n3.9E+01\n1.1E+01\n4.4E+01\n1.1E+01\nND\n79.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/22 15:06:00\nT-4\n141.013889\n37.241667\nTEPCO\n\n\n4\nT-3\nNaN\n7.4E+02\n2.7E+01\n5.1E+01\n2.0E+01\n5.5E+01\n2.0E+01\n2.0E+02\n58.0\n...\nNaN\nNaN\n34.0\n25.0\nNaN\n2011/3/23 13:51:00\nT-3\n141.026389\n37.322222\nTEPCO\n\n\n\n\n5 rows × 53 columns\n\n\n\n\nsource\n\n\nload_data\n\n load_data (fname_coastal_water, fname_clos1F, fname_iaea_orbs)\n\nLoad, align and georeference TEPCO data\n\n\nExported source\ndef load_data(fname_coastal_water, fname_clos1F, fname_iaea_orbs):\n    \"Load, align and georeference TEPCO data\"\n    df_locs = concat_locs(\n        [get_locs_coastal_water(fname_coastal_water), \n         get_locs_clos1F(fname_clos1F),\n         get_locs_orbs(fname_iaea_orbs)])\n    df_meas = concat_dfs(get_coastal_water_df(fname_coastal_water), get_clos1F_df(fname_clos1F))\n    df_meas.dropna(subset=['Sampling point number'], inplace=True)\n    return {'SEAWATER': georef_data(df_meas, df_locs)}\n\n\n\ndfs = load_data(fname_coastal_water, fname_clos1F, fname_iaea_orbs)\ndfs['SEAWATER'].head()\n\n100%|██████████| 11/11 [00:05&lt;00:00,  2.14it/s]\n100%|██████████| 11/11 [00:05&lt;00:00,  2.14it/s]\n\n\n\n\n\n\n\n\n\nSampling point number\nCollection layer of seawater\n131I radioactivity concentration (Bq/L)\n131I detection limit (Bq/L)\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n137Cs radioactivity concentration (Bq/L)\n137Cs detection limit (Bq/L)\n132I radioactivity concentration (Bq/L)\n132I detection limit (Bq/L)\n...\n125Sb radioactivity concentration (Bq/L)\n125Sb detection limit (Bq/L)\n105Ru radioactivity concentration (Bq/L)\n105Ru detection limit (Bq/L)\nUnnamed: 49\nTIME\nSTATION\nLON\nLAT\norg\n\n\n\n\n0\nT-3\nNaN\n1.1E+03\n1.3E+01\n4.8E+01\n9.2E+00\n5.3E+01\n8.8E+00\n1.6E+02\n44.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/21 23:15:00\nT-3\n141.026389\n37.322222\nTEPCO\n\n\n1\nT-4\nNaN\n6.6E+02\n1.2E+01\n3.1E+01\n8.7E+00\n3.3E+01\n8.3E+00\n1.2E+02\n41.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/21 23:45:00\nT-4\n141.013889\n37.241667\nTEPCO\n\n\n2\nT-3\nNaN\n1.1E+03\n2.0E+01\n4.6E+01\n1.4E+01\n4.0E+01\n1.4E+01\nND\n88.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/22 14:28:00\nT-3\n141.026389\n37.322222\nTEPCO\n\n\n3\nT-4\nNaN\n6.7E+02\n1.9E+01\n3.9E+01\n1.1E+01\n4.4E+01\n1.1E+01\nND\n79.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/22 15:06:00\nT-4\n141.013889\n37.241667\nTEPCO\n\n\n4\nT-3\nNaN\n7.4E+02\n2.7E+01\n5.1E+01\n2.0E+01\n5.5E+01\n2.0E+01\n2.0E+02\n58.0\n...\nNaN\nNaN\n34.0\n25.0\nNaN\n2011/3/23 13:51:00\nT-3\n141.026389\n37.322222\nTEPCO\n\n\n\n\n5 rows × 53 columns\n\n\n\n\nprint(f\"# of cols, rows: {dfs['SEAWATER'].shape}\")\ndfs['SEAWATER'].head()\n\n# of cols, rows: (49863, 53)\n\n\n\n\n\n\n\n\n\nSampling point number\nCollection layer of seawater\n131I radioactivity concentration (Bq/L)\n131I detection limit (Bq/L)\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n137Cs radioactivity concentration (Bq/L)\n137Cs detection limit (Bq/L)\n132I radioactivity concentration (Bq/L)\n132I detection limit (Bq/L)\n...\n125Sb radioactivity concentration (Bq/L)\n125Sb detection limit (Bq/L)\n105Ru radioactivity concentration (Bq/L)\n105Ru detection limit (Bq/L)\nUnnamed: 49\nTIME\nSTATION\nLON\nLAT\norg\n\n\n\n\n0\nT-3\nNaN\n1.1E+03\n1.3E+01\n4.8E+01\n9.2E+00\n5.3E+01\n8.8E+00\n1.6E+02\n44.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/21 23:15:00\nT-3\n141.026389\n37.322222\nTEPCO\n\n\n1\nT-4\nNaN\n6.6E+02\n1.2E+01\n3.1E+01\n8.7E+00\n3.3E+01\n8.3E+00\n1.2E+02\n41.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/21 23:45:00\nT-4\n141.013889\n37.241667\nTEPCO\n\n\n2\nT-3\nNaN\n1.1E+03\n2.0E+01\n4.6E+01\n1.4E+01\n4.0E+01\n1.4E+01\nND\n88.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/22 14:28:00\nT-3\n141.026389\n37.322222\nTEPCO\n\n\n3\nT-4\nNaN\n6.7E+02\n1.9E+01\n3.9E+01\n1.1E+01\n4.4E+01\n1.1E+01\nND\n79.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2011/3/22 15:06:00\nT-4\n141.013889\n37.241667\nTEPCO\n\n\n4\nT-3\nNaN\n7.4E+02\n2.7E+01\n5.1E+01\n2.0E+01\n5.5E+01\n2.0E+01\n2.0E+02\n58.0\n...\nNaN\nNaN\n34.0\n25.0\nNaN\n2011/3/23 13:51:00\nT-3\n141.026389\n37.322222\nTEPCO\n\n\n\n\n5 rows × 53 columns\n\n\n\n\ndfs['SEAWATER'].STATION.unique()\n\narray(['T-3', 'T-4', 'T-5', 'T-7', 'T-11', 'T-12', 'T-14', 'T-18', 'T-20',\n       'T-22', 'T-MA', 'T-M10', 'T-A', 'T-D', 'T-E', 'T-B', 'T-C',\n       'T-MG1', 'T-MG2', 'T-MG3', 'T-MG4', 'T-MG5', 'T-MG6', 'T-D1',\n       'T-D5', 'T-D9', 'T-E1', 'T-G4', 'T-H1', 'T-S5', 'T-S6', 'T-17-1',\n       'T-B3', 'T-13-1', 'T-S3', 'T-S4', 'T-B4', 'T-S1', 'T-S2', 'T-MG0',\n       'T-Z', 'T-B1', 'T-B2', 'T-S7', 'T-S8', 'T-0', 'T-4-1', 'T-4-2',\n       'T-6', 'T-0-1', 'T-0-1A', 'T-0-2', 'T-0-3', 'T-0-3A', 'T-1', 'T-2',\n       'T-2-1', 'T-A1', 'T-A2', 'T-A3'], dtype=object)\n\n\n\nnp.sum(dfs['SEAWATER'] == \"ND\")\n\nSampling point number                                 0\nCollection layer of seawater                          0\n131I radioactivity concentration (Bq/L)            8642\n131I detection limit (Bq/L)                           0\n134Cs radioactivity concentration (Bq/L)          30967\n134Cs detection limit (Bq/L)                          0\n137Cs radioactivity concentration (Bq/L)          17232\n137Cs detection limit (Bq/L)                          0\n132I radioactivity concentration (Bq/L)               3\n132I detection limit (Bq/L)                           0\n132Te radioactivity concentration (Bq/L)              0\n132Te detection limit (Bq/L)                          0\n136Cs radioactivity concentration (Bq/L)              2\n136Cs detection limit (Bq/L)                          0\n140La radioactivity concentration (Bq/L)              0\n140La detection limit (Bq/L)                          0\n89Sr radioactivity concentration (Bq/L)             101\n89Sr detection limit (Bq/L)                           0\n90Sr radioactivity concentration (Bq/L)             344\n90Sr detection limit (Bq/L)                           0\n238Pu radioactivity concentration (Bq/L)            309\n238Pu detection limit (Bq/L)                          0\n239Pu+240Pu radioactivity concentration (Bq/L)      231\n239Pu+240Pu statistical error (Bq/L)                  0\n239Pu+240Pu detection limit (Bq/L)                    0\nTotal alpha radioactivity concentration (Bq/L)      983\nTotal alpha detection limit (Bq/L)                    0\nTotal beta radioactivity concentration (Bq/L)      4919\nTotal beta detection limit (Bq/L)                     0\n140Ba radioactivity concentration (Bq/L)              0\n140Ba detection limit (Bq/L)                          0\n106Ru radioactivity concentration (Bq/L)              0\n106Ru detection limit (Bq/L)                          0\n58Co radioactivity concentration (Bq/L)               3\n58Co detection limit (Bq/L)                           0\n60Co radioactivity concentration (Bq/L)               9\n60Co detection limit (Bq/L)                           0\n144Ce radioactivity concentration (Bq/L)              9\n144Ce detection limit (Bq/L)                          0\n54Mn radioactivity concentration (Bq/L)               9\n54Mn detection limit (Bq/L)                           0\n3H radioactivity concentration (Bq/L)              9657\n3H detection limit (Bq/L)                             0\n125Sb radioactivity concentration (Bq/L)            647\n125Sb detection limit (Bq/L)                          0\n105Ru radioactivity concentration (Bq/L)              0\n105Ru detection limit (Bq/L)                          0\nUnnamed: 49                                           0\nTIME                                                  0\nSTATION                                               0\nLON                                                   0\nLAT                                                   0\norg                                                   0\ndtype: int64\n\n\n\ndfs['SEAWATER'][['TIME', '134Cs radioactivity concentration (Bq/L)', '134Cs detection limit (Bq/L)']]\n\n\n\n\n\n\n\n\nTIME\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n\n\n\n\n0\n2011/3/21 23:15:00\n4.8E+01\n9.2E+00\n\n\n1\n2011/3/21 23:45:00\n3.1E+01\n8.7E+00\n\n\n2\n2011/3/22 14:28:00\n4.6E+01\n1.4E+01\n\n\n3\n2011/3/22 15:06:00\n3.9E+01\n1.1E+01\n\n\n4\n2011/3/23 13:51:00\n5.1E+01\n2.0E+01\n\n\n...\n...\n...\n...\n\n\n49858\n2025/06/30 08:05\nND\n0.4\n\n\n49859\n2025/07/07 08:36\nND\n0.37\n\n\n49860\n2025/07/17 08:11\nND\n0.29\n\n\n49861\n2025/07/21 08:20\nND\n0.36\n\n\n49862\n2025/07/24 07:39\nNaN\nNaN\n\n\n\n\n49863 rows × 3 columns",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#remove-約-about-character",
    "href": "handlers/tepco.html#remove-約-about-character",
    "title": "TEPCO",
    "section": "Remove 約 (about) character",
    "text": "Remove 約 (about) character\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nWe systematically remove the 約 character. Please confirm that this is the correct way to handle this. We could imagine that mentioning uncertainty would be less ambiguous in future.\n\n\n\nsource\n\nRemoveJapanaseCharCB\n\n RemoveJapanaseCharCB ()\n\nRemove 約 (about) char\n\n\nExported source\nclass RemoveJapanaseCharCB(Callback):\n    \"Remove 約 (about) char\"\n    def _transform_if_about(self, value, about_char='約'):\n        if pd.isna(value): return value\n        return (value.replace(about_char, '') if str(value).count(about_char) != 0 \n                else value)\n    \n    def __call__(self, tfm): \n        for k in tfm.dfs.keys():\n            cols_rdn = [c for c in tfm.dfs[k].columns if ('(Bq/L)' in c) and (tfm.dfs[k][c].dtype == 'object')]\n            tfm.dfs[k][cols_rdn] = tfm.dfs[k][cols_rdn].map(self._transform_if_about)\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB()])\n\ntfm()['SEAWATER'].sample(10)\n\n\n\n\n\n\n\n\nSampling point number\nCollection layer of seawater\n131I radioactivity concentration (Bq/L)\n131I detection limit (Bq/L)\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n137Cs radioactivity concentration (Bq/L)\n137Cs detection limit (Bq/L)\n132I radioactivity concentration (Bq/L)\n132I detection limit (Bq/L)\n...\n125Sb radioactivity concentration (Bq/L)\n125Sb detection limit (Bq/L)\n105Ru radioactivity concentration (Bq/L)\n105Ru detection limit (Bq/L)\nUnnamed: 49\nTIME\nSTATION\nLON\nLAT\norg\n\n\n\n\n24196\nT-D5\n上層\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2024/1/6 08:28:00\nT-D5\n141.072222\n37.416667\nTEPCO\n\n\n3988\nT-D5\n上層\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2012/10/16 09:40:00\nT-D5\n141.072222\n37.416667\nTEPCO\n\n\n9505\nT-5\n上層\nNaN\nNaN\nND\n1.8E-03\n3.9E-03\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2015/7/6 08:22:00\nT-5\n141.200000\n37.416667\nTEPCO\n\n\n35102\nT-1\n上層\nND\n0.49\nND\n1.2\nND\n1.5\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2012/05/31 08:50:00\nT-1\n141.034444\n37.431111\nTEPCO\n\n\n26240\nT-14\n下層\nNaN\nNaN\nND\n1.4E-03\n2.0E-03\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2025/1/6 07:40:00\nT-14\n141.062500\n37.552778\nTEPCO\n\n\n27384\nT-0-1\nNaN\nNaN\nNaN\nND\n0.71\nND\n0.68\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2014/06/24 09:53:00\nT-0-1\n141.040278\n37.430556\nTEPCO\n\n\n39086\nT-1\n上層\nNaN\nNaN\nND\n0.78\nND\n0.58\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2020/12/24 07:45:00\nT-1\n141.034444\n37.431111\nTEPCO\n\n\n18880\nT-D9\n下層\nNaN\nNaN\nND\n1.3E-03\n4.2E-03\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2020/11/24 09:38:00\nT-D9\n141.072167\n37.333333\nTEPCO\n\n\n17717\nT-MG1\n上層\nNaN\nNaN\nND\n1.2E-03\n3.9E-03\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2020/3/19 10:30:00\nT-MG1\n141.283333\n38.333333\nTEPCO\n\n\n17990\nT-5\n下層\nNaN\nNaN\nND\n1.3E-03\n1.9E-03\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2020/5/18 07:34:00\nT-5\n141.200000\n37.416667\nTEPCO\n\n\n\n\n10 rows × 53 columns",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#fix-values-range-string",
    "href": "handlers/tepco.html#fix-values-range-string",
    "title": "TEPCO",
    "section": "Fix values range string",
    "text": "Fix values range string\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nValue ranges are provided as strings (e.g ‘4.0E+00&lt;&&lt;8.0E+00’ or ‘1.0～2.7’). We replace them by their mean. Please confirm that this is the correct way to handle this. Again, mentioning uncertainty would be less ambiguous in future.\n\n\n\nsource\n\nFixRangeValueStringCB\n\n FixRangeValueStringCB ()\n\nReplace range values (e.g ‘4.0E+00&lt;&&lt;8.0E+00’ or ‘1.0～2.7’) by their mean\n\n\nExported source\nclass FixRangeValueStringCB(Callback):\n    \"Replace range values (e.g '4.0E+00&lt;&&lt;8.0E+00' or '1.0～2.7') by their mean\"\n    \n    def _extract_and_calculate_mean(self, s):\n        # For scientific notation ranges\n        float_strings = re.findall(r\"[+-]?\\d+\\.?\\d*E?[+-]?\\d*\", s)\n        if float_strings:\n            float_numbers = np.array(float_strings, dtype=float)\n            return float_numbers.mean()\n        return s\n    \n    def _transform_if_range(self, value):\n        if pd.isna(value): \n            return value\n        value = str(value)\n        # Check for both range patterns\n        if '&lt;&&lt;' in value or '～' in value:\n            return self._extract_and_calculate_mean(value)\n        return value\n\n    def __call__(self, tfm): \n        for k in tfm.dfs.keys():\n            cols_rdn = [c for c in tfm.dfs[k].columns \n                       if ('(Bq/L)' in c) and (tfm.dfs[k][c].dtype == 'object')]\n            # tfm.dfs[k][cols_rdn] = tfm.dfs[k][cols_rdn].map(self._transform_if_range).astype(float)\n            tfm.dfs[k][cols_rdn] = tfm.dfs[k][cols_rdn].map(self._transform_if_range)\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB()\n    ])\n\ndf_test = tfm()['SEAWATER']\ndf_test.sample(10)\n\n\n\n\n\n\n\n\nSampling point number\nCollection layer of seawater\n131I radioactivity concentration (Bq/L)\n131I detection limit (Bq/L)\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n137Cs radioactivity concentration (Bq/L)\n137Cs detection limit (Bq/L)\n132I radioactivity concentration (Bq/L)\n132I detection limit (Bq/L)\n...\n125Sb radioactivity concentration (Bq/L)\n125Sb detection limit (Bq/L)\n105Ru radioactivity concentration (Bq/L)\n105Ru detection limit (Bq/L)\nUnnamed: 49\nTIME\nSTATION\nLON\nLAT\norg\n\n\n\n\n12202\nT-14\n上層\nNaN\nNaN\nND\n1.3E-03\n6.6E-03\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2016/12/13 08:44:00\nT-14\n141.062500\n37.552778\nTEPCO\n\n\n24157\nT-18\n上層\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2023/12/25 09:21:00\nT-18\n140.922222\n36.905556\nTEPCO\n\n\n39939\nT-1\n上層\nNaN\nNaN\nND\n0.69\nND\n0.81\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2022/10/05 07:40:00\nT-1\n141.034444\n37.431111\nTEPCO\n\n\n30050\nT-0-1A\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2024/05/25 07:00:00\nT-0-1A\n141.046667\n37.430556\nTEPCO\n\n\n34441\nT-0-3A\nNaN\nNaN\nNaN\nND\n0.39\nND\n0.34\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2024/07/15 07:45:00\nT-0-3A\n141.046667\n37.416111\nTEPCO\n\n\n4229\nT-MG5\n上層\nNaN\nNaN\n5.0E-03\nNaN\n8.9E-03\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2012/11/22 09:11:00\nT-MG5\n141.250000\n38.166667\nTEPCO\n\n\n29278\nT-0-1A\nNaN\nNaN\nNaN\nND\n0.81\nND\n0.65\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2018/04/24 06:55:00\nT-0-1A\n141.046667\n37.430556\nTEPCO\n\n\n40055\nT-1\n上層\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2023/01/02 07:57:00\nT-1\n141.034444\n37.431111\nTEPCO\n\n\n38436\nT-1\n上層\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2019/08/19 07:55:00\nT-1\n141.034444\n37.431111\nTEPCO\n\n\n22261\nT-D9\n上層\nNaN\nNaN\nND\n1.2E-03\n2.0E-03\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n2022/11/15 09:01:00\nT-D9\n141.072167\n37.333333\nTEPCO\n\n\n\n\n10 rows × 53 columns",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#select-columns-of-interest",
    "href": "handlers/tepco.html#select-columns-of-interest",
    "title": "TEPCO",
    "section": "Select columns of interest",
    "text": "Select columns of interest\nWe select the columns of interest and in particular the elements of interest, in our case radionuclides.\n\nsource\n\nSelectColsOfInterestCB\n\n SelectColsOfInterestCB (common_coi, nuclides_pattern)\n\nSelect columns of interest.\n\n\nExported source\ncommon_coi = ['LON', 'LAT', 'TIME', 'STATION']\nnuclides_pattern = '(Bq/L)'\n\n\n\n\nExported source\nclass SelectColsOfInterestCB(Callback):\n    \"Select columns of interest.\"\n    def __init__(self, common_coi, nuclides_pattern): fc.store_attr()\n    def __call__(self, tfm):\n        nuc_of_interest = [c for c in tfm.dfs['SEAWATER'].columns if nuclides_pattern in c]\n        tfm.dfs['SEAWATER'] = tfm.dfs['SEAWATER'][self.common_coi + nuc_of_interest]\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern)\n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.sample(5)\n\n\n\n\n\n\n\n\nLON\nLAT\nTIME\nSTATION\n131I radioactivity concentration (Bq/L)\n131I detection limit (Bq/L)\n134Cs radioactivity concentration (Bq/L)\n134Cs detection limit (Bq/L)\n137Cs radioactivity concentration (Bq/L)\n137Cs detection limit (Bq/L)\n...\n144Ce radioactivity concentration (Bq/L)\n144Ce detection limit (Bq/L)\n54Mn radioactivity concentration (Bq/L)\n54Mn detection limit (Bq/L)\n3H radioactivity concentration (Bq/L)\n3H detection limit (Bq/L)\n125Sb radioactivity concentration (Bq/L)\n125Sb detection limit (Bq/L)\n105Ru radioactivity concentration (Bq/L)\n105Ru detection limit (Bq/L)\n\n\n\n\n46500\n141.033611\n37.415833\n2025/05/12 07:50\nT-2\nNaN\nNaN\nND\n0.67\nND\n0.82\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n34387\n141.046667\n37.416111\n2024/03/04 08:22:00\nT-0-3A\nNaN\nNaN\nND\n0.36\nND\n0.23\n...\nNaN\nNaN\nNaN\nNaN\nND\n9.0\nNaN\nNaN\nNaN\nNaN\n\n\n45183\n141.033611\n37.415833\n2022/11/13 08:47:00\nT-2\nNaN\nNaN\nND\n0.74\nND\n0.67\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n44254\n141.033611\n37.415833\n2021/01/28 07:05:00\nT-2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\n1\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n12995\n141.666667\n38.300000\n2017/6/6 08:25:00\nT-MG2\nNaN\nNaN\nND\n1.4E-03\n1.6E-03\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 49 columns",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#reshape-wide-to-long",
    "href": "handlers/tepco.html#reshape-wide-to-long",
    "title": "TEPCO",
    "section": "Reshape: wide to long",
    "text": "Reshape: wide to long\nSo that we can extract information such as nuclide name, unit, derived quantities such as uncertainty, detection limit, …\n\nsource\n\nWideToLongCB\n\n WideToLongCB (id_vars=['LON', 'LAT', 'TIME', 'STATION'])\n\nGet TEPCO nuclide names as values not column names to extract contained information (nuclide name, unc, dl, …).\n\n\nExported source\nclass WideToLongCB(Callback):\n    \"\"\"\n    Get TEPCO nuclide names as values not column names \n    to extract contained information (nuclide name, unc, dl, ...).\n    \"\"\"\n    def __init__(self, id_vars=['LON', 'LAT', 'TIME', 'STATION']): \n        fc.store_attr()\n        \n    def __call__(self, tfm): \n        tfm.dfs['SEAWATER'] = pd.melt(tfm.dfs['SEAWATER'], id_vars=self.id_vars)\n#| eval: false\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB()\n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.head()\n\n\n\n\n\n\n\n\nLON\nLAT\nTIME\nSTATION\nvariable\nvalue\n\n\n\n\n0\n141.026389\n37.322222\n2011/3/21 23:15:00\nT-3\n131I radioactivity concentration (Bq/L)\n1.1E+03\n\n\n1\n141.013889\n37.241667\n2011/3/21 23:45:00\nT-4\n131I radioactivity concentration (Bq/L)\n6.6E+02\n\n\n2\n141.026389\n37.322222\n2011/3/22 14:28:00\nT-3\n131I radioactivity concentration (Bq/L)\n1.1E+03\n\n\n3\n141.013889\n37.241667\n2011/3/22 15:06:00\nT-4\n131I radioactivity concentration (Bq/L)\n6.7E+02\n\n\n4\n141.026389\n37.322222\n2011/3/23 13:51:00\nT-3\n131I radioactivity concentration (Bq/L)\n7.4E+02",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#extract",
    "href": "handlers/tepco.html#extract",
    "title": "TEPCO",
    "section": "Extract",
    "text": "Extract\nNulide name, dl, unc, … are extracted from column names as embedded in TEPCO data source.\n\nNuclide name\n\nsource\n\n\nextract_nuclide\n\n extract_nuclide (text:str)\n\nExtract the nuclide identifier from a measurement variable name using regex.\n\n\nExported source\ndef extract_nuclide(text: str) -&gt; str:\n    \"Extract the nuclide identifier from a measurement variable name using regex.\"\n    pattern = r'^(Total\\s+(?:alpha|beta)|[^\\s]+)'\n    match = re.match(pattern, text, re.IGNORECASE)\n    return match.group(1) if match else text\n\n\nFor instance:\n\nprint(extract_nuclide(\"Total alpha radioactivity concentration (Bq/L)\"))\nprint(extract_nuclide(\"131I radioactivity concentration (Bq/L)\"))\n\nTotal alpha\n131I\n\n\n\nsource\n\n\nExtractNuclideNameCB\n\n ExtractNuclideNameCB (src_col='variable', dest_col='NUCLIDE')\n\nExtract nuclide name from TEPCO data.\n\n\nExported source\nclass ExtractNuclideNameCB(Callback):\n    \"Extract nuclide name from TEPCO data.\"\n    def __init__(self, src_col='variable', dest_col='NUCLIDE'): fc.store_attr()\n    def __call__(self, tfm): \n        tfm.dfs['SEAWATER'][self.dest_col] = tfm.dfs['SEAWATER'][self.src_col].map(extract_nuclide)\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB()\n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.sample(5)\n\n\n\n\n\n\n\n\nLON\nLAT\nTIME\nSTATION\nvariable\nvalue\nNUCLIDE\n\n\n\n\n854848\n141.200000\n37.416667\n2014/5/20 08:34:00\nT-5\n90Sr detection limit (Bq/L)\nNaN\n90Sr\n\n\n125473\n141.026389\n37.322222\n2024/10/15 12:40:00\nT-3\n134Cs radioactivity concentration (Bq/L)\nNaN\n134Cs\n\n\n1684112\n141.034444\n37.431111\n2020/01/15 08:00:00\nT-1\n60Co radioactivity concentration (Bq/L)\nNaN\n60Co\n\n\n869803\n140.702222\n35.987500\n2022/10/21 13:08:00\nT-D\n90Sr detection limit (Bq/L)\nNaN\n90Sr\n\n\n366571\n141.250000\n38.166667\n2020/2/7 09:22:00\nT-MG5\n132I detection limit (Bq/L)\nNaN\n132I\n\n\n\n\n\n\n\n\n\nUnit\n\nsource\n\n\nExtractUnitCB\n\n ExtractUnitCB (src_col='variable', dest_col='UNIT')\n\nExtract unit from TEPCO data.\n\n\nExported source\nclass ExtractUnitCB(Callback):\n    \"Extract unit from TEPCO data.\"\n    def __init__(self, src_col='variable', dest_col='UNIT'): fc.store_attr()\n    def __call__(self, tfm): \n        tfm.dfs['SEAWATER'][self.dest_col] = tfm.dfs['SEAWATER'][self.src_col].str.extract(r'\\((.*?)\\)')\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB()\n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.sample(5)\n\n\n\n\n\n\n\n\nLON\nLAT\nTIME\nSTATION\nvariable\nvalue\nNUCLIDE\nUNIT\n\n\n\n\n938328\n141.034444\n37.431111\n2024/07/08 07:51:00\nT-1\n238Pu radioactivity concentration (Bq/L)\nNaN\n238Pu\nBq/L\n\n\n2221110\n141.072222\n37.500000\n2025/6/2 08:36:00\nT-D1\n105Ru detection limit (Bq/L)\nNaN\n105Ru\nBq/L\n\n\n1951908\n141.078889\n37.458333\n2014/5/29 06:07:00\nT-S3\n3H radioactivity concentration (Bq/L)\nNaN\n3H\nBq/L\n\n\n1077687\n141.046667\n37.423333\n2016/02/01 08:16\nT-0-2\n239Pu+240Pu statistical error (Bq/L)\nNaN\n239Pu+240Pu\nBq/L\n\n\n969934\n141.233333\n37.516667\n2023/1/26 07:26:00\nT-B2\n238Pu detection limit (Bq/L)\nNaN\n238Pu\nBq/L\n\n\n\n\n\n\n\n\n\nValue type\nIs it a measurement or derived detection such as detection limit or uncertainty?\n\nsource\n\n\nExtractValueTypeCB\n\n ExtractValueTypeCB (src_col='variable', dest_col='type')\n\nExtract value type from TEPCO data.\n\n\nExported source\nclass ExtractValueTypeCB(Callback):\n    \"Extract value type from TEPCO data.\"\n    def __init__(self, src_col='variable', dest_col='type'): fc.store_attr()\n    def __call__(self, tfm): \n        tfm.dfs['SEAWATER'][self.dest_col] = np.select(\n            [\n                tfm.dfs['SEAWATER'][self.src_col].str.contains('detection limit', case=False),\n                tfm.dfs['SEAWATER'][self.src_col].str.contains('statistical error', case=False)],\n            ['DL', 'UNC'],\n            default='VALUE'\n        )\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB(),\n    ExtractValueTypeCB()\n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.sample(5)\n\n\n\n\n\n\n\n\nLON\nLAT\nTIME\nSTATION\nvariable\nvalue\nNUCLIDE\nUNIT\ntype\n\n\n\n\n1315127\n141.000000\n36.966667\n2020/10/15 10:53:00\nT-20\nTotal beta detection limit (Bq/L)\nNaN\nTotal beta\nBq/L\nDL\n\n\n1839912\n141.033611\n37.415833\n2022/03/19 09:05:00\nT-2\n144Ce detection limit (Bq/L)\nNaN\n144Ce\nBq/L\nDL\n\n\n1842443\n37.410000\n141.030000\n2014/08/11 05:35:00\nT-2-1\n144Ce detection limit (Bq/L)\nNaN\n144Ce\nBq/L\nDL\n\n\n36817\n141.034444\n37.431111\n2016/03/20 07:45\nT-1\n131I radioactivity concentration (Bq/L)\nND\n131I\nBq/L\nVALUE\n\n\n171526\n141.072222\n37.416667\n2022/9/5 08:09:00\nT-D5\n134Cs detection limit (Bq/L)\n1.4E-03\n134Cs\nBq/L\nDL",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#reshape-long-to-wide",
    "href": "handlers/tepco.html#reshape-long-to-wide",
    "title": "TEPCO",
    "section": "Reshape: long to wide",
    "text": "Reshape: long to wide\nSend type column to columns names (VALUE, DL, UNC)\n\nsource\n\nLongToWideCB\n\n LongToWideCB (src_col='variable', dest_col='type')\n\nReshape: long to wide\n\n\nExported source\nclass LongToWideCB(Callback):\n    \"Reshape: long to wide\"\n    def __init__(self, src_col='variable', dest_col='type'): fc.store_attr()\n    def __call__(self, tfm): \n        tfm.dfs['SEAWATER'] = pd.pivot_table(\n            tfm.dfs['SEAWATER'],\n            values='value',\n            index=['LON', 'LAT', 'TIME', 'STATION', 'NUCLIDE', 'UNIT'],\n            columns='type',\n            aggfunc='first'\n        ).reset_index()\n        tfm.dfs['SEAWATER'].reset_index(inplace=True)\n        tfm.dfs['SEAWATER'].rename(columns={'index': 'SMP_ID'}, inplace=True)\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB(),\n    ExtractValueTypeCB(),\n    LongToWideCB()\n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.sample(5)\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\n\n\n\n\n83649\n83649\n141.200000\n37.233333\n2011/8/26 07:40:00\nT-7\n134Cs\nBq/L\n1.2E+01\nNaN\nND\n\n\n37560\n37560\n141.034444\n37.431111\n2013/10/27 06:45:00\nT-1\n137Cs\nBq/L\nNaN\nNaN\n1.4\n\n\n18204\n18204\n141.026389\n37.322222\n2014/8/5 10:10:00\nT-3\nTotal beta\nBq/L\n1.7E+01\nNaN\nND\n\n\n65718\n65718\n141.046667\n37.430556\n2022/10/03 07:10:00\nT-0-1A\n134Cs\nBq/L\n0.31\nNaN\nND\n\n\n69961\n69961\n141.050761\n37.424686\n2025/07/07 07:35\nT-A2\n3H\nBq/L\n9.4\nNaN\nND\n\n\n\n\n\n\n\n\ndf_test[df_test.VALUE == 'ND'].groupby('NUCLIDE').size().sort_values(ascending=False)\n\nNUCLIDE\n134Cs          25186\n137Cs          16447\n3H              8976\n131I            7958\nTotal beta      4913\nTotal alpha      979\n125Sb            647\n90Sr             342\n238Pu            308\n239Pu+240Pu      231\n89Sr             100\n144Ce              9\n54Mn               9\n60Co               9\n58Co               3\n132I               3\n136Cs              2\ndtype: int64\n\n\n\ndf_test[df_test.VALUE == 'ND']\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\n\n\n\n\n0\n0\n37.210000\n141.01\n2012/10/16 07:25:00\nT-4-1\n131I\nBq/L\n1.3E-01\nNaN\nND\n\n\n1\n1\n37.210000\n141.01\n2012/10/16 07:25:00\nT-4-1\n134Cs\nBq/L\n1.9E-01\nNaN\nND\n\n\n2\n2\n37.210000\n141.01\n2012/10/16 07:25:00\nT-4-1\n137Cs\nBq/L\n2.7E-01\nNaN\nND\n\n\n3\n3\n37.210000\n141.01\n2012/10/2 07:30:00\nT-4-1\n131I\nBq/L\n1.1E-01\nNaN\nND\n\n\n4\n4\n37.210000\n141.01\n2012/10/2 07:30:00\nT-4-1\n134Cs\nBq/L\n2.2E-01\nNaN\nND\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n93158\n93158\n141.666667\n38.30\n2025/4/8 08:20:00\nT-MG2\n134Cs\nBq/L\n1.3E-03\nNaN\nND\n\n\n93160\n93160\n141.666667\n38.30\n2025/5/13 07:36:00\nT-MG2\n134Cs\nBq/L\n1.2E-03\nNaN\nND\n\n\n93162\n93162\n141.666667\n38.30\n2025/5/13 07:50:00\nT-MG2\n134Cs\nBq/L\n8.7E-04\nNaN\nND\n\n\n93164\n93164\n141.666667\n38.30\n2025/6/3 08:15:00\nT-MG2\n134Cs\nBq/L\n1.1E-03\nNaN\nND\n\n\n93166\n93166\n141.666667\n38.30\n2025/6/3 08:24:00\nT-MG2\n134Cs\nBq/L\n1.2E-03\nNaN\nND\n\n\n\n\n66122 rows × 10 columns\n\n\n\n\ndf_test.VALUE == 'ND'\n\n0         True\n1         True\n2         True\n3         True\n4         True\n         ...  \n93163    False\n93164     True\n93165    False\n93166     True\n93167    False\nName: VALUE, Length: 93168, dtype: bool",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#remap-unit-name-to-maris-nomenclature",
    "href": "handlers/tepco.html#remap-unit-name-to-maris-nomenclature",
    "title": "TEPCO",
    "section": "Remap UNIT name to MARIS nomenclature",
    "text": "Remap UNIT name to MARIS nomenclature\nData are reported in Bq/L but MARIS uses Bq/m3 instead. So we assign it to MARIS unit_id = 3 (Bq/L). Later in the processing pipeline, we will convert the values from Bq/L to Bq/m3 by multiplying VALUE, DL, and DLV by 1000.\n\nsource\n\nRemapUnitNameCB\n\n RemapUnitNameCB (unit_mapping)\n\nRemap UNIT name to MARIS id.\n\n\nExported source\nunit_mapping = {'Bq/L': 1}\n\n\n\n\nExported source\nclass RemapUnitNameCB(Callback):\n    \"\"\"\n    Remap `UNIT` name to MARIS id.\n    \"\"\"\n    def __init__(self, unit_mapping): fc.store_attr()\n    def __call__(self, tfm):\n        tfm.dfs['SEAWATER']['UNIT'] = tfm.dfs['SEAWATER']['UNIT'].map(self.unit_mapping)\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB(),\n    ExtractValueTypeCB(),\n    LongToWideCB(),\n    RemapUnitNameCB(unit_mapping)\n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.sample(5)\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\n\n\n\n\n81874\n81874\n141.133333\n38.250000\n2012/7/26 10:25:00\nT-MG4\n134Cs\n1\nNaN\nNaN\n6.8E-03\n\n\n56188\n56188\n141.040556\n37.478889\n2015/2/3 09:15:00\nT-6\n3H\n1\nNaN\nNaN\n4.6E-01\n\n\n74609\n74609\n141.072167\n37.333333\n2024/9/18 08:21:00\nT-D9\n137Cs\n1\nNaN\nNaN\n2.9E-03\n\n\n36442\n36442\n141.034444\n37.431111\n2012/11/19 08:30:00\nT-1\n137Cs\n1\n1.4\nNaN\nND\n\n\n36846\n36846\n141.034444\n37.431111\n2013/03/28 06:50:00\nT-1\n134Cs\n1\n1.1\nNaN\nND",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#remap-nuclide-name-to-maris-nomenclature",
    "href": "handlers/tepco.html#remap-nuclide-name-to-maris-nomenclature",
    "title": "TEPCO",
    "section": "Remap NUCLIDE name to MARIS nomenclature",
    "text": "Remap NUCLIDE name to MARIS nomenclature\n\nsource\n\nRemapNuclideNameCB\n\n RemapNuclideNameCB (nuclide_mapping)\n\nRemap NUCLIDE name to MARIS id.\n\n\nExported source\nnuclide_mapping = {\n    '131I': 29,\n    '134Cs': 31,\n    '137Cs': 33,\n    '125Sb': 24,\n    'Total beta': 103,\n    '238Pu': 67,\n    '239Pu+240Pu': 77,\n    '3H': 1,\n    '89Sr': 11,\n    '90Sr': 12,\n    'Total alpha': 104,\n    '132I': 100,\n    '136Cs': 102,\n    '58Co': 8,\n    '105Ru': 97,\n    '106Ru': 17,\n    '140La': 35,\n    '140Ba': 34,\n    '132Te': 99,\n    '60Co': 9,\n    '144Ce': 37,\n    '54Mn': 6\n}\n\n\n\n\nExported source\nclass RemapNuclideNameCB(Callback):\n    \"Remap `NUCLIDE` name to MARIS id.\"\n    def __init__(self, nuclide_mapping): fc.store_attr()\n    def __call__(self, tfm):\n        tfm.dfs['SEAWATER']['NUCLIDE'] = tfm.dfs['SEAWATER']['NUCLIDE'].map(self.nuclide_mapping)\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB(),\n    ExtractValueTypeCB(),\n    LongToWideCB(),\n    RemapUnitNameCB(unit_mapping),\n    RemapNuclideNameCB(nuclide_mapping)\n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.sample(5)\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\n\n\n\n\n63425\n63425\n141.046667\n37.423333\n2024/04/01 08:15:00\nT-0-2\n31\n1\n0.4\nNaN\nND\n\n\n83876\n83876\n141.200000\n37.233333\n2019/6/6 06:58:00\nT-7\n31\n1\n1.2E-03\nNaN\nND\n\n\n31743\n31743\n141.033611\n37.415833\n2023/07/31 08:55:00\nT-2\n103\n1\nNaN\nNaN\n10\n\n\n68484\n68484\n141.047222\n37.311111\n2023/12/5 05:48:00\nT-S7\n1\n1\nNaN\nNaN\n1.5E-01\n\n\n19853\n19853\n141.026389\n37.322222\n2025/3/4 12:20:00\nT-3\n1\n1\n3.6E-01\nNaN\nND\n\n\n\n\n\n\n\n\ndf_test.dropna(subset=['DL', 'VALUE'], how='any')\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\n\n\n\n\n0\n0\n37.210000\n141.01\n2012/10/16 07:25:00\nT-4-1\n29\n1\n1.3E-01\nNaN\nND\n\n\n1\n1\n37.210000\n141.01\n2012/10/16 07:25:00\nT-4-1\n31\n1\n1.9E-01\nNaN\nND\n\n\n2\n2\n37.210000\n141.01\n2012/10/16 07:25:00\nT-4-1\n33\n1\n2.7E-01\nNaN\nND\n\n\n3\n3\n37.210000\n141.01\n2012/10/2 07:30:00\nT-4-1\n29\n1\n1.1E-01\nNaN\nND\n\n\n4\n4\n37.210000\n141.01\n2012/10/2 07:30:00\nT-4-1\n31\n1\n2.2E-01\nNaN\nND\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n93158\n93158\n141.666667\n38.30\n2025/4/8 08:20:00\nT-MG2\n31\n1\n1.3E-03\nNaN\nND\n\n\n93160\n93160\n141.666667\n38.30\n2025/5/13 07:36:00\nT-MG2\n31\n1\n1.2E-03\nNaN\nND\n\n\n93162\n93162\n141.666667\n38.30\n2025/5/13 07:50:00\nT-MG2\n31\n1\n8.7E-04\nNaN\nND\n\n\n93164\n93164\n141.666667\n38.30\n2025/6/3 08:15:00\nT-MG2\n31\n1\n1.1E-03\nNaN\nND\n\n\n93166\n93166\n141.666667\n38.30\n2025/6/3 08:24:00\nT-MG2\n31\n1\n1.2E-03\nNaN\nND\n\n\n\n\n66093 rows × 10 columns",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#remap-value-dl-dlv",
    "href": "handlers/tepco.html#remap-value-dl-dlv",
    "title": "TEPCO",
    "section": "Remap VALUE, DL, DLV",
    "text": "Remap VALUE, DL, DLV\nWe remap DL (Detection Limit) value to MARIS ids as follows:\n\nFirst check if activity (VALUE) is reported as “ND”, based on reported detection limit DL:\n\nif VALUE is \"ND\":\n    if not DL: \n        VALUE, DLV, DL = NaN, NaN, 3\n    else:\n        VALUE, DLV, DL = DL, DL, 2\n\nThen if activity (VALUE) is reported:\n\nif VALUE:\n    VALUE, DLV, DL = VALUE, DL, 1\nbut if not reported, then based on detection level (DL) reported:\nelse:\n    if DL:\n        VALUE, DLV, DL = DL, DL, 2\n    else:\n        VALUE, DLV, DL = NaN, NaN, NaN (should be dropped)\nWith 1: Detected value (=), 2: Detection limit (&lt;), 3: Not detected (ND) and where:\n\nVALUE is the activity reported by TEPCO\nDL is initially the detection limit as reported by TEPCO but later on remapped to MARIS detection level nomenclature (categorical)\nDLV is the detection limit value as reported by TEPCO (copied from DL)\n\n\nsource\n\nRemapVALUE_DL_DLV_CB\n\n RemapVALUE_DL_DLV_CB ()\n\nRemap DL, DLV, VALUE based on TEPCO -&gt; MARIS rules.\n\n\nExported source\nclass RemapVALUE_DL_DLV_CB(Callback):\n    \"Remap `DL`, `DLV`, `VALUE` based on TEPCO -&gt; MARIS rules.\"    \n    def map_all_columns(self, row):\n        \"\"\"Map all three columns (VALUE, DL, DLV) at once based on TEPCO rules\"\"\"\n        value, dl = row['VALUE'], row['DL']\n        new_value, new_dlv, new_dl = value, dl, 1\n        \n        if value == 'ND':\n            if pd.isna(dl):\n                new_value, new_dlv, new_dl = np.nan, np.nan, 3\n            else:\n                new_value, new_dlv, new_dl = dl, dl, 2\n                \n        elif pd.isna(value):\n            if pd.isna(dl):\n                new_value, new_dlv, new_dl = np.nan, np.nan, np.nan\n            else:\n                new_value, new_dlv, new_dl = dl, dl, 2\n                \n        return pd.Series({\n            'VALUE': new_value,\n            'DLV': new_dlv, \n            'DL': new_dl\n        })\n        \n    def __call__(self, tfm):\n        mapped = tfm.dfs['SEAWATER'].apply(self.map_all_columns, axis=1)\n        tfm.dfs['SEAWATER'][['VALUE', 'DLV', 'DL']] = mapped\n        tfm.dfs['SEAWATER']['DL'] = tfm.dfs['SEAWATER']['DL'].astype(int)\n        tfm.dfs['SEAWATER']['VALUE'] = tfm.dfs['SEAWATER']['VALUE'].astype(float)\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB(),\n    ExtractValueTypeCB(),\n    LongToWideCB(),\n    RemapUnitNameCB(unit_mapping),\n    RemapNuclideNameCB(nuclide_mapping),\n    RemapVALUE_DL_DLV_CB()\n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.sample(20)\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\nDLV\n\n\n\n\n24313\n24313\n141.033611\n37.415833\n2017/10/19 06:50:00\nT-2\n33\n1\n2\nNaN\n0.7100\n0.71\n\n\n81171\n81171\n141.083333\n37.750000\n2011/11/22 07:10:00\nT-MA\n33\n1\n2\nNaN\n1.1000\n1.1E+00\n\n\n62025\n62025\n141.046667\n37.423333\n2018/01/05 07:37:00\nT-0-2\n1\n1\n2\nNaN\n1.7000\n1.7\n\n\n61732\n61732\n141.046667\n37.423333\n2016/08/10 07:54\nT-0-2\n33\n1\n2\nNaN\n0.5300\n0.53\n\n\n52714\n52714\n141.040278\n37.416111\n2023/11/06 06:56:00\nT-0-3\n31\n1\n2\nNaN\n0.3000\n0.3\n\n\n9189\n9189\n140.763889\n36.713889\n2012/2/28 07:41:00\nT-A\n29\n1\n2\nNaN\n1.2000\n1.2E+00\n\n\n16645\n16645\n141.022500\n37.824444\n2022/10/6 05:44:00\nT-22\n31\n1\n2\nNaN\n0.0013\n1.3E-03\n\n\n24544\n24544\n141.033611\n37.415833\n2017/12/15 06:55:00\nT-2\n31\n1\n2\nNaN\n0.5500\n0.55\n\n\n69344\n69344\n141.050761\n37.424686\n2023/03/27 07:26:00\nT-A2\n33\n1\n2\nNaN\n0.2900\n0.29\n\n\n21783\n21783\n141.033611\n37.415833\n2012/05/29 08:15:00\nT-2\n33\n1\n2\nNaN\n1.6000\n1.6\n\n\n30460\n30460\n141.033611\n37.415833\n2022/07/11 09:13:00\nT-2\n103\n1\n1\nNaN\n14.0000\nNaN\n\n\n29837\n29837\n141.033611\n37.415833\n2022/01/03 08:15:00\nT-2\n33\n1\n2\nNaN\n0.8700\n0.87\n\n\n9304\n9304\n140.763889\n36.713889\n2014/11/10 09:34:00\nT-A\n33\n1\n2\nNaN\n1.3000\n1.3E+00\n\n\n28727\n28727\n141.033611\n37.415833\n2021/01/28 07:05:00\nT-2\n31\n1\n2\nNaN\n0.8000\n0.8\n\n\n42442\n42442\n141.034444\n37.431111\n2017/11/27 07:05:00\nT-1\n1\n1\n2\nNaN\n1.9000\n1.9\n\n\n24596\n24596\n141.033611\n37.415833\n2017/12/28 06:55:00\nT-2\n103\n1\n1\nNaN\n12.0000\nNaN\n\n\n6689\n6689\n140.603889\n36.299722\n2019/10/18 13:06:00\nT-C\n33\n1\n2\nNaN\n1.2000\n1.2E+00\n\n\n35904\n35904\n141.034444\n37.431111\n2012/05/28 08:55:00\nT-1\n29\n1\n2\nNaN\n0.4900\n0.49\n\n\n15422\n15422\n141.013889\n37.241667\n2017/12/5 13:30:00\nT-4\n31\n1\n1\nNaN\n0.0038\nNaN\n\n\n59538\n59538\n141.046667\n37.416111\n2018/08/28 07:22:00\nT-0-3A\n103\n1\n2\nNaN\n15.0000\n15.0",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#convert-activity-to-bqm3",
    "href": "handlers/tepco.html#convert-activity-to-bqm3",
    "title": "TEPCO",
    "section": "Convert activity to Bq/m3",
    "text": "Convert activity to Bq/m3\nEarlier in the pipeline, we assigned MARIS unit_id = 3 (Bq/L) to TEPCO UNIT = Bq/L. Now we need to convert the values from Bq/L to Bq/m3 by multiplying VALUE, DL, and DLV by 1000.\n\nsource\n\nConvertToBqM3CB\n\n ConvertToBqM3CB ()\n\nConvert from Bq/L to Bq/m3.\n\n\nExported source\nclass ConvertToBqM3CB(Callback):\n    \"Convert from Bq/L to Bq/m3.\"    \n    def __call__(self, tfm, factor=1000):\n        tfm.dfs['SEAWATER']['VALUE'] = tfm.dfs['SEAWATER']['VALUE'] * factor\n        # Convert DLV to float, handling NaN values\n        tfm.dfs['SEAWATER']['DLV'] = pd.to_numeric(tfm.dfs['SEAWATER']['DLV'], errors='coerce')\n        tfm.dfs['SEAWATER']['DLV'] = tfm.dfs['SEAWATER']['DLV'] * factor\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB(),\n    ExtractValueTypeCB(),\n    LongToWideCB(),\n    RemapUnitNameCB(unit_mapping),\n    RemapNuclideNameCB(nuclide_mapping),\n    RemapVALUE_DL_DLV_CB(),\n    ConvertToBqM3CB()\n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.sample(20)\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\nDLV\n\n\n\n\n11653\n11653\n140.922222\n36.905556\n2018/7/17 09:28:00\nT-18\n33\n1\n1\nNaN\n3.1\nNaN\n\n\n56255\n56255\n141.040556\n37.478889\n2015/8/11 08:00:00\nT-6\n33\n1\n1\nNaN\n57.0\nNaN\n\n\n4414\n4414\n37.410000\n141.030000\n2015/07/13 05:30:00\nT-2-1\n1\n1\n1\nNaN\n2100.0\nNaN\n\n\n71352\n71352\n141.062500\n37.552778\n2015/7/27 08:36:00\nT-14\n31\n1\n1\nNaN\n2.0\n1.4\n\n\n28640\n28640\n141.033611\n37.415833\n2021/01/01 07:11:00\nT-2\n103\n1\n1\nNaN\n15000.0\nNaN\n\n\n51235\n51235\n141.040278\n37.416111\n2017/01/02 07:16:00\nT-0-3\n31\n1\n2\nNaN\n680.0\n680.0\n\n\n66512\n66512\n141.046667\n37.430556\n2025/04/29 07:18\nT-0-1A\n1\n1\n2\nNaN\n8100.0\n8100.0\n\n\n49336\n49336\n141.034444\n37.431111\n2025/03/29 06:50\nT-1\n33\n1\n2\nNaN\n660.0\n660.0\n\n\n71187\n71187\n141.062500\n37.552778\n2014/2/26 09:09:00\nT-14\n33\n1\n1\nNaN\n48.0\nNaN\n\n\n32088\n32088\n141.033611\n37.415833\n2023/10/27 06:45:00\nT-2\n31\n1\n2\nNaN\n750.0\n750.0\n\n\n34498\n34498\n141.034444\n37.431111\n2011/04/20 14:20:00\nT-1\n29\n1\n1\nNaN\n47000.0\nNaN\n\n\n76600\n76600\n141.072222\n37.416667\n2022/6/1 08:40:00\nT-D5\n104\n1\n2\nNaN\n2500.0\n2500.0\n\n\n51215\n51215\n141.040278\n37.416111\n2016/11/28 07:47:00\nT-0-3\n31\n1\n2\nNaN\n790.0\n790.0\n\n\n28565\n28565\n141.033611\n37.415833\n2020/12/09 06:45:00\nT-2\n33\n1\n2\nNaN\n690.0\n690.0\n\n\n28344\n28344\n141.033611\n37.415833\n2020/10/04 06:45:00\nT-2\n103\n1\n1\nNaN\n13000.0\nNaN\n\n\n70632\n70632\n141.062500\n37.552778\n2011/10/25 09:10:00\nT-14\n29\n1\n2\nNaN\n740.0\n740.0\n\n\n7440\n7440\n140.665556\n36.506389\n2014/5/26 13:38:00\nT-B\n31\n1\n2\nNaN\n1100.0\n1100.0\n\n\n20878\n20878\n141.033611\n37.415833\n2011/10/03 08:30:00\nT-2\n29\n1\n2\nNaN\n4000.0\n4000.0\n\n\n4115\n4115\n37.410000\n141.030000\n2015/04/27 05:30:00\nT-2-1\n31\n1\n2\nNaN\n770.0\n770.0\n\n\n44483\n44483\n141.034444\n37.431111\n2019/10/01 08:05:00\nT-1\n31\n1\n2\nNaN\n840.0\n840.0",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#parse-encode-time",
    "href": "handlers/tepco.html#parse-encode-time",
    "title": "TEPCO",
    "section": "Parse & encode time",
    "text": "Parse & encode time\n\nsource\n\nParseTimeCB\n\n ParseTimeCB (time_name='TIME')\n\nParse time column from TEPCO.\n\n\nExported source\nclass ParseTimeCB(Callback):\n    \"Parse time column from TEPCO.\"\n    def __init__(self, time_name='TIME'): fc.store_attr()\n    def __call__(self, tfm):\n        tfm.dfs['SEAWATER'][self.time_name] = pd.to_datetime(tfm.dfs['SEAWATER'][self.time_name], \n                                                             format='%Y/%m/%d %H:%M:%S', errors='coerce')\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB(),\n    ExtractValueTypeCB(),\n    LongToWideCB(),\n    RemapUnitNameCB(unit_mapping),\n    RemapNuclideNameCB(nuclide_mapping),\n    RemapVALUE_DL_DLV_CB(),\n    ConvertToBqM3CB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    \n    ])\n\ndf_test = tfm()['SEAWATER'] \ndf_test.sample(5)\n\nWarning: 4831 missing time value(s) in SEAWATER\n\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\nDLV\n\n\n\n\n69085\n69085\n141.050739\n37.409267\n1733731260\nT-A3\n33\n1\n2\nNaN\n320.0\n320.0\n\n\n44340\n44340\n141.034444\n37.431111\n1564646400\nT-1\n33\n1\n2\nNaN\n720.0\n720.0\n\n\n30532\n30532\n141.033611\n37.415833\n1659345300\nT-2\n103\n1\n1\nNaN\n7000.0\nNaN\n\n\n42965\n42965\n141.034444\n37.431111\n1525677000\nT-1\n1\n1\n2\nNaN\n880.0\n880.0\n\n\n14651\n14651\n141.013889\n37.241667\n1315813500\nT-4\n29\n1\n2\nNaN\n4000.0\n4000.0",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#sanitize-coordinates",
    "href": "handlers/tepco.html#sanitize-coordinates",
    "title": "TEPCO",
    "section": "Sanitize coordinates",
    "text": "Sanitize coordinates\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB(),\n    ExtractValueTypeCB(),\n    LongToWideCB(),\n    RemapUnitNameCB(unit_mapping),\n    RemapNuclideNameCB(nuclide_mapping),\n    RemapVALUE_DL_DLV_CB(),\n    ConvertToBqM3CB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    SanitizeLonLatCB()\n    ])\n\ndf_test = tfm()['SEAWATER']\ndf_test.sample(5)\n\nWarning: 4831 missing time value(s) in SEAWATER\n\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\nDLV\n\n\n\n\n54074\n54074\n141.040278\n37.430556\n1515394980\nT-0-1\n1\n1\n2\nNaN\n1700.0\n1700.0\n\n\n74378\n74378\n141.072167\n37.333333\n1681804920\nT-D9\n103\n1\n2\nNaN\n14000.0\n14000.0\n\n\n91213\n91213\n141.583333\n38.633333\n1377003060\nT-MG0\n31\n1\n2\nNaN\n2.2\n2.2\n\n\n72667\n72667\n141.072167\n37.333333\n1380362220\nT-D9\n31\n1\n1\nNaN\n23.0\nNaN\n\n\n86114\n86114\n141.200000\n37.416667\n1638775740\nT-5\n104\n1\n2\nNaN\n2300.0\n2300.0",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#add-sample-id",
    "href": "handlers/tepco.html#add-sample-id",
    "title": "TEPCO",
    "section": "Add Sample ID",
    "text": "Add Sample ID\nThe SMP_ID_PROVIDER column stores the original sample ID from the data provider. TEPCO does not provide sample IDs, so this column will be set to None for all records.\n\nsource\n\nAddSampleIdCB\n\n AddSampleIdCB ()\n\nConvert from Bq/L to Bq/m3.\n\n\nExported source\nclass AddSampleIdCB(Callback):\n    \"Convert from Bq/L to Bq/m3.\"    \n    def __call__(self, tfm, factor=1000):\n        tfm.dfs['SEAWATER']['SMP_ID'] = range(1, len(tfm.dfs['SEAWATER']) + 1)\n        tfm.dfs['SEAWATER']['SMP_ID_PROVIDER'] = \"\"\n\n\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB(),\n    ExtractValueTypeCB(),\n    LongToWideCB(),\n    RemapUnitNameCB(unit_mapping),\n    RemapNuclideNameCB(nuclide_mapping),\n    RemapVALUE_DL_DLV_CB(),\n    ConvertToBqM3CB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    SanitizeLonLatCB(),\n    AddSampleIdCB(),\n    ])\n\ndf_test = tfm()['SEAWATER']\ndf_test.sample(5)\n\nWarning: 4831 missing time value(s) in SEAWATER\n\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\nDLV\nSMP_ID_PROVIDER\n\n\n\n\n67223\n57586\n141.047222\n37.241667\n1448351640\nT-11\n31\n1\n1\nNaN\n3.5\nNaN\n\n\n\n42868\n35125\n141.034444\n37.431111\n1523173800\nT-1\n33\n1\n2\nNaN\n450.0\n450.0\n\n\n\n44015\n36272\n141.034444\n37.431111\n1553155800\nT-1\n31\n1\n2\nNaN\n760.0\n760.0\n\n\n\n42519\n34776\n141.034444\n37.431111\n1513755900\nT-1\n31\n1\n2\nNaN\n540.0\n540.0\n\n\n\n13882\n7902\n141.013889\n37.241667\n1318838400\nT-4\n29\n1\n2\nNaN\n4000.0\n4000.0",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/tepco.html#encode-to-netcdf",
    "href": "handlers/tepco.html#encode-to-netcdf",
    "title": "TEPCO",
    "section": "Encode to NetCDF",
    "text": "Encode to NetCDF\n\ntfm = Transformer(dfs, cbs=[\n    RemoveJapanaseCharCB(),\n    FixRangeValueStringCB(),\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(),\n    ExtractNuclideNameCB(),\n    ExtractUnitCB(),\n    ExtractValueTypeCB(),\n    LongToWideCB(),\n    RemapUnitNameCB(unit_mapping),\n    RemapNuclideNameCB(nuclide_mapping),\n    RemapVALUE_DL_DLV_CB(),\n    ConvertToBqM3CB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    SanitizeLonLatCB(),\n    AddSampleIdCB(),\n    ])\n\ndfs_tfm = tfm()\ntfm.logs\n\nWarning: 4831 missing time value(s) in SEAWATER\n\n\n['Remove 約 (about) char',\n \"Replace range values (e.g '4.0E+00&lt;&&lt;8.0E+00' or '1.0～2.7') by their mean\",\n 'Select columns of interest.',\n '\\n    Get TEPCO nuclide names as values not column names \\n    to extract contained information (nuclide name, unc, dl, ...).\\n    ',\n 'Extract nuclide name from TEPCO data.',\n 'Extract unit from TEPCO data.',\n 'Extract value type from TEPCO data.',\n 'Reshape: long to wide',\n '\\n    Remap `UNIT` name to MARIS id.\\n    ',\n 'Remap `NUCLIDE` name to MARIS id.',\n 'Remap `DL`, `DLV`, `VALUE` based on TEPCO -&gt; MARIS rules.',\n 'Convert from Bq/L to Bq/m3.',\n 'Parse time column from TEPCO.',\n 'Encode time as seconds since epoch.',\n 'Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.',\n 'Convert from Bq/L to Bq/m3.']\n\n\n\ndfs_tfm['SEAWATER'].sample(10)\n\n\n\n\n\n\n\ntype\nSMP_ID\nLON\nLAT\nTIME\nSTATION\nNUCLIDE\nUNIT\nDL\nUNC\nVALUE\nDLV\nSMP_ID_PROVIDER\n\n\n\n\n64028\n54701\n141.046667\n37.430556\n1408352280\nT-0-1A\n1\n1\n2\nNaN\n1700.0\n1700.0\nNaN\n\n\n82446\n72534\n141.133333\n38.250000\n1522926600\nT-MG4\n31\n1\n2\nNaN\n1.5\n1.5\nNaN\n\n\n39583\n32834\n141.034444\n37.431111\n1438068000\nT-1\n31\n1\n2\nNaN\n790.0\n790.0\nNaN\n\n\n32941\n26702\n141.033611\n37.415833\n1719583500\nT-2\n31\n1\n2\nNaN\n640.0\n640.0\nNaN\n\n\n31333\n25094\n141.033611\n37.415833\n1680244980\nT-2\n33\n1\n2\nNaN\n740.0\n740.0\nNaN\n\n\n87087\n77175\n141.216667\n37.533333\n1659696300\nT-B1\n33\n1\n1\nNaN\n1.4\nNaN\nNaN\n\n\n83010\n73098\n141.148611\n37.348333\n1647327180\nT-B4\n31\n1\n2\nNaN\n1.4\n1.4\nNaN\n\n\n65263\n55757\n141.046667\n37.430556\n1594621980\nT-0-1A\n33\n1\n2\nNaN\n750.0\n750.0\nNaN\n\n\n87045\n77133\n141.216667\n37.533333\n1614322020\nT-B1\n31\n1\n2\nNaN\n1.2\n1.2\nNaN\n\n\n13558\n7578\n141.006944\n37.055556\n1380524760\nT-17-1\n33\n1\n1\nNaN\n8.7\nNaN\nNaN\n\n\n\n\n\n\n\n\nsource\n\nget_attrs\n\n get_attrs (tfm, zotero_key, kw=['oceanography', 'Earth Science &gt; Oceans &gt;\n            Ocean Chemistry&gt; Radionuclides', 'Earth Science &gt; Human\n            Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation\n            Exposure', 'Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean\n            Tracers, Earth Science &gt; Oceans &gt; Marine Sediments', 'Earth\n            Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt;\n            Sea Ice &gt; Isotopes', 'Earth Science &gt; Oceans &gt; Water Quality &gt;\n            Ocean Contaminants', 'Earth Science &gt; Biological\n            Classification &gt; Animals/Vertebrates &gt; Fish', 'Earth Science &gt;\n            Biosphere &gt; Ecosystems &gt; Marine Ecosystems', 'Earth Science &gt;\n            Biological Classification &gt; Animals/Invertebrates &gt; Mollusks',\n            'Earth Science &gt; Biological Classification &gt;\n            Animals/Invertebrates &gt; Arthropods &gt; Crustaceans', 'Earth\n            Science &gt; Biological Classification &gt; Plants &gt; Macroalgae\n            (Seaweeds)'])\n\nRetrieve global attributes from MARIS dump.\n\n\nExported source\nkw = ['oceanography', 'Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides',\n      'Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure',\n      'Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments',\n      'Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes',\n      'Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants',\n      'Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish',\n      'Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems',\n      'Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks',\n      'Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans',\n      'Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)']\n\n\n\n\nExported source\ndef get_attrs(tfm, zotero_key, kw=kw):\n    \"Retrieve global attributes from MARIS dump.\"\n    return GlobAttrsFeeder(tfm.dfs, cbs=[\n        BboxCB(),\n        TimeRangeCB(),\n        ZoteroCB(zotero_key, cfg=cfg()),\n        KeyValuePairCB('keywords', ', '.join(kw)),\n        KeyValuePairCB('publisher_postprocess_logs', ', '.join(tfm.logs))\n        ])()\n\n\n\nget_attrs(tfm, zotero_key='JEV6HP5A', kw=kw)\n\n{'geospatial_lat_min': '141.66666667',\n 'geospatial_lat_max': '38.63333333',\n 'geospatial_lon_min': '140.60388889',\n 'geospatial_lon_max': '35.79611111',\n 'geospatial_bounds': 'POLYGON ((140.60388889 35.79611111, 141.66666667 35.79611111, 141.66666667 38.63333333, 140.60388889 38.63333333, 140.60388889 35.79611111))',\n 'time_coverage_start': '2011-03-21T14:30:00',\n 'time_coverage_end': '2025-07-22T06:17:00',\n 'id': 'JEV6HP5A',\n 'title': \"Readings of Sea Area Monitoring - Monitoring of sea water - Sea area close to TEPCO's Fukushima Daiichi NPS / Coastal area - Readings of Sea Area Monitoring [TEPCO]\",\n 'summary': '',\n 'creator_name': '[{\"creatorType\": \"author\", \"firstName\": \"\", \"lastName\": \"TEPCO - Tokyo Electric Power Company\"}]',\n 'keywords': 'oceanography, Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides, Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure, Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments, Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes, Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants, Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish, Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans, Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)',\n 'publisher_postprocess_logs': \"Remove 約 (about) char, Replace range values (e.g '4.0E+00&lt;&&lt;8.0E+00' or '1.0～2.7') by their mean, Select columns of interest., \\n    Get TEPCO nuclide names as values not column names \\n    to extract contained information (nuclide name, unc, dl, ...).\\n    , Extract nuclide name from TEPCO data., Extract unit from TEPCO data., Extract value type from TEPCO data., Reshape: long to wide, \\n    Remap `UNIT` name to MARIS id.\\n    , Remap `NUCLIDE` name to MARIS id., Remap `DL`, `DLV`, `VALUE` based on TEPCO -&gt; MARIS rules., Convert from Bq/L to Bq/m3., Parse time column from TEPCO., Encode time as seconds since epoch., Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator., Convert from Bq/L to Bq/m3.\"}\n\n\n\nsource\n\n\nencode\n\n encode (fname_out:str, **kwargs)\n\nEncode TEPCO data to NetCDF.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfname_out\nstr\nPath to the folder where the NetCDF output will be saved\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\n\n\nExported source\ndef encode(\n    fname_out: str, # Path to the folder where the NetCDF output will be saved\n    **kwargs # Additional keyword arguments\n    ):\n    \"Encode TEPCO data to NetCDF.\"\n    dfs = load_data(fname_coastal_water, fname_clos1F, fname_iaea_orbs)\n    \n    tfm = Transformer(dfs, cbs=[\n        RemoveJapanaseCharCB(),\n        FixRangeValueStringCB(),\n        SelectColsOfInterestCB(common_coi, nuclides_pattern),\n        WideToLongCB(),\n        ExtractNuclideNameCB(),\n        ExtractUnitCB(),\n        ExtractValueTypeCB(),\n        LongToWideCB(),\n        RemapUnitNameCB(unit_mapping),\n        RemapNuclideNameCB(nuclide_mapping),\n        RemapVALUE_DL_DLV_CB(),\n        ConvertToBqM3CB(),\n        ParseTimeCB(),\n        EncodeTimeCB(),\n        SanitizeLonLatCB(),\n        AddSampleIdCB()\n    ])        \n    tfm()\n    encoder = NetCDFEncoder(tfm.dfs, \n                            dest_fname=fname_out, \n                            global_attrs=get_attrs(tfm, zotero_key='JEV6HP5A', kw=kw),\n                            verbose=kwargs.get('verbose', False)\n                            )\n    encoder.encode()\n\n\n\nencode(fname_out, verbose=False)\n\n100%|██████████| 11/11 [00:05&lt;00:00,  2.17it/s]\n100%|██████████| 11/11 [00:05&lt;00:00,  2.14it/s]\n\n\nWarning: 4831 missing time value(s) in SEAWATER\n\n\n\ndecode(fname_in=fname_out, verbose=True)\n\nSaved SEAWATER to ../../_data/output/tepco_SEAWATER.csv\n\n\n\ndf_output = pd.read_csv(\"../../_data/output/tepco_SEAWATER.csv\")\ndf_output.head()\n\n\n\n\n\n\n\n\nsamplabcode\nlongitude\nlatitude\nbegperiod\nstation\nnuclide_id\nactivity\nunit_id\nuncertaint\ndetection\ndetection_lim\nsamptype_id\nref_id\n\n\n\n\n0\nNaN\n140.60388\n36.29972\n2011-10-13 13:21:00\nT-C\n29\n4000.0\n1\nNaN\n&lt;\n4000.0\n1\n679\n\n\n1\nNaN\n140.60388\n36.29972\n2011-10-13 13:21:00\nT-C\n31\n6000.0\n1\nNaN\n&lt;\n6000.0\n1\n679\n\n\n2\nNaN\n140.60388\n36.29972\n2011-10-13 13:21:00\nT-C\n33\n9000.0\n1\nNaN\n&lt;\n9000.0\n1\n679\n\n\n3\nNaN\n140.60388\n36.29972\n2011-10-13 13:23:00\nT-C\n29\n4000.0\n1\nNaN\n&lt;\n4000.0\n1\n679\n\n\n4\nNaN\n140.60388\n36.29972\n2011-10-13 13:23:00\nT-C\n31\n6000.0\n1\nNaN\n&lt;\n6000.0\n1\n679",
    "crumbs": [
      "Handlers",
      "TEPCO"
    ]
  },
  {
    "objectID": "handlers/ospar.html",
    "href": "handlers/ospar.html",
    "title": "OSPAR",
    "section": "",
    "text": "This data pipeline, known as a “handler” in Marisco terminology, is designed to clean, standardize, and encode OSPAR data into NetCDF format. The handler processes raw OSPAR data, applying various transformations and lookups to align it with MARIS data standards.\nKey functions of this handler:\nThis handler is a crucial component in the Marisco data processing workflow, ensuring OSPAR data is properly integrated into the MARIS database.\nThe present notebook pretends to be an instance of Literate Programming in the sense that it is a narrative that includes code snippets that are interspersed with explanations. When a function or a class needs to be exported in a dedicated python module (in our case marisco/handlers/ospar.py) the code snippet is added to the module using #| export as provided by the wonderful nbdev library.",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#configuration-and-file-paths",
    "href": "handlers/ospar.html#configuration-and-file-paths",
    "title": "OSPAR",
    "section": "Configuration and File Paths",
    "text": "Configuration and File Paths\nThe handler requires several configuration parameters:\n\nsrc_dir: path to the maris-crawlers folder containing the OSPAR data in CSV format\nfname_out: Output path and filename for NetCDF file (relative paths supported)\nzotero_key: Key for retrieving dataset attributes from Zotero\n\n\n\nExported source\nsrc_dir = 'https://raw.githubusercontent.com/franckalbinet/maris-crawlers/refs/heads/main/data/processed/OSPAR'\nfname_out = '../../_data/output/191-OSPAR-2024.nc'\nzotero_key ='LQRA4MMK' # OSPAR MORS zotero key",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#load-data",
    "href": "handlers/ospar.html#load-data",
    "title": "OSPAR",
    "section": "Load data",
    "text": "Load data\nOSPAR data is provided as a zipped Microsoft Access database. To facilitate easier access and integration, we process this dataset and convert it into .csv files. These processed files are then made available in the maris-crawlers repository on GitHub. Once converted, the dataset is in a format that is readily compatible with the marisco data pipeline, ensuring seamless data handling and analysis.\n\nsource\n\nread_csv\n\n read_csv (file_name,\n           dir='https://raw.githubusercontent.com/franckalbinet/maris-\n           crawlers/refs/heads/main/data/processed/OSPAR')\n\n\n\nExported source\ndefault_smp_types = {  \n    'Biota': 'BIOTA', \n    'Seawater': 'SEAWATER', \n}\n\n\n\n\nExported source\ndef read_csv(file_name, dir=src_dir):\n    file_path = f'{dir}/{file_name}'\n    return pd.read_csv(file_path)\n\n\n\nsource\n\n\nload_data\n\n load_data (src_url:str, smp_types:dict={'Biota': 'BIOTA', 'Seawater':\n            'SEAWATER'}, use_cache:bool=False, save_to_cache:bool=False,\n            verbose:bool=False)\n\nLoad OSPAR data and return the data in a dictionary of dataframes with the dictionary key as the sample type.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_url\nstr\n\n\n\n\nsmp_types\ndict\n{‘Biota’: ‘BIOTA’, ‘Seawater’: ‘SEAWATER’}\nSample types to load\n\n\nuse_cache\nbool\nFalse\nUse cache\n\n\nsave_to_cache\nbool\nFalse\nSave to cache\n\n\nverbose\nbool\nFalse\nVerbose\n\n\nReturns\nDict\n\n\n\n\n\n\n\nExported source\ndef load_data(src_url: str, \n              smp_types: dict = default_smp_types, # Sample types to load\n              use_cache: bool = False, # Use cache\n              save_to_cache: bool = False, # Save to cache\n              verbose: bool = False # Verbose\n              ) -&gt; Dict[str, pd.DataFrame]:\n    \"Load OSPAR data and return the data in a dictionary of dataframes with the dictionary key as the sample type.\"\n    \n    def safe_file_path(url: str) -&gt; str:\n        \"Safely encode spaces in a URL.\"\n        return url.replace(\" \", \"%20\")\n\n    def get_file_path(dir_path: str, file_prefix: str) -&gt; str:\n        \"\"\"Construct the full file path based on directory and file prefix.\"\"\"\n        file_path = f\"{dir_path}/{file_prefix} data.csv\"\n        return safe_file_path(file_path) if not use_cache else file_path\n\n    def load_and_process_csv(file_path: str) -&gt; pd.DataFrame:\n        \"\"\"Load a CSV file and process it.\"\"\"\n        if use_cache and not Path(file_path).exists():\n            if verbose:\n                print(f\"{file_path} not found in cache.\")\n            return pd.DataFrame()\n\n        if verbose:\n            start_time = time.time()\n\n        try:\n            df = pd.read_csv(file_path)\n            df.columns = df.columns.str.lower()\n            if verbose:\n                print(f\"Data loaded from {file_path} in {time.time() - start_time:.2f} seconds.\")\n            return df\n        except Exception as e:\n            if verbose:\n                print(f\"Failed to load {file_path}: {e}\")\n            return pd.DataFrame()\n\n    def save_to_cache_dir(df: pd.DataFrame, file_prefix: str):\n        \"\"\"Save the DataFrame to the cache directory.\"\"\"\n        cache_dir = cache_path()\n        cache_file_path = f\"{cache_dir}/{file_prefix} data.csv\"\n        df.to_csv(cache_file_path, index=False)\n        if verbose:\n            print(f\"Data saved to cache at {cache_file_path}\")\n\n    data = {}\n    for file_prefix, smp_type in smp_types.items():\n        dir_path = cache_path() if use_cache else src_url\n        file_path = get_file_path(dir_path, file_prefix)\n        df = load_and_process_csv(file_path)\n\n        if save_to_cache and not df.empty:\n            save_to_cache_dir(df, file_prefix)\n\n        data[smp_type] = df\n\n    return data\n\n\n\ndfs = load_data(src_dir, save_to_cache=True, verbose=False)\n\n\ndfs['SEAWATER'].columns\n\nIndex(['id', 'contracting party', 'rsc sub-division', 'station id',\n       'sample id', 'latd', 'latm', 'lats', 'latdir', 'longd', 'longm',\n       'longs', 'longdir', 'sample type', 'sampling depth', 'sampling date',\n       'nuclide', 'value type', 'activity or mda', 'uncertainty', 'unit',\n       'data provider', 'measurement comment', 'sample comment',\n       'reference comment'],\n      dtype='object')",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#remove-missing-values",
    "href": "handlers/ospar.html#remove-missing-values",
    "title": "OSPAR",
    "section": "Remove Missing Values",
    "text": "Remove Missing Values\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nWe consider records are incomplete if either the activity or mda field or the sampling date field is empty. These are the two key criteria we use to identify missing data.\nAs shown below: 10 rows are missing the sampling date and 10 rows are missing the activity or mda field.\n\n\n\nprint(f'Missing sampling date: {dfs[\"SEAWATER\"][\"sampling date\"].isnull().sum()}')\nprint(f'Missing activity or mda: {dfs[\"SEAWATER\"][\"activity or mda\"].isnull().sum()}')\ndfs['SEAWATER'][dfs['SEAWATER']['sampling date'].isnull()].sample(2)\n\nMissing sampling date: 10\n\n\n\nMissing activity or mda: 10\n\n\n\n\n\n\n\n\n\n\nid\ncontracting party\nrsc sub-division\nstation id\nsample id\nlatd\nlatm\nlats\nlatdir\nlongd\n...\nsampling date\nnuclide\nvalue type\nactivity or mda\nuncertainty\nunit\ndata provider\nmeasurement comment\nsample comment\nreference comment\n\n\n\n\n19191\n120367\nIreland\n4.0\nN9\nNaN\n53\n53.0\n0.0\nN\n5\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2021 data\nThe Irish Navy attempted a few times to collec...\nNaN\n\n\n16161\n120369\nIreland\n1.0\nSalthill\nNaN\n53\n15.0\n40.0\nN\n9\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n2021 data\nWoodstown (County Waterford) and Salthill (Cou...\nNaN\n\n\n\n\n2 rows × 25 columns\n\n\n\nTo quickly remove all missing values, we can use the RemoveAllNAValuesCB callback.\n\n\nExported source\nnan_cols_to_check = ['sampling date', 'activity or mda']\n\n\n\ndfs = load_data(src_dir)\ntfm = Transformer(dfs, cbs = [\n    RemoveAllNAValuesCB(nan_cols_to_check)])\n\ndfs_out = tfm()\n\nNow we can see that the sampling date and activity or mda columns have no missing values.\n\nlen(dfs_out['SEAWATER'][dfs['SEAWATER']['sampling date'].isnull()])\n\n0",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#nuclide-name-normalization",
    "href": "handlers/ospar.html#nuclide-name-normalization",
    "title": "OSPAR",
    "section": "Nuclide Name Normalization",
    "text": "Nuclide Name Normalization\nWe must standardize the nuclide names in the OSPAR dataset to align with the standardized names provided in the MARISCO lookup table. The lookup process utilizes three key columns: - nuclide_id: This serves as a unique identifier for each nuclide - nuclide: Represents the standardized name of the nuclide as per our conventions - nc_name: Denotes the corresponding name used in NetCDF files\nBelow, we will examine the structure and contents of the lookup table:\n\nnuc_lut_df = pd.read_excel(nuc_lut_path())\nnuc_lut_df.sample(5)\n\n\n\n\n\n\n\n\nnuclide_id\nnuclide\natomicnb\nmassnb\nnusymbol\nhalf_life\nhl_unit\nnc_name\n\n\n\n\n119\n128\nYTRIUM\n39.0\n88.0\n88Y\n0.00\n-\ny88\n\n\n40\n42\nLEAD\n82.0\n212.0\n212Pb\n10.64\nH\npb212\n\n\n34\n36\nCERIUM\n58.0\n141.0\n141Ce\n32.55\nD\nce141\n\n\n77\n80\nCURIUM COMB\n96.0\n243.0\n243,244Cm\n0.00\n-\ncm243_244_tot\n\n\n50\n52\nRADIUM\n88.0\n225.0\n225Ra\n14.80\nD\nra225\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nIn OSPAR dataset, the nuclide column has inconsistent naming:\n\nCs-137, 137Cs or CS-137\n239, 240 pu or 239,240 pu\nra-226 and 226ra\nduplicates due to the presence of trailing spaces\n\nSee below:\n\n\n\nprint(get_unique_across_dfs(dfs, 'nuclide', as_df=False))\n\n[\n    'Cs-137',\n    '210Pb',\n    '137Cs  ',\n    '238Pu',\n    nan,\n    '226Ra',\n    '99Tc  ',\n    'CS-137',\n    '239,240Pu',\n    '239, 240 Pu',\n    '241Am',\n    '210Po',\n    '210Po  ',\n    '228Ra',\n    '99Tc',\n    '99Tc   ',\n    '137Cs',\n    '3H'\n]\n\n\n\nRegardless of these inconsistencies, OSPAR’s nuclide column needs to be standardized accorsing to MARIS nomenclature.\n\nLower & strip nuclide names\nTo streamline the process of standardizing nuclide data, we employ the LowerStripNameCB callback. This function is applied to each DataFrame within our dictionary of DataFrames. Specifically, LowerStripNameCB simplifies the nuclide names by converting them to lowercase and removing any leading or trailing whitespace.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='nuclide', col_dst='nuclide')])\ndfs_output=tfm()\nfor key, df in dfs_output.items(): print(f'{key} nuclides: {df[\"nuclide\"].unique()}')\n\nBIOTA nuclides: ['137cs' '226ra' '228ra' '239,240pu' '99tc' '210po' '210pb' '3h' 'cs-137'\n '238pu' '239, 240 pu' '241am']\n\n\n\nSEAWATER nuclides: ['137cs' '239,240pu' '226ra' '228ra' '99tc' '3h' '210po' '210pb' nan]\n\n\n\n\n\nRemap nuclide names to MARIS data formats\nNext, we map nuclide names used by OSPAR to the MARIS standard nuclide names.\n\n\n\n\n\n\nNoteThe “IMFA” MARISCO PATTERN\n\n\n\nRemapping data provider nomenclatures to MARIS standards is a recurrent operation and is done in a semi-automated manner according to the following pattern:\n\nInspect data provider nomenclature\nMatch automatically against MARIS nomenclature (using a fuzzy matching algorithm)\nFix potential mismatches\nApply the lookup table to the DataFrame\n\nWe will refer to this process as IMFA (Inspect, Match, Fix, Apply).\n\n\nLet’s now create an instance of a fuzzy matching algorithm Remapper. This instance will align the nuclide names from the OSPAR dataset with the MARIS standard nuclide names, as defined in the lookup table located at nuc_lut_path and previously shown as nuc_lut_df.\n\nremapper = Remapper(\n    provider_lut_df=get_unique_across_dfs(dfs_output, col_name='nuclide', as_df=True), \n    maris_lut_fn=nuc_lut_path,\n    maris_col_id='nuclide_id',\n    maris_col_name='nc_name',\n    provider_col_to_match='value',\n    provider_col_key='value',\n    fname_cache='nuclides_ospar.pkl'\n    )\n\nLet’s clarify the meaning of the Remapper parameters:\n\nprovider_lut_df: It is the nomenclature/lookup table used by the data provider for a certain attribute/variable. When the data provider does not provide such nomenclature, get_unique_across_dfs is used to extract the unique values from data provider data.\nmaris_lut_fn: The path to the lookup table containing the MARIS standard nuclide names\nmaris_col_id: The column name in the lookup table containing the MARIS standard nuclide names\nmaris_col_name: The column name in the lookup table containing the MARIS standard nuclide names\nprovider_col_to_match: The column name in the OSPAR dataset containing the nuclide names used for the remapping\nprovider_col_key: The column name in the OSPAR dataset containing the nuclide names to remap from\nfname_cache: The filename for the cache file\n\nBoth provider_col_to_match and provider_col_key are the same column name in the OSPAR dataset. In other cases, data providers provide an associated nomenclature such as below for instance (see HELCOM handler for instance).\ndata-provider-nuclide-lut DataFrame:\n\n\n\nnuclide_id\nnuclide\n\n\n\n\n0\nCs-137\n\n\n1\nCs-134\n\n\n2\nI-131\n\n\n\nand uses the nuclide_id value in the data themselves. In such a case: - provider_lut_df: data-provider-nuclide-lut - provider_col_to_match would be nuclide - provider_col_key would be nuclide_id\nNow, we can automatically match the OSPAR nuclide names to the MARIS standard. The match_score column helps us evaluate the results.\n\n\n\n\n\n\nNotePay Attention\n\n\n\nNote that data provider’s name to macth is always transformed to lowercase and stripped of any leading or trailing whitespace to streamline the matching process as mentionned above.\n\n\n\nremapper.generate_lookup_table(as_df=True)\nremapper.select_match(match_score_threshold=0, verbose=True)\n\nProcessing:   0%|          | 0/13 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 13/13 [00:00&lt;00:00, 57.47it/s]\n\n\n0 entries matched the criteria, while 13 entries had a match score of 0 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\n239, 240 pu\npu240\n239, 240 pu\n8\n\n\n239,240pu\npu240\n239,240pu\n6\n\n\n137cs\ni133\n137cs\n4\n\n\n241am\npu241\n241am\n4\n\n\n210po\nru106\n210po\n4\n\n\n226ra\nu234\n226ra\n4\n\n\n210pb\nru106\n210pb\n4\n\n\n228ra\nu235\n228ra\n4\n\n\n99tc\ntu\n99tc\n3\n\n\n238pu\nu238\n238pu\n3\n\n\n3h\ntu\n3h\n2\n\n\ncs-137\ncs137\ncs-137\n1\n\n\nNaN\nUnknown\nNaN\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteFuzzy Matching\n\n\n\nTo try matching/reconciling two nomenclatures, we compute the Levenshtein distance between the OSPAR nuclide names and the MARIS standard nuclide names as indicated in the match_score column. A score of 0 indicates a perfect match.\n\n\nWe now manually review the unmatched nuclide names and construct a dictionary to map them to the MARIS standard.\n\n\nExported source\nfixes_nuclide_names = {\n    '99tc': 'tc99',\n    '238pu': 'pu238',\n    '226ra': 'ra226',\n    'ra-226': 'ra226',\n    'ra-228': 'ra228',    \n    '210pb': 'pb210',\n    '241am': 'am241',\n    '228ra': 'ra228',\n    '137cs': 'cs137',\n    '210po': 'po210',\n    '239,240pu': 'pu239_240_tot',\n    '239, 240 pu': 'pu239_240_tot',\n    '3h': 'h3'\n    }\n\n\nThe dictionary fixes_nuclide_names applies manual corrections to the nuclide names before the remapping process begins. Note that we did not remap cs-137 to cs137 as the fuzzy matching algorithm already matched cs-137 to cs137 (though the match score was 1).\nThe generate_lookup_table function constructs a lookup table for this purpose and includes an overwrite parameter, set to True by default. When activated, this parameter enables the function to update the existing cache with a new pickle file containing the updated lookup table. We are now prepared to test the remapping process.\n\nremapper.generate_lookup_table(as_df=True, fixes=fixes_nuclide_names)\n\nProcessing:   0%|          | 0/13 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 13/13 [00:00&lt;00:00, 52.43it/s]\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\ncs-137\ncs137\ncs-137\n1\n\n\n137cs\ncs137\n137cs\n0\n\n\n241am\nam241\n241am\n0\n\n\n210po\npo210\n210po\n0\n\n\n226ra\nra226\n226ra\n0\n\n\n99tc\ntc99\n99tc\n0\n\n\n239,240pu\npu239_240_tot\n239,240pu\n0\n\n\n238pu\npu238\n238pu\n0\n\n\n3h\nh3\n3h\n0\n\n\n239, 240 pu\npu239_240_tot\n239, 240 pu\n0\n\n\n210pb\npb210\n210pb\n0\n\n\n228ra\nra228\n228ra\n0\n\n\nNaN\nUnknown\nNaN\n0\n\n\n\n\n\n\n\nTo view all remapped nuclides as a lookup table that will be later passed to our RemapNuclideNameCB callback:\n\nremapper.generate_lookup_table(as_df=False, fixes=fixes_nuclide_names, overwrite=True)\n\nProcessing:   0%|          | 0/13 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 13/13 [00:00&lt;00:00, 56.68it/s]\n\n\n{'137cs': Match(matched_id=np.int64(33), matched_maris_name='cs137', source_name='137cs', match_score=np.int64(0)),\n '241am': Match(matched_id=np.int64(72), matched_maris_name='am241', source_name='241am', match_score=np.int64(0)),\n '210po': Match(matched_id=np.int64(47), matched_maris_name='po210', source_name='210po', match_score=np.int64(0)),\n '226ra': Match(matched_id=np.int64(53), matched_maris_name='ra226', source_name='226ra', match_score=np.int64(0)),\n '99tc': Match(matched_id=np.int64(15), matched_maris_name='tc99', source_name='99tc', match_score=np.int64(0)),\n '239,240pu': Match(matched_id=np.int64(77), matched_maris_name='pu239_240_tot', source_name='239,240pu', match_score=np.int64(0)),\n '238pu': Match(matched_id=np.int64(67), matched_maris_name='pu238', source_name='238pu', match_score=np.int64(0)),\n '3h': Match(matched_id=np.int64(1), matched_maris_name='h3', source_name='3h', match_score=np.int64(0)),\n 'cs-137': Match(matched_id=np.int64(33), matched_maris_name='cs137', source_name='cs-137', match_score=np.int64(1)),\n '239, 240 pu': Match(matched_id=np.int64(77), matched_maris_name='pu239_240_tot', source_name='239, 240 pu', match_score=np.int64(0)),\n '210pb': Match(matched_id=np.int64(41), matched_maris_name='pb210', source_name='210pb', match_score=np.int64(0)),\n '228ra': Match(matched_id=np.int64(54), matched_maris_name='ra228', source_name='228ra', match_score=np.int64(0)),\n nan: Match(matched_id=-1, matched_maris_name='Unknown', source_name=nan, match_score=0)}\n\n\nThe nuclide names have been successfully remapped. We now create a callback named RemapNuclideNameCB to translate the OSPAR dataset’s nuclide names into the standard nuclide_ids used by MARIS. This callback employs the lut_nuclides lambda function, which provides the required lookup table. Note that the overwrite=False parameter is specified in the Remapper constructor of the lut_nuclides lambda function to utilize the cached version.\n\nsource\n\n\nRemapNuclideNameCB\n\n RemapNuclideNameCB (fn_lut:Callable, col_name:str)\n\nRemap data provider nuclide names to standardized MARIS nuclide names.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfn_lut\nCallable\nFunction that returns the lookup table dictionary\n\n\ncol_name\nstr\nColumn name to remap\n\n\n\n\n\nExported source\n# Create a lookup table for nuclide names\nlut_nuclides = lambda df: Remapper(provider_lut_df=df,\n                                   maris_lut_fn=nuc_lut_path,\n                                   maris_col_id='nuclide_id',\n                                   maris_col_name='nc_name',\n                                   provider_col_to_match='value',\n                                   provider_col_key='value',\n                                   fname_cache='nuclides_ospar.pkl').generate_lookup_table(fixes=fixes_nuclide_names, \n                                                                                            as_df=False, overwrite=True)\n\n\n\n\nExported source\nclass RemapNuclideNameCB(Callback):\n    \"Remap data provider nuclide names to standardized MARIS nuclide names.\"\n    def __init__(self, \n                 fn_lut: Callable, # Function that returns the lookup table dictionary\n                 col_name: str # Column name to remap\n                ):\n        fc.store_attr()\n\n    def __call__(self, tfm: Transformer):\n        df_uniques = get_unique_across_dfs(tfm.dfs, col_name=self.col_name, as_df=True)\n        lut = {k: v.matched_id for k, v in self.fn_lut(df_uniques).items()}    \n        for k in tfm.dfs.keys():\n            tfm.dfs[k]['NUCLIDE'] = tfm.dfs[k][self.col_name].replace(lut)\n\n\nLet’s see it in action, along with the LowerStripNameCB callback:\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    LowerStripNameCB(col_src='nuclide', col_dst='nuclide'),\n    RemapNuclideNameCB(lut_nuclides, col_name='nuclide')\n    ])\ndfs_out = tfm()\n\n# For instance\nfor key in dfs_out.keys():\n    print(f'Unique nuclide_ids for {key} NUCLIDE column: ', dfs_out[key]['NUCLIDE'].unique())\n\nProcessing: 100%|██████████| 12/12 [00:00&lt;00:00, 46.32it/s]\n\n\nUnique nuclide_ids for BIOTA NUCLIDE column:  [33 53 54 77 15 47 41  1 67 72]\n\n\n\nUnique nuclide_ids for SEAWATER NUCLIDE column:  [33 77 53 54 15  1 47 41]",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#standardize-time",
    "href": "handlers/ospar.html#standardize-time",
    "title": "OSPAR",
    "section": "Standardize Time",
    "text": "Standardize Time\nWe create a callback that remaps the date time format in the dictionary of DataFrames (i.e. %m/%d/%y %H:%M:%S) to a data time object and in the process handle missing date and times.\n\nsource\n\nParseTimeCB\n\n ParseTimeCB (col_src:dict={'BIOTA': 'sampling date', 'SEAWATER':\n              'sampling date'}, col_dst:str='TIME', format:str='%m/%d/%y\n              %H:%M:%S')\n\nParse the time format in the dataframe and check for inconsistencies.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncol_src\ndict\n{‘BIOTA’: ‘sampling date’, ‘SEAWATER’: ‘sampling date’}\nColumn name to remap\n\n\ncol_dst\nstr\nTIME\nColumn name to remap\n\n\nformat\nstr\n%m/%d/%y %H:%M:%S\nTime format\n\n\n\n\n\nExported source\ntime_cols = {'BIOTA': 'sampling date', 'SEAWATER': 'sampling date'}\ntime_format = '%m/%d/%y %H:%M:%S'\n\n\n\n\nExported source\nclass ParseTimeCB(Callback):\n    \"Parse the time format in the dataframe and check for inconsistencies.\"\n    def __init__(self, \n                 col_src: dict=time_cols, # Column name to remap\n                 col_dst: str='TIME', # Column name to remap\n                 format: str=time_format # Time format\n                 ):\n        fc.store_attr()\n    \n    def __call__(self, tfm):\n        for grp, df in tfm.dfs.items():\n            src_col = self.col_src.get(grp)\n            df[self.col_dst] = pd.to_datetime(df[src_col], format=self.format, errors='coerce')\n        return tfm\n\n\nApply the transformer for callback ParseTimeCB.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    ParseTimeCB(),\n    CompareDfsAndTfmCB(dfs)])\ntfm()\n\n\ndisplay(Markdown(\"&lt;b&gt; Row Count Comparison Before and After Transformation:&lt;/b&gt;\"))\nwith pd.option_context('display.max_rows', None):\n    display(pd.DataFrame.from_dict(tfm.compare_stats))\n\ndisplay(Markdown(\"&lt;b&gt; Example of parsed time column:&lt;/b&gt;\"))\nwith pd.option_context('display.max_rows', None):\n    display(tfm.dfs['SEAWATER']['TIME'].head(2))\n\n Row Count Comparison Before and After Transformation:\n\n\n\n\n\n\n\n\n\nBIOTA\nSEAWATER\n\n\n\n\nOriginal row count (dfs)\n15951\n19193\n\n\nTransformed row count (tfm.dfs)\n15951\n19183\n\n\nRows removed from original (tfm.dfs_removed)\n0\n10\n\n\nRows created in transformed (tfm.dfs_created)\n0\n0\n\n\n\n\n\n\n\n Example of parsed time column:\n\n\n0   2010-01-27\n1   2010-01-27\nName: TIME, dtype: datetime64[ns]\n\n\nThe NetCDF time format requires the time to be encoded as number of milliseconds since a time of origin. In our case the time of origin is 1970-01-01 as indicated in configs.ipynb CONFIFS['units']['time'] dictionary.\nEncodeTimeCB transforms the datetime object from ParseTimeCB into the MARIS NetCDF time format.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    CompareDfsAndTfmCB(dfs)\n    ])\n\ntfm()\n\ndisplay(Markdown(\"&lt;b&gt; Row Count Comparison Before and After Transformation:&lt;/b&gt;\"))\nwith pd.option_context('display.max_rows', None):\n    display(pd.DataFrame.from_dict(tfm.compare_stats))\n\n Row Count Comparison Before and After Transformation:\n\n\n\n\n\n\n\n\n\nBIOTA\nSEAWATER\n\n\n\n\nOriginal row count (dfs)\n15951\n19193\n\n\nTransformed row count (tfm.dfs)\n15951\n19183\n\n\nRows removed from original (tfm.dfs_removed)\n0\n10\n\n\nRows created in transformed (tfm.dfs_created)\n0\n0",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#sanitize-value",
    "href": "handlers/ospar.html#sanitize-value",
    "title": "OSPAR",
    "section": "Sanitize value",
    "text": "Sanitize value\nWe create a callback, SanitizeValueCB, to consolidate measurement values into a single column named VALUE and remove any NaN entries.\n\nsource\n\nSanitizeValueCB\n\n SanitizeValueCB (value_col:dict={'BIOTA': 'activity or mda', 'SEAWATER':\n                  'activity or mda'})\n\nSanitize value by removing blank entries and populating value column.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvalue_col\ndict\n{‘BIOTA’: ‘activity or mda’, ‘SEAWATER’: ‘activity or mda’}\nColumn name to sanitize\n\n\n\n\n\nExported source\nvalue_cols = {'BIOTA': 'activity or mda', 'SEAWATER': 'activity or mda'}\n\n\n\n\nExported source\nclass SanitizeValueCB(Callback):\n    \"Sanitize value by removing blank entries and populating `value` column.\"\n    def __init__(self, \n                 value_col: dict = value_cols # Column name to sanitize\n                 ):\n        fc.store_attr()\n\n    def __call__(self, tfm):\n        for grp, df in tfm.dfs.items():\n            # Drop rows where parsing failed (NaT values in TIME column)\n            invalid_rows = df[df[self.value_col.get(grp)].isna()]\n            if not invalid_rows.empty:     \n                print(f\"{len(invalid_rows)} invalid rows found in group '{grp}' during sanitize value callback.\")\n                df.dropna(subset=[self.value_col.get(grp)], inplace=True)\n                \n            df['VALUE'] = df[self.value_col.get(grp)]\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    SanitizeValueCB(),\n    CompareDfsAndTfmCB(dfs)])\n\ntfm()\n\ndisplay(Markdown(\"&lt;b&gt; Example of VALUE column:&lt;/b&gt;\"))\nwith pd.option_context('display.max_rows', None):\n    display(tfm.dfs['SEAWATER'][['VALUE']].head())\n\ndisplay(Markdown(\"&lt;b&gt; Row Count Comparison Before and After Transformation:&lt;/b&gt;\"))\nwith pd.option_context('display.max_rows', None):\n    display(pd.DataFrame.from_dict(tfm.compare_stats))\n\ndisplay(Markdown(\"&lt;b&gt; Example of removed data:&lt;/b&gt;\"))\nwith pd.option_context('display.max_columns', None):\n    display(tfm.dfs_removed['SEAWATER'].head(2))\n\n Example of VALUE column:\n\n\n\n\n\n\n\n\n\nVALUE\n\n\n\n\n0\n0.20\n\n\n1\n0.27\n\n\n2\n0.26\n\n\n3\n0.25\n\n\n4\n0.20\n\n\n\n\n\n\n\n Row Count Comparison Before and After Transformation:\n\n\n\n\n\n\n\n\n\nBIOTA\nSEAWATER\n\n\n\n\nOriginal row count (dfs)\n15951\n19193\n\n\nTransformed row count (tfm.dfs)\n15951\n19183\n\n\nRows removed from original (tfm.dfs_removed)\n0\n10\n\n\nRows created in transformed (tfm.dfs_created)\n0\n0\n\n\n\n\n\n\n\n Example of removed data:\n\n\n\n\n\n\n\n\n\nid\ncontracting party\nrsc sub-division\nstation id\nsample id\nlatd\nlatm\nlats\nlatdir\nlongd\nlongm\nlongs\nlongdir\nsample type\nsampling depth\nsampling date\nnuclide\nvalue type\nactivity or mda\nuncertainty\nunit\ndata provider\nmeasurement comment\nsample comment\nreference comment\n\n\n\n\n14776\n97948\nSweden\n11.0\nSW7\n1\n58\n36.0\n12.0\nN\n11\n14.0\n42.0\nE\nWATER\n1.0\nNaN\n3H\nNaN\nNaN\nNaN\nBq/l\nSwedish Radiation Safety Authority\nno 3H this year due to broken LSC\nNaN\nNaN\n\n\n14780\n97952\nSweden\n12.0\nRinghals (R35)\n7\n57\n14.0\n5.0\nN\n11\n56.0\n8.0\nE\nWATER\n1.0\nNaN\n3H\nNaN\nNaN\nNaN\nBq/l\nSwedish Radiation Safety Authority\nno 3H this year due to broken LSC\nNaN\nNaN",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#normalize-uncertainty",
    "href": "handlers/ospar.html#normalize-uncertainty",
    "title": "OSPAR",
    "section": "Normalize uncertainty",
    "text": "Normalize uncertainty\nWe create a callback, NormalizeUncCB, to standardize the uncertainty value to the MARIS format. For each sample type in the OSPAR dataset, the reported uncertainty is given as an expanded uncertainty with a coverage factor 𝑘=2. For further details, refer to the OSPAR reporting guidelines. In MARIS the uncertainty values are reported as standard uncertainty with a coverage factor 𝑘=1.\nNormalizeUncCB callback normalizes the uncertainty using the following lambda function:\n\nsource\n\nNormalizeUncCB\n\n NormalizeUncCB (col_unc:dict={'BIOTA': 'uncertainty', 'SEAWATER':\n                 'uncertainty'}, fn_convert_unc:Callable=&lt;function\n                 &lt;lambda&gt;&gt;)\n\nNormalize uncertainty values in DataFrames.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncol_unc\ndict\n{‘BIOTA’: ‘uncertainty’, ‘SEAWATER’: ‘uncertainty’}\nColumn name to normalize\n\n\nfn_convert_unc\nCallable\n\nFunction correcting coverage factor\n\n\n\n\n\nExported source\nunc_exp2stan = lambda df, unc_col: df[unc_col] / 2\n\n\n\n\nExported source\nunc_cols = {'BIOTA': 'uncertainty', 'SEAWATER': 'uncertainty'}\n\n\n\n\nExported source\nclass NormalizeUncCB(Callback):\n    \"\"\"Normalize uncertainty values in DataFrames.\"\"\"\n    def __init__(self, \n                 col_unc: dict = unc_cols, # Column name to normalize\n                 fn_convert_unc: Callable=unc_exp2stan, # Function correcting coverage factor\n                 ): \n        fc.store_attr()\n\n    def __call__(self, tfm):\n        for grp, df in tfm.dfs.items():\n            self._convert_commas_to_periods(df, self.col_unc.get(grp)   )\n            self._convert_to_float(df, self.col_unc.get(grp))\n            self._apply_conversion_function(df, self.col_unc.get(grp))\n\n    def _convert_commas_to_periods(self, df, col_unc    ):\n        \"\"\"Convert commas to periods in the uncertainty column.\"\"\"\n        df[col_unc] = df[col_unc].astype(str).str.replace(',', '.')\n\n    def _convert_to_float(self, df, col_unc):\n        \"\"\"Convert uncertainty column to float, handling errors by setting them to NaN.\"\"\"\n        df[col_unc] = pd.to_numeric(df[col_unc], errors='coerce')\n\n    def _apply_conversion_function(self, df, col_unc):\n        \"\"\"Apply the conversion function to normalize the uncertainty values.\"\"\"\n        df['UNC'] = self.fn_convert_unc(df, col_unc)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    SanitizeValueCB(),               \n    NormalizeUncCB()\n    ])\n\ntfm()\n\ndisplay(Markdown(\"&lt;b&gt; Example of VALUE and UNC columns:&lt;/b&gt;\"))  \nfor grp in ['SEAWATER', 'BIOTA']:\n    print(f'\\n{grp}:')\n    print(tfm.dfs[grp][['VALUE', 'UNC']])\n\n Example of VALUE and UNC columns:\n\n\nSEAWATER:\n\n\n\n          VALUE           UNC\n0      0.200000           NaN\n1      0.270000           NaN\n2      0.260000           NaN\n3      0.250000           NaN\n4      0.200000           NaN\n...         ...           ...\n19183  0.000005  2.600000e-07\n19184  6.152000  3.076000e-01\n19185  0.005390  1.078000e-03\n19186  0.001420  2.840000e-04\n19187  6.078000  3.039000e-01\n\n[19183 rows x 2 columns]\n\n\n\nBIOTA:\n\n\n\n          VALUE       UNC\n0      0.326416       NaN\n1      0.442704       NaN\n2      0.412989       NaN\n3      0.202768       NaN\n4      0.652833       NaN\n...         ...       ...\n15946  0.384000  0.012096\n15947  0.456000  0.012084\n15948  0.122000  0.031000\n15949  0.310000       NaN\n15950  0.306000  0.007191\n\n[15951 rows x 2 columns]\n\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nThe SEAWATER dataset includes instances where the uncertainty values significantly exceed the corresponding measurement values. While such occurrences are not inherently erroneous, they merit attention and may warrant further verification.\n\n\nTo demonstrate instances where the uncertainty significantly surpasses the measurement values, we will initially compute the ‘relative uncertainty’ as a percentage for the seawater dataset.\n\ndfs = load_data(src_dir, use_cache=True)\nfor grp in ['SEAWATER', 'BIOTA']:\n    tfm.dfs[grp]['relative_uncertainty'] = (\n    # Divide 'uncertainty' by 'value'\n    (tfm.dfs[grp]['uncertainty'] / tfm.dfs[grp]['activity or mda'])\n    # Multiply by 100 to convert to percentage\n    * 100)\n\nNow we will retrieve all rows where the relative uncertainty exceeds 100% for the seawater dataset.\n\nthreshold = 100\ngrp = 'SEAWATER'\ncols_to_show = ['id', 'contracting party', 'nuclide', 'value type', 'activity or mda', 'uncertainty', 'unit', 'relative_uncertainty']\ndf = tfm.dfs[grp][cols_to_show][tfm.dfs[grp]['relative_uncertainty'] &gt; threshold]\n\nprint(f'Number of rows where relative uncertainty is greater than {threshold}%: \\n {df.shape[0]} \\n')\n\ndisplay(Markdown(f\"&lt;b&gt; Example of data with relative uncertainty greater than {threshold}%:&lt;/b&gt;\"))\nwith pd.option_context('display.max_rows', None):\n    display(df.head())\n\nNumber of rows where relative uncertainty is greater than 100%: \n 95 \n\n\n\n\n Example of data with relative uncertainty greater than 100%:\n\n\n\n\n\n\n\n\n\nid\ncontracting party\nnuclide\nvalue type\nactivity or mda\nuncertainty\nunit\nrelative_uncertainty\n\n\n\n\n969\n11075\nUnited Kingdom\n137Cs\n=\n0.0028\n0.3276\nBq/l\n11700.0\n\n\n971\n11077\nUnited Kingdom\n137Cs\n=\n0.0029\n0.3364\nBq/l\n11600.0\n\n\n973\n11079\nUnited Kingdom\n137Cs\n=\n0.0025\n0.3325\nBq/l\n13300.0\n\n\n975\n11081\nUnited Kingdom\n137Cs\n=\n0.0025\n0.3450\nBq/l\n13800.0\n\n\n977\n11083\nUnited Kingdom\n137Cs\n=\n0.0038\n0.3344\nBq/l\n8800.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nThe BIOTA dataset includes instances where the uncertainty values significantly exceed the corresponding measurement values. While such occurrences are not inherently erroneous, they merit attention and may warrant further verification.\n\n\nNow we will retrieve all rows where the relative uncertainty exceeds 100% for the biota dataset.\n\nthreshold = 100\ngrp = 'BIOTA' \ncols_to_show=['id', 'contracting party', 'nuclide', 'value type', 'activity or mda', 'uncertainty', 'unit', 'relative_uncertainty']\ndf=tfm.dfs[grp][cols_to_show][tfm.dfs[grp]['relative_uncertainty'] &gt; threshold]\n\nprint(f'Number of rows where relative uncertainty is greater than {threshold}%: \\n {df.shape[0]} \\n')\n\ndisplay(Markdown(f\"&lt;b&gt; Example of data with relative uncertainty greater than {threshold}%:&lt;/b&gt;\"))\nwith pd.option_context('display.max_rows', None):\n    display(df.head())\n\nNumber of rows where relative uncertainty is greater than 100%: \n 100 \n\n\n\n\n Example of data with relative uncertainty greater than 100%:\n\n\n\n\n\n\n\n\n\nid\ncontracting party\nnuclide\nvalue type\nactivity or mda\nuncertainty\nunit\nrelative_uncertainty\n\n\n\n\n249\n3101\nNorway\n137Cs\n=\n0.0500\n0.1000\nBq/kg f.w.\n200.000000\n\n\n306\n3158\nNorway\n137Cs\n=\n0.1500\n0.1600\nBq/kg f.w.\n106.666667\n\n\n775\n8152\nNorway\n137Cs\n=\n0.0340\n0.0500\nBq/kg f.w.\n147.058824\n\n\n788\n8165\nNorway\n137Cs\n=\n0.0300\n0.0500\nBq/kg f.w.\n166.666667\n\n\n1839\n19571\nBelgium\n239,240Pu\n=\n0.0074\n0.0093\nBq/kg f.w.\n125.675676",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#remap-units",
    "href": "handlers/ospar.html#remap-units",
    "title": "OSPAR",
    "section": "Remap units",
    "text": "Remap units\nLet’s inspect the unique units used by OSPAR:\n\nget_unique_across_dfs(dfs, col_name='unit', as_df=True)\n\n\n\n\n\n\n\n\nindex\nvalue\n\n\n\n\n0\n0\nBQ/L\n\n\n1\n1\nBq/L\n\n\n2\n2\nBq/l\n\n\n3\n3\nBq/kg f.w.\n\n\n4\n4\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nStandardizing the units would simplify data processing, as the units are not consistent across the dataset. For example, BQ/L, Bq/l, and Bq/L are used interchangeably.\n\n\nWe will establish unit renaming rules for the OSPAR dataset:\n\n\nExported source\n# Define unit names renaming rules\nrenaming_unit_rules = {'Bq/l': 1, #'Bq/m3'\n                       'Bq/L': 1,\n                       'BQ/L': 1,\n                       'Bq/kg f.w.': 5, # Bq/kgw\n                       }\n\n\nNow we will create a callback, RemapUnitCB, to remap the units in the dataframes. For the SEAWATER dataset, we will set a default unit of Bq/l.\n\nsource\n\nRemapUnitCB\n\n RemapUnitCB (lut:Dict[str,str], default_units:Dict[str,str]={'SEAWATER':\n              'Bq/l', 'BIOTA': 'Bq/kg f.w.'}, verbose:bool=False)\n\nCallback to update DataFrame ‘UNIT’ columns based on a lookup table.\n\n\nExported source\ndefault_units = {'SEAWATER': 'Bq/l',\n                 'BIOTA': 'Bq/kg f.w.'}\n\n\n\n\nExported source\nclass RemapUnitCB(Callback):\n    \"\"\"Callback to update DataFrame 'UNIT' columns based on a lookup table.\"\"\"\n\n    def __init__(self,\n                 lut: Dict[str, str],\n                 default_units: Dict[str, str] = default_units,\n                 verbose: bool = False\n                 ):\n        fc.store_attr()  # Store the lookup table as an attribute\n\n    def __call__(self, tfm: 'Transformer'):\n        for grp, df in tfm.dfs.items():\n            # Apply default units to SEAWATER dataset\n            if grp == 'SEAWATER':\n                self._apply_default_units(df, unit=self.default_units.get(grp))\n            # self._print_na_units(df)\n            self._update_units(df)\n\n    def _apply_default_units(self, df: pd.DataFrame , unit = None):\n        df.loc[df['unit'].isnull(), 'unit'] = unit\n\n    # def _print_na_units(self, df: pd.DataFrame):\n    #     na_count = df['unit'].isnull().sum()\n    #     if na_count &gt; 0 and self.verbose:\n    #         print(f\"Number of rows with NaN in 'unit' column: {na_count}\")\n\n    def _update_units(self, df: pd.DataFrame):\n        df['UNIT'] = df['unit'].apply(lambda x: self.lut.get(x, 'Unknown'))\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    SanitizeValueCB(), # Remove blank value entries (also removes NaN values in Unit column) \n    RemapUnitCB(renaming_unit_rules, verbose=True),\n    CompareDfsAndTfmCB(dfs)\n    ])\n\ntfm()\n\ndisplay(Markdown(\"&lt;b&gt; Row Count Comparison Before and After Transformation:&lt;/b&gt;\"))\nwith pd.option_context('display.max_rows', None):\n    display(pd.DataFrame.from_dict(tfm.compare_stats))\n\nprint('Unique Unit values:')\nfor grp in ['BIOTA', 'SEAWATER']:\n    print(f\"{grp}: {tfm.dfs[grp]['UNIT'].unique()}\")\n\n Row Count Comparison Before and After Transformation:\n\n\n\n\n\n\n\n\n\nBIOTA\nSEAWATER\n\n\n\n\nOriginal row count (dfs)\n15951\n19193\n\n\nTransformed row count (tfm.dfs)\n15951\n19183\n\n\nRows removed from original (tfm.dfs_removed)\n0\n10\n\n\nRows created in transformed (tfm.dfs_created)\n0\n0\n\n\n\n\n\n\n\nUnique Unit values:\n\n\n\nBIOTA: [5]\n\n\n\nSEAWATER: [1]",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#remap-detection-limit",
    "href": "handlers/ospar.html#remap-detection-limit",
    "title": "OSPAR",
    "section": "Remap detection limit",
    "text": "Remap detection limit\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nThe Value type column contains numerous nan entries.\n\n\n\n# Count the number of NaN entries in the 'value type' column for 'SEAWATER'\nna_count_seawater = dfs['SEAWATER']['value type'].isnull().sum()\nprint(f\"Number of NaN 'Value type' entries in 'SEAWATER': {na_count_seawater}\")\n\n# Count the number of NaN entries in the 'value type' column for 'BIOTA'\nna_count_biota = dfs['BIOTA']['value type'].isnull().sum()\nprint(f\"Number of NaN 'Value type' entries in 'BIOTA': {na_count_biota}\")\n\nNumber of NaN 'Value type' entries in 'SEAWATER': 64\n\n\n\nNumber of NaN 'Value type' entries in 'BIOTA': 23\n\n\n\nIn the OSPAR dataset, the detection limit is denoted by &lt; in the Value type column. When the Value type is &lt;, the Activity or MDAcolumn specifies the detection limit. Conversely, when the Value type is =, it indicates an actual measurement in theActivity or MDA column. Let’s review the entries in the Value type column for the OSPAR dataset:\n\nfor grp in dfs.keys():\n    print(f'{grp}:')\n    print(tfm.dfs[grp]['value type'].unique())\n\nBIOTA:\n\n\n\n['&lt;' '=' nan]\n\n\n\nSEAWATER:\n\n\n\n['&lt;' '=' nan]\n\n\n\nIn MARIS the Detection limits are encoded as follows:\n\npd.read_excel(detection_limit_lut_path())\n\n\n\n\n\n\n\n\nid\nname\nname_sanitized\n\n\n\n\n0\n-1\nNot applicable\nNot applicable\n\n\n1\n0\nNot Available\nNot available\n\n\n2\n1\n=\nDetected value\n\n\n3\n2\n&lt;\nDetection limit\n\n\n4\n3\nND\nNot detected\n\n\n5\n4\nDE\nDerived\n\n\n\n\n\n\n\nWe can create a lambda function to retrieve the MARIS lookup table.\n\n\nExported source\nlut_dl = lambda: pd.read_excel(detection_limit_lut_path(), usecols=['name','id']).set_index('name').to_dict()['id']\n\n\nWe can define the columns of interest in both the SEAWATER and BIOTA DataFrames for the detection limit column.\n\n\nExported source\ncoi_dl = {'SEAWATER' : {'DL' : 'value type'},\n          'BIOTA':  {'DL' : 'value type'}\n          }\n\n\nWe now create a callback RemapDetectionLimitCB to remap OSPAR detection limit values to MARIS formatted values using the lookup table. Since the dataset contains ‘nan’ entries for the detection limit column, we will create a condition to set the detection limit to ‘=’ when the value and uncertainty columns are present and the current detection limit value is not in the lookup keys.\n\nsource\n\nRemapDetectionLimitCB\n\n RemapDetectionLimitCB (coi:dict, fn_lut:Callable)\n\nRemap detection limit values to MARIS format using a lookup table.\n\n\nExported source\nclass RemapDetectionLimitCB(Callback):\n    \"\"\"Remap detection limit values to MARIS format using a lookup table.\"\"\"\n\n    def __init__(self, coi: dict, fn_lut: Callable):\n        \"\"\"Initialize with column configuration and a function to get the lookup table.\"\"\"\n        fc.store_attr()        \n\n    def __call__(self, tfm: Transformer):\n        \"\"\"Apply the remapping of detection limits across all dataframes\"\"\"\n        lut = self.fn_lut()  # Retrieve the lookup table\n        for grp, df in tfm.dfs.items():\n            df['DL'] = df[self.coi[grp]['DL']]\n            self._set_detection_limits(df, lut)\n\n    def _set_detection_limits(self, df: pd.DataFrame, lut: dict):\n        \"\"\"Set detection limits based on value and uncertainty columns using specified conditions.\"\"\"\n        # Condition to set '=' when value and uncertainty are present and the current detection limit is not in the lookup keys\n        condition_eq = (df['VALUE'].notna() & df['UNC'].notna() & ~df['DL'].isin(lut.keys()))\n        df.loc[condition_eq, 'DL'] = '='\n\n        # Set 'Not Available' for unmatched detection limits\n        df.loc[~df['DL'].isin(lut.keys()), 'DL'] = 'Not Available'\n\n        # Map existing detection limits using the lookup table\n        df['DL'] = df['DL'].map(lut)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    SanitizeValueCB(),\n    NormalizeUncCB(),                  \n    RemapUnitCB(renaming_unit_rules, verbose=True),\n    RemapDetectionLimitCB(coi_dl, lut_dl)])\n\ntfm()\nfor grp in ['BIOTA', 'SEAWATER']:\n    print(f\"{grp}: {tfm.dfs[grp]['DL'].unique()}\")\n\nBIOTA: [2 1]\n\n\n\nSEAWATER: [2 1]",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#remap-biota-species",
    "href": "handlers/ospar.html#remap-biota-species",
    "title": "OSPAR",
    "section": "Remap Biota species",
    "text": "Remap Biota species\nThe OSPAR dataset contains biota species information in the Species column of the biota DataFrame. To ensure consistency with MARIS standards, it is necessary to remap these species names. We will employ a similar approach to that used for standardizing nuclide names, IMFA (Inspect, Match, Fix, Apply).\nWe first inspect the unique Species values of the OSPAR Biota dataset:\n\ndfs = load_data(src_dir, use_cache=True)\nwith pd.option_context('display.max_columns', None):\n    display(get_unique_across_dfs(dfs, col_name='species', as_df=True).T)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n\n\n\n\nindex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n\n\nvalue\nFUCUS SPP.\nBUCCINUM UNDATUM\nBrosme brosme\nThunnus sp.\nMixture of green, red and brown algae\nSALMO SALAR\nGadiculus argenteus thori\nSardina pilchardus\nNephrops norvegicus\nFucus distichus\nAnarhichas minor\nMolva molva\nGALEUS MELASTOMUS\nANARHICHAS LUPUS\nBuccinum undatum\nPhoca vitulina\nGadus morhua\nunknown\nDasyatis pastinaca\nMOLVA DYPTERYGIA\nPLATICHTHYS FLESUS\nSOLEA SOLEA (S.VULGARIS)\nLAMINARIA DIGITATA\nGaleus melastomus\nFucus serratus\nAnarhichas denticulatus\nHippoglossus hippoglossus\nPenaeus vannamei\nAnguilla anguilla\nFUCUS spp\nHIPPOGLOSSUS HIPPOGLOSSUS\nGadus morhua\nSebastes marinus\nScomber scombrus\nMerluccius merluccius\nMallotus villosus\nCapros aper\nHomarus gammarus\nSCOMBER SCOMBRUS\nDIPTURUS BATIS\nSEBASTES MENTELLA\nClupea harengus\nLimanda Limanda\nSepia spp.\nCerastoderma edule\nCHIMAERA MONSTROSA\nMONODONTA LINEATA\nDicentrarchus labrax\nLIMANDA LIMANDA\nPLEURONECTES PLATESSA\nUnknown\nPORPHYRA UMBILICALIS\nPATELLA\nRHODYMENIA spp\nGlyptocephalus cynoglossus\nMOLVA MOLVA\nBoreogadus saida\nTrisopterus esmarki\nReinhardtius hippoglossoides\nBoreogadus Saida\nOstrea edulis\nFlatfish\nPelvetia canaliculata\nBOREOGADUS SAIDA\nPatella sp.\nAscophyllum nodosum\nSCOPHTHALMUS RHOMBUS\nMERLUCCIUS MERLUCCIUS\nMytilus Edulis\nGaidropsarus argenteus\nRHODYMENIA PSEUDOPALAMATA & PALMARIA PALMATA\nCyclopterus lumpus\nMICROMESISTIUS POUTASSOU\nFucus Vesiculosus\nPECTEN MAXIMUS\nREINHARDTIUS HIPPOGLOSSOIDES\nSqualus acanthias\nTRACHURUS TRACHURUS\nSPRATTUS SPRATTUS\nMytilus edulis\nPELVETIA CANALICULATA\nCRASSOSTREA GIGAS\nTrisopterus minutus\nMelanogrammus aeglefinus\nPLUERONECTES PLATESSA\nMERLANGIUS MERLANGUS\nPATELLA VULGATA\nSalmo salar\nCerastoderma (Cardium) Edule\nLimanda limanda\nMYTILUS EDULIS\nClupea Harengus\nPollachius virens\nTrisopterus esmarkii\nASCOPHYLLUN NODOSUM\nFucus sp.\nMerlangius merlangus\nMelanogrammus aeglefinus\nTapes sp.\nFucus vesiculosus\nCoryphaenoides rupestris\nRhodymenia spp.\nArgentina silus\nPALMARIA PALMATA\nDICENTRARCHUS (MORONE) LABRAX\nGadus Morhua\nOSTREA EDULIS\nLumpenus lampretaeformis\nMELANOGRAMMUS AEGLEFINUS\nPollachius pollachius\nSebastes mentella\nPleuronectiformes [order]\nFUCUS SPIRALIS\nNUCELLA LAPILLUS\nCERASTODERMA (CARDIUM) EDULE\nAnarhichas lupus\nGadiculus argenteus\nClupea harengus\nBROSME BROSME\nTrachurus trachurus\nLycodes vahlii\nPlatichthys flesus\nLITTORINA LITTOREA\nOSILINUS LINEATUS\nFUCUS VESICULOSUS\nArgentina sphyraena\nCLUPEA HARENGUS\nPECTINIDAE\nEutrigla gurnardus\nLophius piscatorius\nGLYPTOCEPHALUS CYNOGLOSSUS\nEUTRIGLA GURNARDUS\nSprattus sprattus\nPleuronectes platessa\nSebastes viviparus\nMERLUCCIUS MERLUCCIUS\nPOLLACHIUS VIRENS\nRAJIDAE/BATOIDEA\nOstrea Edulis\nSebastes Mentella\nCrassostrea gigas\nGADUS MORHUA\nLittorina littorea\nPhycis blennoides\nMerlangius Merlangus\nRaja montagui\nMicromesistius poutassou\nASCOPHYLLUM NODOSUM\nHIPPOGLOSSOIDES PLATESSOIDES\nNaN\nCYCLOPTERUS LUMPUS\nHippoglossoides platessoides\nModiolus modiolus\nGadus sp.\nETMOPTERUS SPINAX\nRAJA DIPTURUS BATIS\nHyperoplus lanceolatus\nSebastes norvegicus\nPleuronectes platessa\nSEBASTES MARINUS\nSolea solea (S.vulgaris)\nSebastes vivipares\nPecten maximus\nThunnus thynnus\nMERLANGUIS MERLANGUIS\nMicrostomus kitt\nFUCUS SERRATUS\n\n\n\n\n\n\n\nWe attempt to match the OSPAR species column to the species column of the MARIS nomenclature using the Remapper . First, we initialize the Remapper:\n\nremapper = Remapper(provider_lut_df=get_unique_across_dfs(dfs, col_name='species', as_df=True),\n                    maris_lut_fn=species_lut_path,\n                    maris_col_id='species_id',\n                    maris_col_name='species',\n                    provider_col_to_match='value',\n                    provider_col_key='value',\n                    fname_cache='species_ospar.pkl')\n\nNext, we perform the matching and generate a lookup table that includes the match score, which quantifies the degree of match accuracy:\n\nremapper.generate_lookup_table(as_df=True)\nremapper.select_match(match_score_threshold=1, verbose=True)\n\nProcessing:   0%|          | 0/167 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 167/167 [00:23&lt;00:00,  7.21it/s]\n\n\n129 entries matched the criteria, while 38 entries had a match score of 1 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\nRHODYMENIA PSEUDOPALAMATA & PALMARIA PALMATA\nLomentaria catenata\nRHODYMENIA PSEUDOPALAMATA & PALMARIA PALMATA\n31\n\n\nMixture of green, red and brown algae\nMercenaria mercenaria\nMixture of green, red and brown algae\n26\n\n\nSolea solea (S.vulgaris)\nLoligo vulgaris\nSolea solea (S.vulgaris)\n12\n\n\nSOLEA SOLEA (S.VULGARIS)\nLoligo vulgaris\nSOLEA SOLEA (S.VULGARIS)\n12\n\n\nCerastoderma (Cardium) Edule\nCerastoderma edule\nCerastoderma (Cardium) Edule\n10\n\n\nCERASTODERMA (CARDIUM) EDULE\nCerastoderma edule\nCERASTODERMA (CARDIUM) EDULE\n10\n\n\nDICENTRARCHUS (MORONE) LABRAX\nDicentrarchus labrax\nDICENTRARCHUS (MORONE) LABRAX\n9\n\n\nRAJIDAE/BATOIDEA\nBatoidea\nRAJIDAE/BATOIDEA\n8\n\n\nPleuronectiformes [order]\nPleuronectiformes\nPleuronectiformes [order]\n8\n\n\nPALMARIA PALMATA\nAlaria marginata\nPALMARIA PALMATA\n7\n\n\nGadiculus argenteus\nPampus argenteus\nGadiculus argenteus\n6\n\n\nMONODONTA LINEATA\nMonodonta labio\nMONODONTA LINEATA\n6\n\n\nRAJA DIPTURUS BATIS\nDipturus batis\nRAJA DIPTURUS BATIS\n5\n\n\nRhodymenia spp.\nRhodymenia\nRhodymenia spp.\n5\n\n\nFUCUS SPP.\nFucus\nFUCUS SPP.\n5\n\n\nUnknown\nUndaria\nUnknown\n5\n\n\nunknown\nUndaria\nunknown\n5\n\n\nFlatfish\nLambia\nFlatfish\n5\n\n\nSepia spp.\nSepia\nSepia spp.\n5\n\n\nPatella sp.\nPatella\nPatella sp.\n4\n\n\nGadus sp.\nPenaeus sp.\nGadus sp.\n4\n\n\nThunnus sp.\nThunnus\nThunnus sp.\n4\n\n\nFUCUS spp\nFucus\nFUCUS spp\n4\n\n\nFucus sp.\nFucus\nFucus sp.\n4\n\n\nTapes sp.\nTapes\nTapes sp.\n4\n\n\nRHODYMENIA spp\nRhodymenia\nRHODYMENIA spp\n4\n\n\nMERLANGUIS MERLANGUIS\nMerlangius merlangus\nMERLANGUIS MERLANGUIS\n3\n\n\nPLUERONECTES PLATESSA\nPleuronectes platessa\nPLUERONECTES PLATESSA\n2\n\n\nGaidropsarus argenteus\nGaidropsarus argentatus\nGaidropsarus argenteus\n2\n\n\nTrisopterus esmarki\nTrisopterus esmarkii\nTrisopterus esmarki\n1\n\n\nASCOPHYLLUN NODOSUM\nAscophyllum nodosum\nASCOPHYLLUN NODOSUM\n1\n\n\nClupea harengus\nClupea harengus\nClupea harengus\n1\n\n\nMERLUCCIUS MERLUCCIUS\nMerluccius merluccius\nMERLUCCIUS MERLUCCIUS\n1\n\n\nHippoglossus hippoglossus\nHippoglossus hippoglossus\nHippoglossus hippoglossus\n1\n\n\nMelanogrammus aeglefinus\nMelanogrammus aeglefinus\nMelanogrammus aeglefinus\n1\n\n\nPleuronectes platessa\nPleuronectes platessa\nPleuronectes platessa\n1\n\n\nGadus morhua\nGadus morhua\nGadus morhua\n1\n\n\nSebastes vivipares\nSebastes viviparus\nSebastes vivipares\n1\n\n\n\n\n\n\n\nBelow, we fix the entries that are not properly matched by the Remapper:\n\n\nExported source\nfixes_biota_species = {\n    'RHODYMENIA PSEUDOPALAMATA & PALMARIA PALMATA': NA,  # Mix of species, no direct mapping\n    'Mixture of green, red and brown algae': NA,  # Mix of species, no direct mapping\n    'Solea solea (S.vulgaris)': 'Solea solea',\n    'SOLEA SOLEA (S.VULGARIS)': 'Solea solea',\n    'RAJIDAE/BATOIDEA': NA, #Mix of species, no direct mapping\n    'PALMARIA PALMATA': NA,  # Not defined\n    'Unknown': NA,\n    'unknown': NA,\n    'Flatfish': NA,\n    'Gadus sp.': NA,  # Not defined\n}\n\n\nWe can now review the remapping results, incorporating the adjustments from the fixes_biota_species dictionary:\n\nremapper.generate_lookup_table(fixes=fixes_biota_species)\nremapper.select_match(match_score_threshold=1, verbose=True)\n\nProcessing:   0%|          | 0/167 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 167/167 [00:24&lt;00:00,  6.88it/s]\n\n\n139 entries matched the criteria, while 28 entries had a match score of 1 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\nCERASTODERMA (CARDIUM) EDULE\nCerastoderma edule\nCERASTODERMA (CARDIUM) EDULE\n10\n\n\nCerastoderma (Cardium) Edule\nCerastoderma edule\nCerastoderma (Cardium) Edule\n10\n\n\nDICENTRARCHUS (MORONE) LABRAX\nDicentrarchus labrax\nDICENTRARCHUS (MORONE) LABRAX\n9\n\n\nPleuronectiformes [order]\nPleuronectiformes\nPleuronectiformes [order]\n8\n\n\nGadiculus argenteus\nPampus argenteus\nGadiculus argenteus\n6\n\n\nMONODONTA LINEATA\nMonodonta labio\nMONODONTA LINEATA\n6\n\n\nFUCUS SPP.\nFucus\nFUCUS SPP.\n5\n\n\nRAJA DIPTURUS BATIS\nDipturus batis\nRAJA DIPTURUS BATIS\n5\n\n\nSepia spp.\nSepia\nSepia spp.\n5\n\n\nRhodymenia spp.\nRhodymenia\nRhodymenia spp.\n5\n\n\nTapes sp.\nTapes\nTapes sp.\n4\n\n\nFUCUS spp\nFucus\nFUCUS spp\n4\n\n\nRHODYMENIA spp\nRhodymenia\nRHODYMENIA spp\n4\n\n\nPatella sp.\nPatella\nPatella sp.\n4\n\n\nThunnus sp.\nThunnus\nThunnus sp.\n4\n\n\nFucus sp.\nFucus\nFucus sp.\n4\n\n\nMERLANGUIS MERLANGUIS\nMerlangius merlangus\nMERLANGUIS MERLANGUIS\n3\n\n\nPLUERONECTES PLATESSA\nPleuronectes platessa\nPLUERONECTES PLATESSA\n2\n\n\nGaidropsarus argenteus\nGaidropsarus argentatus\nGaidropsarus argenteus\n2\n\n\nMelanogrammus aeglefinus\nMelanogrammus aeglefinus\nMelanogrammus aeglefinus\n1\n\n\nTrisopterus esmarki\nTrisopterus esmarkii\nTrisopterus esmarki\n1\n\n\nHippoglossus hippoglossus\nHippoglossus hippoglossus\nHippoglossus hippoglossus\n1\n\n\nClupea harengus\nClupea harengus\nClupea harengus\n1\n\n\nMERLUCCIUS MERLUCCIUS\nMerluccius merluccius\nMERLUCCIUS MERLUCCIUS\n1\n\n\nGadus morhua\nGadus morhua\nGadus morhua\n1\n\n\nPleuronectes platessa\nPleuronectes platessa\nPleuronectes platessa\n1\n\n\nSebastes vivipares\nSebastes viviparus\nSebastes vivipares\n1\n\n\nASCOPHYLLUN NODOSUM\nAscophyllum nodosum\nASCOPHYLLUN NODOSUM\n1\n\n\n\n\n\n\n\nVisual inspection of the remaining imperfectly matched entries appears acceptable. We can now define a Remapper Lambda Function that instantiates the Remapper and returns the corrected lookup table.\n\n\nExported source\nlut_biota = lambda: Remapper(provider_lut_df=get_unique_across_dfs(dfs, col_name='species', as_df=True),\n                             maris_lut_fn=species_lut_path,\n                             maris_col_id='species_id',\n                             maris_col_name='species',\n                             provider_col_to_match='value',\n                             provider_col_key='value',\n                             fname_cache='species_ospar.pkl').generate_lookup_table(fixes=fixes_biota_species, \n                                                                                    as_df=False, overwrite=False)\n\n\nPutting it all together, we now apply the RemapCB callback to our data. This process adds a SPECIES column to our BIOTA dataframe, which contains the standardized species IDs.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='species', dest_grps='BIOTA')    \n    ])\n\ntfm()['BIOTA']['SPECIES'].unique()\n\narray([ 377,  129,   96,    0,  192,   99,   50,  378,  270,  379,  380,\n        381,  382,  383,  384,  385,  244,  386,  387,  388,  389,  390,\n        391,  392,  393,  394,  395,  396,  274,  397,  398,  243,  399,\n        400,  401,  402,  403,  404,  405,  406,  407,  191,  139,  408,\n        410,  412,  413,  272,  414,  415,  416,  417,  418,  419,  420,\n        421,  422,  423,  424,  425,  426,  427,  428,  411,  429,  430,\n        431,  432,  433,  434,  435,  436,  437,  438,  439,  440,  441,\n        442,  443,  444,  294, 1684, 1610, 1609, 1605, 1608,   23, 1606,\n        234,  556, 1701, 1752,  158,  223])",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#enhance-species-data-using-biological-group.",
    "href": "handlers/ospar.html#enhance-species-data-using-biological-group.",
    "title": "OSPAR",
    "section": "Enhance Species Data Using Biological group.",
    "text": "Enhance Species Data Using Biological group.\nThe Biological group column in the OSPAR dataset provides valuable insights related to species. We will leverage this information to enrich the SPECIES column. To achieve this, we will employ the generic RemapCB callback to create an enhanced_species column. Subsequently, this enhanced_species column will be used to further enrich the SPECIES column.\nFirst we inspect the unique values in the biological group column.\n\nget_unique_across_dfs(dfs, col_name='biological group', as_df=True)\n\n\n\n\n\n\n\n\nindex\nvalue\n\n\n\n\n0\n0\nSeaweed\n\n\n1\n1\nFISH\n\n\n2\n2\nMolluscs\n\n\n3\n3\nMOLLUSCS\n\n\n4\n4\nseaweed\n\n\n5\n5\nSEAWEED\n\n\n6\n6\nSeaweeds\n\n\n7\n7\nFish\n\n\n8\n8\nfish\n\n\n9\n9\nmolluscs\n\n\n\n\n\n\n\nWe will remap the biological group columns data to the species column of the MARIS nomenclature, again using a Remapper object:\n\nremapper = Remapper(provider_lut_df=get_unique_across_dfs(dfs, col_name='biological group', as_df=True),\n                    maris_lut_fn=species_lut_path,\n                    maris_col_id='species_id',\n                    maris_col_name='species',\n                    provider_col_to_match='value',\n                    provider_col_key='value',\n                    fname_cache='enhance_species_ospar.pkl')\n\nLike before we will inspect the data.\n\nremapper.generate_lookup_table(as_df=True)\nremapper.select_match(match_score_threshold=1)\n\nProcessing: 100%|██████████| 10/10 [00:02&lt;00:00,  4.80it/s]\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\nFISH\nFucus\nFISH\n4\n\n\nFish\nFucus\nFish\n4\n\n\nfish\nFucus\nfish\n4\n\n\nMolluscs\nMollusca\nMolluscs\n1\n\n\nMOLLUSCS\nMollusca\nMOLLUSCS\n1\n\n\nSeaweeds\nSeaweed\nSeaweeds\n1\n\n\nmolluscs\nMollusca\nmolluscs\n1\n\n\n\n\n\n\n\nWe can see that some entries require manual fixes.\n\n\nExported source\nfixes_enhanced_biota_species = {\n    'fish': 'Pisces',\n    'FISH': 'Pisces',\n    'Fish': 'Pisces'    \n}\n\n\nNow we will apply the manual fixes to the lookup table and review.\n\nremapper.generate_lookup_table(fixes=fixes_enhanced_biota_species)\nremapper.select_match(match_score_threshold=1)\n\nProcessing:   0%|          | 0/10 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 10/10 [00:01&lt;00:00,  7.11it/s]\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\nMolluscs\nMollusca\nMolluscs\n1\n\n\nMOLLUSCS\nMollusca\nMOLLUSCS\n1\n\n\nSeaweeds\nSeaweed\nSeaweeds\n1\n\n\nmolluscs\nMollusca\nmolluscs\n1\n\n\n\n\n\n\n\nVisual inspection of the remaining imperfectly matched entries appears acceptable. We can now define a Remapper Lambda Function that instantiates the Remapper and returns the corrected lookup table.\n\n\nExported source\nlut_biota_enhanced = lambda: Remapper(provider_lut_df=get_unique_across_dfs(dfs, col_name='biological group', as_df=True),\n                             maris_lut_fn=species_lut_path,\n                             maris_col_id='species_id',\n                             maris_col_name='species',\n                             provider_col_to_match='value',\n                             provider_col_key='value',\n                             fname_cache='enhance_species_ospar.pkl').generate_lookup_table(\n                                 fixes=fixes_enhanced_biota_species, \n                                 as_df=False, \n                                 overwrite=False)\n\n\nNow we can apply RemapCB which results in the addition of an enhanced_species column in our BIOTA DataFrame.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    RemapCB(fn_lut=lut_biota_enhanced, col_remap='enhanced_species', col_src='biological group', dest_grps='BIOTA')    \n    ])\n\ntfm()['BIOTA']['enhanced_species'].unique()\n\narray([ 873, 1059,  712])\n\n\nWith the enhanced_species column, we can enrich the SPECIES column. We will use the value in enhanced_species column in the absence of a SPECIES match if the enhanced_species column is valid.\n\nsource\n\nEnhanceSpeciesCB\n\n EnhanceSpeciesCB ()\n\nEnhance the ‘SPECIES’ column using the ‘enhanced_species’ column if conditions are met.\n\n\nExported source\nclass EnhanceSpeciesCB(Callback):\n    \"\"\"Enhance the 'SPECIES' column using the 'enhanced_species' column if conditions are met.\"\"\"\n\n    def __init__(self):\n        fc.store_attr()\n\n    def __call__(self, tfm: 'Transformer'):\n        self._enhance_species(tfm.dfs['BIOTA'])\n\n    def _enhance_species(self, df: pd.DataFrame):\n        df['SPECIES'] = df.apply(\n            lambda row: row['enhanced_species'] if row['SPECIES'] in [-1, 0] and pd.notnull(row['enhanced_species']) else row['SPECIES'],\n            axis=1\n        )\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='species', dest_grps='BIOTA'),\n    RemapCB(fn_lut=lut_biota_enhanced, col_remap='enhanced_species', col_src='biological group', dest_grps='BIOTA'),\n    EnhanceSpeciesCB()\n    ])\n\ntfm()['BIOTA']['SPECIES'].unique()\n\narray([ 377,  129,   96,  712,  192,   99,   50,  378,  270,  379,  380,\n        381,  382,  383,  384,  385,  244,  386,  387,  388,  389,  390,\n        391,  392,  393,  394,  395,  396,  274,  397,  398,  243,  399,\n        400,  401,  402,  403,  404,  405,  406,  407, 1059,  191,  139,\n        408,  410,  412,  413,  272,  414,  415,  416,  417,  418,  419,\n        420,  421,  422,  423,  424,  425,  426,  427,  428,  411,  429,\n        430,  431,  432,  433,  434,  435,  436,  437,  438,  439,  440,\n        441,  442,  443,  444,  294, 1684, 1610, 1609, 1605, 1608,   23,\n       1606,  234,  556, 1701, 1752,  158,  223])\n\n\nAll entries are matched for the SPECIES column.",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#remap-biota-tissues",
    "href": "handlers/ospar.html#remap-biota-tissues",
    "title": "OSPAR",
    "section": "Remap Biota tissues",
    "text": "Remap Biota tissues\nThe OSPAR dataset includes entries where the Body Part is labeled as whole. However, the MARIS data standard requires a more specific distinction for the body_part field, differentiating between Whole animal and Whole plant. Fortunately, the OSPAR dataset provides a Biological group field that allows us to make this distinction.\nTo address this discrepancy and ensure compatibility with MARIS standards, we will: 1. Create a temporary column body_part_temp that combines information from both Body Part and Biological group. 2. Use this temporary column to perform the lookup using our Remapper object.\nLets create the temporary column, body_part_temp, that combines Body Part and Biological group.\n\nsource\n\nAddBodypartTempCB\n\n AddBodypartTempCB ()\n\nAdd a temporary column with the body part and biological group combined.\n\n\nExported source\nclass AddBodypartTempCB(Callback):\n    \"Add a temporary column with the body part and biological group combined.\"    \n    def __call__(self, tfm):\n        tfm.dfs['BIOTA']['body_part_temp'] = (\n            tfm.dfs['BIOTA']['body part'] + ' ' + \n            tfm.dfs['BIOTA']['biological group']\n            ).str.strip().str.lower()\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[  \n                            AddBodypartTempCB(),\n                            ])\ndfs_test = tfm()\ndfs_test['BIOTA']['body_part_temp'].unique()\n\narray(['whole animal molluscs', 'whole plant seaweed', 'whole fish fish',\n       'flesh without bones fish', 'whole animal fish', 'muscle fish',\n       'head fish', 'soft parts molluscs', 'growing tips seaweed',\n       'soft parts fish', 'unknown fish', 'flesh without bone fish',\n       'flesh fish', 'flesh with scales fish', 'liver fish',\n       'flesh without bones seaweed', 'whole  fish',\n       'flesh without bones molluscs', 'whole  seaweed',\n       'whole plant seaweeds', 'whole fish', 'whole without head fish',\n       'mix of muscle and whole fish without liver fish',\n       'whole fisk fish', 'muscle  fish', 'cod medallion fish',\n       'tail and claws fish'], dtype=object)\n\n\nTo align the body_part_temp column with the bodypar column in the MARIS nomenclature, we will use the Remapper. However, since the OSPAR dataset lacks a predefined lookup table for the body_part column, we must first create one. This is accomplished by extracting unique values from the body_part_temp column.\n\nget_unique_across_dfs(dfs_test, col_name='body_part_temp', as_df=True).head()\n\n\n\n\n\n\n\n\nindex\nvalue\n\n\n\n\n0\n0\nwhole fish\n\n\n1\n1\nwhole without head fish\n\n\n2\n2\nflesh with scales fish\n\n\n3\n3\nmix of muscle and whole fish without liver fish\n\n\n4\n4\nwhole animal fish\n\n\n\n\n\n\n\nWe can now remap the body_part_temp column to the bodypar column in the MARIS nomenclature using the Remapper. Subsequently, we will inspect the results:\n\nremapper = Remapper(provider_lut_df=get_unique_across_dfs(dfs_test, col_name='body_part_temp', as_df=True),\n                    maris_lut_fn=bodyparts_lut_path,\n                    maris_col_id='bodypar_id',\n                    maris_col_name='bodypar',\n                    provider_col_to_match='value',\n                    provider_col_key='value',\n                    fname_cache='tissues_ospar.pkl'\n                    )\n\nremapper.generate_lookup_table(as_df=True)\nremapper.select_match(match_score_threshold=0, verbose=True)\n\nProcessing:   0%|          | 0/27 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 27/27 [00:00&lt;00:00, 128.32it/s]\n\n\n0 entries matched the criteria, while 27 entries had a match score of 0 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\nmix of muscle and whole fish without liver fish\nFlesh without bones\nmix of muscle and whole fish without liver fish\n31\n\n\ntail and claws fish\nStomach and intestine\ntail and claws fish\n13\n\n\ncod medallion fish\nOld leaf\ncod medallion fish\n13\n\n\nwhole without head fish\nFlesh without bones\nwhole without head fish\n13\n\n\nwhole fish fish\nWhole animal\nwhole fish fish\n9\n\n\nwhole fisk fish\nWhole animal\nwhole fisk fish\n9\n\n\nwhole animal molluscs\nWhole animal\nwhole animal molluscs\n9\n\n\nflesh without bones molluscs\nFlesh without bones\nflesh without bones molluscs\n9\n\n\nwhole plant seaweeds\nWhole plant\nwhole plant seaweeds\n9\n\n\nsoft parts molluscs\nSoft parts\nsoft parts molluscs\n9\n\n\nunknown fish\nGrowing tips\nunknown fish\n9\n\n\nflesh without bones seaweed\nFlesh without bones\nflesh without bones seaweed\n8\n\n\nwhole plant seaweed\nWhole plant\nwhole plant seaweed\n8\n\n\ngrowing tips seaweed\nGrowing tips\ngrowing tips seaweed\n8\n\n\nflesh fish\nShells\nflesh fish\n7\n\n\nwhole seaweed\nWhole plant\nwhole seaweed\n7\n\n\nmuscle fish\nMuscle\nmuscle fish\n6\n\n\nsoft parts fish\nSoft parts\nsoft parts fish\n5\n\n\nwhole fish\nWhole animal\nwhole fish\n5\n\n\nhead fish\nHead\nhead fish\n5\n\n\nliver fish\nLiver\nliver fish\n5\n\n\nwhole fish\nWhole animal\nwhole fish\n5\n\n\nmuscle fish\nMuscle\nmuscle fish\n5\n\n\nwhole animal fish\nWhole animal\nwhole animal fish\n5\n\n\nflesh with scales fish\nFlesh with scales\nflesh with scales fish\n5\n\n\nflesh without bones fish\nFlesh without bones\nflesh without bones fish\n5\n\n\nflesh without bone fish\nFlesh without bones\nflesh without bone fish\n4\n\n\n\n\n\n\n\nMany of the lookup entries are sufficient for our needs. However, for values that don’t find a match, we can use the fixes_biota_bodyparts dictionary to apply manual corrections. First we will create the dictionary.\n\n\nExported source\nfixes_biota_tissues = {\n    'whole seaweed' : 'Whole plant',\n    'flesh fish': 'Flesh with bones', # We assume it as the category 'Flesh with bones' also exists\n    'flesh fish' : 'Flesh with bones',\n    'unknown fish' : NA,\n    'unknown fish' : NA,\n    'cod medallion fish' : NA, # TO BE DETERMINED\n    'mix of muscle and whole fish without liver fish' : NA, # TO BE DETERMINED\n    'whole without head fish' : NA, # TO BE DETERMINED\n    'flesh without bones seaweed' : NA, # TO BE DETERMINED\n    'tail and claws fish' : NA # TO BE DETERMINED\n}\n\n\nNow we will generate the lookup table and apply the manual fixes defined in the fixes_biota_bodyparts dictionary.\n\nremapper.generate_lookup_table(fixes=fixes_biota_tissues)\nremapper.select_match(match_score_threshold=1, verbose=True)\n\nProcessing:   0%|          | 0/27 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 27/27 [00:00&lt;00:00, 143.57it/s]\n\n\n1 entries matched the criteria, while 26 entries had a match score of 1 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\nwhole fisk fish\nWhole animal\nwhole fisk fish\n9\n\n\nsoft parts molluscs\nSoft parts\nsoft parts molluscs\n9\n\n\nwhole fish fish\nWhole animal\nwhole fish fish\n9\n\n\nflesh without bones molluscs\nFlesh without bones\nflesh without bones molluscs\n9\n\n\nwhole animal molluscs\nWhole animal\nwhole animal molluscs\n9\n\n\nwhole plant seaweeds\nWhole plant\nwhole plant seaweeds\n9\n\n\ngrowing tips seaweed\nGrowing tips\ngrowing tips seaweed\n8\n\n\nwhole plant seaweed\nWhole plant\nwhole plant seaweed\n8\n\n\nwhole seaweed\nWhole plant\nwhole seaweed\n7\n\n\nmuscle fish\nMuscle\nmuscle fish\n6\n\n\nmuscle fish\nMuscle\nmuscle fish\n5\n\n\nliver fish\nLiver\nliver fish\n5\n\n\nflesh with scales fish\nFlesh with scales\nflesh with scales fish\n5\n\n\nsoft parts fish\nSoft parts\nsoft parts fish\n5\n\n\nwhole fish\nWhole animal\nwhole fish\n5\n\n\nhead fish\nHead\nhead fish\n5\n\n\nflesh without bones fish\nFlesh without bones\nflesh without bones fish\n5\n\n\nwhole animal fish\nWhole animal\nwhole animal fish\n5\n\n\nwhole fish\nWhole animal\nwhole fish\n5\n\n\nflesh without bone fish\nFlesh without bones\nflesh without bone fish\n4\n\n\ncod medallion fish\n(Not available)\ncod medallion fish\n2\n\n\nflesh without bones seaweed\n(Not available)\nflesh without bones seaweed\n2\n\n\nunknown fish\n(Not available)\nunknown fish\n2\n\n\nwhole without head fish\n(Not available)\nwhole without head fish\n2\n\n\nmix of muscle and whole fish without liver fish\n(Not available)\nmix of muscle and whole fish without liver fish\n2\n\n\ntail and claws fish\n(Not available)\ntail and claws fish\n2\n\n\n\n\n\n\n\nAt this stage, the majority of entries have been successfully matched to the MARIS nomenclature. Entries that remain unmatched are appropriately marked as ‘not available’. We are now ready to proceed with the final remapping process. We will define a lambda function to instantiate the Remapper, which will then generate and return the corrected lookup table.\n\n\nExported source\nlut_bodyparts = lambda: Remapper(provider_lut_df=get_unique_across_dfs(tfm.dfs, col_name='body_part_temp', as_df=True),\n                               maris_lut_fn=bodyparts_lut_path,\n                               maris_col_id='bodypar_id',\n                               maris_col_name='bodypar',\n                               provider_col_to_match='value',\n                               provider_col_key='value',\n                               fname_cache='tissues_ospar.pkl'\n                               ).generate_lookup_table(fixes=fixes_biota_tissues, as_df=False, overwrite=False)\n\n\nPutting it all together, we now apply the RemapCB callback. This process results in the addition of a BODY_PART column to our BIOTA DataFrame.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[  \n                            RemoveAllNAValuesCB(nan_cols_to_check),\n                            AddBodypartTempCB(),\n                            RemapCB(fn_lut=lut_bodyparts, col_remap='BODY_PART', col_src='body_part_temp' , dest_grps='BIOTA')\n                            ])\ntfm()\ntfm.dfs['BIOTA']['BODY_PART'].unique()\n\narray([ 1, 40, 52, 34, 13, 19, 56,  0,  4, 60, 25])",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#remap-biogroup",
    "href": "handlers/ospar.html#remap-biogroup",
    "title": "OSPAR",
    "section": "Remap biogroup",
    "text": "Remap biogroup\nThe MARIS species lookup table contains a biogroup_id column that associates each species with its corresponding biogroup. We will leverage this relationship to create a BIO_GROUP column in the BIOTA DataFrame.\n\n\nExported source\nlut_biogroup_from_biota = lambda: get_lut(src_dir=species_lut_path().parent, fname=species_lut_path().name, \n                               key='species_id', value='biogroup_id')\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[ \n    RemoveAllNAValuesCB(nan_cols_to_check),                            \n    RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='species', dest_grps='BIOTA'),\n    RemapCB(fn_lut=lut_biota_enhanced, col_remap='enhanced_species', col_src='biological group', dest_grps='BIOTA'),\n    EnhanceSpeciesCB(),\n    RemapCB(fn_lut=lut_biogroup_from_biota, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA')\n    ])\n\ntfm()\nprint(tfm.dfs['BIOTA']['BIO_GROUP'].unique())\n\n[14 11  4 13 12  2  5]\n\n\n\n\ntfm.dfs['BIOTA'].head()\n\n\n\n\n\n\n\n\nid\ncontracting party\nrsc sub-division\nstation id\nsample id\nlatd\nlatm\nlats\nlatdir\nlongd\n...\nactivity or mda\nuncertainty\nunit\ndata provider\nmeasurement comment\nsample comment\nreference comment\nSPECIES\nenhanced_species\nBIO_GROUP\n\n\n\n\n0\n1\nBelgium\n8\nKloosterzande-Schelde\nDA 17531\n51\n23.0\n36.0\nN\n4\n...\n0.326416\nNaN\nBq/kg f.w.\nSCK•CEN\nNaN\nNaN\nNaN\n377\n873\n14\n\n\n1\n2\nBelgium\n8\nKloosterzande-Schelde\nDA 17534\n51\n23.0\n36.0\nN\n4\n...\n0.442704\nNaN\nBq/kg f.w.\nSCK•CEN\nNaN\nNaN\nNaN\n377\n873\n14\n\n\n2\n3\nBelgium\n8\nKloosterzande-Schelde\nDA 17537\n51\n23.0\n36.0\nN\n4\n...\n0.412989\nNaN\nBq/kg f.w.\nSCK•CEN\nNaN\nNaN\nNaN\n377\n873\n14\n\n\n3\n4\nBelgium\n8\nKloosterzande-Schelde\nDA 17540\n51\n23.0\n36.0\nN\n4\n...\n0.202768\nNaN\nBq/kg f.w.\nSCK•CEN\nNaN\nNaN\nNaN\n377\n873\n14\n\n\n4\n5\nBelgium\n8\nKloosterzande-Schelde\nDA 17531\n51\n23.0\n36.0\nN\n4\n...\n0.652833\nNaN\nBq/kg f.w.\nSCK•CEN\nNaN\nNaN\nNaN\n377\n873\n14\n\n\n\n\n5 rows × 30 columns",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#add-sample-id",
    "href": "handlers/ospar.html#add-sample-id",
    "title": "OSPAR",
    "section": "Add Sample ID",
    "text": "Add Sample ID\n\nSMP_ID is a internal unique identifier for each sample\nSMP_ID_PROVIDER is data provided by the data provider.\n\n\nsource\n\nAddSampleIdCB\n\n AddSampleIdCB ()\n\nCreate incremental SMP_ID and store original sample id in SMP_ID_PROVIDER\n\n\nExported source\nclass AddSampleIdCB(Callback):\n    \"Create incremental SMP_ID and store original sample id in SMP_ID_PROVIDER\"\n    def __call__(self, tfm):\n        for _, df in tfm.dfs.items():\n            df['SMP_ID'] = range(1, len(df) + 1)\n            df['SMP_ID_PROVIDER'] = df['sample id'].astype(str)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[RemoveAllNAValuesCB(nan_cols_to_check),\n                            AddSampleIdCB(),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\ntfm()\n\ntfm.dfs['BIOTA'].head()\n\n\n\n\n\n\n\n\nid\ncontracting party\nrsc sub-division\nstation id\nsample id\nlatd\nlatm\nlats\nlatdir\nlongd\n...\nvalue type\nactivity or mda\nuncertainty\nunit\ndata provider\nmeasurement comment\nsample comment\nreference comment\nSMP_ID\nSMP_ID_PROVIDER\n\n\n\n\n0\n1\nBelgium\n8\nKloosterzande-Schelde\nDA 17531\n51\n23.0\n36.0\nN\n4\n...\n&lt;\n0.326416\nNaN\nBq/kg f.w.\nSCK•CEN\nNaN\nNaN\nNaN\n1\nDA 17531\n\n\n1\n2\nBelgium\n8\nKloosterzande-Schelde\nDA 17534\n51\n23.0\n36.0\nN\n4\n...\n&lt;\n0.442704\nNaN\nBq/kg f.w.\nSCK•CEN\nNaN\nNaN\nNaN\n2\nDA 17534\n\n\n2\n3\nBelgium\n8\nKloosterzande-Schelde\nDA 17537\n51\n23.0\n36.0\nN\n4\n...\n&lt;\n0.412989\nNaN\nBq/kg f.w.\nSCK•CEN\nNaN\nNaN\nNaN\n3\nDA 17537\n\n\n3\n4\nBelgium\n8\nKloosterzande-Schelde\nDA 17540\n51\n23.0\n36.0\nN\n4\n...\n&lt;\n0.202768\nNaN\nBq/kg f.w.\nSCK•CEN\nNaN\nNaN\nNaN\n4\nDA 17540\n\n\n4\n5\nBelgium\n8\nKloosterzande-Schelde\nDA 17531\n51\n23.0\n36.0\nN\n4\n...\n&lt;\n0.652833\nNaN\nBq/kg f.w.\nSCK•CEN\nNaN\nNaN\nNaN\n5\nDA 17531\n\n\n\n\n5 rows × 29 columns",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#add-depth",
    "href": "handlers/ospar.html#add-depth",
    "title": "OSPAR",
    "section": "Add depth",
    "text": "Add depth\nThe OSPAR dataset features a Sampling depth column specifically for the SEAWATER dataset. In this section, we will develop a callback to integrate the sampling depth, denoted as SMP_DEPTH, into the MARIS dataset.\n\nsource\n\nAddDepthCB\n\n AddDepthCB ()\n\nEnsure depth values are floats and add ‘SMP_DEPTH’ columns.\n\n\nExported source\nclass AddDepthCB(Callback):\n    \"Ensure depth values are floats and add 'SMP_DEPTH' columns.\"\n    def __call__(self, tfm: Transformer):\n        for grp, df in tfm.dfs.items():\n            if grp == 'SEAWATER':\n                if 'sampling depth' in df.columns:\n                    df['SMP_DEPTH'] = df['sampling depth'].astype(float)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    AddDepthCB()\n    ])\ntfm()\nfor grp in tfm.dfs.keys():  \n    if 'SMP_DEPTH' in tfm.dfs[grp].columns:\n        print(f'{grp}:', tfm.dfs[grp][['SMP_DEPTH']].drop_duplicates())\n\nSEAWATER:        SMP_DEPTH\n0            3.0\n80           2.0\n81          21.0\n85          31.0\n87          32.0\n...          ...\n16022       71.0\n16023       66.0\n16025       81.0\n16385     1660.0\n16389     1500.0\n\n[134 rows x 1 columns]",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#standardize-coordinates",
    "href": "handlers/ospar.html#standardize-coordinates",
    "title": "OSPAR",
    "section": "Standardize Coordinates",
    "text": "Standardize Coordinates\nThe OSPAR dataset offers coordinates in degrees, minutes, and seconds (DMS). The following callback is designed to convert DMS to decimal degrees.\n\nsource\n\nConvertLonLatCB\n\n ConvertLonLatCB ()\n\nConvert Coordinates to decimal degrees (DDD.DDDDD°).\n\n\nExported source\nclass ConvertLonLatCB(Callback):\n    \"\"\"Convert Coordinates to decimal degrees (DDD.DDDDD°).\"\"\"\n    def __init__(self):\n        fc.store_attr()\n\n    def __call__(self, tfm: 'Transformer'):\n        for grp, df in tfm.dfs.items():\n            df['LAT'] = self._convert_latitude(df)\n            df['LON'] = self._convert_longitude(df)\n\n    def _convert_latitude(self, df: pd.DataFrame) -&gt; pd.Series:\n        return np.where(\n            df['latdir'].isin(['S']),\n            self._dms_to_decimal(df['latd'], df['latm'], df['lats']) * -1,\n            self._dms_to_decimal(df['latd'], df['latm'], df['lats'])\n        )\n\n    def _convert_longitude(self, df: pd.DataFrame) -&gt; pd.Series:\n        return np.where(\n            df['longdir'].isin(['W']),\n            self._dms_to_decimal(df['longd'], df['longm'], df['longs']) * -1,\n            self._dms_to_decimal(df['longd'], df['longm'], df['longs'])\n        )\n\n    def _dms_to_decimal(self, degrees: pd.Series, minutes: pd.Series, seconds: pd.Series) -&gt; pd.Series:\n        return degrees + minutes / 60 + seconds / 3600\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemoveAllNAValuesCB(nan_cols_to_check),\n    ConvertLonLatCB()\n    ])\n\ntfm()\n\nwith pd.option_context('display.max_columns', None):\n    display(tfm.dfs['SEAWATER'][['LAT','latd', 'latm', 'lats', 'LON', 'latdir', 'longd', 'longm','longs', 'longdir']])\n\n\n\n\n\n\n\n\nLAT\nlatd\nlatm\nlats\nLON\nlatdir\nlongd\nlongm\nlongs\nlongdir\n\n\n\n\n0\n51.375278\n51\n22.0\n31.0\n3.188056\nN\n3\n11.0\n17.0\nE\n\n\n1\n51.223611\n51\n13.0\n25.0\n2.859444\nN\n2\n51.0\n34.0\nE\n\n\n2\n51.184444\n51\n11.0\n4.0\n2.713611\nN\n2\n42.0\n49.0\nE\n\n\n3\n51.420278\n51\n25.0\n13.0\n3.262222\nN\n3\n15.0\n44.0\nE\n\n\n4\n51.416111\n51\n24.0\n58.0\n2.809722\nN\n2\n48.0\n35.0\nE\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19183\n52.831944\n52\n49.0\n55.0\n4.615278\nN\n4\n36.0\n55.0\nE\n\n\n19184\n51.411944\n51\n24.0\n43.0\n3.565556\nN\n3\n33.0\n56.0\nE\n\n\n19185\n51.411944\n51\n24.0\n43.0\n3.565556\nN\n3\n33.0\n56.0\nE\n\n\n19186\n51.411944\n51\n24.0\n43.0\n3.565556\nN\n3\n33.0\n56.0\nE\n\n\n19187\n51.719444\n51\n43.0\n10.0\n3.493889\nN\n3\n29.0\n38.0\nE\n\n\n\n\n19183 rows × 10 columns\n\n\n\nSanitize coordinates drops a row when both longitude & latitude equal 0 or data contains unrealistic longitude & latitude values. Converts longitude & latitude , separator to . separator.”\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n                            ConvertLonLatCB(),\n                            SanitizeLonLatCB(),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\n\ntfm()\n\ndisplay(Markdown(\"&lt;b&gt; Row Count Comparison Before and After Transformation:&lt;/b&gt;\"))\nwith pd.option_context('display.max_rows', None):\n    display(pd.DataFrame.from_dict(tfm.compare_stats))\n\nwith pd.option_context('display.max_columns', None):\n    display(tfm.dfs['SEAWATER'][['LAT','LON']])\n\n Row Count Comparison Before and After Transformation:\n\n\n\n\n\n\n\n\n\nBIOTA\nSEAWATER\n\n\n\n\nOriginal row count (dfs)\n15951\n19193\n\n\nTransformed row count (tfm.dfs)\n15951\n19193\n\n\nRows removed from original (tfm.dfs_removed)\n0\n0\n\n\nRows created in transformed (tfm.dfs_created)\n0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLAT\nLON\n\n\n\n\n0\n51.375278\n3.188056\n\n\n1\n51.223611\n2.859444\n\n\n2\n51.184444\n2.713611\n\n\n3\n51.420278\n3.262222\n\n\n4\n51.416111\n2.809722\n\n\n...\n...\n...\n\n\n19188\n53.600000\n-5.933333\n\n\n19189\n53.733333\n-5.416667\n\n\n19190\n53.650000\n-5.233333\n\n\n19191\n53.883333\n-5.550000\n\n\n19192\n53.866667\n-5.883333\n\n\n\n\n19193 rows × 2 columns",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#add-station",
    "href": "handlers/ospar.html#add-station",
    "title": "OSPAR",
    "section": "Add Station",
    "text": "Add Station\n\nsource\n\nAddStationCB\n\n AddStationCB ()\n\nEnsure station values are floats and add ‘STATION’ columns.\n\n\nExported source\nclass AddStationCB(Callback):\n    \"Ensure station values are floats and add 'STATION' columns.\"\n    def __call__(self, tfm: Transformer):\n        for grp, df in tfm.dfs.items():\n            df['STATION'] = df['station id'].astype(str)",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#review-all-callbacks",
    "href": "handlers/ospar.html#review-all-callbacks",
    "title": "OSPAR",
    "section": "Review all callbacks",
    "text": "Review all callbacks\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n                            RemoveAllNAValuesCB(nan_cols_to_check),\n                            LowerStripNameCB(col_src='nuclide', col_dst='nuclide'),\n                            RemapNuclideNameCB(lut_nuclides, col_name='nuclide'),\n                            ParseTimeCB(),\n                            EncodeTimeCB(),\n                            SanitizeValueCB(),\n                            NormalizeUncCB(),\n                            RemapUnitCB(renaming_unit_rules),\n                            RemapDetectionLimitCB(coi_dl, lut_dl),\n                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='species', dest_grps='BIOTA'),    \n                            RemapCB(fn_lut=lut_biota_enhanced, col_remap='enhanced_species', col_src='biological group', dest_grps='BIOTA'),    \n                            EnhanceSpeciesCB(),\n                            AddBodypartTempCB(),\n                            RemapCB(fn_lut=lut_bodyparts, col_remap='BODY_PART', col_src='body_part_temp' , dest_grps='BIOTA'),\n                            AddSampleIdCB(),\n                            AddDepthCB(),    \n                            ConvertLonLatCB(),\n                            SanitizeLonLatCB(),\n                            AddStationCB(),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\n\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n\nProcessing: 100%|██████████| 12/12 [00:00&lt;00:00, 55.39it/s]\n\n\n                                               BIOTA  SEAWATER\nOriginal row count (dfs)                       15951     19193\nTransformed row count (tfm.dfs)                15951     19183\nRows removed from original (tfm.dfs_removed)       0        10\nRows created in transformed (tfm.dfs_created)      0         0 \n\n\n\n\n\ntfm.dfs['SEAWATER'].head()\n\n\n\n\n\n\n\n\nid\ncontracting party\nrsc sub-division\nstation id\nsample id\nlatd\nlatm\nlats\nlatdir\nlongd\n...\nVALUE\nUNC\nUNIT\nDL\nSMP_ID\nSMP_ID_PROVIDER\nSMP_DEPTH\nLAT\nLON\nSTATION\n\n\n\n\n0\n1\nBelgium\n8.0\nBelgica-W01\nWNZ 01\n51\n22.0\n31.0\nN\n3\n...\n0.20\nNaN\n1\n2\n1\nWNZ 01\n3.0\n51.375278\n3.188056\nBelgica-W01\n\n\n1\n2\nBelgium\n8.0\nBelgica-W02\nWNZ 02\n51\n13.0\n25.0\nN\n2\n...\n0.27\nNaN\n1\n2\n2\nWNZ 02\n3.0\n51.223611\n2.859444\nBelgica-W02\n\n\n2\n3\nBelgium\n8.0\nBelgica-W03\nWNZ 03\n51\n11.0\n4.0\nN\n2\n...\n0.26\nNaN\n1\n2\n3\nWNZ 03\n3.0\n51.184444\n2.713611\nBelgica-W03\n\n\n3\n4\nBelgium\n8.0\nBelgica-W04\nWNZ 04\n51\n25.0\n13.0\nN\n3\n...\n0.25\nNaN\n1\n2\n4\nWNZ 04\n3.0\n51.420278\n3.262222\nBelgica-W04\n\n\n4\n5\nBelgium\n8.0\nBelgica-W05\nWNZ 05\n51\n24.0\n58.0\nN\n2\n...\n0.20\nNaN\n1\n2\n5\nWNZ 05\n3.0\n51.416111\n2.809722\nBelgica-W05\n\n\n\n\n5 rows × 37 columns\n\n\n\n\nExample change logs\nReview the change logs for the netcdf encoding.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n                            RemoveAllNAValuesCB(nan_cols_to_check), \n                            LowerStripNameCB(col_src='nuclide', col_dst='nuclide'),\n                            RemapNuclideNameCB(lut_nuclides, col_name='nuclide'),\n                            ParseTimeCB(),\n                            EncodeTimeCB(),\n                            SanitizeValueCB(),\n                            NormalizeUncCB(),\n                            RemapUnitCB(renaming_unit_rules),\n                            RemapDetectionLimitCB(coi_dl, lut_dl),\n                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='species', dest_grps='BIOTA'),    \n                            RemapCB(fn_lut=lut_biota_enhanced, col_remap='enhanced_species', col_src='biological group', dest_grps='BIOTA'),    \n                            EnhanceSpeciesCB(),\n                            AddBodypartTempCB(),\n                            RemapCB(fn_lut=lut_bodyparts, col_remap='BODY_PART', col_src='body_part_temp' , dest_grps='BIOTA'),\n                            AddSampleIdCB(),\n                            AddDepthCB(),    \n                            ConvertLonLatCB(),\n                            SanitizeLonLatCB(),\n                            AddStationCB(),\n                            ])\n\n# Transform\ntfm()\n# Check transformation logs\ntfm.logs\n\nProcessing: 100%|██████████| 12/12 [00:00&lt;00:00, 56.55it/s]\n\n\n['Remove rows with all NA values in specified columns.',\n \"Convert 'nuclide' column values to lowercase, strip spaces, and store in 'nuclide' column.\",\n 'Remap data provider nuclide names to standardized MARIS nuclide names.',\n 'Parse the time format in the dataframe and check for inconsistencies.',\n 'Encode time as seconds since epoch.',\n 'Sanitize value by removing blank entries and populating `value` column.',\n 'Normalize uncertainty values in DataFrames.',\n \"Callback to update DataFrame 'UNIT' columns based on a lookup table.\",\n 'Remap detection limit values to MARIS format using a lookup table.',\n \"Remap values from 'species' to 'SPECIES' for groups: BIOTA.\",\n \"Remap values from 'biological group' to 'enhanced_species' for groups: BIOTA.\",\n \"Enhance the 'SPECIES' column using the 'enhanced_species' column if conditions are met.\",\n 'Add a temporary column with the body part and biological group combined.',\n \"Remap values from 'body_part_temp' to 'BODY_PART' for groups: BIOTA.\",\n 'Create incremental SMP_ID and store original sample id in SMP_ID_PROVIDER',\n \"Ensure depth values are floats and add 'SMP_DEPTH' columns.\",\n 'Convert Coordinates to decimal degrees (DDD.DDDDD°).',\n 'Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.',\n \"Ensure station values are floats and add 'STATION' columns.\"]",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#feed-global-attributes",
    "href": "handlers/ospar.html#feed-global-attributes",
    "title": "OSPAR",
    "section": "Feed global attributes",
    "text": "Feed global attributes\n\nsource\n\nget_attrs\n\n get_attrs (tfm:marisco.callbacks.Transformer, zotero_key:str,\n            kw:list=['oceanography', 'Earth Science &gt; Oceans &gt; Ocean\n            Chemistry&gt; Radionuclides', 'Earth Science &gt; Human Dimensions &gt;\n            Environmental Impacts &gt; Nuclear Radiation Exposure', 'Earth\n            Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth\n            Science &gt; Oceans &gt; Marine Sediments', 'Earth Science &gt; Oceans\n            &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt;\n            Isotopes', 'Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean\n            Contaminants', 'Earth Science &gt; Biological Classification &gt;\n            Animals/Vertebrates &gt; Fish', 'Earth Science &gt; Biosphere &gt;\n            Ecosystems &gt; Marine Ecosystems', 'Earth Science &gt; Biological\n            Classification &gt; Animals/Invertebrates &gt; Mollusks', 'Earth\n            Science &gt; Biological Classification &gt; Animals/Invertebrates &gt;\n            Arthropods &gt; Crustaceans', 'Earth Science &gt; Biological\n            Classification &gt; Plants &gt; Macroalgae (Seaweeds)'])\n\nRetrieve all global attributes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntfm\nTransformer\n\nTransformer object\n\n\nzotero_key\nstr\n\nZotero dataset record key\n\n\nkw\nlist\n[‘oceanography’, ‘Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides’, ‘Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure’, ‘Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments’, ‘Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes’, ‘Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants’, ‘Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish’, ‘Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems’, ‘Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks’, ‘Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans’, ‘Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)’]\nList of keywords\n\n\nReturns\ndict\n\nGlobal attributes\n\n\n\n\n\nExported source\nkw = ['oceanography', 'Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides',\n      'Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure',\n      'Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments',\n      'Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes',\n      'Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants',\n      'Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish',\n      'Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems',\n      'Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks',\n      'Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans',\n      'Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)']\n\n\n\n\nExported source\ndef get_attrs(\n    tfm: Transformer, # Transformer object\n    zotero_key: str, # Zotero dataset record key\n    kw: list = kw # List of keywords\n    ) -&gt; dict: # Global attributes\n    \"Retrieve all global attributes.\"\n    return GlobAttrsFeeder(tfm.dfs, cbs=[\n        BboxCB(),\n        DepthRangeCB(),\n        TimeRangeCB(),\n        ZoteroCB(zotero_key, cfg=cfg()),\n        KeyValuePairCB('keywords', ', '.join(kw)),\n        KeyValuePairCB('publisher_postprocess_logs', ', '.join(tfm.logs))\n        ])()\n\n\n\nget_attrs(tfm, zotero_key=zotero_key, kw=kw)\n\n{'geospatial_lat_min': '49.43222222222222',\n 'geospatial_lat_max': '81.26805555555555',\n 'geospatial_lon_min': '-58.23166666666667',\n 'geospatial_lon_max': '36.181666666666665',\n 'geospatial_bounds': 'POLYGON ((-58.23166666666667 36.181666666666665, 49.43222222222222 36.181666666666665, 49.43222222222222 81.26805555555555, -58.23166666666667 81.26805555555555, -58.23166666666667 36.181666666666665))',\n 'geospatial_vertical_max': '1850.0',\n 'geospatial_vertical_min': '0.0',\n 'time_coverage_start': '1995-01-01T00:00:00',\n 'time_coverage_end': '2022-12-31T00:00:00',\n 'id': 'LQRA4MMK',\n 'title': 'OSPAR Environmental Monitoring of Radioactive Substances',\n 'summary': '',\n 'creator_name': '[{\"creatorType\": \"author\", \"firstName\": \"\", \"lastName\": \"OSPAR Comission\\'s Radioactive Substances Committee (RSC)\"}]',\n 'keywords': 'oceanography, Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides, Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure, Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments, Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes, Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants, Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish, Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans, Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)',\n 'publisher_postprocess_logs': \"Remove rows with all NA values in specified columns., Convert 'nuclide' column values to lowercase, strip spaces, and store in 'nuclide' column., Remap data provider nuclide names to standardized MARIS nuclide names., Parse the time format in the dataframe and check for inconsistencies., Encode time as seconds since epoch., Sanitize value by removing blank entries and populating `value` column., Normalize uncertainty values in DataFrames., Callback to update DataFrame 'UNIT' columns based on a lookup table., Remap detection limit values to MARIS format using a lookup table., Remap values from 'species' to 'SPECIES' for groups: BIOTA., Remap values from 'biological group' to 'enhanced_species' for groups: BIOTA., Enhance the 'SPECIES' column using the 'enhanced_species' column if conditions are met., Add a temporary column with the body part and biological group combined., Remap values from 'body_part_temp' to 'BODY_PART' for groups: BIOTA., Create incremental SMP_ID and store original sample id in SMP_ID_PROVIDER, Ensure depth values are floats and add 'SMP_DEPTH' columns., Convert Coordinates to decimal degrees (DDD.DDDDD°)., Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator., Ensure station values are floats and add 'STATION' columns.\"}",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#encoding-netcdf",
    "href": "handlers/ospar.html#encoding-netcdf",
    "title": "OSPAR",
    "section": "Encoding NETCDF",
    "text": "Encoding NETCDF\n\nsource\n\nencode\n\n encode (fname_out:str, **kwargs)\n\nEncode data to NetCDF.\n\n\n\n\nType\nDetails\n\n\n\n\nfname_out\nstr\nOutput file name\n\n\nkwargs\nVAR_KEYWORD\n\n\n\nReturns\nNone\nAdditional arguments\n\n\n\n\n\nExported source\ndef encode(\n    fname_out: str, # Output file name\n    **kwargs # Additional arguments\n    ) -&gt; None:\n    \"Encode data to NetCDF.\"\n    dfs = load_data(src_dir, use_cache=True)\n    tfm = Transformer(dfs, cbs=[\n                            RemoveAllNAValuesCB(nan_cols_to_check),\n                            LowerStripNameCB(col_src='nuclide', col_dst='nuclide'),\n                            RemapNuclideNameCB(lut_nuclides, col_name='nuclide'),\n                            ParseTimeCB(),\n                            EncodeTimeCB(),\n                            SanitizeValueCB(),\n                            NormalizeUncCB(),\n                            RemapUnitCB(renaming_unit_rules),\n                            RemapDetectionLimitCB(coi_dl, lut_dl),\n                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='species', dest_grps='BIOTA'),    \n                            RemapCB(fn_lut=lut_biota_enhanced, col_remap='enhanced_species', col_src='biological group', dest_grps='BIOTA'),    \n                            EnhanceSpeciesCB(),\n                            AddBodypartTempCB(),\n                            RemapCB(fn_lut=lut_bodyparts, col_remap='BODY_PART', col_src='body_part_temp' , dest_grps='BIOTA'),\n                            AddSampleIdCB(),\n                            AddDepthCB(),    \n                            ConvertLonLatCB(),\n                            SanitizeLonLatCB(),\n                            AddStationCB()\n                                ])\n    tfm()\n    encoder = NetCDFEncoder(tfm.dfs, \n                            dest_fname=fname_out, \n                            global_attrs=get_attrs(tfm, zotero_key=zotero_key, kw=kw),\n                            verbose=kwargs.get('verbose', False),\n                           )\n    encoder.encode()\n\n\n\nencode(fname_out, verbose=False)\n\nProcessing: 100%|██████████| 12/12 [00:00&lt;00:00, 38.63it/s]",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#netcdf-review",
    "href": "handlers/ospar.html#netcdf-review",
    "title": "OSPAR",
    "section": "NetCDF Review",
    "text": "NetCDF Review\nFirst lets review the global attributes of the NetCDF file:\n\ncontents = ExtractNetcdfContents(fname_out)\nprint(contents.global_attrs)\n\n{\n    'id': 'LQRA4MMK',\n    'title': 'OSPAR Environmental Monitoring of Radioactive Substances',\n    'summary': '',\n    'keywords': 'oceanography, Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides, Earth Science &gt; Human \nDimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure, Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean \nTracers, Earth Science &gt; Oceans &gt; Marine Sediments, Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; \nOceans &gt; Sea Ice &gt; Isotopes, Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants, Earth Science &gt; \nBiological Classification &gt; Animals/Vertebrates &gt; Fish, Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems,\nEarth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks, Earth Science &gt; Biological \nClassification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans, Earth Science &gt; Biological Classification &gt; \nPlants &gt; Macroalgae (Seaweeds)',\n    'history': 'TBD',\n    'keywords_vocabulary': 'GCMD Science Keywords',\n    'keywords_vocabulary_url': 'https://gcmd.earthdata.nasa.gov/static/kms/',\n    'record': 'TBD',\n    'featureType': 'TBD',\n    'cdm_data_type': 'TBD',\n    'Conventions': 'CF-1.10 ACDD-1.3',\n    'publisher_name': 'Paul MCGINNITY, Iolanda OSVATH, Florence DESCROIX-COMANDUCCI',\n    'publisher_email': 'p.mc-ginnity@iaea.org, i.osvath@iaea.org, F.Descroix-Comanducci@iaea.org',\n    'publisher_url': 'https://maris.iaea.org',\n    'publisher_institution': 'International Atomic Energy Agency - IAEA',\n    'creator_name': '[{\"creatorType\": \"author\", \"firstName\": \"\", \"lastName\": \"OSPAR Comission\\'s Radioactive \nSubstances Committee (RSC)\"}]',\n    'institution': 'TBD',\n    'metadata_link': 'TBD',\n    'creator_email': 'TBD',\n    'creator_url': 'TBD',\n    'references': 'TBD',\n    'license': 'Without prejudice to the applicable Terms and Conditions \n(https://nucleus.iaea.org/Pages/Others/Disclaimer.aspx), I hereby agree that any use of the data will contain \nappropriate acknowledgement of the data source(s) and the IAEA Marine Radioactivity Information System (MARIS).',\n    'comment': 'TBD',\n    'geospatial_lat_min': '49.43222222222222',\n    'geospatial_lon_min': '-58.23166666666667',\n    'geospatial_lat_max': '81.26805555555555',\n    'geospatial_lon_max': '36.181666666666665',\n    'geospatial_vertical_min': '0.0',\n    'geospatial_vertical_max': '1850.0',\n    'geospatial_bounds': 'POLYGON ((-58.23166666666667 36.181666666666665, 49.43222222222222 36.181666666666665, \n49.43222222222222 81.26805555555555, -58.23166666666667 81.26805555555555, -58.23166666666667 \n36.181666666666665))',\n    'geospatial_bounds_crs': 'EPSG:4326',\n    'time_coverage_start': '1995-01-01T00:00:00',\n    'time_coverage_end': '2022-12-31T00:00:00',\n    'local_time_zone': 'TBD',\n    'date_created': 'TBD',\n    'date_modified': 'TBD',\n    'publisher_postprocess_logs': \"Remove rows with all NA values in specified columns., Convert 'nuclide' column \nvalues to lowercase, strip spaces, and store in 'nuclide' column., Remap data provider nuclide names to \nstandardized MARIS nuclide names., Parse the time format in the dataframe and check for inconsistencies., Encode \ntime as seconds since epoch., Sanitize value by removing blank entries and populating `value` column., Normalize \nuncertainty values in DataFrames., Callback to update DataFrame 'UNIT' columns based on a lookup table., Remap \ndetection limit values to MARIS format using a lookup table., Remap values from 'species' to 'SPECIES' for groups: \nBIOTA., Remap values from 'biological group' to 'enhanced_species' for groups: BIOTA., Enhance the 'SPECIES' column\nusing the 'enhanced_species' column if conditions are met., Add a temporary column with the body part and \nbiological group combined., Remap values from 'body_part_temp' to 'BODY_PART' for groups: BIOTA., Create \nincremental SMP_ID and store original sample id in SMP_ID_PROVIDER, Ensure depth values are floats and add \n'SMP_DEPTH' columns., Convert Coordinates to decimal degrees (DDD.DDDDD°)., Drop rows with invalid longitude & \nlatitude values. Convert `,` separator to `.` separator., Ensure station values are floats and add 'STATION' \ncolumns.\"\n}\n\n\n\nReview the publisher_postprocess_logs.\n\nprint(contents.global_attrs['publisher_postprocess_logs'])\n\nRemove rows with all NA values in specified columns., Convert 'nuclide' column values to lowercase, strip spaces, \nand store in 'nuclide' column., Remap data provider nuclide names to standardized MARIS nuclide names., Parse the \ntime format in the dataframe and check for inconsistencies., Encode time as seconds since epoch., Sanitize value by\nremoving blank entries and populating `value` column., Normalize uncertainty values in DataFrames., Callback to \nupdate DataFrame 'UNIT' columns based on a lookup table., Remap detection limit values to MARIS format using a \nlookup table., Remap values from 'species' to 'SPECIES' for groups: BIOTA., Remap values from 'biological group' to\n'enhanced_species' for groups: BIOTA., Enhance the 'SPECIES' column using the 'enhanced_species' column if \nconditions are met., Add a temporary column with the body part and biological group combined., Remap values from \n'body_part_temp' to 'BODY_PART' for groups: BIOTA., Create incremental SMP_ID and store original sample id in \nSMP_ID_PROVIDER, Ensure depth values are floats and add 'SMP_DEPTH' columns., Convert Coordinates to decimal \ndegrees (DDD.DDDDD°)., Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.,\nEnsure station values are floats and add 'STATION' columns.\n\n\n\nLets review the data of the NetCDF file:\n\ndfs = contents.dfs\ndfs\n\n{'BIOTA':       SMP_ID_PROVIDER        LON        LAT        TIME  \\\n 0            DA 17531   4.031111  51.393333  1267574400   \n 1            DA 17534   4.031111  51.393333  1276473600   \n 2            DA 17537   4.031111  51.393333  1285545600   \n 3            DA 17540   4.031111  51.393333  1291766400   \n 4            DA 17531   4.031111  51.393333  1267574400   \n ...               ...        ...        ...         ...   \n 15946             nan  12.087778  57.252499  1660003200   \n 15947             nan  12.107500  57.306389  1663891200   \n 15948             nan  11.245000  58.603333  1667779200   \n 15949             nan  11.905278  57.302502  1663632000   \n 15950             nan  12.076667  57.335278  1662076800   \n \n                      STATION  SMP_ID  NUCLIDE     VALUE  UNIT       UNC  DL  \\\n 0      Kloosterzande-Schelde       1       33  0.326416     5       NaN   2   \n 1      Kloosterzande-Schelde       2       33  0.442704     5       NaN   2   \n 2      Kloosterzande-Schelde       3       33  0.412989     5       NaN   2   \n 3      Kloosterzande-Schelde       4       33  0.202768     5       NaN   2   \n 4      Kloosterzande-Schelde       5       53  0.652833     5       NaN   2   \n ...                      ...     ...      ...       ...   ...       ...  ..   \n 15946         Ringhals (R22)   15947       33  0.384000     5  0.012096   1   \n 15947         Ringhals (R23)   15948       33  0.456000     5  0.012084   1   \n 15948                    SW7   15949       33  0.122000     5  0.031000   1   \n 15949                   SW6a   15950       33  0.310000     5       NaN   2   \n 15950         Ringhals (R25)   15951       33  0.306000     5  0.007191   1   \n \n        SPECIES  BODY_PART  \n 0          377          1  \n 1          377          1  \n 2          377          1  \n 3          377          1  \n 4          377          1  \n ...        ...        ...  \n 15946      272         52  \n 15947      272         52  \n 15948      129         19  \n 15949      129         19  \n 15950       96         40  \n \n [15951 rows x 13 columns],\n 'SEAWATER':       SMP_ID_PROVIDER       LON        LAT  SMP_DEPTH        TIME  \\\n 0              WNZ 01  3.188056  51.375278        3.0  1264550400   \n 1              WNZ 02  2.859444  51.223610        3.0  1264550400   \n 2              WNZ 03  2.713611  51.184444        3.0  1264550400   \n 3              WNZ 04  3.262222  51.420277        3.0  1264550400   \n 4              WNZ 05  2.809722  51.416111        3.0  1264464000   \n ...               ...       ...        ...        ...         ...   \n 19178      2019010074  4.615278  52.831944        1.0  1573649640   \n 19179      2019010420  3.565556  51.411945        1.0  1575977820   \n 19180      2019010420  3.565556  51.411945        1.0  1575977820   \n 19181      2019010420  3.565556  51.411945        1.0  1575977820   \n 19182      2019010526  3.493889  51.719444        1.0  1576680180   \n \n             STATION  SMP_ID  NUCLIDE     VALUE  UNIT           UNC  DL  \n 0       Belgica-W01       1       33  0.200000     1           NaN   2  \n 1       Belgica-W02       2       33  0.270000     1           NaN   2  \n 2       Belgica-W03       3       33  0.260000     1           NaN   2  \n 3       Belgica-W04       4       33  0.250000     1           NaN   2  \n 4       Belgica-W05       5       33  0.200000     1           NaN   2  \n ...             ...     ...      ...       ...   ...           ...  ..  \n 19178         PETT5   19179       77  0.000005     1  2.600000e-07   1  \n 19179  VLISSGBISSVH   19180        1  6.152000     1  3.076000e-01   1  \n 19180  VLISSGBISSVH   19181       53  0.005390     1  1.078000e-03   1  \n 19181  VLISSGBISSVH   19182       54  0.001420     1  2.840000e-04   1  \n 19182     SCHOUWN10   19183        1  6.078000     1  3.039000e-01   1  \n \n [19183 rows x 12 columns]}\n\n\nLets review the biota data:\n\nnc_dfs_biota = dfs['BIOTA']\nnc_dfs_biota\n\n\n\n\n\n\n\n\nSMP_ID_PROVIDER\nLON\nLAT\nTIME\nSTATION\nSMP_ID\nNUCLIDE\nVALUE\nUNIT\nUNC\nDL\nSPECIES\nBODY_PART\n\n\n\n\n0\nDA 17531\n4.031111\n51.393333\n1267574400\nKloosterzande-Schelde\n1\n33\n0.326416\n5\nNaN\n2\n377\n1\n\n\n1\nDA 17534\n4.031111\n51.393333\n1276473600\nKloosterzande-Schelde\n2\n33\n0.442704\n5\nNaN\n2\n377\n1\n\n\n2\nDA 17537\n4.031111\n51.393333\n1285545600\nKloosterzande-Schelde\n3\n33\n0.412989\n5\nNaN\n2\n377\n1\n\n\n3\nDA 17540\n4.031111\n51.393333\n1291766400\nKloosterzande-Schelde\n4\n33\n0.202768\n5\nNaN\n2\n377\n1\n\n\n4\nDA 17531\n4.031111\n51.393333\n1267574400\nKloosterzande-Schelde\n5\n53\n0.652833\n5\nNaN\n2\n377\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n15946\nnan\n12.087778\n57.252499\n1660003200\nRinghals (R22)\n15947\n33\n0.384000\n5\n0.012096\n1\n272\n52\n\n\n15947\nnan\n12.107500\n57.306389\n1663891200\nRinghals (R23)\n15948\n33\n0.456000\n5\n0.012084\n1\n272\n52\n\n\n15948\nnan\n11.245000\n58.603333\n1667779200\nSW7\n15949\n33\n0.122000\n5\n0.031000\n1\n129\n19\n\n\n15949\nnan\n11.905278\n57.302502\n1663632000\nSW6a\n15950\n33\n0.310000\n5\nNaN\n2\n129\n19\n\n\n15950\nnan\n12.076667\n57.335278\n1662076800\nRinghals (R25)\n15951\n33\n0.306000\n5\n0.007191\n1\n96\n40\n\n\n\n\n15951 rows × 13 columns\n\n\n\nLets review the seawater data:\n\nnc_dfs_seawater = dfs['SEAWATER']\nnc_dfs_seawater\n\n\n\n\n\n\n\n\nSMP_ID_PROVIDER\nLON\nLAT\nSMP_DEPTH\nTIME\nSTATION\nSMP_ID\nNUCLIDE\nVALUE\nUNIT\nUNC\nDL\n\n\n\n\n0\nWNZ 01\n3.188056\n51.375278\n3.0\n1264550400\nBelgica-W01\n1\n33\n0.200000\n1\nNaN\n2\n\n\n1\nWNZ 02\n2.859444\n51.223610\n3.0\n1264550400\nBelgica-W02\n2\n33\n0.270000\n1\nNaN\n2\n\n\n2\nWNZ 03\n2.713611\n51.184444\n3.0\n1264550400\nBelgica-W03\n3\n33\n0.260000\n1\nNaN\n2\n\n\n3\nWNZ 04\n3.262222\n51.420277\n3.0\n1264550400\nBelgica-W04\n4\n33\n0.250000\n1\nNaN\n2\n\n\n4\nWNZ 05\n2.809722\n51.416111\n3.0\n1264464000\nBelgica-W05\n5\n33\n0.200000\n1\nNaN\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19178\n2019010074\n4.615278\n52.831944\n1.0\n1573649640\nPETT5\n19179\n77\n0.000005\n1\n2.600000e-07\n1\n\n\n19179\n2019010420\n3.565556\n51.411945\n1.0\n1575977820\nVLISSGBISSVH\n19180\n1\n6.152000\n1\n3.076000e-01\n1\n\n\n19180\n2019010420\n3.565556\n51.411945\n1.0\n1575977820\nVLISSGBISSVH\n19181\n53\n0.005390\n1\n1.078000e-03\n1\n\n\n19181\n2019010420\n3.565556\n51.411945\n1.0\n1575977820\nVLISSGBISSVH\n19182\n54\n0.001420\n1\n2.840000e-04\n1\n\n\n19182\n2019010526\n3.493889\n51.719444\n1.0\n1576680180\nSCHOUWN10\n19183\n1\n6.078000\n1\n3.039000e-01\n1\n\n\n\n\n19183 rows × 12 columns",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "handlers/ospar.html#data-format-conversion",
    "href": "handlers/ospar.html#data-format-conversion",
    "title": "OSPAR",
    "section": "Data Format Conversion",
    "text": "Data Format Conversion\nThe MARIS data processing workflow involves two key steps:\n\nNetCDF to Standardized CSV Compatible with OpenRefine Pipeline\n\nConvert standardized NetCDF files to CSV formats compatible with OpenRefine using the NetCDFDecoder.\nPreserve data integrity and variable relationships.\nMaintain standardized nomenclature and units.\n\nDatabase Integration\n\nProcess the converted CSV files using OpenRefine.\nApply data cleaning and standardization rules.\nExport validated data to the MARIS master database.\n\n\nThis section focuses on the first step: converting NetCDF files to a format suitable for OpenRefine processing using the NetCDFDecoder class.\n\ndecode(fname_in=fname_out, verbose=True)\n\nSaved BIOTA to ../../_data/output/191-OSPAR-2024_BIOTA.csv\nSaved SEAWATER to ../../_data/output/191-OSPAR-2024_SEAWATER.csv",
    "crumbs": [
      "Handlers",
      "OSPAR"
    ]
  },
  {
    "objectID": "api/metadata.html",
    "href": "api/metadata.html",
    "title": "Metadata",
    "section": "",
    "text": "source",
    "crumbs": [
      "API",
      "Metadata"
    ]
  },
  {
    "objectID": "api/metadata.html#how-to-use",
    "href": "api/metadata.html#how-to-use",
    "title": "Metadata",
    "section": "How to use",
    "text": "How to use\n\ndfs = pd.read_pickle('../files/pkl/dfs_test.pkl')\n\n\nkw = ['oceanography', 'Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides',\n      'Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure',\n      'Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments',\n      'Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes',\n      'Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants',\n      'Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish',\n      'Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems',\n      'Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks',\n      'Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans',\n      'Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)']\n\n\nfeed = GlobAttrsFeeder(dfs, cbs=[\n    BboxCB(),\n    DepthRangeCB(),\n    TimeRangeCB(),\n    ZoteroCB('26VMZZ2Q', cfg()),\n    KeyValuePairCB('keywords', ', '.join(kw))\n    ])\n\nattrs = feed(); attrs\n\n{'geospatial_lat_min': '179.9986',\n 'geospatial_lat_max': '89.9905',\n 'geospatial_lon_min': '-180.0',\n 'geospatial_lon_max': '-70.5744',\n 'geospatial_bounds': 'POLYGON ((-180 -70.5744, 179.9986 -70.5744, 179.9986 89.9905, -180 89.9905, -180 -70.5744))',\n 'geospatial_vertical_max': '5815.3',\n 'geospatial_vertical_min': '0.5',\n 'time_coverage_start': '2007-07-30T10:37:19',\n 'time_coverage_end': '2018-11-22T07:33:10',\n 'title': 'Environmental database - Helsinki Commission Monitoring of Radioactive Substances',\n 'summary': 'MORS Environment database has been used to collate data resulting from monitoring of environmental radioactivity in the Baltic Sea based on HELCOM Recommendation 26/3.\\n\\nThe database is structured according to HELCOM Guidelines on Monitoring of Radioactive Substances (https://www.helcom.fi/wp-content/uploads/2019/08/Guidelines-for-Monitoring-of-Radioactive-Substances.pdf), which specifies reporting format, database structure, data types and obligatory parameters used for reporting data under Recommendation 26/3.\\n\\nThe database is updated and quality assured annually by HELCOM MORS EG.',\n 'creator_name': '[{\"creatorType\": \"author\", \"name\": \"HELCOM MORS\"}]',\n 'keywords': 'oceanography, Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides, Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure, Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments, Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes, Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants, Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish, Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans, Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)'}",
    "crumbs": [
      "API",
      "Metadata"
    ]
  },
  {
    "objectID": "api/callbacks.html",
    "href": "api/callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "The Transformer class is designed to facilitate the application of a series of callbacks to a set of dataframes. It provides a structured way to apply transformations (i.e Callback) to the data, with a focus on flexibility and ease of use.\n\nsource\n\n\n\n Callback ()\n\nBase class for callbacks.\n\n\nExported source\nclass Callback(): \n    \"Base class for callbacks.\"\n    order = 0\n\n\n\nsource\n\n\n\n\n run_cbs (cbs:List[__main__.Callback], obj:Any)\n\nRun the callbacks in the order they are specified.\n\n\n\n\nType\nDetails\n\n\n\n\ncbs\nList\nList of callbacks to run\n\n\nobj\nAny\nObject to pass to the callbacks\n\n\n\n\n\nExported source\ndef run_cbs(\n    cbs: List[Callback], # List of callbacks to run\n    obj: Any # Object to pass to the callbacks\n    ):\n    \"Run the callbacks in the order they are specified.\"\n    for cb in sorted(cbs, key=attrgetter('order')):\n        if cb.__doc__: obj.logs.append(cb.__doc__)\n        cb(obj)\n\n\n\nsource\n\n\n\n\n Transformer (data:Union[Dict[str,pandas.core.frame.DataFrame],pandas.core\n              .frame.DataFrame],\n              cbs:Optional[List[__main__.Callback]]=None,\n              custom_maps:Dict=None, inplace:bool=False)\n\nTransform the dataframe(s) according to the specified callbacks.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nUnion\n\nData to be transformed\n\n\ncbs\nOptional\nNone\nList of callbacks to run\n\n\ncustom_maps\nDict\nNone\n\n\n\ninplace\nbool\nFalse\nWhether to modify the dataframe(s) in place\n\n\n\n\n\nExported source\nclass Transformer():\n    \"Transform the dataframe(s) according to the specified callbacks.\"\n    def __init__(self, \n                 data: Union[Dict[str, pd.DataFrame], pd.DataFrame], # Data to be transformed\n                 cbs: Optional[List[Callback]]=None, # List of callbacks to run\n                 custom_maps: Dict = None,\n                 inplace: bool=False # Whether to modify the dataframe(s) in place\n                 ): \n        fc.store_attr()\n        self.is_single_df = isinstance(data, pd.DataFrame)\n        self.df, self.dfs = self._prepare_data(data, inplace)\n        self.logs = []\n        self.custom_maps = custom_maps or defaultdict(lambda: defaultdict(dict))\n            \n    def _prepare_data(self, data, inplace):\n        if self.is_single_df:\n            return (data if inplace else data.copy()), None\n        else:\n            return None, (data if inplace else {k: v.copy() for k, v in data.items()})\n    \n    def unique(self, col_name: str) -&gt; np.ndarray:\n        \"Distinct values of a specific column present in all groups.\"\n        if self.is_single_df:\n            values = self.df.get(col_name, pd.Series()).dropna().values\n        else:\n            columns = [df.get(col_name) for df in self.dfs.values() if df.get(col_name) is not None]\n            values = np.concatenate([col.dropna().values for col in columns]) if columns else []\n        return np.unique(values)\n        \n    def __call__(self):\n        \"Transform the dataframe(s) according to the specified callbacks.\"\n        if self.cbs: run_cbs(self.cbs, self)\n        return self.df if self.dfs is None else self.dfs\n\n\nBelow, a few examples of how to use the Transformer class. Let’s define first a test callback that adds 1 to the depth:\n\nclass TestCB(Callback):\n    \"A test callback to add 1 to the depth.\"\n    def __call__(self, tfm: Transformer):\n        for grp, df in tfm.dfs.items(): \n            df['depth'] = df['depth'].apply(lambda x: x+1)\n\nAnd apply it to the following dataframes:\n\ndfs = {'biota': pd.DataFrame({'id': [0, 1, 2], 'species': [0, 2, 0], 'depth': [2, 3, 4]}),\n       'seawater': pd.DataFrame({'id': [0, 1, 2], 'depth': [3, 4, 5]})}\n\ntfm = Transformer(dfs, cbs=[TestCB()])\ndfs_test = tfm()\n\nfc.test_eq(dfs_test['biota']['depth'].to_list(), [3, 4, 5])\nfc.test_eq(dfs_test['seawater']['depth'].to_list(), [4, 5, 6])\n\n\nclass TestCB(Callback):\n    \"A test callback to add 1 to the depth.\"\n    def __call__(self, tfm: Transformer):\n        tfm.df['depth'] = tfm.df['depth'].apply(lambda x: x+1)\n\n\ndf = pd.DataFrame({'id': [0, 1, 2], 'species': [0, 2, 0], 'depth': [2, 3, 4]})\n\ntfm = Transformer(df, cbs=[TestCB()])\ndf_test = tfm()\n\nfc.test_eq(df_test['depth'].to_list(), [3, 4, 5])",
    "crumbs": [
      "API",
      "Callbacks"
    ]
  },
  {
    "objectID": "api/callbacks.html#core",
    "href": "api/callbacks.html#core",
    "title": "Callbacks",
    "section": "",
    "text": "The Transformer class is designed to facilitate the application of a series of callbacks to a set of dataframes. It provides a structured way to apply transformations (i.e Callback) to the data, with a focus on flexibility and ease of use.\n\nsource\n\n\n\n Callback ()\n\nBase class for callbacks.\n\n\nExported source\nclass Callback(): \n    \"Base class for callbacks.\"\n    order = 0\n\n\n\nsource\n\n\n\n\n run_cbs (cbs:List[__main__.Callback], obj:Any)\n\nRun the callbacks in the order they are specified.\n\n\n\n\nType\nDetails\n\n\n\n\ncbs\nList\nList of callbacks to run\n\n\nobj\nAny\nObject to pass to the callbacks\n\n\n\n\n\nExported source\ndef run_cbs(\n    cbs: List[Callback], # List of callbacks to run\n    obj: Any # Object to pass to the callbacks\n    ):\n    \"Run the callbacks in the order they are specified.\"\n    for cb in sorted(cbs, key=attrgetter('order')):\n        if cb.__doc__: obj.logs.append(cb.__doc__)\n        cb(obj)\n\n\n\nsource\n\n\n\n\n Transformer (data:Union[Dict[str,pandas.core.frame.DataFrame],pandas.core\n              .frame.DataFrame],\n              cbs:Optional[List[__main__.Callback]]=None,\n              custom_maps:Dict=None, inplace:bool=False)\n\nTransform the dataframe(s) according to the specified callbacks.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\nUnion\n\nData to be transformed\n\n\ncbs\nOptional\nNone\nList of callbacks to run\n\n\ncustom_maps\nDict\nNone\n\n\n\ninplace\nbool\nFalse\nWhether to modify the dataframe(s) in place\n\n\n\n\n\nExported source\nclass Transformer():\n    \"Transform the dataframe(s) according to the specified callbacks.\"\n    def __init__(self, \n                 data: Union[Dict[str, pd.DataFrame], pd.DataFrame], # Data to be transformed\n                 cbs: Optional[List[Callback]]=None, # List of callbacks to run\n                 custom_maps: Dict = None,\n                 inplace: bool=False # Whether to modify the dataframe(s) in place\n                 ): \n        fc.store_attr()\n        self.is_single_df = isinstance(data, pd.DataFrame)\n        self.df, self.dfs = self._prepare_data(data, inplace)\n        self.logs = []\n        self.custom_maps = custom_maps or defaultdict(lambda: defaultdict(dict))\n            \n    def _prepare_data(self, data, inplace):\n        if self.is_single_df:\n            return (data if inplace else data.copy()), None\n        else:\n            return None, (data if inplace else {k: v.copy() for k, v in data.items()})\n    \n    def unique(self, col_name: str) -&gt; np.ndarray:\n        \"Distinct values of a specific column present in all groups.\"\n        if self.is_single_df:\n            values = self.df.get(col_name, pd.Series()).dropna().values\n        else:\n            columns = [df.get(col_name) for df in self.dfs.values() if df.get(col_name) is not None]\n            values = np.concatenate([col.dropna().values for col in columns]) if columns else []\n        return np.unique(values)\n        \n    def __call__(self):\n        \"Transform the dataframe(s) according to the specified callbacks.\"\n        if self.cbs: run_cbs(self.cbs, self)\n        return self.df if self.dfs is None else self.dfs\n\n\nBelow, a few examples of how to use the Transformer class. Let’s define first a test callback that adds 1 to the depth:\n\nclass TestCB(Callback):\n    \"A test callback to add 1 to the depth.\"\n    def __call__(self, tfm: Transformer):\n        for grp, df in tfm.dfs.items(): \n            df['depth'] = df['depth'].apply(lambda x: x+1)\n\nAnd apply it to the following dataframes:\n\ndfs = {'biota': pd.DataFrame({'id': [0, 1, 2], 'species': [0, 2, 0], 'depth': [2, 3, 4]}),\n       'seawater': pd.DataFrame({'id': [0, 1, 2], 'depth': [3, 4, 5]})}\n\ntfm = Transformer(dfs, cbs=[TestCB()])\ndfs_test = tfm()\n\nfc.test_eq(dfs_test['biota']['depth'].to_list(), [3, 4, 5])\nfc.test_eq(dfs_test['seawater']['depth'].to_list(), [4, 5, 6])\n\n\nclass TestCB(Callback):\n    \"A test callback to add 1 to the depth.\"\n    def __call__(self, tfm: Transformer):\n        tfm.df['depth'] = tfm.df['depth'].apply(lambda x: x+1)\n\n\ndf = pd.DataFrame({'id': [0, 1, 2], 'species': [0, 2, 0], 'depth': [2, 3, 4]})\n\ntfm = Transformer(df, cbs=[TestCB()])\ndf_test = tfm()\n\nfc.test_eq(df_test['depth'].to_list(), [3, 4, 5])",
    "crumbs": [
      "API",
      "Callbacks"
    ]
  },
  {
    "objectID": "api/callbacks.html#geographical",
    "href": "api/callbacks.html#geographical",
    "title": "Callbacks",
    "section": "Geographical",
    "text": "Geographical\nThis section gathers callbacks that are used to transform the geographical coordinates.\n\nsource\n\nSanitizeLonLatCB\n\n SanitizeLonLatCB (lon_col:str='LON', lat_col:str='LAT',\n                   verbose:bool=False)\n\nDrop rows with invalid longitude & latitude values. Convert , separator to . separator.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlon_col\nstr\nLON\nLongitude column name\n\n\nlat_col\nstr\nLAT\nLatitude column name\n\n\nverbose\nbool\nFalse\nWhether to print the number of invalid longitude & latitude values\n\n\n\n\n\nExported source\nclass SanitizeLonLatCB(Callback):\n    \"Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.\"\n    def __init__(self, \n                 lon_col: str='LON', # Longitude column name\n                 lat_col: str='LAT', # Latitude column name\n                 verbose: bool=False # Whether to print the number of invalid longitude & latitude values\n                 ):\n        fc.store_attr()\n        \n    def __call__(self, tfm: Transformer):\n        for grp, df in tfm.dfs.items():\n            # Convert `,` separator to `.` separator\n            df[self.lon_col] = df[self.lon_col].apply(lambda x: float(str(x).replace(',', '.')))\n            df[self.lat_col] = df[self.lat_col].apply(lambda x: float(str(x).replace(',', '.')))\n            \n            # Mask zero values\n            mask_zeroes = (df[self.lon_col] == 0) & (df[self.lat_col] == 0) \n            nZeroes = mask_zeroes.sum()\n            if nZeroes and self.verbose: \n                print(f'The \"{grp}\" group contains {nZeroes} data points whose ({self.lon_col}, {self.lat_col}) = (0, 0)')\n            \n            # Mask out of bounds values\n            mask_goob = (df[self.lon_col] &lt; -180) | (df[self.lon_col] &gt; 180) | (df[self.lat_col] &lt; -90) | (df[self.lat_col] &gt; 90)\n            nGoob = mask_goob.sum()\n            if nGoob and self.verbose: \n                print(f'The \"{grp}\" group contains {nGoob} data points with unrealistic {self.lon_col} or {self.lat_col} values.')\n                \n            tfm.dfs[grp] = df.loc[~(mask_zeroes | mask_goob)]\n\n\n\n# Check that measurements located at (0,0) get removed\ndfs = {'BIOTA': pd.DataFrame({'LON': [0, 1, 0], 'LAT': [0, 2, 0]})}\ntfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\ntfm()['BIOTA']\n\nexpected = [1., 2.]\nfc.test_eq(tfm()['BIOTA'].iloc[0].to_list(), expected)\n\n\n# Check that comma decimal separator get replaced by point instead\ndfs = {'BIOTA': pd.DataFrame({'LON': ['45,2'], 'LAT': ['43,1']})}\ntfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\ntfm()['BIOTA']\n\nexpected = [45.2, 43.1]\nfc.test_eq(tfm()['BIOTA'].iloc[0].to_list(), expected)\n\n\n# Check that out of bounds lon or lat get removed\ndfs = {'BIOTA': pd.DataFrame({'LON': [-190, 190, 1, 2, 1.1], 'LAT': [1, 2, 91, -91, 2.2]})}\ntfm = Transformer(dfs, cbs=[SanitizeLonLatCB()])\ntfm()['BIOTA']\n\nexpected = [1.1, 2.2]\nfc.test_eq(tfm()['BIOTA'].iloc[0].to_list(), expected)",
    "crumbs": [
      "API",
      "Callbacks"
    ]
  },
  {
    "objectID": "api/callbacks.html#map-standardize",
    "href": "api/callbacks.html#map-standardize",
    "title": "Callbacks",
    "section": "Map & Standardize",
    "text": "Map & Standardize\n\nsource\n\nRemapCB\n\n RemapCB (fn_lut:Callable, col_remap:str, col_src:str,\n          dest_grps:list[str]|str=dict_keys(['BIOTA', 'SEAWATER',\n          'SEDIMENT', 'SUSPENDED_MATTER']), default_value:Any=0,\n          verbose:bool=False)\n\nGeneric MARIS remapping callback.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfn_lut\nCallable\n\nFunction that returns the lookup table dictionary\n\n\ncol_remap\nstr\n\nName of the column to remap\n\n\ncol_src\nstr\n\nName of the column with the source values\n\n\ndest_grps\nlist[str] | str\ndict_keys([‘BIOTA’, ‘SEAWATER’, ‘SEDIMENT’, ‘SUSPENDED_MATTER’])\nList of destination groups\n\n\ndefault_value\nAny\n0\nDefault value for unmatched entries\n\n\nverbose\nbool\nFalse\nWhether to print the number of unmatched entries\n\n\n\n\n\nExported source\nclass RemapCB(Callback):\n    \"Generic MARIS remapping callback.\"\n    def __init__(self, \n                 fn_lut: Callable, # Function that returns the lookup table dictionary\n                 col_remap: str, # Name of the column to remap\n                 col_src: str, # Name of the column with the source values\n                 dest_grps: list[str]|str=NC_GROUPS.keys(), # List of destination groups\n                 default_value: Any = 0, # Default value for unmatched entries\n                 verbose: bool=False # Whether to print the number of unmatched entries\n                ):\n        fc.store_attr()\n        self.lut = None\n        if isinstance(dest_grps, str): self.dest_grps = [dest_grps]\n        # Format the documentation string based on the type and content of dest_grps\n        if isinstance(self.dest_grps, list):\n            if len(self.dest_grps) &gt; 1:\n                grp_str = ', '.join(self.dest_grps[:-1]) + ' and ' + self.dest_grps[-1]\n            else:\n                grp_str = self.dest_grps[0]\n        else:\n            grp_str = self.dest_grps\n                \n        self.__doc__ = f\"Remap values from '{col_src}' to '{col_remap}' for groups: {grp_str}.\"\n\n    def __call__(self, tfm):\n        self.lut = self.fn_lut()\n        for grp in self.dest_grps:\n            if grp in tfm.dfs:\n                self._remap_group(tfm.dfs[grp])\n            else:\n                print(f\"Group {grp} not found in the dataframes.\")\n\n    def _remap_group(self, df: pd.DataFrame):\n        df[self.col_remap] = df[self.col_src].apply(self._remap_value)\n\n    def _remap_value(self, value: str) -&gt; Any:\n        value = value.strip() if isinstance(value, str) else value\n        match = self.lut.get(value, Match(self.default_value, None, None, None))\n        if isinstance(match, Match):\n            if match.matched_id == self.default_value:\n                if self.verbose:\n                    print(f\"Unmatched value: {value}\")\n            return match.matched_id \n        else:\n            return match\n\n\n\nsource\n\n\nLowerStripNameCB\n\n LowerStripNameCB (col_src:str, col_dst:str=None,\n                   fn_transform:Callable=&lt;function &lt;lambda&gt;&gt;)\n\nConvert values to lowercase and strip any trailing spaces.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncol_src\nstr\n\nSource column name e.g. ‘Nuclide’\n\n\ncol_dst\nstr\nNone\nDestination column name\n\n\nfn_transform\nCallable\n\nTransformation function\n\n\n\n\n\nExported source\nclass LowerStripNameCB(Callback):\n    \"Convert values to lowercase and strip any trailing spaces.\"\n    def __init__(self, \n                 col_src: str, # Source column name e.g. 'Nuclide'\n                 col_dst: str=None, # Destination column name\n                 fn_transform: Callable=lambda x: x.lower().strip() # Transformation function\n                 ):\n        fc.store_attr()\n        self.__doc__ = f\"Convert '{col_src}' column values to lowercase, strip spaces, and store in '{col_dst}' column.\"\n        if not col_dst: self.col_dst = col_src\n        \n    def _safe_transform(self, value):\n        \"Ensure value is not NA and apply transformation function.\"\n        return value if pd.isna(value) else self.fn_transform(str(value))\n            \n    def __call__(self, tfm):\n        for key in tfm.dfs.keys():\n            tfm.dfs[key][self.col_dst] = tfm.dfs[key][self.col_src].apply(self._safe_transform)\n\n\nLet’s test the callback:\n\ndfs = {'seawater': pd.DataFrame({'Nuclide': ['CS137', '226RA']})}\n\ntfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='Nuclide', col_dst='NUCLIDE')])\nfc.test_eq(tfm()['seawater']['NUCLIDE'].to_list(), ['cs137', '226ra'])\n\n\ntfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='Nuclide')])\nfc.test_eq(tfm()['seawater']['Nuclide'].to_list(), ['cs137', '226ra'])\n\nThe point is when (semi-automatic) remapping names generally:\n\nwe need first to guess (fuzzy matching or other) the right nuclide name.\nThen manually check the result and eventually update the lookup table.\nFinally we can apply the lookup table to the dataframe.",
    "crumbs": [
      "API",
      "Callbacks"
    ]
  },
  {
    "objectID": "api/callbacks.html#change-structure",
    "href": "api/callbacks.html#change-structure",
    "title": "Callbacks",
    "section": "Change structure",
    "text": "Change structure\n\nsource\n\nAddSampleTypeIdColumnCB\n\n AddSampleTypeIdColumnCB (lut:dict={'SEAWATER': 1, 'BIOTA': 2, 'SEDIMENT':\n                          3, 'SUSPENDED_MATTER': 4},\n                          col_name:str='SAMPLE_TYPE')\n\nBase class for callbacks.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlut\ndict\n{‘SEAWATER’: 1, ‘BIOTA’: 2, ‘SEDIMENT’: 3, ‘SUSPENDED_MATTER’: 4}\nLookup table for sample type\n\n\ncol_name\nstr\nSAMPLE_TYPE\nColumn name to store the sample type id\n\n\n\n\n\nExported source\nclass AddSampleTypeIdColumnCB(Callback):\n    def __init__(self, \n                 lut: dict=SMP_TYPE_LUT, # Lookup table for sample type\n                 col_name: str='SAMPLE_TYPE' # Column name to store the sample type id\n                 ): \n        \"Add a column with the sample type as defined in the CDL.\"\n        fc.store_attr()\n        \n    def __call__(self, tfm):\n        for grp, df in tfm.dfs.items():             \n            df[self.col_name] = self.lut[grp]\n\n\nLet’s test the callback:\n\ndfs = {smp_type: pd.DataFrame({'col_test': [0, 1, 2]}) for smp_type in SMP_TYPE_LUT.keys()};\n\ntfm = Transformer(dfs, cbs=[AddSampleTypeIdColumnCB()])\ndfs_test = tfm()\n\nfor smp_type in SMP_TYPE_LUT.keys():\n    fc.test_eq(dfs_test[smp_type]['SAMPLE_TYPE'].unique().item(), SMP_TYPE_LUT[smp_type])\n\n\n''' 05 Feb 2025 - NM - AddNuclideIdColumnCB is not used anymore.\n#| exports\nclass AddNuclideIdColumnCB(Callback):\n    def __init__(self, \n                 col_value: str, # Column name containing the nuclide name\n                 lut_fname_fn: Callable=nuc_lut_path, # Function returning the lut path\n                 col_name: str='nuclide_id' # Column name to store the nuclide id\n                 ): \n        \"Add a column with the nuclide id.\"\n        fc.store_attr()\n        self.lut = get_lut(lut_fname_fn().parent, lut_fname_fn().name, \n                           key='nc_name', value='nuclide_id', reverse=False)\n        \n    def __call__(self, tfm: Transformer):\n        for grp, df in tfm.dfs.items(): \n            df[self.col_name] = df[self.col_value].map(self.lut)\n'''\n\n' 05 Feb 2025 - NM - AddNuclideIdColumnCB is not used anymore.\\n#| exports\\nclass AddNuclideIdColumnCB(Callback):\\n    def __init__(self, \\n                 col_value: str, # Column name containing the nuclide name\\n                 lut_fname_fn: Callable=nuc_lut_path, # Function returning the lut path\\n                 col_name: str=\\'nuclide_id\\' # Column name to store the nuclide id\\n                 ): \\n        \"Add a column with the nuclide id.\"\\n        fc.store_attr()\\n        self.lut = get_lut(lut_fname_fn().parent, lut_fname_fn().name, \\n                           key=\\'nc_name\\', value=\\'nuclide_id\\', reverse=False)\\n        \\n    def __call__(self, tfm: Transformer):\\n        for grp, df in tfm.dfs.items(): \\n            df[self.col_name] = df[self.col_value].map(self.lut)\\n'\n\n\n\nsource\n\n\nSelectColumnsCB\n\n SelectColumnsCB (cois:dict)\n\nSelect columns of interest.\n\n\n\n\nType\nDetails\n\n\n\n\ncois\ndict\nColumns of interest\n\n\n\n\n\nExported source\nclass SelectColumnsCB(Callback):\n    \"Select columns of interest.\"\n    def __init__(self, \n                 cois: dict # Columns of interest\n                 ): \n        fc.store_attr()\n        \n    def __call__(self, tfm):\n        \"Select columns of interest.\"\n        for grp, df in tfm.dfs.items(): \n            tfm.dfs[grp] = df.loc[:, self.cois.keys()]\n\n\n\nsource\n\n\nRenameColumnsCB\n\n RenameColumnsCB (renaming_rules:dict)\n\nRenaming variables to MARIS standard names.\n\n\n\n\nType\nDetails\n\n\n\n\nrenaming_rules\ndict\nRenaming rules\n\n\n\n\n\nExported source\nclass RenameColumnsCB(Callback):\n    \"Renaming variables to MARIS standard names.\"\n    def __init__(self,\n                 renaming_rules: dict # Renaming rules\n                 ): \n        fc.store_attr()\n        \n    def __call__(self, tfm):\n        for grp in tfm.dfs.keys(): \n            tfm.dfs[grp].rename(columns=self.renaming_rules, inplace=True)\n\n\n\nsource\n\n\nRemoveAllNAValuesCB\n\n RemoveAllNAValuesCB (cols_to_check:Union[Dict[str,list],list],\n                      how:str='all')\n\nRemove rows with all NA values in specified columns.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncols_to_check\nUnion\n\nDict or list of columns to check\n\n\nhow\nstr\nall\nHow to handle NA values ‘all’ or ‘any’\n\n\n\n\n\nExported source\nclass RemoveAllNAValuesCB(Callback):\n    \"Remove rows with all NA values in specified columns.\"\n    def __init__(self, \n                 cols_to_check: Union[Dict[str, list], list],  # Dict or list of columns to check\n                 how: str='all'  # How to handle NA values 'all' or 'any'\n                ):\n        fc.store_attr()\n\n    def __call__(self, tfm):\n        # Convert list to dict if cols_to_check is a list\n        cols_dict = (self.cols_to_check if isinstance(self.cols_to_check, dict) \n                    else {k: self.cols_to_check for k in tfm.dfs.keys()})\n        \n        for sample_type, columns in cols_dict.items():\n            tfm.dfs[sample_type].dropna(\n                subset=columns,\n                how=self.how,\n                inplace=True\n            )\n\n\nLet’s test the callback:\n\ndef test_remove_all_na_values_cb():\n    \"\"\"Test RemoveAllNAValuesCB with both dict and list inputs.\"\"\"\n    # Create sample data\n    sample_data = {\n        'SEAWATER': pd.DataFrame({\n            'value': [1.0, np.nan, np.nan, 4.0],\n            'uncertainty': [0.1, np.nan, np.nan, 0.4],\n            'other_col': ['a', 'b', 'c', 'd']\n        }),\n        'SEDIMENT': pd.DataFrame({\n            'value': [np.nan, 2.0, np.nan, np.nan],\n            'uncertainty': [np.nan, 0.2, np.nan, np.nan],\n            'meta': ['x', 'y', 'z', 'w']\n        })\n    }\n    \n    tfm = Transformer(sample_data)\n    \n    # Test with list input\n    cb1 = RemoveAllNAValuesCB(cols_to_check=['value', 'uncertainty'], how='all')\n    tfm_list = copy.deepcopy(tfm)\n    cb1(tfm_list)\n    \n    # Test with dict input\n    cb2 = RemoveAllNAValuesCB(\n        cols_to_check={'SEAWATER': ['value', 'uncertainty'], \n                       'SEDIMENT': ['value', 'uncertainty']},\n        how='all'\n    )\n    tfm_dict = copy.deepcopy(tfm)\n    cb2(tfm_dict)\n    \n    # Verify both approaches give same results\n    for k in tfm.dfs.keys():\n        assert tfm_list.dfs[k].equals(tfm_dict.dfs[k])\n        assert len(tfm_list.dfs[k]) == len(tfm_dict.dfs[k])\n\n\nsource\n\n\nCompareDfsAndTfmCB\n\n CompareDfsAndTfmCB (dfs:Dict[str,pandas.core.frame.DataFrame])\n\nCreate a dataframe of removed data and track changes in row counts due to transformations.\n\n\n\n\nType\nDetails\n\n\n\n\ndfs\nDict\nOriginal dataframes\n\n\n\n\n\nExported source\nclass CompareDfsAndTfmCB(Callback):\n    \"Create a dataframe of removed data and track changes in row counts due to transformations.\"\n    def __init__(self, \n                 dfs: Dict[str, pd.DataFrame]  # Original dataframes\n                 ): \n        fc.store_attr()\n        \n    def __call__(self, tfm: Transformer) -&gt; None:\n        self._initialize_tfm_attributes(tfm)\n        for grp in tfm.dfs.keys():\n            self._compute_changes(grp, tfm)\n\n    def _initialize_tfm_attributes(self, tfm: Transformer) -&gt; None:\n        tfm.dfs_removed = {}\n        tfm.compare_stats = {}\n\n    def _compute_changes(self, \n                         grp: str,  # The group key\n                         tfm: Transformer  # The transformation object containing `dfs`\n                        ) -&gt; None:\n        \"Compute and store changes including data removed and created during transformation.\"\n        original_df = self.dfs[grp]\n        transformed_df = tfm.dfs[grp]\n\n        # Calculate differences\n        original_count = len(original_df.index)\n        transformed_count = len(transformed_df.index)\n        removed_count = len(original_df.index.difference(transformed_df.index))\n        created_count = len(transformed_df.index.difference(original_df.index))\n\n        # Store results\n        tfm.dfs_removed[grp] = original_df.loc[original_df.index.difference(transformed_df.index)]\n        tfm.compare_stats[grp] = {\n            'Original row count (dfs)': original_count,\n            'Transformed row count (tfm.dfs)': transformed_count,\n            'Rows removed from original (tfm.dfs_removed)': removed_count,\n            'Rows created in transformed (tfm.dfs_created)': created_count\n        }\n\n\nCompareDfsAndTfmCB compares the original dataframes to the transformed dataframe. A dictionary of dataframes, tfm.dfs_dropped, is created to include the data present in the original dataset but absent from the transformed data. tfm.compare_stats provides a quick overview of the number of rows in both the original dataframes and the transformed dataframe.\nFor instance:\n\nsource\n\n\nUniqueIndexCB\n\n UniqueIndexCB (index_name='ID')\n\nSet unique index for each group.\n\n\nExported source\nclass UniqueIndexCB(Callback):\n    \"Set unique index for each group.\"\n    def __init__(self,\n                 index_name='ID'):\n        fc.store_attr()\n        \n    def __call__(self, tfm):\n        for k in tfm.dfs.keys():\n            # Reset the index of the DataFrame and drop the old index\n            tfm.dfs[k] = tfm.dfs[k].reset_index(drop=True)\n            # Reset the index again and set the name of the new index to `ìndex_name``\n            tfm.dfs[k] = tfm.dfs[k].reset_index(names=[self.index_name])",
    "crumbs": [
      "API",
      "Callbacks"
    ]
  },
  {
    "objectID": "api/callbacks.html#time",
    "href": "api/callbacks.html#time",
    "title": "Callbacks",
    "section": "Time",
    "text": "Time\n\nsource\n\nEncodeTimeCB\n\n EncodeTimeCB (col_time:str='TIME', fn_units:Callable=&lt;function\n               get_time_units&gt;)\n\nEncode time as seconds since epoch.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncol_time\nstr\nTIME\n\n\n\nfn_units\nCallable\nget_time_units\nFunction returning the time units\n\n\n\n\n\nExported source\nclass EncodeTimeCB(Callback):\n    \"Encode time as seconds since epoch.\"    \n    def __init__(self, \n                 col_time: str='TIME',\n                 fn_units: Callable=get_time_units # Function returning the time units\n                 ): \n        fc.store_attr()\n        self.units = fn_units()\n    \n    def __call__(self, tfm): \n        for grp, df in tfm.dfs.items():\n            n_missing = df[self.col_time].isna().sum()\n            if n_missing:\n                print(f\"Warning: {n_missing} missing time value(s) in {grp}\")\n            \n            # Remove NaN times and convert to seconds since epoch\n            tfm.dfs[grp] = tfm.dfs[grp][tfm.dfs[grp][self.col_time].notna()]\n            tfm.dfs[grp][self.col_time] = tfm.dfs[grp][self.col_time].apply(lambda x: date2num(x, units=self.units))\n\n\n\ndfs_test = {\n    'SEAWATER': pd.DataFrame({\n        'TIME': [pd.Timestamp(f'2023-01-0{t}') for t in [1, 2]],\n        'value': [1, 2]\n        }),\n    'SEDIMENT': pd.DataFrame({\n        'TIME': [pd.Timestamp(f'2023-01-0{t}') for t in [3, 4]],\n        'value': [3, 4]\n        }),\n}\n\nunits = 'seconds since 1970-01-01 00:00:00.0'\ntfm = Transformer(dfs_test, cbs=[\n    EncodeTimeCB(fn_units=lambda: units)\n    ], inplace=False)\ndfs_result = tfm()\n\nfc.test_eq(dfs_result['SEAWATER'].TIME.dtype, 'int64')\nfc.test_eq(dfs_result['SEDIMENT'].TIME.dtype, 'int64')\n\n\nfc.test_eq(dfs_result['SEAWATER'].TIME, dfs_test['SEAWATER'].TIME.apply(lambda x: date2num(x, units=units)))\nfc.test_eq(dfs_result['SEDIMENT'].TIME, dfs_test['SEDIMENT'].TIME.apply(lambda x: date2num(x, units=units)))\n\n\nsource\n\n\nDecodeTimeCB\n\n DecodeTimeCB (col_time:str='TIME', fn_units:Callable=&lt;function\n               get_time_units&gt;)\n\nDecode time from seconds since epoch to datetime format.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncol_time\nstr\nTIME\n\n\n\nfn_units\nCallable\nget_time_units\nFunction returning the time units\n\n\n\n\n\nExported source\nclass DecodeTimeCB(Callback):\n    \"Decode time from seconds since epoch to datetime format.\"    \n    def __init__(self, \n                 col_time: str='TIME',\n                 fn_units: Callable=get_time_units # Function returning the time units\n                 ): \n        fc.store_attr()\n        self.units = fn_units()\n\n    def __call__(self, tfm): \n        for grp, df in tfm.dfs.items():\n            n_missing = df[self.col_time].isna().sum()\n            if n_missing:\n                print(f\"Warning: {n_missing} missing time value(s) in {grp}.\")\n            \n            # Remove NaN times and convert to datetime\n            tfm.dfs[grp] = tfm.dfs[grp][tfm.dfs[grp][self.col_time].notna()]\n            tfm.dfs[grp][self.col_time] = df[self.col_time].apply(\n                lambda x: num2date(x, units=self.units, only_use_cftime_datetimes=False)\n            )\n\n\n\ndfs_test = {\n    'SEAWATER': pd.DataFrame({\n        'TIME': [1672531200, 1672617600],  # 2023-01-01, 2023-01-02 in seconds since epoch\n        'value': [1, 2]\n        }),\n    'SEDIMENT': pd.DataFrame({\n        'TIME': [1672704000, 1672790400],  # 2023-01-03, 2023-01-04 in seconds since epoch\n        'value': [3, 4]\n        }),\n}\n\nunits = 'seconds since 1970-01-01 00:00:00.0'\ntfm = Transformer(dfs_test, cbs=[\n    DecodeTimeCB(fn_units=lambda: units)\n    ], inplace=False)\ndfs_result = tfm()\n\n\n# Test that times were converted to datetime\nfc.test_eq(dfs_result['SEAWATER'].TIME.dtype, 'datetime64[ns]')\nfc.test_eq(dfs_result['SEDIMENT'].TIME.dtype, 'datetime64[ns]')\n\n# Test specific datetime values\nexpected_times_seawater = pd.to_datetime(['2023-01-01', '2023-01-02'])\nexpected_times_sediment = pd.to_datetime(['2023-01-03', '2023-01-04'])\n\nfc.test_eq(dfs_result['SEAWATER'].TIME.dt.date, expected_times_seawater.date)\nfc.test_eq(dfs_result['SEDIMENT'].TIME.dt.date, expected_times_sediment.date)",
    "crumbs": [
      "API",
      "Callbacks"
    ]
  },
  {
    "objectID": "api/utils.html",
    "href": "api/utils.html",
    "title": "Utilities",
    "section": "",
    "text": "We define below useful constants throughout the package.\nExported source\n# TBD: move to configs\nNA = 'Not available'",
    "crumbs": [
      "API",
      "Utilities"
    ]
  },
  {
    "objectID": "api/utils.html#core",
    "href": "api/utils.html#core",
    "title": "Utilities",
    "section": "Core",
    "text": "Core\nAbstracting some common operations.\n\nsource\n\nget_unique_across_dfs\n\n get_unique_across_dfs (dfs:Dict[str,pandas.core.frame.DataFrame],\n                        col_name:str='NUCLIDE', as_df:bool=False,\n                        include_nchars:bool=False)\n\nGet a list of unique column values across dataframes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndfs\nDict\n\nDictionary of dataframes\n\n\ncol_name\nstr\nNUCLIDE\nColumn name to extract unique values from\n\n\nas_df\nbool\nFalse\nReturn a DataFrame of unique values\n\n\ninclude_nchars\nbool\nFalse\nAdd a column with the number of characters in the value\n\n\nReturns\nList\n\nReturns a list of unique column values across dataframes\n\n\n\n\n\nExported source\ndef get_unique_across_dfs(dfs: Dict[str, pd.DataFrame],  # Dictionary of dataframes\n                          col_name: str='NUCLIDE', # Column name to extract unique values from\n                          as_df: bool=False, # Return a DataFrame of unique values\n                          include_nchars: bool=False # Add a column with the number of characters in the value\n                          ) -&gt; List[str]: # Returns a list of unique column values across dataframes\n    \"Get a list of unique column values across dataframes.\"\n    unique_values = list(set().union(*(df[col_name].unique() for df in dfs.values() if col_name in df.columns)))\n    if not as_df:\n        return unique_values\n    else:\n        df_uniques = pd.DataFrame(unique_values, columns=['value']).reset_index()\n        if include_nchars: df_uniques['n_chars'] = df_uniques['value'].str.len()\n        return df_uniques\n\n\nExample of use:\n\ndfs_test = {'SEAWATER': pd.DataFrame({'NUCLIDE': ['cs137', 'cs134_137_tot', 'cs134_137_tot']}),\n            'BIOTA': pd.DataFrame({'NUCLIDE': ['cs137', 'cs134', 'cs134_137_tot']}),\n            'SEDIMENT': pd.DataFrame({'NUCLIDE': ['cs134_137_tot', 'cs134_137_tot', 'cs134_137_tot']})}\n\nfc.test_eq(set(get_unique_across_dfs(dfs_test, col_name='NUCLIDE')), \n           set(['cs134', 'cs137', 'cs134_137_tot']))\n\nWhat if the column name is not in one of the dataframe?\n\ndfs_test = {'SEAWATER': pd.DataFrame({'NUCLIDE': ['cs137', 'cs134_137_tot', 'cs134_137_tot']}),\n            'BIOTA': pd.DataFrame({'NUCLIDE': ['cs137', 'cs134', 'cs134_137_tot']}),\n            'SEDIMENT': pd.DataFrame({'NONUCLIDE': ['cs134_137_tot', 'cs134_137_tot', 'cs134_137_tot']})}\n\nfc.test_eq(set(get_unique_across_dfs(dfs_test, col_name='NUCLIDE')), \n           set(['cs134', 'cs137', 'cs134_137_tot']))\n\n\nget_unique_across_dfs(dfs_test, col_name='NUCLIDE', as_df=True, include_nchars=True)\n\n\n\n\n\n\n\n\nindex\nvalue\nn_chars\n\n\n\n\n0\n0\ncs134_137_tot\n13\n\n\n1\n1\ncs137\n5\n\n\n2\n2\ncs134\n5\n\n\n\n\n\n\n\n\nsource\n\n\nRemapper\n\n Remapper (provider_lut_df:pandas.core.frame.DataFrame,\n           maris_lut_fn:Union[Callable,pandas.core.frame.DataFrame],\n           maris_col_id:str, maris_col_name:str,\n           provider_col_to_match:str, provider_col_key:str,\n           fname_cache:str)\n\nRemap a data provider lookup table to a MARIS lookup table using fuzzy matching.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nprovider_lut_df\nDataFrame\nData provider lookup table to be remapped\n\n\nmaris_lut_fn\nUnion\nMARIS lookup table or function returning the path\n\n\nmaris_col_id\nstr\nMARIS lookup table column name for the id\n\n\nmaris_col_name\nstr\nMARIS lookup table column name for the name\n\n\nprovider_col_to_match\nstr\nData provider lookup table column name for the name to match\n\n\nprovider_col_key\nstr\nData provider lookup table column name for the key\n\n\nfname_cache\nstr\nCache file name\n\n\n\n\n\nExported source\nclass Remapper():\n    \"Remap a data provider lookup table to a MARIS lookup table using fuzzy matching.\"\n    def __init__(self,\n                 provider_lut_df: pd.DataFrame, # Data provider lookup table to be remapped\n                 maris_lut_fn: Union[Callable, pd.DataFrame], # MARIS lookup table or function returning the path\n                 maris_col_id: str, # MARIS lookup table column name for the id\n                 maris_col_name: str, # MARIS lookup table column name for the name\n                 provider_col_to_match: str, # Data provider lookup table column name for the name to match\n                 provider_col_key: str, # Data provider lookup table column name for the key\n                 fname_cache: str # Cache file name\n                 ):\n        fc.store_attr()\n        self.cache_file = cache_path() / fname_cache\n        # Check if maris_lut is a callable function or already a DataFrame\n        if callable(maris_lut_fn):\n            self.maris_lut = maris_lut_fn()\n        else:\n            self.maris_lut = maris_lut_fn\n        self.lut = {}\n\n    def generate_lookup_table(self, \n                              fixes={}, # Lookup table fixes\n                              as_df=True, # Whether to return a DataFrame\n                              overwrite=True):\n        \"Generate a lookup table from a data provider lookup table to a MARIS lookup table using fuzzy matching.\"\n        self.fixes = fixes\n        self.as_df = as_df\n        if overwrite or not self.cache_file.exists():\n            self._create_lookup_table()\n            fc.save_pickle(self.cache_file, self.lut)\n        else:\n            self.lut = fc.load_pickle(self.cache_file)\n\n        return self._format_output()\n\n    def _create_lookup_table(self):\n        df = self.provider_lut_df\n        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"): \n            self._process_row(row)\n\n    def _process_row(self, row):\n        value_to_match = row[self.provider_col_to_match]\n        if isinstance(value_to_match, str):  # Only process if value is a string\n            # If value is in fixes, use the fixed value\n            name_to_match = self.fixes.get(value_to_match, value_to_match)\n            result = match_maris_lut(self.maris_lut, name_to_match, self.maris_col_id, self.maris_col_name).iloc[0]\n            match = Match(result[self.maris_col_id], result[self.maris_col_name], \n                          value_to_match, result['score'])\n            self.lut[row[self.provider_col_key]] = match\n        else:\n            # Handle non-string values (e.g., NaN)\n            self.lut[row[self.provider_col_key]] = Match(-1, \"Unknown\", value_to_match, 0)\n            \n    def select_match(self, match_score_threshold:int=1, verbose:bool=False):\n        if verbose:\n            matched_len= len([v for v in self.lut.values() if v.match_score &lt; match_score_threshold])\n            print(f\"{matched_len} entries matched the criteria, while {len(self.lut) - matched_len} entries had a match score of {match_score_threshold} or higher.\")\n        \n        self.lut = {k: v for k, v in self.lut.items() if v.match_score &gt;= match_score_threshold}\n        return self._format_output()\n\n    def _format_output(self):\n        if not self.as_df: return self.lut\n        df_lut = pd.DataFrame.from_dict(self.lut, orient='index', \n                                        columns=['matched_maris_name', 'source_name', 'match_score'])\n        df_lut.index.name = 'source_key'\n        return df_lut.sort_values(by='match_score', ascending=False)",
    "crumbs": [
      "API",
      "Utilities"
    ]
  },
  {
    "objectID": "api/utils.html#validation",
    "href": "api/utils.html#validation",
    "title": "Utilities",
    "section": "Validation",
    "text": "Validation\n\nsource\n\nhas_valid_varname\n\n has_valid_varname (var_names:List[str], cdl_path:str,\n                    group:Optional[str]=None)\n\nCheck that proposed variable names are in MARIS CDL\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvar_names\nList\n\nvariable names\n\n\ncdl_path\nstr\n\nPath to MARIS CDL file (point of truth)\n\n\ngroup\nOptional\nNone\nCheck if the variable names is contained in the group\n\n\n\n\n\nExported source\n# TBD: Assess if still needed\ndef has_valid_varname(\n    var_names: List[str], # variable names\n    cdl_path: str, # Path to MARIS CDL file (point of truth)\n    group: Optional[str] = None, # Check if the variable names is contained in the group\n):\n    \"Check that proposed variable names are in MARIS CDL\"\n    has_valid = True\n    with Dataset(cdl_path) as nc:\n        cdl_vars={}\n        all_vars=[]\n        # get variable names in CDL \n        for grp in nc.groups.values():\n            # Create a list of var for each group\n            vars = list(grp.variables.keys())\n            cdl_vars[grp.name] = vars\n            all_vars.extend(vars)\n        \n    if group != None:\n        allowed_vars= cdl_vars[group]\n    else: \n        # get unique \n        allowed_vars = list(set(all_vars))\n        \n    for name in var_names:\n        if name not in allowed_vars:\n            has_valid = False\n            if group != None:\n                print(f'\"{name}\" variable name not found in group \"{group}\" of MARIS CDL')\n            else:\n                print(f'\"{name}\" variable name not found in MARIS CDL')\n    return has_valid\n\n\n\n# VARNAMES = ['lat', 'lon']\n# test_eq(has_valid_varname(VARNAMES, './files/nc/maris-cdl.nc'), True)\n\n\n# VARNAMES = ['ba140_invalid', 'ba140_dl']\n# test_eq(has_valid_varname(VARNAMES, './files/nc/maris-cdl.nc'), False)",
    "crumbs": [
      "API",
      "Utilities"
    ]
  },
  {
    "objectID": "api/utils.html#geoprocessing",
    "href": "api/utils.html#geoprocessing",
    "title": "Utilities",
    "section": "Geoprocessing",
    "text": "Geoprocessing\n\nsource\n\nget_bbox\n\n get_bbox (df, coord_cols:Tuple[str,str]=('LON', 'LAT'))\n\nGet the bounding box of a DataFrame.\n\n\nExported source\ndef get_bbox(df,\n             coord_cols: Tuple[str, str] = ('LON', 'LAT')\n            ):\n    \"Get the bounding box of a DataFrame.\"\n    x, y = coord_cols        \n    arr = [(row[x], row[y]) for _, row in df.iterrows()]\n    return MultiPoint(arr).envelope\n\n\n\ndf = pd.DataFrame({'LON': np.linspace(-10, 5, 20), 'LAT':  np.linspace(40, 50, 20)})\nbbox = get_bbox(df);\n\n\n# To get `lon_min`, `lon_max`, `lat_min`, `lat_max`\nbbox.bounds\n\n(-10.0, 40.0, 5.0, 50.0)\n\n\n\n# And its Well-Know Text representation\nbbox.wkt\n\n'POLYGON ((-10 40, 5 40, 5 50, -10 50, -10 40))'\n\n\n\n# If unique (lon, lat)\ndf = pd.DataFrame({'LON': [0, 0], 'LAT':  [1, 1]})\nbbox = get_bbox(df);\n\n\nbbox.bounds\n\n(0.0, 1.0, 0.0, 1.0)\n\n\n\nsource\n\n\nddmm_to_dd\n\n ddmm_to_dd (ddmmmm:float)\n\n\n\n\n\nType\nDetails\n\n\n\n\nddmmmm\nfloat\nCoordinates in degrees/minutes decimal format\n\n\nReturns\nfloat\nCoordinates in degrees decimal format\n\n\n\n\n\nExported source\ndef ddmm_to_dd(\n    ddmmmm: float # Coordinates in degrees/minutes decimal format\n    ) -&gt; float: # Coordinates in degrees decimal format\n    # Convert degrees/minutes decimal to degrees decimal.\n    mins, degs = modf(ddmmmm)\n    mins = mins * 100\n    return round(int(degs) + (mins / 60), 6)\n\n\n\nfc.test_close(ddmm_to_dd(45.34), 45.566667)",
    "crumbs": [
      "API",
      "Utilities"
    ]
  },
  {
    "objectID": "api/utils.html#downloaders",
    "href": "api/utils.html#downloaders",
    "title": "Utilities",
    "section": "Downloaders",
    "text": "Downloaders\n\nsource\n\ndownload_file\n\n download_file (owner, repo, src_dir, dest_dir, fname)\n\n\n\nExported source\ndef download_files_in_folder(\n    owner: str, # GitHub owner\n    repo: str, # GitHub repository\n    src_dir: str, # Source directory\n    dest_dir: str # Destination directory\n    ):\n    \"Make a GET request to the GitHub API to get the contents of the folder.\"\n    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{src_dir}\"\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        contents = response.json()\n\n        # Iterate over the files and download them\n        for item in contents:\n            if item[\"type\"] == \"file\":\n                fname = item[\"name\"]\n                download_file(owner, repo, src_dir, dest_dir, fname)\n    else:\n        print(f\"Error: {response.status_code}\")\n\ndef download_file(owner, repo, src_dir, dest_dir, fname):\n    # Make a GET request to the GitHub API to get the raw file contents\n    url = f\"https://raw.githubusercontent.com/{owner}/{repo}/master/{src_dir}/{fname}\"\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        # Save the file locally\n        with open(Path(dest_dir) / fname, \"wb\") as file:\n            file.write(response.content)\n        print(f\"{fname} downloaded successfully.\")\n    else:\n        print(f\"Error: {response.status_code}\")\n\n\n\nsource\n\n\ndownload_files_in_folder\n\n download_files_in_folder (owner:str, repo:str, src_dir:str, dest_dir:str)\n\nMake a GET request to the GitHub API to get the contents of the folder.\n\n\n\n\nType\nDetails\n\n\n\n\nowner\nstr\nGitHub owner\n\n\nrepo\nstr\nGitHub repository\n\n\nsrc_dir\nstr\nSource directory\n\n\ndest_dir\nstr\nDestination directory",
    "crumbs": [
      "API",
      "Utilities"
    ]
  },
  {
    "objectID": "api/utils.html#worrms",
    "href": "api/utils.html#worrms",
    "title": "Utilities",
    "section": "WorRMS",
    "text": "WorRMS\nThe World Register of Marine Species (WorMS) is an authoritative classification and catalogue of marine names. It provides a REST API (among others) allowing to “fuzzy” match any species name you might encounter in marine data sources names againt their own database. There are several types of matches as described here.\n\nsource\n\nmatch_worms\n\n match_worms (name:str)\n\nLookup name in WoRMS (fuzzy match).\n\n\n\n\nType\nDetails\n\n\n\n\nname\nstr\nName of species to look up in WoRMS\n\n\n\n\n\nExported source\ndef match_worms(\n    name: str # Name of species to look up in WoRMS\n    ):\n    \"Lookup `name` in WoRMS (fuzzy match).\"\n    url = 'https://www.marinespecies.org/rest/AphiaRecordsByMatchNames'\n    params = {\n        'scientificnames[]': [name],\n        'marine_only': 'true'\n    }\n    headers = {\n        'accept': 'application/json'\n    }\n    \n    response = requests.get(url, params=params, headers=headers)\n    \n    # Check if the request was successful (status code 200)\n    if response.status_code == 200:\n        data = response.json()\n        return data\n    else:\n        return -1\n\n\nFor instance:\n\nmatch_worms('Aristeus antennatus')\n\n[[{'AphiaID': 107083,\n   'url': 'https://www.marinespecies.org/aphia.php?p=taxdetails&id=107083',\n   'scientificname': 'Aristeus antennatus',\n   'authority': '(Risso, 1816)',\n   'status': 'accepted',\n   'unacceptreason': None,\n   'taxonRankID': 220,\n   'rank': 'Species',\n   'valid_AphiaID': 107083,\n   'valid_name': 'Aristeus antennatus',\n   'valid_authority': '(Risso, 1816)',\n   'parentNameUsageID': 106807,\n   'kingdom': 'Animalia',\n   'phylum': 'Arthropoda',\n   'class': 'Malacostraca',\n   'order': 'Decapoda',\n   'family': 'Aristeidae',\n   'genus': 'Aristeus',\n   'citation': 'DecaNet eds. (2025). DecaNet. Aristeus antennatus (Risso, 1816). Accessed through: World Register of Marine Species at: https://www.marinespecies.org/aphia.php?p=taxdetails&id=107083 on 2025-05-28',\n   'lsid': 'urn:lsid:marinespecies.org:taxname:107083',\n   'isMarine': 1,\n   'isBrackish': 0,\n   'isFreshwater': 0,\n   'isTerrestrial': 0,\n   'isExtinct': 0,\n   'match_type': 'exact',\n   'modified': '2022-08-24T09:48:14.813Z'}]]",
    "crumbs": [
      "API",
      "Utilities"
    ]
  },
  {
    "objectID": "api/utils.html#fuzzy-matching-for-maris-lookup-tables",
    "href": "api/utils.html#fuzzy-matching-for-maris-lookup-tables",
    "title": "Utilities",
    "section": "Fuzzy matching for MARIS Lookup Tables",
    "text": "Fuzzy matching for MARIS Lookup Tables\nUsing https://jamesturk.github.io/jellyfish fuzzy matching distance metrics.\n\nsource\n\nMatch\n\n Match (matched_id:int, matched_maris_name:str, source_name:str,\n        match_score:int)\n\nMatch between a data provider name and a MARIS lookup table.\n\n\nExported source\n@dataclass\nclass Match:\n    \"Match between a data provider name and a MARIS lookup table.\"\n    matched_id: int\n    matched_maris_name: str\n    source_name: str\n    match_score: int\n\n\n\nsource\n\n\nmatch_maris_lut\n\n match_maris_lut (lut:Union[str,pandas.core.frame.DataFrame,pathlib.Path],\n                  data_provider_name:str, maris_id:str, maris_name:str,\n                  dist_fn:Callable=&lt;built-in function\n                  levenshtein_distance&gt;, nresults:int=10)\n\nFuzzy matching data provider and MARIS lookup tables (e.g biota species, sediments, …).\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlut\nUnion\n\nEither str, Path or DataFrame\n\n\ndata_provider_name\nstr\n\nName of data provider nomenclature item to look up\n\n\nmaris_id\nstr\n\nId of MARIS lookup table nomenclature item to match\n\n\nmaris_name\nstr\n\nName of MARIS lookup table nomenclature item to match\n\n\ndist_fn\nCallable\nlevenshtein_distance\nDistance function\n\n\nnresults\nint\n10\nMaximum number of results to return\n\n\nReturns\nDataFrame\n\n\n\n\n\n\n\nExported source\ndef match_maris_lut(\n    lut: Union[str, pd.DataFrame, Path], # Either str, Path or DataFrame\n    data_provider_name: str, # Name of data provider nomenclature item to look up \n    maris_id: str, # Id of MARIS lookup table nomenclature item to match\n    maris_name: str, # Name of MARIS lookup table nomenclature item to match\n    dist_fn: Callable = jf.levenshtein_distance, # Distance function\n    nresults: int = 10 # Maximum number of results to return\n) -&gt; pd.DataFrame:\n    \"Fuzzy matching data provider and MARIS lookup tables (e.g biota species, sediments, ...).\"\n    if isinstance(lut, str) or isinstance(lut, Path):\n        df = pd.read_excel(lut)  # Load the LUT if a path is provided\n    elif isinstance(lut, pd.DataFrame):\n        df = lut  # Use the DataFrame directly if provided\n    else:\n        raise ValueError(\"lut must be either a file path or a DataFrame\")\n\n    df = df.dropna(subset=[maris_name])\n    df = df.astype({maris_id: 'int'})\n    df['score'] = df[maris_name].str.lower().apply(lambda x: dist_fn(data_provider_name.lower(), x))\n    df = df.sort_values(by='score', ascending=True)[:nresults]\n    return df[[maris_id, maris_name, 'score']]\n\n\nBelow an example trying to match the name “PLANKTON” with dbo_species_cleaned.xlsx MARIS biota species lookup table:\n\nlut_fname = '../files/lut/dbo_species_cleaned.xlsx'\nmatch_maris_lut(lut_fname, data_provider_name='PLANKTON', \n                maris_id='species_id', maris_name='species')\n\n\n\n\n\n\n\n\nspecies_id\nspecies\nscore\n\n\n\n\n281\n280\nPlankton\n0\n\n\n696\n695\nZooplankton\n3\n\n\n633\n632\nPalaemon\n4\n\n\n697\n696\nPhytoplankton\n5\n\n\n812\n811\nChanos\n5\n\n\n160\n159\nNeuston\n5\n\n\n234\n233\nPenaeus\n6\n\n\n1458\n1457\nLamnidae\n6\n\n\n1438\n1437\nLabrus\n6\n\n\n1527\n1526\nFavites\n6\n\n\n\n\n\n\n\nBelow, we demonstrate matching the laboratory name “Central Mining Institute, Poland” with the MARIS lab lookup table from dbo_lab.xlsx. This example utilizes the lab and country columns. Note that in this instance, df_lut is passed directly as the lut argument.\n\nlut_fname = '../files/lut/dbo_lab.xlsx'\ndf_lut=pd.read_excel(lut_fname)\ndf_lut['lab_country'] = df_lut['lab'] + '_' + df_lut['country']\n\nmatch_maris_lut(lut=df_lut, data_provider_name='Central Mining Institute, Poland', \n                maris_id='lab_id', maris_name='lab_country')\n\n\n\n\n\n\n\n\nlab_id\nlab_country\nscore\n\n\n\n\n6\n5\nCentral Mining Institute_Poland\n2\n\n\n203\n202\nPolytechnic Institute_Romania\n18\n\n\n282\n281\nNorwegian Polar Institute_Norway\n21\n\n\n113\n112\nNuclear Research Institute_Vietnam\n22\n\n\n246\n245\nPaul Scherrer Institute_Switzerland\n22\n\n\n136\n135\nNuclear Energy Board_Ireland\n23\n\n\n471\n474\nKobe University_Japan\n23\n\n\n429\n432\nQatar University_Qatar\n23\n\n\n174\n173\nInterfaculty Reactor Institute_Netherlands\n23\n\n\n177\n176\nRIKILT_Netherlands\n23\n\n\n\n\n\n\n\nBelow an example trying to match the name “GLACIAL” with dbo_sedtype.xlsx MARIS sediment lookup table:\n\nlut_fname = '../files/lut/dbo_sedtype.xlsx'\nmatch_maris_lut(lut_fname, data_provider_name='GLACIAL', \n                maris_id='sedtype_id', maris_name='sedtype')\n\n\n\n\n\n\n\n\nsedtype_id\nsedtype\nscore\n\n\n\n\n26\n25\nGlacial\n0\n\n\n3\n2\nGravel\n4\n\n\n2\n1\nClay\n5\n\n\n51\n50\nGlacial clay\n5\n\n\n4\n3\nMarsh\n6\n\n\n7\n6\nSand\n6\n\n\n13\n12\nSilt\n6\n\n\n15\n14\nSludge\n6\n\n\n27\n26\nSoft\n7\n\n\n52\n51\nSoft clay\n7\n\n\n\n\n\n\n\n\nlut_fname = '../files/lut/dbo_nuclide.xlsx'\nmatch_maris_lut(lut_fname, data_provider_name='CS-137', \n                maris_id='nuclide_id', maris_name='nc_name')\n\n\n\n\n\n\n\n\nnuclide_id\nnc_name\nscore\n\n\n\n\n31\n33\ncs137\n1\n\n\n30\n31\ncs134\n2\n\n\n29\n30\ncs127\n2\n\n\n99\n102\ncs136\n2\n\n\n109\n112\nsb127\n3\n\n\n111\n114\nce139\n3\n\n\n25\n24\nsb125\n4\n\n\n36\n38\npm147\n4\n\n\n28\n29\ni131\n4\n\n\n110\n113\nba133\n4",
    "crumbs": [
      "API",
      "Utilities"
    ]
  },
  {
    "objectID": "api/utils.html#downloaders-1",
    "href": "api/utils.html#downloaders-1",
    "title": "Utilities",
    "section": "Downloaders",
    "text": "Downloaders\n\nsource\n\ndownload_file\n\n download_file (owner, repo, src_dir, dest_dir, fname)\n\n\n\nExported source\ndef download_files_in_folder(\n    owner: str, # GitHub owner\n    repo: str, # GitHub repository\n    src_dir: str, # Source directory\n    dest_dir: str # Destination directory\n    ):\n    \"Make a GET request to the GitHub API to get the contents of the folder\"\n    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{src_dir}\"\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        contents = response.json()\n\n        # Iterate over the files and download them\n        for item in contents:\n            if item[\"type\"] == \"file\":\n                fname = item[\"name\"]\n                download_file(owner, repo, src_dir, dest_dir, fname)\n    else:\n        print(f\"Error: {response.status_code}\")\n\ndef download_file(owner, repo, src_dir, dest_dir, fname):\n    # Make a GET request to the GitHub API to get the raw file contents\n    url = f\"https://raw.githubusercontent.com/{owner}/{repo}/master/{src_dir}/{fname}\"\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        # Save the file locally\n        with open(Path(dest_dir) / fname, \"wb\") as file:\n            file.write(response.content)\n        print(f\"{fname} downloaded successfully.\")\n    else:\n        print(f\"Error: {response.status_code}\")\n\n\n\nsource\n\n\ndownload_files_in_folder\n\n download_files_in_folder (owner:str, repo:str, src_dir:str, dest_dir:str)\n\nMake a GET request to the GitHub API to get the contents of the folder\n\n\n\n\nType\nDetails\n\n\n\n\nowner\nstr\nGitHub owner\n\n\nrepo\nstr\nGitHub repository\n\n\nsrc_dir\nstr\nSource directory\n\n\ndest_dir\nstr\nDestination directory",
    "crumbs": [
      "API",
      "Utilities"
    ]
  },
  {
    "objectID": "api/utils.html#test",
    "href": "api/utils.html#test",
    "title": "Utilities",
    "section": "Test",
    "text": "Test\n\nsource\n\ntest_dfs\n\n test_dfs (dfs1:Dict[str,pandas.core.frame.DataFrame],\n           dfs2:Dict[str,pandas.core.frame.DataFrame])\n\nCompare two dictionaries of DataFrames for equality (also ensuring that columns are in the same order).\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndfs1\nDict\nFirst dictionary of DataFrames to compare\n\n\ndfs2\nDict\nSecond dictionary of DataFrames to compare\n\n\nReturns\nNone\nIt raises an AssertionError if the DataFrames are not equal\n\n\n\n\n\nExported source\ndef test_dfs(\n    dfs1: Dict[str, pd.DataFrame], # First dictionary of DataFrames to compare \n    dfs2: Dict[str, pd.DataFrame] # Second dictionary of DataFrames to compare\n    ) -&gt; None: # It raises an `AssertionError` if the DataFrames are not equal\n    \"Compare two dictionaries of DataFrames for equality (also ensuring that columns are in the same order).\"\n    for grp in dfs1.keys():\n        df1, df2 = (df.sort_index() for df in (dfs1[grp], dfs2[grp]))\n        fc.test_eq(df1, df2.reindex(columns=df1.columns))",
    "crumbs": [
      "API",
      "Utilities"
    ]
  },
  {
    "objectID": "api/utils.html#netcdf-utilities",
    "href": "api/utils.html#netcdf-utilities",
    "title": "Utilities",
    "section": "NetCDF Utilities",
    "text": "NetCDF Utilities\nExtract NetCDF contents\n\nsource\n\nExtractNetcdfContents\n\n ExtractNetcdfContents (filename:str, verbose:bool=False)\n\nInitialize and extract data from a NetCDF file.\n\n\nExported source\nclass ExtractNetcdfContents:\n    def __init__(self, filename: str, verbose: bool = False):\n        \"Initialize and extract data from a NetCDF file.\"\n        self.filename = filename\n        self.verbose = verbose\n        self.dfs = {}  # DataFrames extracted from the NetCDF file\n        self.enum_dicts = {}  # Enum dictionaries extracted from the NetCDF file\n        self.global_attrs = {}  # Global attributes extracted from the NetCDF file\n        self.custom_maps = {}  # Custom maps extracted from the NetCDF file\n        self.extract_all()\n\n    def extract_all(self):\n        \"Extract data, enums, and global attributes from the NetCDF file.\"\n        if not Path(self.filename).exists():\n            print(f'File {self.filename} not found.')\n            return\n        \n        with Dataset(self.filename, 'r') as nc:\n            self.global_attrs = self.extract_global_attributes(nc)\n            for group_name in nc.groups:\n                group = nc.groups[group_name]\n                self.dfs[group_name.upper()] = self.extract_data(group)\n                self.enum_dicts[group_name.upper()] = self.extract_enums(group, group_name)\n                self.custom_maps[group_name.upper()] = self.extract_custom_maps(group, group_name)\n                \n            if self.verbose:\n                print(\"Data extraction complete.\")\n\n    def extract_data(self, group) -&gt; pd.DataFrame:\n        \"Extract data from a group and convert to DataFrame.\"\n        data = {var_name: var[:] for var_name, var in group.variables.items() if var_name not in group.dimensions}\n        df = pd.DataFrame(data)\n        rename_map = {nc_var: col for col, nc_var in NC_VARS.items() if nc_var in df.columns}\n        df = df.rename(columns=rename_map)\n        return df\n\n    def extract_enums(self, group, group_name: str) -&gt; Dict:\n        \"Extract enum dictionaries for variables in a group.\"\n        local_enum_dicts = {}\n        for var_name, var in group.variables.items():\n            if hasattr(var.datatype, 'enum_dict'):\n                local_enum_dicts[var_name] = {str(k): str(v) for k, v in var.datatype.enum_dict.items()}\n                if self.verbose:\n                    print(f\"Extracted enum_dict for {var_name} in {group_name}\")\n        return local_enum_dicts\n\n    def extract_global_attributes(self, nc) -&gt; Dict:\n        \"Extract global attributes from the NetCDF file.\"\n        globattrs = {attr: getattr(nc, attr) for attr in nc.ncattrs()}\n        return globattrs\n    \n    def extract_custom_maps(self, group, group_name: str) -&gt; Dict:\n        \"Extract custom maps from the NetCDF file.\"\n        reverse_nc_vars = {v: k for k, v in NC_VARS.items()}        \n        custom_maps = {}\n        for var_name, var in group.variables.items():\n            attr=f\"{var_name}_map\"\n            if hasattr(var, attr):\n                custom_maps[reverse_nc_vars[var.name]] =  literal_eval(getattr(var, attr))\n        return custom_maps\n\n\n\n# fname = Path('../../_data/output/190-geotraces-2021.nc')\n#fname = Path('../../_data/output/tepco.nc')\nfname = Path('../../_data/output/tepco.nc')\n#fname = Path('./files/nc/encoding-test.nc')\ncontents= ExtractNetcdfContents(fname)\n\nprint(contents.dfs)\nprint(contents.enum_dicts)\nprint(contents.global_attrs)\nprint(contents.custom_maps)\n\n{'SEAWATER':               LON        LAT        TIME STATION  NUCLIDE   VALUE  UNIT  UNC  \\\n0      140.603882  36.299721  1318512060     T-C       29     NaN     3  NaN   \n1      140.603882  36.299721  1318512060     T-C       31     NaN     3  NaN   \n2      140.603882  36.299721  1318512060     T-C       33     NaN     3  NaN   \n3      140.603882  36.299721  1318512180     T-C       29     NaN     3  NaN   \n4      140.603882  36.299721  1318512180     T-C       31     NaN     3  NaN   \n...           ...        ...         ...     ...      ...     ...   ...  ...   \n81491  141.666672  38.299999  1722930660   T-MG2       33  0.0016     3  NaN   \n81492  141.666672  38.299999  1725956340   T-MG2       31     NaN     3  NaN   \n81493  141.666672  38.299999  1725956340   T-MG2       33  0.0015     3  NaN   \n81494  141.666672  38.299999  1725957120   T-MG2       31     NaN     3  NaN   \n81495  141.666672  38.299999  1725957120   T-MG2       33  0.0014     3  NaN   \n\n       DL     DLV  \n0       3  4.0000  \n1       3  6.0000  \n2       3  9.0000  \n3       3  4.0000  \n4       3  6.0000  \n...    ..     ...  \n81491   1     NaN  \n81492   3  0.0011  \n81493   1     NaN  \n81494   3  0.0011  \n81495   1     NaN  \n\n[81496 rows x 10 columns]}\n{'SEAWATER': {'nuclide': {'NOT APPLICABLE': '-1', 'NOT AVAILABLE': '0', 'h3': '1', 'be7': '2', 'c14': '3', 'k40': '4', 'cr51': '5', 'mn54': '6', 'co57': '7', 'co58': '8', 'co60': '9', 'zn65': '10', 'sr89': '11', 'sr90': '12', 'zr95': '13', 'nb95': '14', 'tc99': '15', 'ru103': '16', 'ru106': '17', 'rh106': '18', 'ag106m': '19', 'ag108': '20', 'ag108m': '21', 'ag110m': '22', 'sb124': '23', 'sb125': '24', 'te129m': '25', 'i129': '28', 'i131': '29', 'cs127': '30', 'cs134': '31', 'cs137': '33', 'ba140': '34', 'la140': '35', 'ce141': '36', 'ce144': '37', 'pm147': '38', 'eu154': '39', 'eu155': '40', 'pb210': '41', 'pb212': '42', 'pb214': '43', 'bi207': '44', 'bi211': '45', 'bi214': '46', 'po210': '47', 'rn220': '48', 'rn222': '49', 'ra223': '50', 'ra224': '51', 'ra225': '52', 'ra226': '53', 'ra228': '54', 'ac228': '55', 'th227': '56', 'th228': '57', 'th232': '59', 'th234': '60', 'pa234': '61', 'u234': '62', 'u235': '63', 'u238': '64', 'np237': '65', 'np239': '66', 'pu238': '67', 'pu239': '68', 'pu240': '69', 'pu241': '70', 'am240': '71', 'am241': '72', 'cm242': '73', 'cm243': '74', 'cm244': '75', 'cs134_137_tot': '76', 'pu239_240_tot': '77', 'pu239_240_iii_iv_tot': '78', 'pu239_240_v_vi_tot': '79', 'cm243_244_tot': '80', 'pu238_pu239_240_tot_ratio': '81', 'am241_pu239_240_tot_ratio': '82', 'cs137_134_ratio': '83', 'cd109': '84', 'eu152': '85', 'fe59': '86', 'gd153': '87', 'ir192': '88', 'pu238_240_tot': '89', 'rb86': '90', 'sc46': '91', 'sn113': '92', 'sn117m': '93', 'tl208': '94', 'mo99': '95', 'tc99m': '96', 'ru105': '97', 'te129': '98', 'te132': '99', 'i132': '100', 'i135': '101', 'cs136': '102', 'tbeta': '103', 'talpha': '104', 'i133': '105', 'th230': '106', 'pa231': '107', 'u236': '108', 'ag111': '109', 'in116m': '110', 'te123m': '111', 'sb127': '112', 'ba133': '113', 'ce139': '114', 'tl201': '116', 'hg203': '117', 'na22': '122', 'pa234m': '123', 'am243': '124', 'se75': '126', 'sr85': '127', 'y88': '128', 'ce140': '129', 'bi212': '130', 'u236_238_ratio': '131', 'i125': '132', 'ba137m': '133', 'u232': '134', 'pa233': '135', 'ru106_rh106_tot': '136', 'tu': '137', 'tbeta40k': '138', 'fe55': '139', 'ce144_pr144_tot': '140', 'pu240_pu239_ratio': '141', 'u233': '142', 'pu239_242_tot': '143', 'ac227': '144'}, 'unit': {'Not applicable': '-1', 'NOT AVAILABLE': '0', 'Bq per m3': '1', 'Bq per m2': '2', 'Bq per kg': '3', 'Bq per kgd': '4', 'Bq per kgw': '5', 'kg per kg': '6', 'TU': '7', 'DELTA per mill': '8', 'atom per kg': '9', 'atom per kgd': '10', 'atom per kgw': '11', 'atom per l': '12', 'Bq per kgC': '13'}, 'dlt': {'Not applicable': '-1', 'Not available': '0', 'Detected value': '1', 'Detection limit': '2', 'Not detected': '3', 'Derived': '4'}}}\n{'id': 'JEV6HP5A', 'title': \"Readings of Sea Area Monitoring - Monitoring of sea water - Sea area close to TEPCO's Fukushima Daiichi NPS / Coastal area - Readings of Sea Area Monitoring [TEPCO]\", 'summary': '', 'keywords': 'oceanography, Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides, Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure, Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments, Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes, Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants, Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish, Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans, Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)', 'history': 'TBD', 'keywords_vocabulary': 'GCMD Science Keywords', 'keywords_vocabulary_url': 'https://gcmd.earthdata.nasa.gov/static/kms/', 'record': 'TBD', 'featureType': 'TBD', 'cdm_data_type': 'TBD', 'Conventions': 'CF-1.10 ACDD-1.3', 'publisher_name': 'Paul MCGINNITY, Iolanda OSVATH, Florence DESCROIX-COMANDUCCI', 'publisher_email': 'p.mc-ginnity@iaea.org, i.osvath@iaea.org, F.Descroix-Comanducci@iaea.org', 'publisher_url': 'https://maris.iaea.org', 'publisher_institution': 'International Atomic Energy Agency - IAEA', 'creator_name': '[{\"creatorType\": \"author\", \"firstName\": \"\", \"lastName\": \"TEPCO - Tokyo Electric Power Company\"}]', 'institution': 'TBD', 'metadata_link': 'TBD', 'creator_email': 'TBD', 'creator_url': 'TBD', 'references': 'TBD', 'license': 'Without prejudice to the applicable Terms and Conditions (https://nucleus.iaea.org/Pages/Others/Disclaimer.aspx), I hereby agree that any use of the data will contain appropriate acknowledgement of the data source(s) and the IAEA Marine Radioactivity Information System (MARIS).', 'comment': 'TBD', 'geospatial_lat_min': '141.66666667', 'geospatial_lon_min': '140.60388889', 'geospatial_lat_max': '38.63333333', 'geospatial_lon_max': '35.79611111', 'geospatial_vertical_min': 'TBD', 'geospatial_vertical_max': 'TBD', 'geospatial_bounds': 'POLYGON ((140.60388889 35.79611111, 141.66666667 35.79611111, 141.66666667 38.63333333, 140.60388889 38.63333333, 140.60388889 35.79611111))', 'geospatial_bounds_crs': 'EPSG:4326', 'time_coverage_start': '2011-03-21T14:30:00', 'time_coverage_end': '2025-01-25T07:24:00', 'local_time_zone': 'TBD', 'date_created': 'TBD', 'date_modified': 'TBD', 'publisher_postprocess_logs': \"Remove 約 (about) char, Replace range values (e.g '4.0E+00&lt;&&lt;8.0E+00' or '1.0～2.7') by their mean, Select columns of interest., \\n    Get TEPCO nuclide names as values not column names \\n    to extract contained information (nuclide name, unc, dl, ...).\\n    , Extract nuclide name from TEPCO data., Extract unit from TEPCO data., Extract value type from TEPCO data., Reshape: long to wide, \\n    Remap `UNIT` name to MARIS id.\\n    , Remap `NUCLIDE` name to MARIS id., Remap `DL` name to MARIS id., Cast `VALUE` to float., Parse time column from TEPCO., Encode time as seconds since epoch., Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.\"}\n{'SEAWATER': {}}",
    "crumbs": [
      "API",
      "Utilities"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html",
    "href": "api/netcdf2csv.html",
    "title": "NetCDF2CSV",
    "section": "",
    "text": "A data pipeline that converts MARIS NetCDF files into MARIS Standard OpenRefine CSV format.\nThis module converts NetCDF files into CSV files that follow the MARIS Standard OpenRefine format. While MARISCO has replaced OpenRefine in the data cleaning and preparation pipeline, the MARIS master database still requires input files to conform to this CSV format specification. The conversion is performed using the marisco library.",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#configuration-and-file-paths",
    "href": "api/netcdf2csv.html#configuration-and-file-paths",
    "title": "NetCDF2CSV",
    "section": "Configuration and File Paths",
    "text": "Configuration and File Paths\n\n# fname_in =  Path('../../_data/output/tepco.nc')\nfname_in =  Path('../../_data/output/100-HELCOM-MORS-2024.nc')\nfname_out = fname_in.with_suffix('.csv')\noutput_format = 'openrefine_csv'",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#data-loading",
    "href": "api/netcdf2csv.html#data-loading",
    "title": "NetCDF2CSV",
    "section": "Data Loading",
    "text": "Data Loading\nLoad data from standardized MARIS NetCDF files using ExtractNetcdfContents. The NetCDF files follow CF conventions and include standardized variable names and metadata according to MARIS specifications.\n\ncontents = ExtractNetcdfContents(fname_in)\n\nShow the dictionary of dataframes extracted from the NetCDF file.\n\ncontents.dfs.keys()\n\ndict_keys(['BIOTA', 'SEAWATER', 'SEDIMENT'])\n\n\nShow an example of the DataFrame extracted from the NetCDF file.\n\nwith pd.option_context('display.max_columns', None):\n    display(contents.dfs['SEAWATER'].head())\n\n\n\n\n\n\n\n\nSMP_ID_PROVIDER\nLON\nLAT\nSMP_DEPTH\nTOT_DEPTH\nTIME\nSMP_ID\nNUCLIDE\nVALUE\nUNIT\nUNC\nDL\nFILT\n\n\n\n\n0\nWKRIL2012003\n29.333300\n60.083302\n0.0\nNaN\n1337731200\n1\n33\n5.300000\n1\n1.696\n1\n0\n\n\n1\nWKRIL2012004\n29.333300\n60.083302\n29.0\nNaN\n1337731200\n2\n33\n19.900000\n1\n3.980\n1\n0\n\n\n2\nWKRIL2012005\n23.150000\n59.433300\n0.0\nNaN\n1339891200\n3\n33\n25.500000\n1\n5.100\n1\n0\n\n\n3\nWKRIL2012006\n27.983299\n60.250000\n0.0\nNaN\n1337817600\n4\n33\n17.000000\n1\n4.930\n1\n0\n\n\n4\nWKRIL2012007\n27.983299\n60.250000\n39.0\nNaN\n1337817600\n5\n33\n22.200001\n1\n3.996\n1\n0\n\n\n\n\n\n\n\nShow an example of the dictionary of enums extracted from the NetCDF file as a DataFrame.\n\ngrp='SEAWATER'\nprint(f'Variables in {grp} group: {contents.enum_dicts[grp].keys()}')\nvar='dl'\nwith pd.option_context('display.max_columns', None):\n    display(pd.DataFrame.from_dict(contents.enum_dicts[grp][var], orient='index').T)\n\nVariables in SEAWATER group: dict_keys(['nuclide', 'unit', 'dl', 'filt'])\n\n\n\n\n\n\n\n\n\nNot applicable\nNot available\nDetected value\nDetection limit\nNot detected\nDerived\n\n\n\n\n0\n-1\n0\n1\n2\n3\n4\n\n\n\n\n\n\n\nShow the global attributes extracted from the NetCDF file.\n\nprint(\"First few attributes from global attributes:\", list(contents.global_attrs.items())[:5])\n\nFirst few attributes from global attributes: [('id', '26VMZZ2Q'), ('title', 'Environmental database - Helsinki Commission Monitoring of Radioactive Substances'), ('summary', 'MORS Environment database has been used to collate data resulting from monitoring of environmental radioactivity in the Baltic Sea based on HELCOM Recommendation 26/3.\\n\\nThe database is structured according to HELCOM Guidelines on Monitoring of Radioactive Substances (https://www.helcom.fi/wp-content/uploads/2019/08/Guidelines-for-Monitoring-of-Radioactive-Substances.pdf), which specifies reporting format, database structure, data types and obligatory parameters used for reporting data under Recommendation 26/3.\\n\\nThe database is updated and quality assured annually by HELCOM MORS EG.'), ('keywords', 'oceanography, Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides, Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure, Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments, Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes, Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants, Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish, Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans, Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)'), ('history', 'TBD')]\n\n\nShow the custom maps extracted from the NetCDF file.\n\ngrp='SEAWATER'\nprint(f'Custom maps in {grp} group: {contents.custom_maps[grp].keys()}')\nwith pd.option_context('display.max_columns', None):\n    display(pd.DataFrame.from_dict(contents.custom_maps[grp], orient='index'))\n\nCustom maps in SEAWATER group: dict_keys([])",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#validate-netcdf-enumerations",
    "href": "api/netcdf2csv.html#validate-netcdf-enumerations",
    "title": "NetCDF2CSV",
    "section": "Validate NetCDF Enumerations",
    "text": "Validate NetCDF Enumerations\nVerify that enumerated values in the NetCDF file match current MARIS lookup tables.\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDERS\n\n\n\nThe enumeration validation process is a diagnostic step that identifies inconsistencies between NetCDF enumerations and MARIS lookup tables. While this validation does not modify the dataset, it generates detailed feedback about any mismatches or undefined values.\n\n\n\nNC_VARS\n\n{'SMP_ID': 'smp_id',\n 'SMP_ID_PROVIDER': 'id_provider',\n 'LON': 'lon',\n 'LAT': 'lat',\n 'SMP_DEPTH': 'smp_depth',\n 'TOT_DEPTH': 'tot_depth',\n 'TIME': 'time',\n 'AREA': 'area',\n 'NUCLIDE': 'nuclide',\n 'VALUE': 'value',\n 'UNIT': 'unit',\n 'UNC': 'unc',\n 'DL': 'dl',\n 'DLV': 'dlv',\n 'FILT': 'filt',\n 'COUNT_MET': 'count_met',\n 'SAMP_MET': 'samp_met',\n 'PREP_MET': 'prep_met',\n 'VOL': 'vol',\n 'SAL': 'sal',\n 'TEMP': 'temp',\n 'PH': 'ph',\n 'BIO_GROUP': 'bio_group',\n 'SPECIES': 'species',\n 'BODY_PART': 'body_part',\n 'SED_TYPE': 'sed_type',\n 'TOP': 'top',\n 'BOTTOM': 'bottom',\n 'DRYWT': 'drywt',\n 'WETWT': 'wetwt',\n 'PERCENTWT': 'percentwt',\n 'LAB': 'lab',\n 'PROFILE_ID': 'profile_id',\n 'STATION': 'station'}\n\n\n\nsource\n\nValidateEnumsCB\n\n ValidateEnumsCB (contents, maris_enums, verbose=False)\n\nValidate enumeration mappings between NetCDF file and MARIS lookup tables.\n\ncontents = ExtractNetcdfContents(fname_in)\ntfm = Transformer(\n    data= contents.dfs,\n    custom_maps=contents.custom_maps,\n    cbs=[\n        ValidateEnumsCB(\n            contents = contents,\n            maris_enums=Enums(lut_src_dir=lut_path())\n        ),\n    ]\n)\ntfm()\n\n{'BIOTA':       SMP_ID_PROVIDER        LON        LAT  SMP_DEPTH        TIME  SMP_ID  \\\n 0        BVTIG2012041  12.316667  54.283333        NaN  1348358400       1   \n 1        BVTIG2012041  12.316667  54.283333        NaN  1348358400       2   \n 2        BVTIG2012041  12.316667  54.283333        NaN  1348358400       3   \n 3        BVTIG2012041  12.316667  54.283333        NaN  1348358400       4   \n 4        BVTIG2012040  12.316667  54.283333        NaN  1348358400       5   \n ...               ...        ...        ...        ...         ...     ...   \n 16089    BSTUK2022010  21.395000  61.241501        2.0  1652140800   16090   \n 16090    BSTUK2022010  21.395000  61.241501        2.0  1652140800   16091   \n 16091    BSTUK2022011  21.385000  61.343334        NaN  1663200000   16092   \n 16092    BSTUK2022011  21.385000  61.343334        NaN  1663200000   16093   \n 16093    BSTUK2022011  21.385000  61.343334        NaN  1663200000   16094   \n \n        NUCLIDE       VALUE  UNIT       UNC  DL  BIO_GROUP  SPECIES  BODY_PART  \\\n 0           31    0.010140     5       NaN   2          4       99         52   \n 1            4  135.300003     5  4.830210   1          4       99         52   \n 2            9    0.013980     5       NaN   2          4       99         52   \n 3           33    4.338000     5  0.150962   1          4       99         52   \n 4           31    0.009614     5       NaN   2          4       99         52   \n ...        ...         ...   ...       ...  ..        ...      ...        ...   \n 16089       33   13.700000     4  0.520600   1         11       96         55   \n 16090        9    0.500000     4  0.045500   1         11       96         55   \n 16091        4   50.700001     4  4.106700   1         14      129          1   \n 16092       33    0.880000     4  0.140800   1         14      129          1   \n 16093       12    6.600000     4  0.349800   1         14      129          1   \n \n             DRYWT  WETWT  PERCENTWT  \n 0      174.934433  948.0    0.18453  \n 1      174.934433  948.0    0.18453  \n 2      174.934433  948.0    0.18453  \n 3      174.934433  948.0    0.18453  \n 4      177.935120  964.0    0.18458  \n ...           ...    ...        ...  \n 16089         NaN    NaN        NaN  \n 16090         NaN    NaN        NaN  \n 16091         NaN    NaN        NaN  \n 16092         NaN    NaN        NaN  \n 16093         NaN    NaN        NaN  \n \n [16094 rows x 17 columns],\n 'SEAWATER':       SMP_ID_PROVIDER        LON        LAT  SMP_DEPTH  TOT_DEPTH        TIME  \\\n 0        WKRIL2012003  29.333300  60.083302        0.0        NaN  1337731200   \n 1        WKRIL2012004  29.333300  60.083302       29.0        NaN  1337731200   \n 2        WKRIL2012005  23.150000  59.433300        0.0        NaN  1339891200   \n 3        WKRIL2012006  27.983299  60.250000        0.0        NaN  1337817600   \n 4        WKRIL2012007  27.983299  60.250000       39.0        NaN  1337817600   \n ...               ...        ...        ...        ...        ...         ...   \n 21468    WDHIG2023112  13.499833  54.600334        0.0       47.0  1686441600   \n 21469    WDHIG2023113  13.499833  54.600334       45.0       47.0  1686441600   \n 21470    WDHIG2023143  14.200833  54.600334        0.0       11.0  1686614400   \n 21471    WDHIG2023145  14.665500  54.600334        0.0       20.0  1686614400   \n 21472    WDHIG2023147  14.330000  54.600334        0.0       17.0  1686614400   \n \n        SMP_ID  NUCLIDE       VALUE  UNIT        UNC  DL  FILT  \n 0           1       33    5.300000     1   1.696000   1     0  \n 1           2       33   19.900000     1   3.980000   1     0  \n 2           3       33   25.500000     1   5.100000   1     0  \n 3           4       33   17.000000     1   4.930000   1     0  \n 4           5       33   22.200001     1   3.996000   1     0  \n ...       ...      ...         ...   ...        ...  ..   ...  \n 21468   21469        1  702.838074     1  51.276207   1     0  \n 21469   21470        1  725.855713     1  52.686260   1     0  \n 21470   21471        1  648.992920     1  48.154419   1     0  \n 21471   21472        1  627.178406     1  46.245316   1     0  \n 21472   21473        1  605.715088     1  45.691143   1     0  \n \n [21473 rows x 13 columns],\n 'SEDIMENT':       SMP_ID_PROVIDER        LON        LAT  TOT_DEPTH        TIME  SMP_ID  \\\n 0        SKRIL2012116  27.799999  60.466667       25.0  1337904000       1   \n 1        SKRIL2012117  27.799999  60.466667       25.0  1337904000       2   \n 2        SKRIL2012118  27.799999  60.466667       25.0  1337904000       3   \n 3        SKRIL2012119  27.799999  60.466667       25.0  1337904000       4   \n 4        SKRIL2012120  27.799999  60.466667       25.0  1337904000       5   \n ...               ...        ...        ...        ...         ...     ...   \n 70444    SCLOR2022071  15.537800  54.617832       62.0  1654646400   70445   \n 70445    SCLOR2022071  15.537800  54.617832       62.0  1654646400   70446   \n 70446    SCLOR2022072  15.537800  54.617832       62.0  1654646400   70447   \n 70447    SCLOR2022072  15.537800  54.617832       62.0  1654646400   70448   \n 70448    SCLOR2022072  15.537800  54.617832       62.0  1654646400   70449   \n \n        NUCLIDE        VALUE  UNIT         UNC  DL  SED_TYPE   TOP  BOTTOM  \\\n 0           33  1200.000000     3  240.000000   1         0  15.0    20.0   \n 1           33   250.000000     3   50.000000   1         0  20.0    25.0   \n 2           33   140.000000     3   29.400000   1         0  25.0    30.0   \n 3           33    79.000000     3   15.800000   1         0  30.0    35.0   \n 4           33    29.000000     3    6.960000   1         0  35.0    40.0   \n ...        ...          ...   ...         ...  ..       ...   ...     ...   \n 70444       67     0.044000     2    0.015312   1        10  15.0    17.0   \n 70445       77     2.500000     2    0.185000   1        10  15.0    17.0   \n 70446        4  5873.000000     2  164.444000   1        10  17.0    19.0   \n 70447       33    21.200001     2    2.162400   1        10  17.0    19.0   \n 70448       77     0.370000     2    0.048100   1        10  17.0    19.0   \n \n        PERCENTWT  \n 0            NaN  \n 1            NaN  \n 2            NaN  \n 3            NaN  \n 4            NaN  \n ...          ...  \n 70444   0.257642  \n 70445   0.257642  \n 70446   0.263965  \n 70447   0.263965  \n 70448   0.263965  \n \n [70449 rows x 15 columns]}",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#remove-non-compatible-columns",
    "href": "api/netcdf2csv.html#remove-non-compatible-columns",
    "title": "NetCDF2CSV",
    "section": "Remove Non Compatible Columns",
    "text": "Remove Non Compatible Columns\nThe [RemoveNonCompatibleVariablesCB](https://franckalbinet.github.io/marisco/api/netcdf2csv.html#removenoncompatiblevariablescb) callback filters out variables from the NetCDF format that are not listed in the VARS configuration.\n\nsource\n\nRemoveNonCompatibleVariablesCB\n\n RemoveNonCompatibleVariablesCB (vars:Dict[str,str]={'LON': 'longitude',\n                                 'LAT': 'latitude', 'SMP_DEPTH':\n                                 'sampdepth', 'TOT_DEPTH': 'totdepth',\n                                 'TIME': 'begperiod', 'AREA': 'area',\n                                 'NUCLIDE': 'nuclide_id', 'VALUE':\n                                 'activity', 'UNIT': 'unit_id', 'UNC':\n                                 'uncertaint', 'DL': 'detection', 'DLV':\n                                 'detection_lim', 'FILT': 'filtered',\n                                 'COUNT_MET': 'counmet_id', 'SAMP_MET':\n                                 'sampmet_id', 'PREP_MET': 'prepmet_id',\n                                 'VOL': 'volume', 'SAL': 'salinity',\n                                 'TEMP': 'temperatur', 'SPECIES':\n                                 'species_id', 'BODY_PART': 'bodypar_id',\n                                 'SED_TYPE': 'sedtype_id', 'TOP':\n                                 'sliceup', 'BOTTOM': 'slicedown',\n                                 'DRYWT': 'drywt', 'WETWT': 'wetwt',\n                                 'PERCENTWT': 'percentwt', 'LAB':\n                                 'lab_id', 'PROFILE_ID': 'profile_id',\n                                 'SAMPLE_TYPE': 'samptype_id',\n                                 'TAXONNAME': 'taxonname', 'TAXONREPNAME':\n                                 'taxonrepname', 'TAXONRANK': 'taxonrank',\n                                 'TAXONDB': 'taxondb', 'TAXONDBID':\n                                 'taxondb_id', 'TAXONDBURL':\n                                 'taxondb_url', 'REF_ID': 'ref_id',\n                                 'SMP_ID_PROVIDER': 'samplabcode',\n                                 'STATION': 'station'},\n                                 verbose:bool=False)\n\nRemove variables not listed in VARS configuration.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvars\nDict\n{‘LON’: ‘longitude’, ‘LAT’: ‘latitude’, ‘SMP_DEPTH’: ‘sampdepth’, ‘TOT_DEPTH’: ‘totdepth’, ‘TIME’: ‘begperiod’, ‘AREA’: ‘area’, ‘NUCLIDE’: ‘nuclide_id’, ‘VALUE’: ‘activity’, ‘UNIT’: ‘unit_id’, ‘UNC’: ‘uncertaint’, ‘DL’: ‘detection’, ‘DLV’: ‘detection_lim’, ‘FILT’: ‘filtered’, ‘COUNT_MET’: ‘counmet_id’, ‘SAMP_MET’: ‘sampmet_id’, ‘PREP_MET’: ‘prepmet_id’, ‘VOL’: ‘volume’, ‘SAL’: ‘salinity’, ‘TEMP’: ‘temperatur’, ‘SPECIES’: ‘species_id’, ‘BODY_PART’: ‘bodypar_id’, ‘SED_TYPE’: ‘sedtype_id’, ‘TOP’: ‘sliceup’, ‘BOTTOM’: ‘slicedown’, ‘DRYWT’: ‘drywt’, ‘WETWT’: ‘wetwt’, ‘PERCENTWT’: ‘percentwt’, ‘LAB’: ‘lab_id’, ‘PROFILE_ID’: ‘profile_id’, ‘SAMPLE_TYPE’: ‘samptype_id’, ‘TAXONNAME’: ‘taxonname’, ‘TAXONREPNAME’: ‘taxonrepname’, ‘TAXONRANK’: ‘taxonrank’, ‘TAXONDB’: ‘taxondb’, ‘TAXONDBID’: ‘taxondb_id’, ‘TAXONDBURL’: ‘taxondb_url’, ‘REF_ID’: ‘ref_id’, ‘SMP_ID_PROVIDER’: ‘samplabcode’, ‘STATION’: ‘station’}\nDictionary mapping OR vars to NC vars\n\n\nverbose\nbool\nFalse\n\n\n\n\n\ncontents = ExtractNetcdfContents(fname_in)\ntfm = Transformer(\n    data=contents.dfs,\n    custom_maps=contents.custom_maps,\n    cbs=[\n        RemoveNonCompatibleVariablesCB(vars=CSV_VARS, verbose=True),\n    ]\n)\ntfm()\nprint(tfm.dfs['BIOTA'].head())\n\nRemoving variables that are not compatible with vars provided. \nRemoving SMP_ID, BIO_GROUP from BIOTA dataset.\nRemoving variables that are not compatible with vars provided. \nRemoving SMP_ID from SEAWATER dataset.\nRemoving variables that are not compatible with vars provided. \nRemoving SMP_ID from SEDIMENT dataset.\n  SMP_ID_PROVIDER        LON        LAT  SMP_DEPTH        TIME  NUCLIDE  \\\n0    BVTIG2012041  12.316667  54.283333        NaN  1348358400       31   \n1    BVTIG2012041  12.316667  54.283333        NaN  1348358400        4   \n2    BVTIG2012041  12.316667  54.283333        NaN  1348358400        9   \n3    BVTIG2012041  12.316667  54.283333        NaN  1348358400       33   \n4    BVTIG2012040  12.316667  54.283333        NaN  1348358400       31   \n\n        VALUE  UNIT       UNC  DL  SPECIES  BODY_PART       DRYWT  WETWT  \\\n0    0.010140     5       NaN   2       99         52  174.934433  948.0   \n1  135.300003     5  4.830210   1       99         52  174.934433  948.0   \n2    0.013980     5       NaN   2       99         52  174.934433  948.0   \n3    4.338000     5  0.150962   1       99         52  174.934433  948.0   \n4    0.009614     5       NaN   2       99         52  177.935120  964.0   \n\n   PERCENTWT  \n0    0.18453  \n1    0.18453  \n2    0.18453  \n3    0.18453  \n4    0.18458",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#sample-id-handling",
    "href": "api/netcdf2csv.html#sample-id-handling",
    "title": "NetCDF2CSV",
    "section": "Sample ID Handling",
    "text": "Sample ID Handling\nThe MARISCO NetCDF encoding pipeline uses SMP_ID to store unique identifiers for each sample measurement. These IDs can come from two sources:\n\nDirectly from data providers who supply their own unique identifiers\nAuto-generated by MARISCO as incremented integers when providers don’t supply IDs\n\nThe Maris database ingestion pipeline expects:\n\nThe original SMP_ID if provided by the data source\nNone if the ID was auto-generated by MARISCO\n\nNote: When writing to CSV, SMP_ID is renamed to samplabcode as defined in the CSV_VARS mapping in configs.ipynb.\nThe callback below implements this logic by providing an option to convert auto-generated SMP_ID values to None while preserving original provider-supplied IDs.\n\nsource\n\nSampleIDConversionCB\n\n SampleIDConversionCB (nonify:bool=False)\n\nConvert auto-generated SMP_ID values to None while preserving original provider-supplied IDs.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnonify\nbool\nFalse\nIf True, convert auto-generated SMP_ID values to None\n\n\n\n\ncontents = ExtractNetcdfContents(fname_in)\ntfm = Transformer(\n    data=contents.dfs,\n    custom_maps=contents.custom_maps,\n    cbs=[\n        RemoveNonCompatibleVariablesCB(vars=CSV_VARS, verbose=True),\n        # SampleIDConversionCB(nonify=True)\n    ]\n)\ntfm()\nprint(tfm.dfs['BIOTA'].head())\n\nRemoving variables that are not compatible with vars provided. \nRemoving SMP_ID, BIO_GROUP from BIOTA dataset.\nRemoving variables that are not compatible with vars provided. \nRemoving SMP_ID from SEAWATER dataset.\nRemoving variables that are not compatible with vars provided. \nRemoving SMP_ID from SEDIMENT dataset.\n  SMP_ID_PROVIDER        LON        LAT  SMP_DEPTH        TIME  NUCLIDE  \\\n0    BVTIG2012041  12.316667  54.283333        NaN  1348358400       31   \n1    BVTIG2012041  12.316667  54.283333        NaN  1348358400        4   \n2    BVTIG2012041  12.316667  54.283333        NaN  1348358400        9   \n3    BVTIG2012041  12.316667  54.283333        NaN  1348358400       33   \n4    BVTIG2012040  12.316667  54.283333        NaN  1348358400       31   \n\n        VALUE  UNIT       UNC  DL  SPECIES  BODY_PART       DRYWT  WETWT  \\\n0    0.010140     5       NaN   2       99         52  174.934433  948.0   \n1  135.300003     5  4.830210   1       99         52  174.934433  948.0   \n2    0.013980     5       NaN   2       99         52  174.934433  948.0   \n3    4.338000     5  0.150962   1       99         52  174.934433  948.0   \n4    0.009614     5       NaN   2       99         52  177.935120  964.0   \n\n   PERCENTWT  \n0    0.18453  \n1    0.18453  \n2    0.18453  \n3    0.18453  \n4    0.18458",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#add-taxon-information",
    "href": "api/netcdf2csv.html#add-taxon-information",
    "title": "NetCDF2CSV",
    "section": "Add Taxon Information",
    "text": "Add Taxon Information\n\nsource\n\nget_taxon_info_lut\n\n get_taxon_info_lut (maris_lut:str, key_names:dict={'Taxonname':\n                     'TAXONNAME', 'Taxonrank': 'TAXONRANK', 'TaxonDB':\n                     'TAXONDB', 'TaxonDBID': 'TAXONDBID', 'TaxonDBURL':\n                     'TAXONDBURL'})\n\nCreate lookup dictionary for taxon information from MARIS species lookup table.\n\nsource\n\n\nAddTaxonInformationCB\n\n AddTaxonInformationCB (fn_lut:Callable=&lt;function &lt;lambda&gt;&gt;,\n                        verbose:bool=False)\n\nAdd taxon information to BIOTA group based on species lookup table.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfn_lut\nCallable\n\nFunction that returns taxon lookup dictionary\n\n\nverbose\nbool\nFalse\n\n\n\n\n\ncontents = ExtractNetcdfContents(fname_in)\ntfm = Transformer(\n    data=contents.dfs,\n    custom_maps=contents.custom_maps,\n    cbs=[\n        AddTaxonInformationCB(\n            fn_lut=lut_taxon\n        ),\n    ]\n)\n\ntfm()\n\n{'BIOTA':       SMP_ID_PROVIDER        LON        LAT  SMP_DEPTH        TIME  SMP_ID  \\\n 0        BVTIG2012041  12.316667  54.283333        NaN  1348358400       1   \n 1        BVTIG2012041  12.316667  54.283333        NaN  1348358400       2   \n 2        BVTIG2012041  12.316667  54.283333        NaN  1348358400       3   \n 3        BVTIG2012041  12.316667  54.283333        NaN  1348358400       4   \n 4        BVTIG2012040  12.316667  54.283333        NaN  1348358400       5   \n ...               ...        ...        ...        ...         ...     ...   \n 16089    BSTUK2022010  21.395000  61.241501        2.0  1652140800   16090   \n 16090    BSTUK2022010  21.395000  61.241501        2.0  1652140800   16091   \n 16091    BSTUK2022011  21.385000  61.343334        NaN  1663200000   16092   \n 16092    BSTUK2022011  21.385000  61.343334        NaN  1663200000   16093   \n 16093    BSTUK2022011  21.385000  61.343334        NaN  1663200000   16094   \n \n        NUCLIDE       VALUE  UNIT       UNC  ...  SPECIES  BODY_PART  \\\n 0           31    0.010140     5       NaN  ...       99         52   \n 1            4  135.300003     5  4.830210  ...       99         52   \n 2            9    0.013980     5       NaN  ...       99         52   \n 3           33    4.338000     5  0.150962  ...       99         52   \n 4           31    0.009614     5       NaN  ...       99         52   \n ...        ...         ...   ...       ...  ...      ...        ...   \n 16089       33   13.700000     4  0.520600  ...       96         55   \n 16090        9    0.500000     4  0.045500  ...       96         55   \n 16091        4   50.700001     4  4.106700  ...      129          1   \n 16092       33    0.880000     4  0.140800  ...      129          1   \n 16093       12    6.600000     4  0.349800  ...      129          1   \n \n             DRYWT  WETWT  PERCENTWT          TAXONNAME  TAXONRANK   TAXONDB  \\\n 0      174.934433  948.0    0.18453       Gadus morhua    species  Wikidata   \n 1      174.934433  948.0    0.18453       Gadus morhua    species  Wikidata   \n 2      174.934433  948.0    0.18453       Gadus morhua    species  Wikidata   \n 3      174.934433  948.0    0.18453       Gadus morhua    species  Wikidata   \n 4      177.935120  964.0    0.18458       Gadus morhua    species  Wikidata   \n ...           ...    ...        ...                ...        ...       ...   \n 16089         NaN    NaN        NaN  Fucus vesiculosus    species  Wikidata   \n 16090         NaN    NaN        NaN  Fucus vesiculosus    species  Wikidata   \n 16091         NaN    NaN        NaN     Mytilus edulis    species  Wikidata   \n 16092         NaN    NaN        NaN     Mytilus edulis    species  Wikidata   \n 16093         NaN    NaN        NaN     Mytilus edulis    species  Wikidata   \n \n       TAXONDBID                             TAXONDBURL  \n 0       Q199788  https://www.wikidata.org/wiki/Q199788  \n 1       Q199788  https://www.wikidata.org/wiki/Q199788  \n 2       Q199788  https://www.wikidata.org/wiki/Q199788  \n 3       Q199788  https://www.wikidata.org/wiki/Q199788  \n 4       Q199788  https://www.wikidata.org/wiki/Q199788  \n ...         ...                                    ...  \n 16089   Q754755  https://www.wikidata.org/wiki/Q754755  \n 16090   Q754755  https://www.wikidata.org/wiki/Q754755  \n 16091    Q27855   https://www.wikidata.org/wiki/Q27855  \n 16092    Q27855   https://www.wikidata.org/wiki/Q27855  \n 16093    Q27855   https://www.wikidata.org/wiki/Q27855  \n \n [16094 rows x 22 columns],\n 'SEAWATER':       SMP_ID_PROVIDER        LON        LAT  SMP_DEPTH  TOT_DEPTH        TIME  \\\n 0        WKRIL2012003  29.333300  60.083302        0.0        NaN  1337731200   \n 1        WKRIL2012004  29.333300  60.083302       29.0        NaN  1337731200   \n 2        WKRIL2012005  23.150000  59.433300        0.0        NaN  1339891200   \n 3        WKRIL2012006  27.983299  60.250000        0.0        NaN  1337817600   \n 4        WKRIL2012007  27.983299  60.250000       39.0        NaN  1337817600   \n ...               ...        ...        ...        ...        ...         ...   \n 21468    WDHIG2023112  13.499833  54.600334        0.0       47.0  1686441600   \n 21469    WDHIG2023113  13.499833  54.600334       45.0       47.0  1686441600   \n 21470    WDHIG2023143  14.200833  54.600334        0.0       11.0  1686614400   \n 21471    WDHIG2023145  14.665500  54.600334        0.0       20.0  1686614400   \n 21472    WDHIG2023147  14.330000  54.600334        0.0       17.0  1686614400   \n \n        SMP_ID  NUCLIDE       VALUE  UNIT        UNC  DL  FILT  \n 0           1       33    5.300000     1   1.696000   1     0  \n 1           2       33   19.900000     1   3.980000   1     0  \n 2           3       33   25.500000     1   5.100000   1     0  \n 3           4       33   17.000000     1   4.930000   1     0  \n 4           5       33   22.200001     1   3.996000   1     0  \n ...       ...      ...         ...   ...        ...  ..   ...  \n 21468   21469        1  702.838074     1  51.276207   1     0  \n 21469   21470        1  725.855713     1  52.686260   1     0  \n 21470   21471        1  648.992920     1  48.154419   1     0  \n 21471   21472        1  627.178406     1  46.245316   1     0  \n 21472   21473        1  605.715088     1  45.691143   1     0  \n \n [21473 rows x 13 columns],\n 'SEDIMENT':       SMP_ID_PROVIDER        LON        LAT  TOT_DEPTH        TIME  SMP_ID  \\\n 0        SKRIL2012116  27.799999  60.466667       25.0  1337904000       1   \n 1        SKRIL2012117  27.799999  60.466667       25.0  1337904000       2   \n 2        SKRIL2012118  27.799999  60.466667       25.0  1337904000       3   \n 3        SKRIL2012119  27.799999  60.466667       25.0  1337904000       4   \n 4        SKRIL2012120  27.799999  60.466667       25.0  1337904000       5   \n ...               ...        ...        ...        ...         ...     ...   \n 70444    SCLOR2022071  15.537800  54.617832       62.0  1654646400   70445   \n 70445    SCLOR2022071  15.537800  54.617832       62.0  1654646400   70446   \n 70446    SCLOR2022072  15.537800  54.617832       62.0  1654646400   70447   \n 70447    SCLOR2022072  15.537800  54.617832       62.0  1654646400   70448   \n 70448    SCLOR2022072  15.537800  54.617832       62.0  1654646400   70449   \n \n        NUCLIDE        VALUE  UNIT         UNC  DL  SED_TYPE   TOP  BOTTOM  \\\n 0           33  1200.000000     3  240.000000   1         0  15.0    20.0   \n 1           33   250.000000     3   50.000000   1         0  20.0    25.0   \n 2           33   140.000000     3   29.400000   1         0  25.0    30.0   \n 3           33    79.000000     3   15.800000   1         0  30.0    35.0   \n 4           33    29.000000     3    6.960000   1         0  35.0    40.0   \n ...        ...          ...   ...         ...  ..       ...   ...     ...   \n 70444       67     0.044000     2    0.015312   1        10  15.0    17.0   \n 70445       77     2.500000     2    0.185000   1        10  15.0    17.0   \n 70446        4  5873.000000     2  164.444000   1        10  17.0    19.0   \n 70447       33    21.200001     2    2.162400   1        10  17.0    19.0   \n 70448       77     0.370000     2    0.048100   1        10  17.0    19.0   \n \n        PERCENTWT  \n 0            NaN  \n 1            NaN  \n 2            NaN  \n 3            NaN  \n 4            NaN  \n ...          ...  \n 70444   0.257642  \n 70445   0.257642  \n 70446   0.263965  \n 70447   0.263965  \n 70448   0.263965  \n \n [70449 rows x 15 columns]}",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#standardize-time",
    "href": "api/netcdf2csv.html#standardize-time",
    "title": "NetCDF2CSV",
    "section": "Standardize Time",
    "text": "Standardize Time\n\ncontents = ExtractNetcdfContents(fname_in)\ntfm = Transformer(\n    data=contents.dfs,\n    custom_maps=contents.custom_maps,\n    cbs=[\n        DecodeTimeCB(),\n    ]\n)\n\ntfm()\n\nprint(tfm.dfs['SEAWATER']['TIME'])\n\n0       2012-05-23\n1       2012-05-23\n2       2012-06-17\n3       2012-05-24\n4       2012-05-24\n           ...    \n21468   2023-06-11\n21469   2023-06-11\n21470   2023-06-13\n21471   2023-06-13\n21472   2023-06-13\nName: TIME, Length: 21473, dtype: datetime64[ns]",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#add-sample-type-id",
    "href": "api/netcdf2csv.html#add-sample-type-id",
    "title": "NetCDF2CSV",
    "section": "Add Sample Type ID",
    "text": "Add Sample Type ID\n\ncontents = ExtractNetcdfContents(fname_in)\ntfm = Transformer(\n    data=contents.dfs,\n    custom_maps=contents.custom_maps,\n    cbs=[\n        AddSampleTypeIdColumnCB(),\n    ]\n)\n\ntfm()\nprint(tfm.dfs['SEAWATER']['SAMPLE_TYPE'].unique())\n# print(tfm.dfs['BIOTA']['SAMPLE_TYPE'].unique())\n# print(tfm.dfs['SEDIMENT']['SAMPLE_TYPE'].unique())\n\n[1]",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#add-reference-id",
    "href": "api/netcdf2csv.html#add-reference-id",
    "title": "NetCDF2CSV",
    "section": "Add Reference ID",
    "text": "Add Reference ID\nInclude the ref_id (i.e., Zotero Archive Location). The ZoteroArchiveLocationCB performs a lookup of the Zotero Archive Location based on the Zotero key defined in the global attributes of the MARIS NetCDF file as id.\n\ncontents.global_attrs['id']\n\n'26VMZZ2Q'\n\n\n\nsource\n\nAddZoteroArchiveLocationCB\n\n AddZoteroArchiveLocationCB (attrs:str, cfg:dict)\n\nFetch and append ‘Loc. in Archive’ from Zotero to DataFrame.\n\ncontents = ExtractNetcdfContents(fname_in)\ntfm = Transformer(\n    data=contents.dfs,\n    custom_maps=contents.custom_maps,\n    cbs=[\n        AddZoteroArchiveLocationCB(contents.global_attrs, cfg=cfg()),\n    ]\n)\ntfm()\nprint(tfm.dfs['SEAWATER']['REF_ID'].unique())\n\n[100]",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#remap-to-open-refine-specific-mappings",
    "href": "api/netcdf2csv.html#remap-to-open-refine-specific-mappings",
    "title": "NetCDF2CSV",
    "section": "Remap to Open Refine specific mappings",
    "text": "Remap to Open Refine specific mappings\n\n\n\n\n\n\nWarningFEEDBACK FOR NEXT VERSION\n\n\n\n[To be further clarified]\nThe current approach of remapping to OR-specific mappings should be reconsidered. Considering that we already utilize MARISCO lookup tables in NetCDF for creating enums, it would be beneficial to extend their use to OpenRefine data formats as well. By doing so, we could eliminate the need for OpenRefine-specific mappings, streamlining the data transformation process. Lets review the lookup tables used to create the enums for NetCDF:\n\n\n\nenums = Enums(lut_src_dir=lut_path())\nprint(f'DL enums: {enums.types[\"DL\"]}')\nprint(f'FILT enums: {enums.types[\"FILT\"]}')\n\nDL enums: {'Not applicable': -1, 'Not available': 0, 'Detected value': 1, 'Detection limit': 2, 'Not detected': 3, 'Derived': 4}\nFILT enums: {'Not applicable': -1, 'Not available': 0, 'Yes': 1, 'No': 2}\n\n\nFor the detection limit lookup table (LUT), as shown below, the values required for the OpenRefine CSV format are listed under the ‘name’ column, whereas the enums utilize the ‘name_sanitized’ column.\nAdditionally, for the filtered LUT, also shown below, the values do not align consistently with the OpenRefine CSV format, which uses (Y, N, NA).\n\ndl_lut = pd.read_excel(detection_limit_lut_path())\ndl_lut\n\n\n\n\n\n\n\n\nid\nname\nname_sanitized\n\n\n\n\n0\n-1\nNot applicable\nNot applicable\n\n\n1\n0\nNot Available\nNot available\n\n\n2\n1\n=\nDetected value\n\n\n3\n2\n&lt;\nDetection limit\n\n\n4\n3\nND\nNot detected\n\n\n5\n4\nDE\nDerived\n\n\n\n\n\n\n\n\nfiltered_lut = pd.read_excel(filtered_lut_path())\nfiltered_lut\n\n\n\n\n\n\n\n\nid\nname\n\n\n\n\n0\n-1\nNot applicable\n\n\n1\n0\nNot available\n\n\n2\n1\nYes\n\n\n3\n2\nNo\n\n\n\n\n\n\n\nWe will create OpenRefine specific mappings for the detection limit and filtered data:\nRemapToORSpecificMappingsCB remaps the values of the detection limit and filtered data to the OpenRefine CSV format.\n\nsource\n\nRemapToORSpecificMappingsCB\n\n RemapToORSpecificMappingsCB (or_mappings:Dict[str,Dict]={'DL': {3: 'ND',\n                              1: '=', 2: '&lt;'}, 'FILT': {0: 'NA', 1: 'Y',\n                              2: 'N'}},\n                              output_format:str='openrefine_csv',\n                              verbose:bool=False)\n\nConvert values using OR mappings if columns exist in dataframe.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nor_mappings\nDict\n{‘DL’: {3: ‘ND’, 1: ‘=’, 2: ‘&lt;’}, ‘FILT’: {0: ‘NA’, 1: ‘Y’, 2: ‘N’}}\nDictionary of column mappings,\n\n\noutput_format\nstr\nopenrefine_csv\n\n\n\nverbose\nbool\nFalse\n\n\n\n\n\ncontents = ExtractNetcdfContents(fname_in)\ntfm = Transformer(\n    data= contents.dfs,\n    custom_maps=contents.custom_maps,\n    cbs=[\n        RemapToORSpecificMappingsCB(),\n    ]\n)\n\ntfm()\n\n# Loop through each group in the 'dfs' dictionary\nfor group_name, df in tfm.dfs.items():\n    # Check if the group dataframe contains any of the columns specified in or_mappings.keys()\n    relevant_columns = [col for col in or_mappings.keys() if col in df.columns]\n    if relevant_columns:\n        # Print the unique values from the relevant columns\n        print(f\"\\nUnique values in {group_name} for columns {relevant_columns}:\")\n        for col in relevant_columns:\n            print(f\"{col}: {df[col].unique()}\")\n    else:\n        print(f\"No relevant columns found in {group_name} based on or_mappings keys.\")\n\n\nUnique values in BIOTA for columns ['DL']:\nDL: ['&lt;' '=' nan]\n\nUnique values in SEAWATER for columns ['DL', 'FILT']:\nDL: ['=' '&lt;' nan]\nFILT: ['NA' 'N' 'Y']\n\nUnique values in SEDIMENT for columns ['DL']:\nDL: ['=' '&lt;' nan]",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#remap-to-csv-data-type-format",
    "href": "api/netcdf2csv.html#remap-to-csv-data-type-format",
    "title": "NetCDF2CSV",
    "section": "Remap to CSV data type format",
    "text": "Remap to CSV data type format\nCSV_DTYPES (defined in configs.ipynb) defines a state for each variable that contains a lookup table (i.e. enums). The state is either ‘decoded’ or ‘encoded’. Lets review the variable states as a DataFrame:\n\nwith pd.option_context('display.max_columns', None, 'display.max_colwidth', None):\n    display(pd.DataFrame.from_dict(CSV_DTYPES, orient='index').T)\n\n\n\n\n\n\n\n\nAREA\nNUCLIDE\nUNIT\nDL\nFILT\nCOUNT_MET\nSAMP_MET\nPREP_MET\nSPECIES\nBODY_PART\nSED_TYPE\nLAB\n\n\n\n\nstate\ndecoded\nencoded\nencoded\ndecoded\ndecoded\nencoded\nencoded\nencoded\nencoded\nencoded\nencoded\nencoded\n\n\n\n\n\n\n\n\nenums = Enums(lut_src_dir=lut_path())\nenums.types.keys()\n\ndict_keys(['AREA', 'BIO_GROUP', 'BODY_PART', 'COUNT_MET', 'DL', 'FILT', 'NUCLIDE', 'PREP_MET', 'SAMP_MET', 'SED_TYPE', 'SPECIES', 'UNIT', 'LAB'])\n\n\n\nsource\n\nget_excluded_enums\n\n get_excluded_enums (output_format:str='openrefine_csv')\n\nGet excluded enums based on output format.\n\nsource\n\n\nDataFormatConversionCB\n\n DataFormatConversionCB (dtypes:Dict, excluded_mappings:Callable=&lt;function\n                         get_excluded_enums&gt;,\n                         output_format:str='openrefine_csv',\n                         verbose:bool=False)\n\nA callback to convert DataFrame enum values between encoded and decoded formats based on specified settings.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndtypes\nDict\n\nDictionary defining data types and states for each lookup table\n\n\nexcluded_mappings\nCallable\nget_excluded_enums\nDictionary of columns to exclude from conversion\n\n\noutput_format\nstr\nopenrefine_csv\n\n\n\nverbose\nbool\nFalse\nFlag for verbose output\n\n\n\n\ncontents = ExtractNetcdfContents(fname_in)\ntfm = Transformer(\n    contents.dfs,\n    cbs=[\n        RemoveNonCompatibleVariablesCB(vars=CSV_VARS, verbose=True),\n        DataFormatConversionCB(\n            dtypes=CSV_DTYPES,\n            excluded_mappings = get_excluded_enums,\n            output_format='openrefine_csv',\n            verbose=True\n        ),\n    ]\n)\ntfm()\n\nRemoving variables that are not compatible with vars provided. \nRemoving SMP_ID, BIO_GROUP from BIOTA dataset.\nRemoving variables that are not compatible with vars provided. \nRemoving SMP_ID from SEAWATER dataset.\nRemoving variables that are not compatible with vars provided. \nRemoving SMP_ID from SEDIMENT dataset.\nLoaded enums: dict_keys(['AREA', 'BIO_GROUP', 'BODY_PART', 'COUNT_MET', 'DL', 'FILT', 'NUCLIDE', 'PREP_MET', 'SAMP_MET', 'SED_TYPE', 'SPECIES', 'UNIT', 'LAB'])\n\n\n{'BIOTA':       SMP_ID_PROVIDER        LON        LAT  SMP_DEPTH        TIME  NUCLIDE  \\\n 0        BVTIG2012041  12.316667  54.283333        NaN  1348358400       31   \n 1        BVTIG2012041  12.316667  54.283333        NaN  1348358400        4   \n 2        BVTIG2012041  12.316667  54.283333        NaN  1348358400        9   \n 3        BVTIG2012041  12.316667  54.283333        NaN  1348358400       33   \n 4        BVTIG2012040  12.316667  54.283333        NaN  1348358400       31   \n ...               ...        ...        ...        ...         ...      ...   \n 16089    BSTUK2022010  21.395000  61.241501        2.0  1652140800       33   \n 16090    BSTUK2022010  21.395000  61.241501        2.0  1652140800        9   \n 16091    BSTUK2022011  21.385000  61.343334        NaN  1663200000        4   \n 16092    BSTUK2022011  21.385000  61.343334        NaN  1663200000       33   \n 16093    BSTUK2022011  21.385000  61.343334        NaN  1663200000       12   \n \n             VALUE  UNIT       UNC  DL  SPECIES  BODY_PART       DRYWT  WETWT  \\\n 0        0.010140     5       NaN   2       99         52  174.934433  948.0   \n 1      135.300003     5  4.830210   1       99         52  174.934433  948.0   \n 2        0.013980     5       NaN   2       99         52  174.934433  948.0   \n 3        4.338000     5  0.150962   1       99         52  174.934433  948.0   \n 4        0.009614     5       NaN   2       99         52  177.935120  964.0   \n ...           ...   ...       ...  ..      ...        ...         ...    ...   \n 16089   13.700000     4  0.520600   1       96         55         NaN    NaN   \n 16090    0.500000     4  0.045500   1       96         55         NaN    NaN   \n 16091   50.700001     4  4.106700   1      129          1         NaN    NaN   \n 16092    0.880000     4  0.140800   1      129          1         NaN    NaN   \n 16093    6.600000     4  0.349800   1      129          1         NaN    NaN   \n \n        PERCENTWT  \n 0        0.18453  \n 1        0.18453  \n 2        0.18453  \n 3        0.18453  \n 4        0.18458  \n ...          ...  \n 16089        NaN  \n 16090        NaN  \n 16091        NaN  \n 16092        NaN  \n 16093        NaN  \n \n [16094 rows x 15 columns],\n 'SEAWATER':       SMP_ID_PROVIDER        LON        LAT  SMP_DEPTH  TOT_DEPTH        TIME  \\\n 0        WKRIL2012003  29.333300  60.083302        0.0        NaN  1337731200   \n 1        WKRIL2012004  29.333300  60.083302       29.0        NaN  1337731200   \n 2        WKRIL2012005  23.150000  59.433300        0.0        NaN  1339891200   \n 3        WKRIL2012006  27.983299  60.250000        0.0        NaN  1337817600   \n 4        WKRIL2012007  27.983299  60.250000       39.0        NaN  1337817600   \n ...               ...        ...        ...        ...        ...         ...   \n 21468    WDHIG2023112  13.499833  54.600334        0.0       47.0  1686441600   \n 21469    WDHIG2023113  13.499833  54.600334       45.0       47.0  1686441600   \n 21470    WDHIG2023143  14.200833  54.600334        0.0       11.0  1686614400   \n 21471    WDHIG2023145  14.665500  54.600334        0.0       20.0  1686614400   \n 21472    WDHIG2023147  14.330000  54.600334        0.0       17.0  1686614400   \n \n        NUCLIDE       VALUE  UNIT        UNC  DL  FILT  \n 0           33    5.300000     1   1.696000   1     0  \n 1           33   19.900000     1   3.980000   1     0  \n 2           33   25.500000     1   5.100000   1     0  \n 3           33   17.000000     1   4.930000   1     0  \n 4           33   22.200001     1   3.996000   1     0  \n ...        ...         ...   ...        ...  ..   ...  \n 21468        1  702.838074     1  51.276207   1     0  \n 21469        1  725.855713     1  52.686260   1     0  \n 21470        1  648.992920     1  48.154419   1     0  \n 21471        1  627.178406     1  46.245316   1     0  \n 21472        1  605.715088     1  45.691143   1     0  \n \n [21473 rows x 12 columns],\n 'SEDIMENT':       SMP_ID_PROVIDER        LON        LAT  TOT_DEPTH        TIME  NUCLIDE  \\\n 0        SKRIL2012116  27.799999  60.466667       25.0  1337904000       33   \n 1        SKRIL2012117  27.799999  60.466667       25.0  1337904000       33   \n 2        SKRIL2012118  27.799999  60.466667       25.0  1337904000       33   \n 3        SKRIL2012119  27.799999  60.466667       25.0  1337904000       33   \n 4        SKRIL2012120  27.799999  60.466667       25.0  1337904000       33   \n ...               ...        ...        ...        ...         ...      ...   \n 70444    SCLOR2022071  15.537800  54.617832       62.0  1654646400       67   \n 70445    SCLOR2022071  15.537800  54.617832       62.0  1654646400       77   \n 70446    SCLOR2022072  15.537800  54.617832       62.0  1654646400        4   \n 70447    SCLOR2022072  15.537800  54.617832       62.0  1654646400       33   \n 70448    SCLOR2022072  15.537800  54.617832       62.0  1654646400       77   \n \n              VALUE  UNIT         UNC  DL  SED_TYPE   TOP  BOTTOM  PERCENTWT  \n 0      1200.000000     3  240.000000   1         0  15.0    20.0        NaN  \n 1       250.000000     3   50.000000   1         0  20.0    25.0        NaN  \n 2       140.000000     3   29.400000   1         0  25.0    30.0        NaN  \n 3        79.000000     3   15.800000   1         0  30.0    35.0        NaN  \n 4        29.000000     3    6.960000   1         0  35.0    40.0        NaN  \n ...            ...   ...         ...  ..       ...   ...     ...        ...  \n 70444     0.044000     2    0.015312   1        10  15.0    17.0   0.257642  \n 70445     2.500000     2    0.185000   1        10  15.0    17.0   0.257642  \n 70446  5873.000000     2  164.444000   1        10  17.0    19.0   0.263965  \n 70447    21.200001     2    2.162400   1        10  17.0    19.0   0.263965  \n 70448     0.370000     2    0.048100   1        10  17.0    19.0   0.263965  \n \n [70449 rows x 14 columns]}",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#review-all-callbacks",
    "href": "api/netcdf2csv.html#review-all-callbacks",
    "title": "NetCDF2CSV",
    "section": "Review all callbacks",
    "text": "Review all callbacks\n\ncontents = ExtractNetcdfContents(fname_in)\noutput_format = 'openrefine_csv'\ntfm = Transformer(\n    data=contents.dfs,\n    custom_maps=contents.custom_maps,\n    cbs=[\n        ValidateEnumsCB(\n            contents = contents,\n            maris_enums=Enums(lut_src_dir=lut_path())\n        ),\n        RemoveNonCompatibleVariablesCB(vars=CSV_VARS),\n        # SampleIDConversionCB(nonify=False),\n        # RemapCustomMapsCB(),\n        RemapToORSpecificMappingsCB(output_format=output_format),\n        AddTaxonInformationCB(\n            fn_lut=lut_taxon\n        ),\n        DecodeTimeCB(),\n        AddSampleTypeIdColumnCB(),\n        AddZoteroArchiveLocationCB(contents.global_attrs, cfg=cfg()),\n        DataFormatConversionCB(\n            dtypes=CSV_DTYPES,\n            excluded_mappings = get_excluded_enums,\n            output_format=output_format,\n        ) \n        ]\n)\ntfm()\nfor grp in ['SEAWATER', 'BIOTA']:\n    display(Markdown(f\"&lt;b&gt;Head of the transformed `{grp}` DataFrame:&lt;/b&gt;\"))\n    with pd.option_context('display.max_rows', None):\n        display(tfm.dfs[grp].head())\n\nHead of the transformed SEAWATER DataFrame:\n\n\n\n\n\n\n\n\n\nSMP_ID_PROVIDER\nLON\nLAT\nSMP_DEPTH\nTOT_DEPTH\nTIME\nNUCLIDE\nVALUE\nUNIT\nUNC\nDL\nFILT\nSAMPLE_TYPE\nREF_ID\n\n\n\n\n0\nWKRIL2012003\n29.333300\n60.083302\n0.0\nNaN\n2012-05-23\n33\n5.300000\n1\n1.696\n=\nNA\n1\n100\n\n\n1\nWKRIL2012004\n29.333300\n60.083302\n29.0\nNaN\n2012-05-23\n33\n19.900000\n1\n3.980\n=\nNA\n1\n100\n\n\n2\nWKRIL2012005\n23.150000\n59.433300\n0.0\nNaN\n2012-06-17\n33\n25.500000\n1\n5.100\n=\nNA\n1\n100\n\n\n3\nWKRIL2012006\n27.983299\n60.250000\n0.0\nNaN\n2012-05-24\n33\n17.000000\n1\n4.930\n=\nNA\n1\n100\n\n\n4\nWKRIL2012007\n27.983299\n60.250000\n39.0\nNaN\n2012-05-24\n33\n22.200001\n1\n3.996\n=\nNA\n1\n100\n\n\n\n\n\n\n\nHead of the transformed BIOTA DataFrame:\n\n\n\n\n\n\n\n\n\nSMP_ID_PROVIDER\nLON\nLAT\nSMP_DEPTH\nTIME\nNUCLIDE\nVALUE\nUNIT\nUNC\nDL\n...\nDRYWT\nWETWT\nPERCENTWT\nTAXONNAME\nTAXONRANK\nTAXONDB\nTAXONDBID\nTAXONDBURL\nSAMPLE_TYPE\nREF_ID\n\n\n\n\n0\nBVTIG2012041\n12.316667\n54.283333\nNaN\n2012-09-23\n31\n0.010140\n5\nNaN\n&lt;\n...\n174.934433\n948.0\n0.18453\nGadus morhua\nspecies\nWikidata\nQ199788\nhttps://www.wikidata.org/wiki/Q199788\n2\n100\n\n\n1\nBVTIG2012041\n12.316667\n54.283333\nNaN\n2012-09-23\n4\n135.300003\n5\n4.830210\n=\n...\n174.934433\n948.0\n0.18453\nGadus morhua\nspecies\nWikidata\nQ199788\nhttps://www.wikidata.org/wiki/Q199788\n2\n100\n\n\n2\nBVTIG2012041\n12.316667\n54.283333\nNaN\n2012-09-23\n9\n0.013980\n5\nNaN\n&lt;\n...\n174.934433\n948.0\n0.18453\nGadus morhua\nspecies\nWikidata\nQ199788\nhttps://www.wikidata.org/wiki/Q199788\n2\n100\n\n\n3\nBVTIG2012041\n12.316667\n54.283333\nNaN\n2012-09-23\n33\n4.338000\n5\n0.150962\n=\n...\n174.934433\n948.0\n0.18453\nGadus morhua\nspecies\nWikidata\nQ199788\nhttps://www.wikidata.org/wiki/Q199788\n2\n100\n\n\n4\nBVTIG2012040\n12.316667\n54.283333\nNaN\n2012-09-23\n31\n0.009614\n5\nNaN\n&lt;\n...\n177.935120\n964.0\n0.18458\nGadus morhua\nspecies\nWikidata\nQ199788\nhttps://www.wikidata.org/wiki/Q199788\n2\n100\n\n\n\n\n5 rows × 22 columns",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/netcdf2csv.html#decode",
    "href": "api/netcdf2csv.html#decode",
    "title": "NetCDF2CSV",
    "section": "Decode",
    "text": "Decode\n\nsource\n\ndecode\n\n decode (fname_in:str, dest_out:str|None=None,\n         output_format:str='openrefine_csv',\n         remap_vars:Dict[str,str]={'LON': 'longitude', 'LAT': 'latitude',\n         'SMP_DEPTH': 'sampdepth', 'TOT_DEPTH': 'totdepth', 'TIME':\n         'begperiod', 'AREA': 'area', 'NUCLIDE': 'nuclide_id', 'VALUE':\n         'activity', 'UNIT': 'unit_id', 'UNC': 'uncertaint', 'DL':\n         'detection', 'DLV': 'detection_lim', 'FILT': 'filtered',\n         'COUNT_MET': 'counmet_id', 'SAMP_MET': 'sampmet_id', 'PREP_MET':\n         'prepmet_id', 'VOL': 'volume', 'SAL': 'salinity', 'TEMP':\n         'temperatur', 'SPECIES': 'species_id', 'BODY_PART': 'bodypar_id',\n         'SED_TYPE': 'sedtype_id', 'TOP': 'sliceup', 'BOTTOM':\n         'slicedown', 'DRYWT': 'drywt', 'WETWT': 'wetwt', 'PERCENTWT':\n         'percentwt', 'LAB': 'lab_id', 'PROFILE_ID': 'profile_id',\n         'SAMPLE_TYPE': 'samptype_id', 'TAXONNAME': 'taxonname',\n         'TAXONREPNAME': 'taxonrepname', 'TAXONRANK': 'taxonrank',\n         'TAXONDB': 'taxondb', 'TAXONDBID': 'taxondb_id', 'TAXONDBURL':\n         'taxondb_url', 'REF_ID': 'ref_id', 'SMP_ID_PROVIDER':\n         'samplabcode', 'STATION': 'station'},\n         remap_dtypes:Dict[str,str]={'AREA': {'state': 'decoded'},\n         'NUCLIDE': {'state': 'encoded'}, 'UNIT': {'state': 'encoded'},\n         'DL': {'state': 'decoded'}, 'FILT': {'state': 'decoded'},\n         'COUNT_MET': {'state': 'encoded'}, 'SAMP_MET': {'state':\n         'encoded'}, 'PREP_MET': {'state': 'encoded'}, 'SPECIES':\n         {'state': 'encoded'}, 'BODY_PART': {'state': 'encoded'},\n         'SED_TYPE': {'state': 'encoded'}, 'LAB': {'state': 'encoded'}},\n         verbose:bool=False, **kwargs)\n\nDecode data from NetCDF.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname_in\nstr\n\nInput file name\n\n\ndest_out\nstr | None\nNone\nOutput file name (optional)\n\n\noutput_format\nstr\nopenrefine_csv\nOutput format\n\n\nremap_vars\nDict\n{‘LON’: ‘longitude’, ‘LAT’: ‘latitude’, ‘SMP_DEPTH’: ‘sampdepth’, ‘TOT_DEPTH’: ‘totdepth’, ‘TIME’: ‘begperiod’, ‘AREA’: ‘area’, ‘NUCLIDE’: ‘nuclide_id’, ‘VALUE’: ‘activity’, ‘UNIT’: ‘unit_id’, ‘UNC’: ‘uncertaint’, ‘DL’: ‘detection’, ‘DLV’: ‘detection_lim’, ‘FILT’: ‘filtered’, ‘COUNT_MET’: ‘counmet_id’, ‘SAMP_MET’: ‘sampmet_id’, ‘PREP_MET’: ‘prepmet_id’, ‘VOL’: ‘volume’, ‘SAL’: ‘salinity’, ‘TEMP’: ‘temperatur’, ‘SPECIES’: ‘species_id’, ‘BODY_PART’: ‘bodypar_id’, ‘SED_TYPE’: ‘sedtype_id’, ‘TOP’: ‘sliceup’, ‘BOTTOM’: ‘slicedown’, ‘DRYWT’: ‘drywt’, ‘WETWT’: ‘wetwt’, ‘PERCENTWT’: ‘percentwt’, ‘LAB’: ‘lab_id’, ‘PROFILE_ID’: ‘profile_id’, ‘SAMPLE_TYPE’: ‘samptype_id’, ‘TAXONNAME’: ‘taxonname’, ‘TAXONREPNAME’: ‘taxonrepname’, ‘TAXONRANK’: ‘taxonrank’, ‘TAXONDB’: ‘taxondb’, ‘TAXONDBID’: ‘taxondb_id’, ‘TAXONDBURL’: ‘taxondb_url’, ‘REF_ID’: ‘ref_id’, ‘SMP_ID_PROVIDER’: ‘samplabcode’, ‘STATION’: ‘station’}\nMapping of OR vars to NC vars\n\n\nremap_dtypes\nDict\n{‘AREA’: {‘state’: ‘decoded’}, ‘NUCLIDE’: {‘state’: ‘encoded’}, ‘UNIT’: {‘state’: ‘encoded’}, ‘DL’: {‘state’: ‘decoded’}, ‘FILT’: {‘state’: ‘decoded’}, ‘COUNT_MET’: {‘state’: ‘encoded’}, ‘SAMP_MET’: {‘state’: ‘encoded’}, ‘PREP_MET’: {‘state’: ‘encoded’}, ‘SPECIES’: {‘state’: ‘encoded’}, ‘BODY_PART’: {‘state’: ‘encoded’}, ‘SED_TYPE’: {‘state’: ‘encoded’}, ‘LAB’: {‘state’: ‘encoded’}}\nMapping of OR vars to NC dtypes\n\n\nverbose\nbool\nFalse\nIf True, print verbose output\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\nReturns\nNone\n\nAdditional arguments\n\n\n\n\nfname = Path('../../_data/output/100-HELCOM-MORS-2024.nc')\ndecode(fname_in=fname, dest_out=fname.with_suffix(''), output_format='openrefine_csv')",
    "crumbs": [
      "API",
      "NetCDF2CSV"
    ]
  },
  {
    "objectID": "api/encoders.html",
    "href": "api/encoders.html",
    "title": "Encoders",
    "section": "",
    "text": "source\n\n\n\n NetCDFEncoder (dfs:Dict[str,pandas.core.frame.DataFrame], dest_fname:str,\n                global_attrs:Dict[str,str],\n                fn_src_fname:Callable=&lt;function nc_tpl_path&gt;,\n                verbose:bool=False)\n\nMARIS NetCDF encoder.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndfs\nDict\n\ndict of Dataframes to encode with group name as key {‘sediment’: df_sed, …}\n\n\ndest_fname\nstr\n\nName of output file to produce\n\n\nglobal_attrs\nDict\n\nGlobal attributes\n\n\nfn_src_fname\nCallable\nnc_tpl_path\nFunction returning file name and path to the MARIS CDL template\n\n\nverbose\nbool\nFalse\nPrint currently written NetCDF group and variable names\n\n\n\n\n\nExported source\nclass NetCDFEncoder:\n    \"MARIS NetCDF encoder.\"\n    def __init__(self, \n                 dfs: Dict[str, pd.DataFrame], # dict of Dataframes to encode with group name as key {'sediment': df_sed, ...}\n                 dest_fname: str, # Name of output file to produce\n                 global_attrs: Dict[str, str], # Global attributes\n                 fn_src_fname: Callable=nc_tpl_path, # Function returning file name and path to the MARIS CDL template\n                 verbose: bool=False, # Print currently written NetCDF group and variable names\n                 ):\n        store_attr()\n        self.src_fname = fn_src_fname()\n        self.enum_dtypes = {}\n        self.nc_to_cols = {v:k for k,v in NC_VARS.items()}\n\n\n\ndf_seawater = pd.DataFrame({\n    'SMP_ID': [0, 1, 2],\n    'SMP_ID_PROVIDER': ['1', '2', '3'],\n    'LON': [141, 142, 143], \n    'LAT': [37.3, 38.3, 39.3], \n    'TIME': [1234, 1235, 1236], \n    'NUCLIDE': [1, 2, 3],\n    'VALUE': [0.1, 1.1, 2.1], \n    'AREA': [2374, 2379, 2401],\n    'STATION': ['A0', 'A11', 'B234']\n    })\n\ndf_biota = pd.DataFrame({\n    'SMP_ID': [0, 1, 2, 3], \n    'SMP_ID_PROVIDER': ['ID1', 'ID2', 'ID3', 'ID4'],\n    'LON': [141, 142, 143, 144], \n    'LAT': [37.3, 38.3, 39.3, 40.3], \n    'TIME': [1234, 1235, 1236, 1237], \n    'NUCLIDE': [1, 2, 3, 3],\n    'VALUE': [0.1, 1.1, 2.1, 3.1], \n    'SPECIES': [1, 2, 3, 3]\n    })\n\ndfs = {'SEAWATER': df_seawater, 'BIOTA': df_biota}\nattrs = {'id': '123', 'title': 'Test title', 'summary': 'Summary test'}\ndest = './files/nc/encoding-test.nc'\n\n\nsource\n\n\n\n\n NetCDFEncoder.copy_global_attributes ()\n\nUpdate NetCDF template global attributes as specified by global_attrs argument.\n\n\nExported source\n@patch \ndef copy_global_attributes(self:NetCDFEncoder):\n    \"Update NetCDF template global attributes as specified by `global_attrs` argument.\"\n    self.dest.setncatts(self.src.__dict__)\n    for k, v in self.global_attrs.items(): self.dest.setncattr(k, v)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.copy_dimensions (grp_dest)\n\nCopy dimensions to root and all groups from template.\n\n\nExported source\n@patch\ndef copy_dimensions(self:NetCDFEncoder, grp_dest):\n    \"Copy dimensions to root and all groups from template.\"\n    src_dim = self.src.groups[grp_dest.name].dimensions\n    for name, dim in src_dim.items():\n        grp_dest.createDimension(name, (len(dim) if not dim.isunlimited() else None))\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.process_groups ()\n\n\n\nExported source\n@patch\ndef process_groups(self:NetCDFEncoder):\n    for grp_name, df in self.dfs.items():\n        self.process_group(NC_GROUPS[grp_name], df)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.process_group (grp_name, df)\n\n\n\nExported source\n@patch\ndef process_group(self:NetCDFEncoder, grp_name, df):\n    grp_dest = self.dest.createGroup(grp_name)\n    self.copy_dimensions(grp_dest)\n    self.copy_variables(grp_name, df, grp_dest)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.copy_variables (grp_name, df, grp_dest)\n\n\n\nExported source\n@patch\ndef copy_variables(self:NetCDFEncoder, grp_name, df, grp_dest):\n    cols = [NC_VARS[col] for col in df.columns if col in NC_VARS]\n    for var_name, var_src in self.src.groups[grp_name].variables.items():\n        if var_name in cols: \n            self.copy_variable(var_name, var_src, df, grp_dest)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.copy_variable (var_name, var_src, df, grp_dest)\n\n\n\nExported source\n@patch\ndef copy_variable(self:NetCDFEncoder, var_name, var_src, df, grp_dest):\n    dtype_name = var_src.datatype.name\n    if self.verbose: \n        print(80*'-')\n        print(f'Group: {grp_dest.name}, Variable: {var_name}')\n    self._create_and_copy_variable(var_name, var_src, df, grp_dest, dtype_name)\n    self.copy_variable_attributes(var_name, var_src, grp_dest)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.sanitize_if_enum_and_nan (values, fill_value=-1)\n\n\n\nExported source\n@patch\ndef _create_and_copy_variable(self:NetCDFEncoder, var_name:str, var_src, df, grp_dest, dtype_name:str):\n    \"Create and populate a NetCDF variable with data from the dataframe\"\n    variable_type = self._get_variable_type(dtype_name, var_src)\n    self._create_netcdf_variable(grp_dest, var_name, variable_type)\n    self._populate_variable_data(grp_dest, var_name, variable_type, df)\n\n\n\n\nExported source\n@patch\ndef _get_variable_type(self:NetCDFEncoder, dtype_name:str, var_src):\n    \"Determine the appropriate variable type for NetCDF creation\"\n    if var_src.dtype == str: return str\n    return self.enum_dtypes.get(dtype_name, var_src.datatype)\n\n\n\n\nExported source\n@patch\ndef _create_netcdf_variable(self:NetCDFEncoder, grp_dest, var_name:str, variable_type):\n    \"Create a NetCDF variable with appropriate compression settings\"\n    compression_kwargs = {'compression': None} if variable_type == str else {'compression': 'zlib', 'complevel': 9}\n    grp_dest.createVariable(var_name, variable_type, (NC_DIM,), **compression_kwargs)\n\n\n\n\nExported source\n@patch\ndef _populate_variable_data(self:NetCDFEncoder, grp_dest, var_name:str, variable_type, df):\n    \"Populate the NetCDF variable with data from the dataframe\"\n    values = df[self.nc_to_cols[var_name]].values\n    is_enum_type = hasattr(variable_type, '__class__') and 'EnumType' in str(type(variable_type))\n    if is_enum_type: values = self.sanitize_if_enum_and_nan(values) \n    if variable_type == str:\n        for i, v in enumerate(values): grp_dest[var_name][i] = v\n    else:\n        grp_dest[var_name][:] = values\n\n\n\n\nExported source\n@patch\ndef sanitize_if_enum_and_nan(self:NetCDFEncoder, values, fill_value=-1):\n    values[np.isnan(values)] = int(fill_value)\n    values = values.astype(int)\n    return values\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.copy_variable_attributes (var_name, var_src, grp_dest)\n\n\n\nExported source\n@patch\ndef copy_variable_attributes(self:NetCDFEncoder, var_name, var_src, grp_dest):\n    grp_dest[var_name].setncatts(var_src.__dict__)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.retrieve_all_cols (dtypes={'AREA': {'name': 'area_t',\n                                  'fname': 'dbo_area.xlsx', 'key':\n                                  'displayName', 'value': 'areaId'},\n                                  'BIO_GROUP': {'name': 'bio_group_t',\n                                  'fname': 'dbo_biogroup.xlsx', 'key':\n                                  'biogroup', 'value': 'biogroup_id'},\n                                  'BODY_PART': {'name': 'body_part_t',\n                                  'fname': 'dbo_bodypar.xlsx', 'key':\n                                  'bodypar', 'value': 'bodypar_id'},\n                                  'COUNT_MET': {'name': 'count_met_t',\n                                  'fname': 'dbo_counmet.xlsx', 'key':\n                                  'counmet', 'value': 'counmet_id'}, 'DL':\n                                  {'name': 'dl_t', 'fname':\n                                  'dbo_detectlimit.xlsx', 'key':\n                                  'name_sanitized', 'value': 'id'},\n                                  'FILT': {'name': 'filt_t', 'fname':\n                                  'dbo_filtered.xlsx', 'key': 'name',\n                                  'value': 'id'}, 'NUCLIDE': {'name':\n                                  'nuclide_t', 'fname':\n                                  'dbo_nuclide.xlsx', 'key': 'nc_name',\n                                  'value': 'nuclide_id'}, 'PREP_MET':\n                                  {'name': 'prep_met_t', 'fname':\n                                  'dbo_prepmet.xlsx', 'key': 'prepmet',\n                                  'value': 'prepmet_id'}, 'SAMP_MET':\n                                  {'name': 'samp_met_t', 'fname':\n                                  'dbo_sampmet.xlsx', 'key': 'sampmet',\n                                  'value': 'sampmet_id'}, 'SED_TYPE':\n                                  {'name': 'sed_type_t', 'fname':\n                                  'dbo_sedtype.xlsx', 'key': 'sedtype',\n                                  'value': 'sedtype_id'}, 'SPECIES':\n                                  {'name': 'species_t', 'fname':\n                                  'dbo_species_2024_11_19.xlsx', 'key':\n                                  'species', 'value': 'species_id'},\n                                  'UNIT': {'name': 'unit_t', 'fname':\n                                  'dbo_unit.xlsx', 'key':\n                                  'unit_sanitized', 'value': 'unit_id'},\n                                  'LAB': {'name': 'lab_t', 'fname':\n                                  'dbo_lab_cleaned.xlsx', 'key': 'lab',\n                                  'value': 'lab_id'}})\n\nRetrieve all unique columns from the dict of dataframes.\n\n\nExported source\n@patch\ndef retrieve_all_cols(self:NetCDFEncoder, \n                      dtypes=NC_DTYPES\n                      ):\n    \"Retrieve all unique columns from the dict of dataframes.\" \n    return list(set(col for df in self.dfs.values() for col in df.columns if col in dtypes.keys()))\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.create_enums ()\n\n\n\nExported source\n@patch\ndef create_enums(self:NetCDFEncoder):\n    cols = self.retrieve_all_cols()\n    enums = Enums(lut_src_dir=lut_path())\n    for col in cols:\n        name = NC_DTYPES[col]['name']\n        if self.verbose: print(f'Creating enum for {name} with values {enums.types[col]}.')\n        dtype = self.dest.createEnumType(np.int64, name, enums.types[col])\n        self.enum_dtypes[name] = dtype\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.encode ()\n\nEncode MARIS NetCDF based on template and dataframes.\n\n\nExported source\n@patch\ndef encode(self:NetCDFEncoder):\n    \"Encode MARIS NetCDF based on template and dataframes.\"\n    with Dataset(self.src_fname, format='NETCDF4') as self.src, Dataset(self.dest_fname, 'w', format='NETCDF4') as self.dest:\n        self.copy_global_attributes()\n        self.create_enums()\n        self.process_groups()\n\n\n\nencoder = NetCDFEncoder(dfs, \n                        dest_fname=dest, \n                        global_attrs=attrs,\n                        # custom_maps=custom_maps,\n                        verbose=False\n                        )\nencoder.encode()\n\n\n# Test that global attributes are copied\n#with Dataset(dest, 'r', format='NETCDF4') as nc:\n#    for k, v in {'id': '123', 'title': 'Test title', 'summary': 'Summary test'}.items():\n#        fc.test_eq(getattr(nc, k), v)\n\n\n# Test that dimension is `sample` and unlimited\n# with Dataset(dest, 'r', format='NETCDF4') as nc:\n#     fc.test_eq('sample' in nc.dimensions, True)\n#     fc.test_eq(nc.dimensions['sample'].isunlimited(), True)\n\n\n# Test that groups are created\n# with Dataset(dest, 'r', format='NETCDF4') as nc:\n#     fc.test_eq(nc.groups.keys(), ['seawater', 'biota'])\n\n\n# Test that groups are created\n# with Dataset(dest, 'r', format='NETCDF4') as nc:\n#     fc.test_eq(nc.groups.keys(), ['seawater', 'biota'])\n\n\n# Test that correct variables are created in groups\n# with Dataset(dest, 'r', format='NETCDF4') as nc:\n#     fc.test_eq(nc['biota'].variables.keys(), \n#                ['sample', 'lon', 'lat', 'time', 'species', 'i131', 'i131_dl', 'i131_unit'])\n    \n#     fc.test_eq(nc['seawater'].variables.keys(), \n#                ['sample', 'lon', 'lat', 'time', 'i131', 'i131_dl', 'i131_unit'])\n\n\n# Test that correct variables are created in groups\n# with Dataset(dest, 'r', format='NETCDF4') as nc:\n#     print(nc.dimensions.items())\n#     print(nc['biota'].dimensions.items())\n#     print(nc['seawater'].dimensions.items())\n\n\n# Test that custom maps are copied\n#with Dataset(dest, 'r', format='NETCDF4') as nc:\n#    print(nc['seawater'].variables.items())\n#    print(nc['biota'].variables.items())",
    "crumbs": [
      "API",
      "Encoders"
    ]
  },
  {
    "objectID": "api/encoders.html#netcdf",
    "href": "api/encoders.html#netcdf",
    "title": "Encoders",
    "section": "",
    "text": "source\n\n\n\n NetCDFEncoder (dfs:Dict[str,pandas.core.frame.DataFrame], dest_fname:str,\n                global_attrs:Dict[str,str],\n                fn_src_fname:Callable=&lt;function nc_tpl_path&gt;,\n                verbose:bool=False)\n\nMARIS NetCDF encoder.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndfs\nDict\n\ndict of Dataframes to encode with group name as key {‘sediment’: df_sed, …}\n\n\ndest_fname\nstr\n\nName of output file to produce\n\n\nglobal_attrs\nDict\n\nGlobal attributes\n\n\nfn_src_fname\nCallable\nnc_tpl_path\nFunction returning file name and path to the MARIS CDL template\n\n\nverbose\nbool\nFalse\nPrint currently written NetCDF group and variable names\n\n\n\n\n\nExported source\nclass NetCDFEncoder:\n    \"MARIS NetCDF encoder.\"\n    def __init__(self, \n                 dfs: Dict[str, pd.DataFrame], # dict of Dataframes to encode with group name as key {'sediment': df_sed, ...}\n                 dest_fname: str, # Name of output file to produce\n                 global_attrs: Dict[str, str], # Global attributes\n                 fn_src_fname: Callable=nc_tpl_path, # Function returning file name and path to the MARIS CDL template\n                 verbose: bool=False, # Print currently written NetCDF group and variable names\n                 ):\n        store_attr()\n        self.src_fname = fn_src_fname()\n        self.enum_dtypes = {}\n        self.nc_to_cols = {v:k for k,v in NC_VARS.items()}\n\n\n\ndf_seawater = pd.DataFrame({\n    'SMP_ID': [0, 1, 2],\n    'SMP_ID_PROVIDER': ['1', '2', '3'],\n    'LON': [141, 142, 143], \n    'LAT': [37.3, 38.3, 39.3], \n    'TIME': [1234, 1235, 1236], \n    'NUCLIDE': [1, 2, 3],\n    'VALUE': [0.1, 1.1, 2.1], \n    'AREA': [2374, 2379, 2401],\n    'STATION': ['A0', 'A11', 'B234']\n    })\n\ndf_biota = pd.DataFrame({\n    'SMP_ID': [0, 1, 2, 3], \n    'SMP_ID_PROVIDER': ['ID1', 'ID2', 'ID3', 'ID4'],\n    'LON': [141, 142, 143, 144], \n    'LAT': [37.3, 38.3, 39.3, 40.3], \n    'TIME': [1234, 1235, 1236, 1237], \n    'NUCLIDE': [1, 2, 3, 3],\n    'VALUE': [0.1, 1.1, 2.1, 3.1], \n    'SPECIES': [1, 2, 3, 3]\n    })\n\ndfs = {'SEAWATER': df_seawater, 'BIOTA': df_biota}\nattrs = {'id': '123', 'title': 'Test title', 'summary': 'Summary test'}\ndest = './files/nc/encoding-test.nc'\n\n\nsource\n\n\n\n\n NetCDFEncoder.copy_global_attributes ()\n\nUpdate NetCDF template global attributes as specified by global_attrs argument.\n\n\nExported source\n@patch \ndef copy_global_attributes(self:NetCDFEncoder):\n    \"Update NetCDF template global attributes as specified by `global_attrs` argument.\"\n    self.dest.setncatts(self.src.__dict__)\n    for k, v in self.global_attrs.items(): self.dest.setncattr(k, v)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.copy_dimensions (grp_dest)\n\nCopy dimensions to root and all groups from template.\n\n\nExported source\n@patch\ndef copy_dimensions(self:NetCDFEncoder, grp_dest):\n    \"Copy dimensions to root and all groups from template.\"\n    src_dim = self.src.groups[grp_dest.name].dimensions\n    for name, dim in src_dim.items():\n        grp_dest.createDimension(name, (len(dim) if not dim.isunlimited() else None))\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.process_groups ()\n\n\n\nExported source\n@patch\ndef process_groups(self:NetCDFEncoder):\n    for grp_name, df in self.dfs.items():\n        self.process_group(NC_GROUPS[grp_name], df)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.process_group (grp_name, df)\n\n\n\nExported source\n@patch\ndef process_group(self:NetCDFEncoder, grp_name, df):\n    grp_dest = self.dest.createGroup(grp_name)\n    self.copy_dimensions(grp_dest)\n    self.copy_variables(grp_name, df, grp_dest)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.copy_variables (grp_name, df, grp_dest)\n\n\n\nExported source\n@patch\ndef copy_variables(self:NetCDFEncoder, grp_name, df, grp_dest):\n    cols = [NC_VARS[col] for col in df.columns if col in NC_VARS]\n    for var_name, var_src in self.src.groups[grp_name].variables.items():\n        if var_name in cols: \n            self.copy_variable(var_name, var_src, df, grp_dest)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.copy_variable (var_name, var_src, df, grp_dest)\n\n\n\nExported source\n@patch\ndef copy_variable(self:NetCDFEncoder, var_name, var_src, df, grp_dest):\n    dtype_name = var_src.datatype.name\n    if self.verbose: \n        print(80*'-')\n        print(f'Group: {grp_dest.name}, Variable: {var_name}')\n    self._create_and_copy_variable(var_name, var_src, df, grp_dest, dtype_name)\n    self.copy_variable_attributes(var_name, var_src, grp_dest)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.sanitize_if_enum_and_nan (values, fill_value=-1)\n\n\n\nExported source\n@patch\ndef _create_and_copy_variable(self:NetCDFEncoder, var_name:str, var_src, df, grp_dest, dtype_name:str):\n    \"Create and populate a NetCDF variable with data from the dataframe\"\n    variable_type = self._get_variable_type(dtype_name, var_src)\n    self._create_netcdf_variable(grp_dest, var_name, variable_type)\n    self._populate_variable_data(grp_dest, var_name, variable_type, df)\n\n\n\n\nExported source\n@patch\ndef _get_variable_type(self:NetCDFEncoder, dtype_name:str, var_src):\n    \"Determine the appropriate variable type for NetCDF creation\"\n    if var_src.dtype == str: return str\n    return self.enum_dtypes.get(dtype_name, var_src.datatype)\n\n\n\n\nExported source\n@patch\ndef _create_netcdf_variable(self:NetCDFEncoder, grp_dest, var_name:str, variable_type):\n    \"Create a NetCDF variable with appropriate compression settings\"\n    compression_kwargs = {'compression': None} if variable_type == str else {'compression': 'zlib', 'complevel': 9}\n    grp_dest.createVariable(var_name, variable_type, (NC_DIM,), **compression_kwargs)\n\n\n\n\nExported source\n@patch\ndef _populate_variable_data(self:NetCDFEncoder, grp_dest, var_name:str, variable_type, df):\n    \"Populate the NetCDF variable with data from the dataframe\"\n    values = df[self.nc_to_cols[var_name]].values\n    is_enum_type = hasattr(variable_type, '__class__') and 'EnumType' in str(type(variable_type))\n    if is_enum_type: values = self.sanitize_if_enum_and_nan(values) \n    if variable_type == str:\n        for i, v in enumerate(values): grp_dest[var_name][i] = v\n    else:\n        grp_dest[var_name][:] = values\n\n\n\n\nExported source\n@patch\ndef sanitize_if_enum_and_nan(self:NetCDFEncoder, values, fill_value=-1):\n    values[np.isnan(values)] = int(fill_value)\n    values = values.astype(int)\n    return values\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.copy_variable_attributes (var_name, var_src, grp_dest)\n\n\n\nExported source\n@patch\ndef copy_variable_attributes(self:NetCDFEncoder, var_name, var_src, grp_dest):\n    grp_dest[var_name].setncatts(var_src.__dict__)\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.retrieve_all_cols (dtypes={'AREA': {'name': 'area_t',\n                                  'fname': 'dbo_area.xlsx', 'key':\n                                  'displayName', 'value': 'areaId'},\n                                  'BIO_GROUP': {'name': 'bio_group_t',\n                                  'fname': 'dbo_biogroup.xlsx', 'key':\n                                  'biogroup', 'value': 'biogroup_id'},\n                                  'BODY_PART': {'name': 'body_part_t',\n                                  'fname': 'dbo_bodypar.xlsx', 'key':\n                                  'bodypar', 'value': 'bodypar_id'},\n                                  'COUNT_MET': {'name': 'count_met_t',\n                                  'fname': 'dbo_counmet.xlsx', 'key':\n                                  'counmet', 'value': 'counmet_id'}, 'DL':\n                                  {'name': 'dl_t', 'fname':\n                                  'dbo_detectlimit.xlsx', 'key':\n                                  'name_sanitized', 'value': 'id'},\n                                  'FILT': {'name': 'filt_t', 'fname':\n                                  'dbo_filtered.xlsx', 'key': 'name',\n                                  'value': 'id'}, 'NUCLIDE': {'name':\n                                  'nuclide_t', 'fname':\n                                  'dbo_nuclide.xlsx', 'key': 'nc_name',\n                                  'value': 'nuclide_id'}, 'PREP_MET':\n                                  {'name': 'prep_met_t', 'fname':\n                                  'dbo_prepmet.xlsx', 'key': 'prepmet',\n                                  'value': 'prepmet_id'}, 'SAMP_MET':\n                                  {'name': 'samp_met_t', 'fname':\n                                  'dbo_sampmet.xlsx', 'key': 'sampmet',\n                                  'value': 'sampmet_id'}, 'SED_TYPE':\n                                  {'name': 'sed_type_t', 'fname':\n                                  'dbo_sedtype.xlsx', 'key': 'sedtype',\n                                  'value': 'sedtype_id'}, 'SPECIES':\n                                  {'name': 'species_t', 'fname':\n                                  'dbo_species_2024_11_19.xlsx', 'key':\n                                  'species', 'value': 'species_id'},\n                                  'UNIT': {'name': 'unit_t', 'fname':\n                                  'dbo_unit.xlsx', 'key':\n                                  'unit_sanitized', 'value': 'unit_id'},\n                                  'LAB': {'name': 'lab_t', 'fname':\n                                  'dbo_lab_cleaned.xlsx', 'key': 'lab',\n                                  'value': 'lab_id'}})\n\nRetrieve all unique columns from the dict of dataframes.\n\n\nExported source\n@patch\ndef retrieve_all_cols(self:NetCDFEncoder, \n                      dtypes=NC_DTYPES\n                      ):\n    \"Retrieve all unique columns from the dict of dataframes.\" \n    return list(set(col for df in self.dfs.values() for col in df.columns if col in dtypes.keys()))\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.create_enums ()\n\n\n\nExported source\n@patch\ndef create_enums(self:NetCDFEncoder):\n    cols = self.retrieve_all_cols()\n    enums = Enums(lut_src_dir=lut_path())\n    for col in cols:\n        name = NC_DTYPES[col]['name']\n        if self.verbose: print(f'Creating enum for {name} with values {enums.types[col]}.')\n        dtype = self.dest.createEnumType(np.int64, name, enums.types[col])\n        self.enum_dtypes[name] = dtype\n\n\n\nsource\n\n\n\n\n NetCDFEncoder.encode ()\n\nEncode MARIS NetCDF based on template and dataframes.\n\n\nExported source\n@patch\ndef encode(self:NetCDFEncoder):\n    \"Encode MARIS NetCDF based on template and dataframes.\"\n    with Dataset(self.src_fname, format='NETCDF4') as self.src, Dataset(self.dest_fname, 'w', format='NETCDF4') as self.dest:\n        self.copy_global_attributes()\n        self.create_enums()\n        self.process_groups()\n\n\n\nencoder = NetCDFEncoder(dfs, \n                        dest_fname=dest, \n                        global_attrs=attrs,\n                        # custom_maps=custom_maps,\n                        verbose=False\n                        )\nencoder.encode()\n\n\n# Test that global attributes are copied\n#with Dataset(dest, 'r', format='NETCDF4') as nc:\n#    for k, v in {'id': '123', 'title': 'Test title', 'summary': 'Summary test'}.items():\n#        fc.test_eq(getattr(nc, k), v)\n\n\n# Test that dimension is `sample` and unlimited\n# with Dataset(dest, 'r', format='NETCDF4') as nc:\n#     fc.test_eq('sample' in nc.dimensions, True)\n#     fc.test_eq(nc.dimensions['sample'].isunlimited(), True)\n\n\n# Test that groups are created\n# with Dataset(dest, 'r', format='NETCDF4') as nc:\n#     fc.test_eq(nc.groups.keys(), ['seawater', 'biota'])\n\n\n# Test that groups are created\n# with Dataset(dest, 'r', format='NETCDF4') as nc:\n#     fc.test_eq(nc.groups.keys(), ['seawater', 'biota'])\n\n\n# Test that correct variables are created in groups\n# with Dataset(dest, 'r', format='NETCDF4') as nc:\n#     fc.test_eq(nc['biota'].variables.keys(), \n#                ['sample', 'lon', 'lat', 'time', 'species', 'i131', 'i131_dl', 'i131_unit'])\n    \n#     fc.test_eq(nc['seawater'].variables.keys(), \n#                ['sample', 'lon', 'lat', 'time', 'i131', 'i131_dl', 'i131_unit'])\n\n\n# Test that correct variables are created in groups\n# with Dataset(dest, 'r', format='NETCDF4') as nc:\n#     print(nc.dimensions.items())\n#     print(nc['biota'].dimensions.items())\n#     print(nc['seawater'].dimensions.items())\n\n\n# Test that custom maps are copied\n#with Dataset(dest, 'r', format='NETCDF4') as nc:\n#    print(nc['seawater'].variables.items())\n#    print(nc['biota'].variables.items())",
    "crumbs": [
      "API",
      "Encoders"
    ]
  },
  {
    "objectID": "api/inout.html",
    "href": "api/inout.html",
    "title": "Input/Output",
    "section": "",
    "text": "source\n\nwrite_toml\n\n write_toml (fname:str, cfg:Dict[str,Any])\n\nWrite a TOML file from a dictionary.\n\n\nExported source\ndef write_toml(fname: str, cfg: Dict[str, Any]):\n    \"Write a TOML file from a dictionary.\"\n    none_keys = [k for k, v in flatten_dict(cfg).items() if v is None]\n    if none_keys:\n        print(f\"Warning: The following config keys have None values: {', '.join(none_keys)}\")\n        \n    print(f'Creating {fname}')\n    with open(fname, \"wb\") as f:\n        tomli_w.dump(cfg, f)\n\n\n\nsource\n\n\nflatten_dict\n\n flatten_dict (d:Dict[str,Any], parent_key:str='', sep:str='.')\n\nFlatten a nested dictionary.\n\n\nExported source\ndef flatten_dict(d: Dict[str, Any], parent_key: str = '', sep: str = '.') -&gt; Dict[str, Any]:\n    \"\"\"Flatten a nested dictionary.\"\"\"\n    items = []\n    for k, v in d.items():\n        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\n\nsource\n\n\nread_toml\n\n read_toml (fname)\n\nRead a TOML file into a dictionary.\n\n\nExported source\ndef read_toml(fname):\n    \"Read a TOML file into a dictionary.\"\n    with open(fname, \"rb\") as f:\n        config = tomli.load(f)\n    return config"
  },
  {
    "objectID": "api/decoders.html",
    "href": "api/decoders.html",
    "title": "Decoders",
    "section": "",
    "text": "MARIS NetCDF files can be converted to OpenRefine CSV files. The OpenRefine CSV files are compatible with the OpenRefine data cleaning tool which are used during the MARIS data cleaning process before loading into the MARIS database.\n\nsource\n\n\n\n NetCDFDecoder (dfs:Dict[str,pandas.core.frame.DataFrame], fname_in:str,\n                dest_out:str, output_format:str, remap_vars:Dict[str,str],\n                verbose:bool=False)\n\nDecode MARIS NetCDF files to human readable formats.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndfs\nDict\n\n\n\n\nfname_in\nstr\n\nPath to NetCDF file\n\n\ndest_out\nstr\n\n\n\n\noutput_format\nstr\n\n\n\n\nremap_vars\nDict\n\n\n\n\nverbose\nbool\nFalse\n\n\n\n\n\n\nExported source\nclass NetCDFDecoder:\n    \"\"\"Decode MARIS NetCDF files to human readable formats.\"\"\"\n    def __init__(self, \n                 dfs: Dict[str, pd.DataFrame], \n                 fname_in: str,  # Path to NetCDF file\n                 dest_out: str, \n                 output_format:str, \n                 remap_vars: Dict[str, str],\n                 verbose: bool=False\n                ):\n        fc.store_attr()\n\n\n\nsource\n\n\n\n\n NetCDFDecoder.process_groups ()\n\nProcess all groups in the dataset.\n\n\nExported source\n@patch\ndef process_groups(self: NetCDFDecoder):\n    \"\"\"Process all groups in the dataset.\"\"\"\n    for group_name, df in self.dfs.items():\n        self.process_group(group_name, df, self.remap_vars)\n\n\n\nsource\n\n\n\n\n NetCDFDecoder.process_group (group_name:str,\n                              df:pandas.core.frame.DataFrame,\n                              remap_vars:Dict[str,str])\n\nProcess a single group, mapping column names using remap_vars.\n\n\nExported source\n@patch\ndef process_group(self: NetCDFDecoder, group_name: str, df: pd.DataFrame, remap_vars: Dict[str, str]):\n    \"\"\"Process a single group, mapping column names using remap_vars.\"\"\"\n    # Map column names using remap_vars\n    df.columns = [remap_vars.get(col, col) for col in df.columns]\n\n\n\nsource\n\n\n\n\n NetCDFDecoder.save_dataframes ()\n\n*Save DataFrames to CSV files.\nEach group in the DataFrame dictionary will be saved as a separate CSV file with the naming pattern: {base_path}_{group_name}.csv\nRaises: ValueError: If no destination path is provided or if output format is not CSV*\n\n\nExported source\n@patch\ndef save_dataframes(self: NetCDFDecoder):\n    \"\"\"\n    Save DataFrames to CSV files.\n    \n    Each group in the DataFrame dictionary will be saved as a separate CSV file\n    with the naming pattern: {base_path}_{group_name}.csv\n    \n    Raises:\n        ValueError: If no destination path is provided or if output format is not CSV\n    \"\"\"\n    # Validate destination path\n    if self.dest_out is None:\n        self.dest_out  = str(Path(self.fname_in).with_suffix(''))\n    \n    # Validate output format\n    if self.output_format != 'csv':\n        raise ValueError(\"Only CSV format is supported\")\n    \n    # Get base path without extension\n    base_path = str(Path(self.dest_out).with_suffix(''))\n    \n    # Save each DataFrame to a CSV file\n    for group_name, df in self.dfs.items():\n        output_path = f\"{base_path}_{group_name}.csv\"\n        df.to_csv(output_path, index=False)\n        \n        if self.verbose:\n            print(f\"Saved {group_name} to {output_path}\")\n\n\n\nsource\n\n\n\n\n NetCDFDecoder.decode ()\n\nDecode NetCDF to Human readable files.\n\n\nExported source\n@patch\ndef decode(self: NetCDFDecoder):\n    \"Decode NetCDF to Human readable files.\"\n    # Function to rename the columns. \n    self.process_groups()\n    self.save_dataframes()\n    return self.dfs\n\n\n\ndf_seawater = pd.DataFrame({\n    'ID': [0, 1, 2], \n    'LON': [141, 142, 143], \n    'LAT': [37.3, 38.3, 39.3], \n    'TIME': [1234, 1235, 1236], \n    'NUCLIDE': [1, 2, 3],\n    'VALUE': [0.1, 1.1, 2.1], \n    'AREA': [2374, 2379, 2401],\n    })\n\ndf_biota = pd.DataFrame({\n    'ID': [0, 1, 2, 3], \n    'LON': [141, 142, 143, 144], \n    'LAT': [37.3, 38.3, 39.3, 40.3], \n    'TIME': [1234, 1235, 1236, 1237], \n    'NUCLIDE': [1, 2, 3, 3],\n    'VALUE': [0.1, 1.1, 2.1, 3.1], \n    'SPECIES': [1, 2, 3, 3]\n    })\ndfs = {'SEAWATER': df_seawater, 'BIOTA': df_biota}\n\n\nfname = Path('../../_data/output/100-HELCOM-MORS-2024.nc')\n\ndecoder = NetCDFDecoder( \n                        dfs=dfs,\n                        fname_in=fname,  \n                        dest_out=fname.with_suffix(''),\n                        output_format='csv',\n                        remap_vars=CSV_VARS,\n                        verbose=True\n                 )\ndecoder.decode()\n\nSaved SEAWATER to ../../_data/output/100-HELCOM-MORS-2024_SEAWATER.csv\nSaved BIOTA to ../../_data/output/100-HELCOM-MORS-2024_BIOTA.csv\n\n\n{'SEAWATER':    ID  longitude  latitude  begperiod  nuclide_id  activity  area\n 0   0        141      37.3       1234           1       0.1  2374\n 1   1        142      38.3       1235           2       1.1  2379\n 2   2        143      39.3       1236           3       2.1  2401,\n 'BIOTA':    ID  longitude  latitude  begperiod  nuclide_id  activity  species_id\n 0   0        141      37.3       1234           1       0.1           1\n 1   1        142      38.3       1235           2       1.1           2\n 2   2        143      39.3       1236           3       2.1           3\n 3   3        144      40.3       1237           3       3.1           3}",
    "crumbs": [
      "API",
      "Decoders"
    ]
  },
  {
    "objectID": "api/decoders.html#convert-netcdf-to-openrefine-csv",
    "href": "api/decoders.html#convert-netcdf-to-openrefine-csv",
    "title": "Decoders",
    "section": "",
    "text": "MARIS NetCDF files can be converted to OpenRefine CSV files. The OpenRefine CSV files are compatible with the OpenRefine data cleaning tool which are used during the MARIS data cleaning process before loading into the MARIS database.\n\nsource\n\n\n\n NetCDFDecoder (dfs:Dict[str,pandas.core.frame.DataFrame], fname_in:str,\n                dest_out:str, output_format:str, remap_vars:Dict[str,str],\n                verbose:bool=False)\n\nDecode MARIS NetCDF files to human readable formats.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndfs\nDict\n\n\n\n\nfname_in\nstr\n\nPath to NetCDF file\n\n\ndest_out\nstr\n\n\n\n\noutput_format\nstr\n\n\n\n\nremap_vars\nDict\n\n\n\n\nverbose\nbool\nFalse\n\n\n\n\n\n\nExported source\nclass NetCDFDecoder:\n    \"\"\"Decode MARIS NetCDF files to human readable formats.\"\"\"\n    def __init__(self, \n                 dfs: Dict[str, pd.DataFrame], \n                 fname_in: str,  # Path to NetCDF file\n                 dest_out: str, \n                 output_format:str, \n                 remap_vars: Dict[str, str],\n                 verbose: bool=False\n                ):\n        fc.store_attr()\n\n\n\nsource\n\n\n\n\n NetCDFDecoder.process_groups ()\n\nProcess all groups in the dataset.\n\n\nExported source\n@patch\ndef process_groups(self: NetCDFDecoder):\n    \"\"\"Process all groups in the dataset.\"\"\"\n    for group_name, df in self.dfs.items():\n        self.process_group(group_name, df, self.remap_vars)\n\n\n\nsource\n\n\n\n\n NetCDFDecoder.process_group (group_name:str,\n                              df:pandas.core.frame.DataFrame,\n                              remap_vars:Dict[str,str])\n\nProcess a single group, mapping column names using remap_vars.\n\n\nExported source\n@patch\ndef process_group(self: NetCDFDecoder, group_name: str, df: pd.DataFrame, remap_vars: Dict[str, str]):\n    \"\"\"Process a single group, mapping column names using remap_vars.\"\"\"\n    # Map column names using remap_vars\n    df.columns = [remap_vars.get(col, col) for col in df.columns]\n\n\n\nsource\n\n\n\n\n NetCDFDecoder.save_dataframes ()\n\n*Save DataFrames to CSV files.\nEach group in the DataFrame dictionary will be saved as a separate CSV file with the naming pattern: {base_path}_{group_name}.csv\nRaises: ValueError: If no destination path is provided or if output format is not CSV*\n\n\nExported source\n@patch\ndef save_dataframes(self: NetCDFDecoder):\n    \"\"\"\n    Save DataFrames to CSV files.\n    \n    Each group in the DataFrame dictionary will be saved as a separate CSV file\n    with the naming pattern: {base_path}_{group_name}.csv\n    \n    Raises:\n        ValueError: If no destination path is provided or if output format is not CSV\n    \"\"\"\n    # Validate destination path\n    if self.dest_out is None:\n        self.dest_out  = str(Path(self.fname_in).with_suffix(''))\n    \n    # Validate output format\n    if self.output_format != 'csv':\n        raise ValueError(\"Only CSV format is supported\")\n    \n    # Get base path without extension\n    base_path = str(Path(self.dest_out).with_suffix(''))\n    \n    # Save each DataFrame to a CSV file\n    for group_name, df in self.dfs.items():\n        output_path = f\"{base_path}_{group_name}.csv\"\n        df.to_csv(output_path, index=False)\n        \n        if self.verbose:\n            print(f\"Saved {group_name} to {output_path}\")\n\n\n\nsource\n\n\n\n\n NetCDFDecoder.decode ()\n\nDecode NetCDF to Human readable files.\n\n\nExported source\n@patch\ndef decode(self: NetCDFDecoder):\n    \"Decode NetCDF to Human readable files.\"\n    # Function to rename the columns. \n    self.process_groups()\n    self.save_dataframes()\n    return self.dfs\n\n\n\ndf_seawater = pd.DataFrame({\n    'ID': [0, 1, 2], \n    'LON': [141, 142, 143], \n    'LAT': [37.3, 38.3, 39.3], \n    'TIME': [1234, 1235, 1236], \n    'NUCLIDE': [1, 2, 3],\n    'VALUE': [0.1, 1.1, 2.1], \n    'AREA': [2374, 2379, 2401],\n    })\n\ndf_biota = pd.DataFrame({\n    'ID': [0, 1, 2, 3], \n    'LON': [141, 142, 143, 144], \n    'LAT': [37.3, 38.3, 39.3, 40.3], \n    'TIME': [1234, 1235, 1236, 1237], \n    'NUCLIDE': [1, 2, 3, 3],\n    'VALUE': [0.1, 1.1, 2.1, 3.1], \n    'SPECIES': [1, 2, 3, 3]\n    })\ndfs = {'SEAWATER': df_seawater, 'BIOTA': df_biota}\n\n\nfname = Path('../../_data/output/100-HELCOM-MORS-2024.nc')\n\ndecoder = NetCDFDecoder( \n                        dfs=dfs,\n                        fname_in=fname,  \n                        dest_out=fname.with_suffix(''),\n                        output_format='csv',\n                        remap_vars=CSV_VARS,\n                        verbose=True\n                 )\ndecoder.decode()\n\nSaved SEAWATER to ../../_data/output/100-HELCOM-MORS-2024_SEAWATER.csv\nSaved BIOTA to ../../_data/output/100-HELCOM-MORS-2024_BIOTA.csv\n\n\n{'SEAWATER':    ID  longitude  latitude  begperiod  nuclide_id  activity  area\n 0   0        141      37.3       1234           1       0.1  2374\n 1   1        142      38.3       1235           2       1.1  2379\n 2   2        143      39.3       1236           3       2.1  2401,\n 'BIOTA':    ID  longitude  latitude  begperiod  nuclide_id  activity  species_id\n 0   0        141      37.3       1234           1       0.1           1\n 1   1        142      38.3       1235           2       1.1           2\n 2   2        143      39.3       1236           3       2.1           3\n 3   3        144      40.3       1237           3       3.1           3}",
    "crumbs": [
      "API",
      "Decoders"
    ]
  },
  {
    "objectID": "api/configs.html",
    "href": "api/configs.html",
    "title": "Configs",
    "section": "",
    "text": "Exported source\nAVOGADRO = 6.02214076e23\nExported source\nNC_DIM = 'id'\nExported source\n# Lookup variables names to MARIS NetCDF4 template variable names\nNC_VARS = {\n    'SMP_ID': 'id',\n    'SMP_ID_PROVIDER': 'id_provider',\n    'LON': 'lon',\n    'LAT': 'lat',\n    'SMP_DEPTH': 'smp_depth',\n    'TOT_DEPTH': 'tot_depth',\n    'TIME': 'time',\n    'AREA': 'area',\n    'SMP_ID': 'smp_id',\n    'NUCLIDE': 'nuclide',\n    'VALUE': 'value',\n    'UNIT': 'unit',\n    'UNC': 'unc',\n    'DL': 'dl',\n    'DLV': 'dlv',\n    'FILT': 'filt',\n    'COUNT_MET': 'count_met',\n    'SAMP_MET': 'samp_met',\n    'PREP_MET': 'prep_met',\n    #'DRY_MET': 'dry_met', # NOT IN USE, no LUT for this. \n    'VOL': 'vol',\n    'SAL': 'sal',\n    'TEMP': 'temp',\n    'PH': 'ph',\n    'BIO_GROUP': 'bio_group',\n    'SPECIES': 'species',\n    'BODY_PART': 'body_part',\n    'SED_TYPE': 'sed_type',\n    'TOP': 'top',\n    'BOTTOM': 'bottom',\n    'DRYWT': 'drywt',\n    'WETWT': 'wetwt',\n    'PERCENTWT': 'percentwt',\n    'LAB': 'lab',\n    'PROFILE_ID': 'profile_id', # NEED TO INCLUDE.\n    'STATION': 'station'\n    }\nCSV_VARS includes variables as defined in the OpenRefine MARIS CSV format.\nsource",
    "crumbs": [
      "API",
      "Configs"
    ]
  },
  {
    "objectID": "api/configs.html#lookup-tables-paths",
    "href": "api/configs.html#lookup-tables-paths",
    "title": "Configs",
    "section": "Lookup tables paths",
    "text": "Lookup tables paths\n\nsource\n\nnuc_lut_path\n\n nuc_lut_path ()\n\nReturn the path to the nuclide lookup table.\n\n\nExported source\ndef nuc_lut_path(): \n    \"Return the path to the nuclide lookup table.\"\n    return Path(cfg()['dirs']['lut']) / NUCLIDE_LOOKUP_FNAME\n\n\n\nsource\n\n\nspecies_lut_path\n\n species_lut_path ()\n\nReturn the path to the species lookup table.\n\n\nExported source\ndef species_lut_path():\n    \"Return the path to the species lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['SPECIES']['fname']\n    \n    return src_dir / fname\n\n\n\nspecies_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_species_2024_11_19.xlsx')\n\n\n\nsource\n\n\nbodyparts_lut_path\n\n bodyparts_lut_path ()\n\nReturn the path to the body parts lookup table.\n\n\nExported source\ndef bodyparts_lut_path():\n    \"Return the path to the body parts lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['BODY_PART']['fname']\n    return src_dir / fname\n\n\n\nsource\n\n\nlab_lut_path\n\n lab_lut_path ()\n\nReturn the path to the laboratory lookup table.\n\n\nExported source\ndef lab_lut_path():\n    \"Return the path to the laboratory lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['LAB']['fname']\n    return src_dir / fname\n\n\n\nlab_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_lab_cleaned.xlsx')\n\n\n\nsource\n\n\nbiogroup_lut_path\n\n biogroup_lut_path ()\n\nReturn the path to the biota group lookup table.\n\n\nExported source\ndef biogroup_lut_path():\n    \"Return the path to the biota group lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['BIO_GROUP']['fname']\n    return src_dir / fname\n\n\n\nbiogroup_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_biogroup.xlsx')\n\n\n\nsource\n\n\nsediments_lut_path\n\n sediments_lut_path ()\n\nReturn the path to the sediment type lookup table.\n\n\nExported source\ndef sediments_lut_path():\n    \"Return the path to the sediment type lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['SED_TYPE']['fname']\n    return src_dir / fname\n\n\n\nsediments_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_sedtype.xlsx')\n\n\n\nsource\n\n\nunit_lut_path\n\n unit_lut_path ()\n\nReturn the path to the unit lookup table.\n\n\nExported source\ndef unit_lut_path():\n    \"Return the path to the unit lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['UNIT']['fname']\n    return src_dir / fname\n\n\n\nunit_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_unit.xlsx')\n\n\n\nsource\n\n\ndetection_limit_lut_path\n\n detection_limit_lut_path ()\n\nReturn the path to the detection limit lookup table.\n\n\nExported source\ndef detection_limit_lut_path():\n    \"Return the path to the detection limit lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['DL']['fname']\n    return src_dir / fname\n\n\n\ndetection_limit_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_detectlimit.xlsx')\n\n\n\nsource\n\n\nfiltered_lut_path\n\n filtered_lut_path ()\n\nReturn the path to the filtered lookup table.\n\n\nExported source\ndef filtered_lut_path():\n    \"Return the path to the filtered lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['FILT']['fname']\n    return src_dir / fname\n\n\n\nfiltered_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_filtered.xlsx')\n\n\n\nsource\n\n\narea_lut_path\n\n area_lut_path ()\n\nReturn the path to the area lookup table.\n\n\nExported source\ndef area_lut_path():\n    \"Return the path to the area lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['AREA']['fname']\n    return src_dir / fname\n\n# area_lut_path()\n\n\n\narea_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_area.xlsx')\n\n\n\nsource\n\n\nprepmet_lut_path\n\n prepmet_lut_path ()\n\nReturn the path to the prepmet lookup table.\n\n\nExported source\ndef prepmet_lut_path():\n    \"Return the path to the prepmet lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['PREP_MET']['fname']\n    return src_dir / fname\n\n# prepmet_lut_path()\n\n\n\nprepmet_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_prepmet.xlsx')\n\n\n\nsource\n\n\nsampmet_lut_path\n\n sampmet_lut_path ()\n\nReturn the path to the sampmet lookup table.\n\n\nExported source\ndef sampmet_lut_path():\n    \"Return the path to the sampmet lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['SAMP_MET']['fname']\n    return src_dir / fname\n\n# sampmet_lut_path()\n\n\n\nsampmet_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_sampmet.xlsx')\n\n\n\nsource\n\n\ncounmet_lut_path\n\n counmet_lut_path ()\n\nReturn the path to the counmet lookup table.\n\n\nExported source\ndef counmet_lut_path():\n    \"Return the path to the counmet lookup table.\"\n    src_dir = lut_path()\n    fname = NC_DTYPES['COUNT_MET']['fname']\n    return src_dir / fname\n\n# counmet_lut_path()\n\n\n\ncounmet_lut_path()\n\nPath('/Users/franckalbinet/.marisco/lut/dbo_counmet.xlsx')",
    "crumbs": [
      "API",
      "Configs"
    ]
  },
  {
    "objectID": "api/configs.html#utilities-function",
    "href": "api/configs.html#utilities-function",
    "title": "Configs",
    "section": "Utilities function",
    "text": "Utilities function\n\n\nExported source\nNETCDF_TO_PYTHON_TYPE = {\n    'u8': int,\n    'f4': float\n    }\n\n\n\n\nExported source\n# def name2grp(\n#     name: str, # Group name\n#     cdl: dict, # CDL configuration\n#     ):\n#     # Reverse `cdl.toml` config group dict so that group config key can be retrieve based on its name\n#     return {v['name']:k  for k, v in cdl['grps'].items()}[name]\n\n\nExample:\n\n# TO BE REMOVED: Now read directly from `maris.cdl` file\n# name2grp('seawater', cdl=CONFIGS_CDL)\n\n\nsource\n\nnc_tpl_name\n\n nc_tpl_name ()\n\nReturn the name of the MARIS NetCDF template as defined in configs.toml\n\n\nExported source\ndef nc_tpl_name():\n    \"Return the name of the MARIS NetCDF template as defined in `configs.toml`\"\n    p = base_path()\n    return read_toml(p / 'configs.toml')['names']['nc_template']\n\n\n\nsource\n\n\nnc_tpl_path\n\n nc_tpl_path ()\n\nReturn the path of the MARIS NetCDF template as defined in configs.toml\n\n\nExported source\ndef nc_tpl_path():\n    \"Return the path of the MARIS NetCDF template as defined in `configs.toml`\"\n    p = base_path()\n    return p / read_toml(p / 'configs.toml')['names']['nc_template']\n\n\n\nsource\n\n\nget_time_units\n\n get_time_units (nc_path:Callable=&lt;function nc_tpl_path&gt;)\n\nGet the units attribute of the time variable from a NetCDF file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnc_path\nCallable\nnc_tpl_path\nFunction returning Path to NetCDF template file\n\n\nReturns\nstr\n\nTime units string (e.g. ‘seconds since 1970-01-01 00:00:00.0’)\n\n\n\n\n\nExported source\ndef get_time_units(\n    nc_path: Callable=nc_tpl_path # Function returning Path to NetCDF template file\n    ) -&gt; str: # Time units string (e.g. 'seconds since 1970-01-01 00:00:00.0')\n    \"Get the units attribute of the time variable from a NetCDF file.\"\n    with Dataset(nc_tpl_path(), 'r') as nc:\n        for group in nc.groups.values():\n            if 'time' in group.variables:\n                return group.variables['time'].units\n                \n    raise ValueError(\"Time variable not found in NetCDF file\")\n\n\nUsage example:\n\ntime_units = get_time_units(); time_units\n\n'seconds since 1970-01-01 00:00:00.0'",
    "crumbs": [
      "API",
      "Configs"
    ]
  },
  {
    "objectID": "api/configs.html#enumeration-types",
    "href": "api/configs.html#enumeration-types",
    "title": "Configs",
    "section": "Enumeration types",
    "text": "Enumeration types\nEnumeration types are used to avoid using strings as NetCDF4 variable values. Instead, enumeration types (lookup tables) such as {'Crustaceans': 2, 'Echinoderms': 3, ...} are prepended to the NetCDF file template and associated ids (integers) are used as values.\n\nsource\n\nsanitize\n\n sanitize (s:str|float)\n\n*Sanitize dictionary key to comply with NetCDF enumeration type:\n\nRemove (, ), ., /, -\nStrip the string\nReturn original value if it’s not a string (e.g., NaN)*\n\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ns\nstr | float\nString or float to sanitize\n\n\nReturns\nstr | float\nSanitized string or original float\n\n\n\n\n\nExported source\ndef sanitize(\n    s: str|float # String or float to sanitize\n    ) -&gt; str|float:  # Sanitized string or original float\n    \"\"\"\n    Sanitize dictionary key to comply with NetCDF enumeration type:\n    \n    - Remove `(`, `)`, `.`, `/`, `-`\n    - Strip the string\n    - Return original value if it's not a string (e.g., NaN)\n    \"\"\"\n    if isinstance(s, str):\n        s = re.sub(r'[().]', '', s)\n        return re.sub(r'[/-]', ' ', s).strip()\n    elif pd.isna(s):  # This covers np.nan, None, and pandas NaT\n        return s\n    else:\n        return str(s).strip()\n\n\nFor example:\n\nfc.test_eq(sanitize('key (sanitized)'), 'key sanitized')\nfc.test_eq(sanitize('key san.itized'), 'key sanitized')\nfc.test_eq(sanitize('key-sanitized'), 'key sanitized')\nfc.test_eq(sanitize('key/sanitized'), 'key sanitized')\n\nNetCDF4 enumeration type seems to not accept keys containing non alphanumeric characters like parentheses, dots, slash, … As a result, MARIS lookup table needs to be sanitized.\n\nsource\n\n\nget_lut\n\n get_lut (src_dir:str, fname:str, key:str, value:str,\n          do_sanitize:bool=True, reverse:bool=False,\n          check_duplicates:bool=False)\n\nConvert MARIS db lookup table excel file to dictionary {'name': id, ...} or {id: name, ...} if reverse is True.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc_dir\nstr\n\nDirectory containing lookup tables\n\n\nfname\nstr\n\nExcel file lookup table name\n\n\nkey\nstr\n\nExcel file column name to be used as dict keys\n\n\nvalue\nstr\n\nExcel file column name to be used as dict values\n\n\ndo_sanitize\nbool\nTrue\nSanitization required?\n\n\nreverse\nbool\nFalse\nReverse lookup table (value, key)\n\n\ncheck_duplicates\nbool\nFalse\nCheck for duplicates in lookup table\n\n\nReturns\nDict\n\nMARIS lookup table (key, value)\n\n\n\n\n\nExported source\ndef get_lut(\n    src_dir: str, # Directory containing lookup tables\n    fname: str, # Excel file lookup table name\n    key: str, # Excel file column name to be used as dict keys \n    value: str, # Excel file column name to be used as dict values \n    do_sanitize: bool=True, # Sanitization required?\n    reverse: bool=False, # Reverse lookup table (value, key)\n    check_duplicates: bool=False # Check for duplicates in lookup table\n    ) -&gt; Dict[str, int]: # MARIS lookup table (key, value)\n    \"Convert MARIS db lookup table excel file to dictionary `{'name': id, ...}` or `{id: name, ...}` if `reverse` is True.\"\n    fname = Path(src_dir) / fname\n    df = pd.read_excel(fname, usecols=[key, value]).dropna(subset=value)\n    \n    if check_duplicates:\n        duplicates = df[key][df[key].duplicated()].tolist()\n        if duplicates: print(f\"Warning: {fname.name}: found duplicate keys: {duplicates}\")\n        \n    df[value] = df[value].astype('int')\n    df = df.set_index(key)\n    lut = df[value].to_dict()\n    if do_sanitize: lut = {sanitize(k): v for k, v in lut.items()}\n    lut = {try_int(k): try_int(v) for k, v in lut.items()}    \n    return {v: k for k, v in lut.items()} if reverse else lut\n\n\nFor example:\n\nlut_src_dir = './files/lut'\nget_lut(lut_src_dir, 'dbo_biogroup.xlsx', key='biogroup', value='biogroup_id', reverse=False)\n\n{'Not applicable': -1,\n 'Not available': 0,\n 'Birds': 1,\n 'Crustaceans': 2,\n 'Echinoderms': 3,\n 'Fish': 4,\n 'Mammals': 5,\n 'Molluscs': 6,\n 'Others': 7,\n 'Plankton': 8,\n 'Polychaete worms': 9,\n 'Reptile': 10,\n 'Seaweeds and plants': 11,\n 'Cephalopods': 12,\n 'Gastropods': 13,\n 'Bivalves': 14}\n\n\n\nsource\n\n\nEnums\n\n Enums (lut_src_dir:str, dtypes:Dict[str,Dict[str,str]]={'AREA': {'name':\n        'area_t', 'fname': 'dbo_area.xlsx', 'key': 'displayName', 'value':\n        'areaId'}, 'BIO_GROUP': {'name': 'bio_group_t', 'fname':\n        'dbo_biogroup.xlsx', 'key': 'biogroup', 'value': 'biogroup_id'},\n        'BODY_PART': {'name': 'body_part_t', 'fname': 'dbo_bodypar.xlsx',\n        'key': 'bodypar', 'value': 'bodypar_id'}, 'COUNT_MET': {'name':\n        'count_met_t', 'fname': 'dbo_counmet.xlsx', 'key': 'counmet',\n        'value': 'counmet_id'}, 'DL': {'name': 'dl_t', 'fname':\n        'dbo_detectlimit.xlsx', 'key': 'name_sanitized', 'value': 'id'},\n        'FILT': {'name': 'filt_t', 'fname': 'dbo_filtered.xlsx', 'key':\n        'name', 'value': 'id'}, 'NUCLIDE': {'name': 'nuclide_t', 'fname':\n        'dbo_nuclide.xlsx', 'key': 'nc_name', 'value': 'nuclide_id'},\n        'PREP_MET': {'name': 'prep_met_t', 'fname': 'dbo_prepmet.xlsx',\n        'key': 'prepmet', 'value': 'prepmet_id'}, 'SAMP_MET': {'name':\n        'samp_met_t', 'fname': 'dbo_sampmet.xlsx', 'key': 'sampmet',\n        'value': 'sampmet_id'}, 'SED_TYPE': {'name': 'sed_type_t',\n        'fname': 'dbo_sedtype.xlsx', 'key': 'sedtype', 'value':\n        'sedtype_id'}, 'SPECIES': {'name': 'species_t', 'fname':\n        'dbo_species_2024_11_19.xlsx', 'key': 'species', 'value':\n        'species_id'}, 'UNIT': {'name': 'unit_t', 'fname':\n        'dbo_unit.xlsx', 'key': 'unit_sanitized', 'value': 'unit_id'},\n        'LAB': {'name': 'lab_t', 'fname': 'dbo_lab_cleaned.xlsx', 'key':\n        'lab', 'value': 'lab_id'}})\n\nReturn dictionaries of MARIS NetCDF’s enumeration types.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlut_src_dir\nstr\n\nDirectory containing lookup tables\n\n\ndtypes\nDict\n{‘AREA’: {‘name’: ‘area_t’, ‘fname’: ‘dbo_area.xlsx’, ‘key’: ‘displayName’, ‘value’: ‘areaId’}, ‘BIO_GROUP’: {‘name’: ‘bio_group_t’, ‘fname’: ‘dbo_biogroup.xlsx’, ‘key’: ‘biogroup’, ‘value’: ‘biogroup_id’}, ‘BODY_PART’: {‘name’: ‘body_part_t’, ‘fname’: ‘dbo_bodypar.xlsx’, ‘key’: ‘bodypar’, ‘value’: ‘bodypar_id’}, ‘COUNT_MET’: {‘name’: ‘count_met_t’, ‘fname’: ‘dbo_counmet.xlsx’, ‘key’: ‘counmet’, ‘value’: ‘counmet_id’}, ‘DL’: {‘name’: ‘dl_t’, ‘fname’: ‘dbo_detectlimit.xlsx’, ‘key’: ‘name_sanitized’, ‘value’: ‘id’}, ‘FILT’: {‘name’: ‘filt_t’, ‘fname’: ‘dbo_filtered.xlsx’, ‘key’: ‘name’, ‘value’: ‘id’}, ‘NUCLIDE’: {‘name’: ‘nuclide_t’, ‘fname’: ‘dbo_nuclide.xlsx’, ‘key’: ‘nc_name’, ‘value’: ‘nuclide_id’}, ‘PREP_MET’: {‘name’: ‘prep_met_t’, ‘fname’: ‘dbo_prepmet.xlsx’, ‘key’: ‘prepmet’, ‘value’: ‘prepmet_id’}, ‘SAMP_MET’: {‘name’: ‘samp_met_t’, ‘fname’: ‘dbo_sampmet.xlsx’, ‘key’: ‘sampmet’, ‘value’: ‘sampmet_id’}, ‘SED_TYPE’: {‘name’: ‘sed_type_t’, ‘fname’: ‘dbo_sedtype.xlsx’, ‘key’: ‘sedtype’, ‘value’: ‘sedtype_id’}, ‘SPECIES’: {‘name’: ‘species_t’, ‘fname’: ‘dbo_species_2024_11_19.xlsx’, ‘key’: ‘species’, ‘value’: ‘species_id’}, ‘UNIT’: {‘name’: ‘unit_t’, ‘fname’: ‘dbo_unit.xlsx’, ‘key’: ‘unit_sanitized’, ‘value’: ‘unit_id’}, ‘LAB’: {‘name’: ‘lab_t’, ‘fname’: ‘dbo_lab_cleaned.xlsx’, ‘key’: ‘lab’, ‘value’: ‘lab_id’}}\nCustom NetCDF types\n\n\n\n\n\nExported source\nclass Enums():\n    \"Return dictionaries of MARIS NetCDF's enumeration types.\"\n    def __init__(self, \n                 lut_src_dir: str, # Directory containing lookup tables\n                 dtypes: Dict[str, Dict[str, str]]=NC_DTYPES, # Custom NetCDF types\n                 ):\n        fc.store_attr()\n        self.types = self.lookup()\n        \n    def filter(self, var_name, values):\n        return {name: id for name, id in self.types[var_name].items() if id in values}\n    \n    def lookup(self):\n        types = {}\n        for var_name, dtype in self.dtypes.items():\n            name, fname, key, value = dtype.values()\n            lut = get_lut(self.lut_src_dir, fname, key=key, value=value)\n            types[var_name] = lut\n        return types\n\n\n\nlut_src_dir_test = './files/lut'\nenums = Enums(lut_src_dir=lut_src_dir_test)\n\n\nenums.types['DL']\n\n{'Not applicable': -1,\n 'Not available': 0,\n 'Detected value': 1,\n 'Detection limit': 2,\n 'Not detected': 3,\n 'Derived': 4}\n\n\n\nfor k,v in enums.types['SPECIES'].items():\n    print(k, v)\n\nNOT AVAILABLE 0\nAristeus antennatus 1\nApostichopus 2\nSaccharina japonica var religiosa 3\nSiganus fuscescens 4\nAlpheus dentipes 5\nHexagrammos agrammus 6\nDitrema temminckii 7\nParapristipoma trilineatum 8\nScombrops boops 9\nPseudopleuronectes schrenki 10\nDesmarestia ligulata 11\nSaccharina japonica 12\nNeodilsea yendoana 13\nCostaria costata 14\nSargassum yezoense 15\nAcanthephyra pelagica 16\nSargassum ringgoldianum 17\nAcanthephyra quadrispinosa 18\nSargassum thunbergii 19\nSargassum patens 20\nAsterias rubens 21\nSargassum miyabei 22\nHomarus gammarus 23\nAcanthephyra stylorostratis 24\nAcanthocybium solandri 25\nAcanthopagrus bifasciatus 26\nAcanthophora muscoides 27\nAcanthophora spicifera 28\nAcanthurus triostegus 29\nActinopterygii 30\nAdamussium colbecki 31\nAhnfeltiopsis densa 32\nAlepes melanoptera 33\nAmpharetidae 34\nAnchoviella lepidentostole 35\nAnguillidae 36\nAphroditidae 37\nArnoglossus 38\nAurigequula fasciata 39\nBalaenoptera musculus 40\nBalaenoptera physalus 41\nBalistes 42\nBeryciformes 43\nBryopsis maxima 44\nCallinectes sp 45\nCallorhinus ursinus 46\nCarassius auratus auratus 47\nCarcharhinus sorrah 48\nCaridae 49\nClupea harengus 50\nCathorops spixii 51\nCaulerpa racemosa 52\nCaulerpa scalpelliformis 53\nCaulerpa sertularioides 54\nCellana radiata 55\nCoscinasterias tenuispina 56\nCentroceras clavulatum 57\nCentropomus parallelus 58\nCrangon crangon 59\nCeramium diaphanum 60\nCeramium rubrum 61\nChaenocephalus aceratus 62\nChaetodipterus faber 63\nChaetomorpha antennina 64\nChaetomorpha linoides 65\nChelidonichthys kumu 66\nChelon ramada 67\nChiloscyllium 68\nChionodraco hamatus 69\nChlamys islandica 70\nChlorophyta 71\nChondrichthyes 72\nChrysaora 73\nCladophora nitellopsis 74\nCladophora vagabunda 75\nCladophoropsis membranacea 76\nClupea 77\nCoccotylus truncatus 78\nCodium fragile 79\nCrassostrea 80\nCynoscion acoupa 81\nCynoscion jamaicensis 82\nCynoscion leiarchus 83\nEngraulis encrasicolus 84\nCypselurus agoo agoo 85\nCystophora cristata 86\nCystoseira barbata 87\nCystoseira crinita 88\nDecapodiformes 89\nDecapterus russelli 90\nDecapterus scombrinus 91\nDelphinapterus leucas 92\nDelphinus capensis 93\nDiapterus rhombeus 94\nDicentrarchus punctatus 95\nFucus vesiculosus 96\nFunchalia woodwardi 97\nEcklonia bicyclis 98\nGadus morhua 99\nEcklonia kurome 100\nGennadas elegans 101\nEisenia arborea 102\nEncrasicholina devisi 103\nEnteromorpha 104\nEnteromorpha flexuosa 105\nEnteromorpha intestinalis 106\nEpinephelinae 107\nEpinephelus diacanthus 108\nExocoetidae 109\nSaccharina latissima 110\nGracilaria corticata 111\nLigur ensiferus 112\nGracilaria debilis 113\nGracilaria edulis 114\nGracilariales 115\nGrateloupia elliptica 116\nGrateloupia filicina 117\nLysmata seticaudata 118\nGymnogongrus griffithsiae 119\nMya arenaria 120\nHalichoerus grypus 121\nMacoma balthica 122\nMarthasterias glacialis 123\nHalimeda macroloba 124\nHarengula clupeola 125\nHarpagifer antarcticus 126\nHemifusus ternatanus 127\nHemiramphus brasiliensis 128\nMytilus edulis 129\nMetapenaeus affinis 130\nHeteroscleromorpha 131\nHeterosigma akashiwo 132\nHilsa ilisha 133\nMetapenaeus monoceros 134\nMetapenaeus stebbingi 135\nHolothuria 136\nHoplobrotula armata 137\nHypnea musciformis 138\nMerlangius merlangus 139\nIridaea cordata 140\nJania rubens 141\nMeganyctiphanes norvegica 142\nJohnius glaucus 143\nKappaphycus 144\nKappaphycus alvarezii 145\nLaevistrombus canarium 146\nLagenodelphis hosei 147\nLambia 148\nLaminaria japonica 149\nLaminaria longissima 150\nLarimus breviceps 151\nLaurencia papillosa 152\nLeiognathidae 153\nLeiognathus dussumieri 154\nLepidochelys olivacea 155\nLeptonychotes weddellii 156\nLimanda yokohamae 157\nNephrops norvegicus 158\nNeuston 159\nLittoraria undulata 160\nLoligo vulgaris 161\nLumbrineridae 162\nLutjanus fulviflamma 163\nMarginisporum aberrans 164\nMegalaspis cordyla 165\nOctopus vulgaris 166\nMenticirrhus americanus 167\nMesoplodon densirostris 168\nPalaemon longirostris 169\nMetapenaeus brevicornis 170\nPasiphaea multidentata 171\nPasiphaea sivado 172\nParapenaeopsis stylifera 173\nMiichthys miiuy 174\nMirounga leonina 175\nBrachidontes striatulus 176\nMonodon monoceros 177\nMugil platanus 178\nPenaeus semisulcatus 179\nMullus barbatus 180\nMycteroperca rubra 181\nPhilocheras echinulatus 182\nMyelophycus simplex 183\nMytilus coruscus 184\nPenaeus indicus 185\nNatator depressus 186\nPandalus jordani 187\nMelicertus kerathurus 188\nParapenaeus longirostris 189\nPlesionika 190\nPlatichthys flesus 191\nPleuronectes platessa 192\nNematopalaemon tenuipes 193\nNematoscelis difficilis 194\nNemipterus 195\nAegaeon lacazei 196\nNephtyidae 197\nNereididae 198\nNetuma bilineata 199\nNibea maculata 200\nOceana serrulata 201\nPalaemon serratus 202\nOcypode 203\nOdobenus rosmarus 204\nOgcocephalus vespertilio 205\nOligoplites saurus 206\nOnuphidae 207\nOpheliidae 208\nOpisthonema oglinum 209\nOpisthopterus tardoore 210\nOrientomysis mitsukurii 211\nOtolithes cuvieri 212\nPadina pavonica 213\nPadina tetrastromatica 214\nPadina vickersiae 215\nPagellus affinis 216\nPagophilus groenlandicus 217\nPaguroidea 218\nPagurus 219\nSystellaspis debilis 220\nSergestes 221\nSergestes arcticus 222\nPampus argenteus 223\nSergestes arachnipodus 224\nSergestes henseni 225\nSergestes prehensilis 226\nSergestes robustus 227\nPangasius pangasius 228\nPanulirus homarus 229\nParacentrotus lividus 230\nPasiphaea sp 231\nPectinariidae 232\nPenaeus 233\nPhoca vitulina 234\nPhotopectoralis bindus 235\nPhyllospadix iwatensis 236\nPlectorhinchus mediterraneus 237\nPleuronectes mochigarei 238\nPleuronectes obscurus 239\nPlocamium brasiliense 240\nPolynemus paradiseus 241\nPolysiphonia 242\nSprattus sprattus 243\nScomber scombrus 244\nPolysiphonia fucoides 245\nGonostomatidae 246\nPerca fluviatilis 247\nPomadasys crocro 248\nPorphyra tenera 249\nPotamogeton pectinatus 250\nPriacanthus hamrur 251\nPseudorhombus malayanus 252\nPterocladiella capillacea 253\nPusa caspica 254\nPusa sibirica 255\nPylaiella littoralis 256\nSabellidae 257\nSalangichthys ishikawae 258\nSarconema filiforme 259\nSardinella albella 260\nSardinella brasiliensis 261\nSardinops melanostictus 262\nSargassum cymosum 263\nSargassum linearifolium 264\nSargassum micracanthum 265\nXiphias gladius 266\nSargassum novae hollandiae 267\nSargassum oligocystum 268\nEsox lucius 269\nLimanda limanda 270\nAbramis brama 271\nAnguilla anguilla 272\nArctica islandica 273\nCerastoderma edule 274\nCyprinus carpio 275\nEchinodermata 276\nFish larvae 277\nMyoxocephalus scorpius 278\nOsmerus eperlanus 279\nPlankton 280\nScophthalmus maximus 281\nRhodophyta 282\nRutilus rutilus 283\nSaduria entomon 284\nSander lucioperca 285\nGasterosteus aculeatus 286\nZoarces viviparus 287\nGymnocephalus cernua 288\nFurcellaria lumbricalis 289\nCladophora glomerata 290\nLateolabrax japonicus 291\nOkamejei kenojei 292\nSebastes pachycephalus 293\nSqualus acanthias 294\nGadus macrocephalus 295\nParalichthys olivaceus 296\nOvalipes punctatus 297\nPseudopleuronectes yokohamae 298\nHemitripterus villosus 299\nClidoderma asperrimum 300\nMicrostomus achne 301\nLepidotrigla microptera 302\nHexagrammos otakii 303\nKareius bicoloratus 304\nPleuronichthys cornutus 305\nEnteroctopus dofleini 306\nAmmodytes personatus 307\nLophius litulon 308\nEopsetta grigorjewi 309\nTakifugu porphyreus 310\nLoliolus japonica 311\nSepia andreana 312\nSebastes cheni 313\nPortunus trituberculatus 314\nSebastes schlegelii 315\nPennahia argentata 316\nPlatichthys stellatus 317\nGadus chalcogrammus 318\nChelidonichthys spinosus 319\nConger myriaster 320\nHeterololigo bleekeri 321\nStichaeus grigorjewi 322\nPseudopleuronectes herzensteini 323\nOctopus conispadiceus 324\nHippoglossoides dubius 325\nCleisthenes pinetorum 326\nGlyptocephalus stelleri 327\nTanakius kitaharae 328\nNibea mitsukurii 329\nDasyatis matsubarai 330\nVerasper moseri 331\nHemitrygon akajei 332\nTriakis scyllium 333\nTrachurus japonicus 334\nZeus faber 335\nPagrus major 336\nAcanthopagrus schlegelii 337\nDentex tumifrons 338\nMustelus manazo 339\nSeriola quinqueradiata 340\nHyperoglyphe japonica 341\nCarcharhinus 342\nPlatycephalus 343\nScomber japonicus 344\nSquatina japonica 345\nAlopias pelagicus 346\nZenopsis nebulosa 347\nCynoglossus joyneri 348\nVerasper variegatus 349\nOncorhynchus keta 350\nPhysiculus japonicus 351\nOplegnathus punctatus 352\nArothron hispidus 353\nStereolepis doederleini 354\nTakifugu snyderi 355\nScomber australasicus 356\nLiparis tanakae 357\nThamnaconus modestus 358\nGnathophis nystromi 359\nSebastes oblongus 360\nSebastiscus marmoratus 361\nTakifugu pardalis 362\nMugil cephalus 363\nDitrema temminckii temminckii 364\nKonosirus punctatus 365\nTribolodon brandtii 366\nOncorhynchus masou 367\nAluterus monoceros 368\nTodarodes pacificus 369\nMyoxocephalus stelleri 370\nMyliobatis tobijei 371\nScyliorhinus torazame 372\nLophiomus setigerus 373\nHeterodontus japonicus 374\nSebastes vulpes 375\nParaplagusia japonica 376\nOstrea edulis 377\nMelanogrammus aeglefinus 378\nPollachius virens 379\nPollachius pollachius 380\nSebastes marinus 381\nAnarhichas minor 382\nAnarhichas denticulatus 383\nReinhardtius hippoglossoides 384\nTrisopterus esmarkii 385\nMicromesistius poutassou 386\nCoryphaenoides rupestris 387\nArgentina silus 388\nSalmo salar 389\nSebastes viviparus 390\nBuccinum undatum 391\nFucus serratus 392\nMerluccius merluccius 393\nLittorina littorea 394\nFucus 395\nRhodymenia 396\nSolea solea 397\nTrachurus trachurus 398\nEutrigla gurnardus 399\nPelvetia canaliculata 400\nAscophyllum nodosum 401\nMallotus villosus 402\nPecten maximus 403\nHippoglossoides platessoides 404\nSebastes mentella 405\nModiolus modiolus 406\nBoreogadus saida 407\nSepia 408\nGadus 409\nSardina pilchardus 410\nPleuronectiformes 411\nMolva molva 412\nPatella 413\nCrassostrea gigas 414\nDasyatis pastinaca 415\nLophius piscatorius 416\nPorphyra umbilicalis 417\nPatella vulgata 418\nBrosme brosme 419\nGlyptocephalus cynoglossus 420\nGaleus melastomus 421\nChimaera monstrosa 422\nEtmopterus spinax 423\nDicentrarchus labrax 424\nOsilinus lineatus 425\nHippoglossus hippoglossus 426\nCyclopterus lumpus 427\nMolva dypterygia 428\nMicrostomus kitt 429\nFucus distichus 430\nTapes 431\nSebastes norvegicus 432\nPhycis blennoides 433\nFucus spiralis 434\nLaminaria digitata 435\nDipturus batis 436\nAnarhichas lupus 437\nLumpenus lampretaeformis 438\nLycodes vahlii 439\nArgentina sphyraena 440\nTrisopterus minutus 441\nThunnus 442\nHyperoplus lanceolatus 443\nGaidropsarus argentatus 444\nEngraulis japonicus 445\nMytilus galloprovincialis 446\nUndaria pinnatifida 447\nChlorophthalmus albatrossis 448\nSargassum fusiforme 449\nEisenia bicyclis 450\nSpisula sachalinensis 451\nStrongylocentrotus nudus 452\nHaliotis discus hannai 453\nDexistes rikuzenius 454\nRuditapes philippinarum 455\nApostichopus japonicus 456\nPterothrissus gissu 457\nHelicolenus hilgendorfii 458\nBuccinum isaotakii 459\nNeptunea intersculpta 460\nApostichopus nigripunctatus 461\nSebastes thompsoni 462\nOratosquilla oratoria 463\nOncorhynchus kisutch 464\nErimacrus isenbeckii 465\nSillago japonica 466\nTrachysalambria curvirostris 467\nMytilus unguiculatus 468\nCrassostrea nippona 469\nLaminariales 470\nUroteuthis edulis 471\nTakifugu poecilonotus 472\nNeptunea arthritica 473\nKatsuwonus pelamis 474\nDoederleinia berycoides 475\nMetapenaeopsis dalei 476\nSeriola dumerili 477\nPseudorhombus pentophthalmus 478\nStephanolepis cirrhifer 479\nCookeolus japonicus 480\nPanulirus japonicus 481\nThunnus orientalis 482\nHalocynthia roretzi 483\nEtrumeus sadina 484\nCololabis saira 485\nCoryphaena hippurus 486\nSarda orientalis 487\nOctopus ocellatus 488\nSardinops sagax 489\nSphyraena pinguis 490\nSebastes ventricosus 491\nOccella iburia 492\nGlossanodon semifasciatus 493\nMizuhopecten yessoensis 494\nNeosalangichthys ishikawae 495\nBothrocara tanakae 496\nMalacocottus zonurus 497\nCoelorinchus macrochir 498\nNeptunea constricta 499\nBeringius polynematicus 500\nSebastes nivosus 501\nPandalus eous 502\nSynaphobranchus kaupii 503\nSebastolobus macrochir 504\nMarsupenaeus japonicus 505\nJapelion hirasei 506\nPleurogrammus azonus 507\nMonostroma nitidum 508\nAtheresthes evermanni 509\nTakifugu rubripes 510\nChionoecetes opilio 511\nPandalopsis coccinata 512\nChionoecetes japonicus 513\nSebastes matsubarae 514\nScombrops gilberti 515\nHyporhamphus sajori 516\nTrichiurus lepturus 517\nAlcichthys elongatus 518\nVolutharpa perryi 519\nMercenaria stimpsoni 520\nBerryteuthis magister 521\nAptocyclus ventricosus 522\nEuphausia pacifica 523\nSalangichthys microdon 524\nTelmessus acutidens 525\nCeratophyllum demersum 526\nPandalus nipponensis 527\nSebastes owstoni 528\nCociella crocodilus 529\nConger japonicus 530\nSardinella zunasi 531\nCheilopogon pinnatibarbatus japonicus 532\nOplegnathus fasciatus 533\nMacridiscus aequilatera 534\nRepomucenus ornatipinnis 535\nClupea pallasii 536\nScorpaena neglecta 537\nScomberomorus niphonius 538\nLeucopsarion petersii 539\nSebastes scythropus 540\nStrongylura anastomella 541\nLaemonema longipes 542\nFusitriton oregonensis 543\nJapelion pericochlion 544\nSebastes steindachneri 545\nAuxis rochei 546\nLobotes surinamensis 547\nAuxis thazard 548\nChlorophthalmus borealis 549\nEtelis coruscans 550\nSebastes inermis 551\nCynoglossus interruptus 552\nErilepis zonifer 553\nTridentiger obscurus 554\nCaranx sexfasciatus 555\nThunnus thynnus 556\nTakifugu stictonotus 557\nEuthynnus affinis 558\nSynagrops japonicus 559\nOkamejei schmidti 560\nSuggrundus meerdervoortii 561\nSebastes baramenuke 562\nPleurogrammus monopterygius 563\nDecapterus maruadsi 564\nGirella punctata 565\nSphyraena japonica 566\nOmmastrephes bartramii 567\nSepiella japonica 568\nSepioteuthis lessoniana 569\nEucleoteuthis luminosa 570\nGloiopeltis furcata 571\nMacrobrachium nipponense 572\nSepia kobiensis 573\nEriocheir japonica 574\nMagallana nippona 575\nMeretrix lusoria 576\nChondrus ocellatus 577\nChondrus elatus 578\nGloiopeltis 579\nHolothuroidea 580\nCorbicula japonica 581\nSunetta menstrualis 582\nPseudorhombus cinnamoneus 583\nTakifugu niphobles 584\nLagocephalus gloveri 585\nBeryx splendens 586\nParastichopus nigripunctatus 587\nVenerupis philippinarum 588\nHaliotis 589\nLiparis agassizii 590\nSeriola lalandi 591\nNiphon spinosus 592\nPleuronichthys japonicus 593\nSergia lucens 594\nSphoeroides pachygaster 595\nCoryphaenoides acrolepis 596\nPseudopleuronectes obscurus 597\nPyropia yezoensis 598\nIsurus oxyrinchus 599\nSargassum fulvellum 600\nPrionace glauca 601\nKajikia audax 602\nThunnus albacares 603\nThunnus alalunga 604\nThunnus obesus 605\nLamna ditropis 606\nGlyptocidaris crenularis 607\nAsterias amurensis 608\nSepiida 609\nCongridae 610\nTakifugu 611\nSargassum horneri 612\nHaliotis discus 613\nPleuronectidae 614\nAcanthogobius flavimanus 615\nAcanthogobius lactipes 616\nPholis nebulosa 617\nHemigrapsus penicillatus 618\nPalaemon paucidens 619\nMysidae 620\nZostera marina 621\nUlva pertusa 622\nGobiidae 623\nAtherinidae 624\nTribolodon 625\nAlpheus 626\nPolychaeta 627\nSebastes 628\nCharybdis japonica 629\nHemigrapsus 630\nFavonigobius gymnauchen 631\nPalaemon 632\nPlaniliza haematocheila 633\nPalaemonidae 634\nPholis crassispina 635\nLaminaria 636\nDistolasterias nipon 637\nLophiiformes 638\nAlpheus brevicristatus 639\nUndaria undariodes 640\nNeomysis awatschensis 641\nAlpheidae 642\nMacrobrachium 643\nHediste 644\nGymnogobius breunigii 645\nLuidia quinaria 646\nRhizoprionodon acutus 647\nCarangoides equula 648\nCarcinoplax longimana 649\nAnomura 650\nSpatangoida 651\nPlesiobatis daviesi 652\nEusphyra blochii 653\nRuditapes variegata 654\nSinonovacula constricta 655\nPenaeus monodon 656\nLitopenaeus vannamei 657\nSolenocera crassicornis 658\nStomatopoda 659\nTeuthida 660\nOctopus 661\nLarimichthys polyactis 662\nScomberomorini 663\nChanna argus 664\nRanina ranina 665\nLates calcarifer 666\nScomberomorus commerson 667\nLutjanus malabaricus 668\nThenus parindicus 669\nAmusium pleuronectes 670\nLoligo 671\nPlectropomus leopardus 672\nSillago ciliata 673\nScylla serrata 674\nPinctada maxima 675\nLutjanus argentimaculatus 676\nProtonibea diacanthus 677\nPolydactylus macrochir 678\nRachycentron canadum 679\nIbacus peronii 680\nArripis trutta 681\nSarda australis 682\nSeriola hippos 683\nChoerodon schoenleinii 684\nPanulirus ornatus 685\nNeotrygon kuhlii 686\nLethrinus nebulosus 687\nParupeneus multifasciatus 688\nSaccostrea cucullata 689\nLutjanus sebae 690\nThunnus maccoyii 691\nAcanthopagrus butcheri 692\nLambis lambis 693\nGerres subfasciatus 694\nZooplankton 695\nPhytoplankton 696\nRapana venosa 697\nScapharca inaequivalvis 698\nUlva intestinalis 699\nUlva linza 700\nCeramium virgatum 701\nGayralia oxysperma 702\nVertebrata fucoides 703\nStuckenia pectinata 704\nRochia nilotica 705\nCtenochaetus striatus 706\nSerranidae 707\nTurbo setosus 708\nPandalidae 709\nGymnosarda unicolor 710\nEpinephelini 711\nPisces 712\nLiza klunzingeri 713\nAcanthopagrus latus 714\nLiza subviridis 715\nSparidentex hasta 716\nOtolithes ruber 717\nCrenidens crenidens 718\nEnsis 719\nGastropoda 720\nEuheterodonta 721\nScomber 722\nTheragra chalcogramma 723\nEngraulidae 724\nOstreidae 725\nPhaeophyceae 726\nPorphyra 727\nUlva reticulata 728\nPerna viridis 729\nFenneropenaeus indicus 730\nMerluccius 731\nSoleidae 732\nMugilidae 733\nMarine algae 734\nScarus rivulatus 735\nScarus coeruleus 736\nSardinella fimbriata 737\nDussumieria acuta 738\nLutjanus kasmira 739\nLutjanus rivulatus 740\nLutjanus bohar 741\nPriacanthus blochii 742\nPelates quadrilineatus 743\nEpinephelus fasciatus 744\nUpeneus vittatus 745\nLethrinus laticaudis 746\nLethrinus lentjan 747\nLethrinus microdon 748\nSphyraena barracuda 749\nAlectis indica 750\nEpinephelus latifasciatus 751\nNemipterus japonicus 752\nRaconda russeliana 753\nLactarius lactarius 754\nAetomylaeus bovinus 755\nPennahia anea 756\nLeiognathus fasciatus 757\nSardinella longiceps 758\nTenualosa ilisha 759\nPellona ditchela 760\nStolephorus indicus 761\nSetipinna breviceps 762\nRastrelliger kanagurta 763\nChanos chanos 764\nLepturacanthus savala 765\nEpinephelus niveatus 766\nLutjanus johnii 767\nCarangoides malabaricus 768\nAblennes hians 769\nChirocentrus dorab 770\nScomberomorus cavalla 771\nScomberomorus semifasciatus 772\nScomberomorus guttatus 773\nEtrumeus teres 774\nSpondyliosoma cantharus 775\nBrama brama 776\nDasyatis zugei 777\nHarpadon nehereus 778\nCarcharhinus melanopterus 779\nPenaeus plebejus 780\nSepia officinalis 781\nJohnius dussumieri 782\nLutjanus campechanus 783\nRuditapes decussatus 784\nCarcinus aestuarii 785\nSquilla mantis 786\nEpinephelus polyphekadion 787\nLutjanus gibbus 788\nLethrinus mahsena 789\nEpinephelus chlorostigma 790\nCarangoides bajad 791\nAethaloperca rogaa 792\nAtule mate 793\nMacolor niger 794\nCarangoides fulvoguttatus 795\nPlectropomus areolatus 796\nCephalopholis argus 797\nCephalopholis 798\nScarus sordidus 799\nScomberomorus tritor 800\nTriaenodon obesus 801\nPomadasys commersonnii 802\nMonotaxis grandoculis 803\nPlectropomus maculatus 804\nTrachinotus blochii 805\nPristipomoides filamentosus 806\nAcanthurus gahhm 807\nAcanthurus sohal 808\nSiganus argenteus 809\nNaso unicornis 810\nChanos 811\nOedalechilus labiosus 812\nPlectorhinchus gaterinus 813\nMercenaria mercenaria 814\nMytilus 815\nTurbo cornutus 816\nDecapoda 817\nSphyraena 818\nArius maculatus 819\nPenaeus merguiensis 820\nTegillarca granosa 821\nMullus barbatus barbatus 822\nChamelea gallina 823\nMetanephrops thomsoni 824\nMagallana gigas 825\nBranchiostegus japonicus 826\nCephalopoda 827\nLutjanidae 828\nLethrinidae 829\nSphyraena argentea 830\nChirocentrus nudus 831\nTrachinotus 832\nMugil auratus 833\nEuthynnus alletteratus 834\nSparus aurata 835\nPagrus caeruleostictus 836\nScorpaena scrofa 837\nPagellus erythrinus 838\nEpinephelus aeneus 839\nDentex maroccanus 840\nCaranx rhonchus 841\nSardinella 842\nSiganus 843\nSolea 844\nDiplodus sargus 845\nLithognathus mormyrus 846\nOblada melanura 847\nSiganus rivulatus 848\nChelon labrosus 849\nCynoscion microlepidotus 850\nGenypterus brasiliensis 851\nMyoxocephalus polyacanthocephalus 852\nHexagrammos lagocephalus 853\nHexagrammos decagrammus 854\nSebastes ciliatus 855\nLepidopsetta polyxystra 856\nClupeiformes 857\nGadidae 858\nBrachyura 859\nDasyatis 860\nCarcharias 861\nSaurida 862\nUpeneus 863\nCynoglossus 864\nScomberomorus 865\nTerapon 866\nLeiognathus 867\nTerapontidae 868\nCaranx 869\nDiplodus 870\nPlectorhinchus flavomaculatus 871\nSalmonidae 872\nMollusca 873\nBoops boops 874\nSarpa salpa 875\nPagellus acarne 876\nSpicara smaris 877\nDiplodus vulgaris 878\nChelidonichthys lucerna 879\nSarda sarda 880\nSerranus cabrilla 881\nDiplodus annularis 882\nPagrus pagrus 883\nAlosa fallax 884\nBelone belone 885\nDentex dentex 886\nSphyraena viridensis 887\nTrisopterus capelanus 888\nArnoglossus laterna 889\nProcambarus clarkii 890\nNemadactylus macropterus 891\nPagrus auratus 892\nJasus edwardsii 893\nPerna canaliculus 894\nPseudophycis bachus 895\nHaliotis iris 896\nHoplostethus atlanticus 897\nRhombosolea leporina 898\nZygochlamys delicatula 899\nGaleorhinus galeus 900\nParapercis colias 901\nTiostrea chilensis 902\nGenypterus blacodes 903\nEvechinus chloroticus 904\nAustrovenus stutchburyi 905\nMicromesistius australis 906\nMacruronus novaezelandiae 907\nNototodarus 908\nPerna perna 909\nSepia pharaonis 910\nTurbo bruneus 911\nPortunus sanguinolentus 912\nCharybdis natator 913\nCharybdis lucifera 914\nPanulirus argus 915\nEthmalosa fimbriata 916\nSardinella brachysoma 917\nThryssa mystax 918\nPlicofollis dussumieri 919\nNibea soldado 920\nEpinephelus melanostigma 921\nMegalops cyprinoides 922\nDecapterus macarellus 923\nDrepane punctata 924\nSillago sihama 925\nTylosurus crocodilus crocodilus 926\nSaurida tumbil 927\nCynoglossus macrostomus 928\nParupeneus indicus 929\nSynechogobius hasta 930\nBusycotypus canaliculatus 931\nPampus cinereus 932\nPomadasys kaakan 933\nEpinephelus coioides 934\nSepiella inermis 935\nUroteuthis duvauceli 936\nStomatella auricula 937\nCerithium scabridum 938\nMarcia recens 939\nCirce intermedia 940\nMarcia opima 941\nFulvia fragile 942\nCharybdis feriatus 943\nCharybdis annulata 944\nAtergatis integerrimus 945\nMatuta lunaris 946\nCalappa lophos 947\nUca annulipes 948\nChlamys varia 949\nCololabis adocetus 950\nSeriola lalandi dorsalis 951\nBrunneifusus ternatanus 952\nMetapenaeus joyneri 953\nEpinephelus tauvina 954\nCoilia dussumieri 955\nCarcharhinus dussumieri 956\nUpeneus tragula 957\nSartoriana spinigera 958\nLamellidens marginalis 959\nPolydactylus sextarius 960\nJohnius macrorhynus 961\nHexanematichthys sagor 962\nSargassum swartzii 963\nArgyrops spinifer 964\nSynodus intermedius 965\nMuraenesox cinereus 966\nCarangoides armatus 967\nEleutheronema tetradactylum 968\nMustelus mosis 969\nNemipterus bipunctatus 970\nLutjanus quinquelineatus 971\nPlatycephalus indicus 972\nRhabdosargus haffara 973\nArgyrops filamentosus 974\nBrachirus orientalis 975\nMene maculata 976\nHemiramphus marginatus 977\nEncrasicholina heteroloba 978\nTrachinotus africanus 979\nBramidae 980\nEscualosa thoracata 981\nSepia arabica 982\nScatophagus argus 983\nParastromateus niger 984\nPlaniliza subviridis 985\nLabeo rohita 986\nOreochromis niloticus 987\nCardiidae 988\nSargassum angustifolium 989\nPomacea bridgesii 990\nSebastes fasciatus 991\nBatoidea 992\nUrophycis chuss 993\nDalatias licha 994\nTrisopterus luscus 995\nScyliorhinus canicula 996\nRuvettus pretiosus 997\nAphanopus carbo 998\nAlepocephalus bairdii 999\nCentroscymnus coelolepis 1000\nLoligo forbesii 1001\nLutjanus cyanopterus 1002\nMugil liza 1003\nMicropogonias furnieri 1004\nBalistes capriscus 1005\nHaemulidae 1006\nStenotomus caprinus 1007\nHemanthias leptus 1008\nMicropogonias undulatus 1009\nCynoscion nebulosus 1010\nRhomboplites aurorubens 1011\nBothidae 1012\nPogonias cromis 1013\nLutjanus synagris 1014\nNetuma thalassina 1015\nSillaginopsis panijus 1016\nLeptomelanosoma indicum 1017\nTherapon 1018\nPterotolithus maculatus 1019\nIlisha filigera 1020\nHilsa kelee 1021\nPampus chinensis 1022\nPalaemon styliferus 1023\nArgyrosomus regius 1024\nLutjanus 1025\nSciades 1026\nMullus 1027\nAlbula vulpes 1028\nSelar crumenophthalmus 1029\nCentropomus 1030\nSardinella aurita 1031\nHarengula humeralis 1032\nDiapterus auratus 1033\nGerres cinereus 1034\nHaemulon parra 1035\nOcyurus chrysurus 1036\nSphyraena guachancho 1037\nAnoplopoma fimbria 1038\nNerita versicolor 1039\nBulla striata 1040\nMelongena melongena 1041\nTrachycardium muricatum 1042\nIsognomon alatus 1043\nBrachidontes exustus 1044\nCrassostrea virginica 1045\nProtothaca granulata 1046\nCittarium pica 1047\nPenaeus schmitti 1048\nPenaeus notialis 1049\nCallinectes sapidus 1050\nCallinectes danae 1051\nDasyatidae 1052\nCaridea 1053\nNephropidae 1054\nSparus 1055\nSargassum boveanum 1056\nHaliotis tuberculata 1057\nLittorinidae 1058\nSeaweed 1059\nEchinoidea 1060\nOstreida 1061\nDonax trunculus 1062\nScrobicularia plana 1063\nVenus verrucosa 1064\nSolen marginatus 1065\nTestudines 1066\nMullidae 1067\nAmphipoda 1068\nCystosphaera jacquinotii 1069\nDaption capense 1070\nDesmarestia anceps 1071\nHimantothallus grandifolius 1072\nMirounga 1073\nNacella concinna 1074\nNotothenia coriiceps 1075\nPygoscelis antarcticus 1076\nPygoscelis papua 1077\nOncorhynchus gorbuscha 1078\nOncorhynchus mykiss 1079\nOncorhynchus nerka 1080\nOncorhynchus tshawytscha 1081\nErignathus barbatus 1082\nPusa hispida 1083\nHippoglossus stenolepis 1084\nSqualus suckleyi 1085\nSargassum 1086\nCodium 1087\nMembranoptera alata 1088\nDictyota dichotoma 1089\nPlocamium cartilagineum 1090\nGalatea paradoxa 1091\nCrassostrea tulipa 1092\nMacrobrachium sp 1093\nPortunus 1094\nTympanotonos fuscatus 1095\nThais 1096\nBivalvia 1097\nCynoglossus senegalensis 1098\nCarlarius heudelotii 1099\nFontitrygon margarita 1100\nChrysichthys nigrodigitatus 1101\nAcanthephyra purpurea 1102\nActinauge abyssorum 1103\nAlaria marginata 1104\nAnadara transversa 1105\nAnthomedusae 1106\nArchosargus probatocephalus 1107\nArgyropelecus aculeatus 1108\nAriopsis felis 1109\nAstrometis sertulifera 1110\nAstropecten 1111\nAtherina breviceps 1112\nAtolla 1113\nAulacomya atra 1114\nAuxis rochei rochei 1115\nAuxis thazard thazard 1116\nAvicennia marina 1117\nBalaena mysticetus 1118\nBalaenoptera acutorostrata 1119\nBalanus 1120\nBerardius bairdii 1121\nBeroe 1122\nBoopsoidea inornata 1123\nCalanoida 1124\nCalanus finmarchicus finmarchicus 1125\nCallorhinchus milii 1126\nCepphus columba 1127\nCladonia rangiferina 1128\nClinus superciliosus 1129\nCodium tomentosum 1130\nCopepoda 1131\nCoregonus autumnalis 1132\nCoregonus nasus 1133\nCoregonus sardinella 1134\nCoryphaenoides armatus 1135\nCoryphoblennius galerita 1136\nCreseis sp 1137\nCrinoidea 1138\nCrossota 1139\nCryptochiton stelleri 1140\nDelphinus delphis 1141\nDiacria 1142\nDichistius capensis 1143\nDosinia alta 1144\nDugong dugon 1145\nElectrona risso 1146\nEngraulis capensis 1147\nEnsis siliqua 1148\nEryonidae 1149\nEualaria fistulosa 1150\nEupasiphae gilesii 1151\nEuphausiacea 1152\nEuphausiidae 1153\nEurypharynx pelecanoides 1154\nEurythenes gryllus 1155\nEuthynnus lineatus 1156\nFratercula cirrhata 1157\nGaleichthys feliceps 1158\nGelidium corneum 1159\nGibbula umbilicalis 1160\nGnathophausia ingens 1161\nGonatus fabricii 1162\nHaliaeetus leucocephalus 1163\nHaliclona 1164\nHalodule uninervis 1165\nHemilepidotus 1166\nHemilepidotus jordani 1167\nHeterocarpus ensifer 1168\nHeterodontus portusjacksoni 1169\nHippasteria phrygiana 1170\nHomola barbata 1171\nHyperoodon planifrons 1172\nHypleurochilus geminatus 1173\nInvertebrata 1174\nIsognomon bicolor 1175\nIsopoda 1176\nKogia breviceps 1177\nLabrus bergylta 1178\nLagenorhynchus obliquidens 1179\nLampris guttatus 1180\nLarus glaucescens 1181\nLeander serratus 1182\nLibinia emarginata 1183\nLichia amia 1184\nLipophrys pholis 1185\nLipophrys trigloides 1186\nLithognathus lithognathus 1187\nLithophaga aristata 1188\nLobianchia gemellarii 1189\nLoliginidae 1190\nLoligo reynaudii 1191\nLophius budegassa 1192\nMagallana angulata 1193\nMajoidea 1194\nMegachasma pelagios 1195\nMegaptera novaeangliae 1196\nMenippe mercenaria 1197\nMesoplodon carlhubbsi 1198\nMesoplodon stejnegeri 1199\nMicrostomus pacificus 1200\nMorone saxatilis 1201\nMullus surmuletus 1202\nMycteroperca xenarcha 1203\nMyliobatis australis 1204\nMysida 1205\nMytilus californianus 1206\nMytilus trossulus 1207\nNephasoma Nephasoma flagriferum 1208\nNudibranchia 1209\nOdobenus rosmarus divergens 1210\nOmmastrephidae 1211\nOphiomusa lymani 1212\nOphiothrix lineata 1213\nOrcinus orca 1214\nOstracoda 1215\nPagellus bogaraveo 1216\nPandalus borealis 1217\nPaphies subtriangulata 1218\nParabrotula 1219\nParacalanus 1220\nPatella aspera 1221\nPeriphylla 1222\nPhocoena phocoena 1223\nPhocoenoides dalli 1224\nPhronima 1225\nPhyseter macrocephalus 1226\nPinctada radiata 1227\nPlesionika edwardsii 1228\nPododesmus macrochisma 1229\nPomatomus saltatrix 1230\nPortunus pelagicus 1231\nPraunus 1232\nPyrosoma 1233\nRangifer tarandus 1234\nRhabdosargus globiceps 1235\nSaccorhiza polyschides 1236\nSagitta 1237\nSalpa 1238\nSalvelinus alpinus 1239\nSalvelinus malma 1240\nSarda chiliensis 1241\nSargassum aquifolium 1242\nScalibregmatidae 1243\nSebastes alutus 1244\nSebastes melanops 1245\nSeriola dorsalis 1246\nSerranus scriba 1247\nSigmops bathyphilus 1248\nSilicula fragilis 1249\nSipunculidae 1250\nSomateria mollissima 1251\nSomateria spectabilis 1252\nSparodon durbanensis 1253\nSpicara maena 1254\nSquatina australis 1255\nStriostrea margaritacea 1256\nStromateus fiatola 1257\nStrongylocentrotus polyacanthus 1258\nTaractichthys steindachneri 1259\nTectura scutum 1260\nTegula viridula 1261\nThais haemastoma 1262\nThegrefg 1263\nThemisto 1264\nThunnus tonggol 1265\nTrachurus picturatus 1266\nTrachurus symmetricus 1267\nTrygonorrhina fasciata 1268\nUlva lactuca 1269\nUrsus maritimus 1270\nVampyroteuthis infernalis 1271\nZiphius cavirostris 1272\nAlepes kleinii 1273\nAlepes vari 1274\nDecapterus macrosoma 1275\nLutjanus madras 1276\nLutjanus russellii 1277\nRastrelliger brachysoma 1278\nRastrelliger faughni 1279\nSelar boops 1280\nSelaroides leptolepis 1281\nSphyraena obtusata 1282\nGeloina expansa 1283\nCaesio erythrogaster 1284\nEuristhmus microceps 1285\nPomacanthus annularis 1286\nScylla 1287\nPlotosus lineatus 1288\nPrionotus stephanophrys 1289\nTrachurus murphyi 1290\nDosidicus gigas 1291\nSarda chiliensis chiliensis 1292\nCynoscion analis 1293\nMerluccius gayi peruanus 1294\nBrotula ordwayi 1295\nLoligo gahi 1296\nMerluccius gayi 1297\nOphichthus remiger 1298\nPenaeus sp 1299\nTrachinotus paitensis 1300\nCheilopogon heterurus 1301\nEngraulis ringens 1302\nSciaena deliciosa 1303\nIsacia conceptionis 1304\nOdontesthes regia 1305\nBodianus diplotaenia 1306\nConcholepas concholepas 1307\nDiplectrum conceptione 1308\nGenypterus maculatus 1309\nLabrisomus philippii 1310\nParalabrax humeralis 1311\nPrionotus horrens 1312\nDasyatis akajei 1313\nArctoscopus japonicus 1314\nSepia esculenta 1315\nBothrocara hollandi 1316\nCynoglossidae 1317\nLepidotrigla 1318\nLepidotrigla alata 1319\nOctopus sinensis 1320\nRhabdosargus sarba 1321\nLophiidae 1322\nMuraenesox 1323\nPhysiculus maximowiczi 1324\nPleuronectoidei 1325\nSciaenidae 1326\nTriglidae 1327\nAtherina presbyter 1328\nBentheogennema intermedia 1329\nBenthesicymidae 1330\nBenthesicymus 1331\nBuccinum striatissimum 1332\nCallinectes 1333\nCancer pagurus 1334\nChaetognatha 1335\nChama macerophylla 1336\nCirripedia 1337\nCyclosalpa 1338\nCymopolia barbata 1339\nCynoscion 1340\nCystoseira amentacea 1341\nEctocarpus siliculosus 1342\nEllisolandia elongata 1343\nEnteromorpha linza 1344\nEuphausia superba 1345\nGaidropsarus mediterraneus 1346\nGennadas valens 1347\nGlobicephala 1348\nHaliptilon virgatum 1349\nHalocynthia aurantium 1350\nHeliocidaris crassispina 1351\nHymenodora gracilis 1352\nLagodon rhomboides 1353\nLepas Anatifa anatifera 1354\nLobophora variegata 1355\nMacrocystis pyrifera 1356\nMaculabatis gerrardi 1357\nNemacystus decipiens 1358\nNeptunea polycostata 1359\nPadina pavonia 1360\nPenaeidae 1361\nPetricolinae 1362\nPolynemidae 1363\nPristipomoides aquilonaris 1364\nPyropia fallax 1365\nRadiolaria 1366\nSalpidae 1367\nSardinops melanosticta 1368\nSargassum vulgare 1369\nSciaena umbra 1370\nScorpaena porcus 1371\nSergestidae 1372\nSicyonia brevirostris 1373\nSphaerococcus coronopifolius 1374\nStenella coeruleoalba 1375\nStichopus japonicus 1376\nThalia democratica 1377\nThemisto gaudichaudii 1378\nUndaria 1379\nAnalipus japonicus 1380\nSargassum yamadae 1381\nAhnfeltiopsis paradoxa 1382\nScytosiphon lomentaria 1383\nChondria crassicaulis 1384\nGrateloupia lanceolata 1385\nColpomenia sinuosa 1386\nChondrus giganteus 1387\nSargassum muticum 1388\nUlva prolifera 1389\nPetalonia fascia 1390\nBalanus roseus 1391\nChaetomorpha moniligera 1392\nLomentaria hakodatensis 1393\nNeodilsea longissima 1394\nPolyopes affinis 1395\nSchizymenia dubyi 1396\nDictyopteris pacifica 1397\nAhnfeltiopsis flabelliformis 1398\nBangia fuscopurpurea 1399\nCalliarthron 1400\nCladophora 1401\nCladophora albida 1402\nDasya sessilis 1403\nDelesseria serrulata 1404\nEcklonia cava 1405\nGelidium elegans 1406\nGrateloupia turuturu 1407\nHypnea asiatica 1408\nMazzaella japonica 1409\nPachydictyon coriaceum 1410\nPadina arborescens 1411\nPterosiphonia pinnulata 1412\nAlatocladia yessoensis 1413\nBryopsis plumosa 1414\nCeramium kondoi 1415\nChondracanthus intermedius 1416\nCodium contractum 1417\nCodium lucasii 1418\nCorallina pilulifera 1419\nDictyopteris undulata 1420\nGastroclonium pacificum 1421\nGelidium amansii 1422\nGrateloupia sparsa 1423\nLaurencia okamurae 1424\nLeathesia marina 1425\nLomentaria catenata 1426\nMeristotheca papulosa 1427\nSargassum confusum 1428\nSargassum siliquastrum 1429\nTinocladia crassa 1430\nSaccharina yendoana 1431\nThalassiophyllum clathrus 1432\nMytilida 1433\nPteriomorphia 1434\nConger 1435\nScyliorhinidae 1436\nLabrus 1437\nAlgae 1438\nNecora puber 1439\nAnguilla 1440\nRajidae 1441\nBuccinidae 1442\nCrustacea 1443\nGreen algae 1444\nAmmodytes japonicus 1445\nEvynnis tumifrons 1446\nGnathophis nystromi nystromi 1447\nLoligo bleekeri 1448\nPlatichthys bicoloratus 1449\nLimanda punctatissima 1450\nLoliolus Nipponololigo japonica 1451\nAcanthopagrus schlegelii schlegelii 1452\nSepiolina 1453\nGelidium 1454\nAtrina pectinata 1455\nEchinocardium cordatum 1456\nLamnidae 1457\nMeretrix lamarckii 1458\nNoctiluca scintillans 1459\nPhiline argentata 1460\nSergestes lucens 1461\nCorbicula sandai 1462\nUlva 1463\nActiniaria 1464\nCtenopharyngodon idella 1465\nOphiuroidea 1466\nScomberoides lysan 1467\nScomberoides tol 1468\nSebastolobus 1469\nSelachimorpha 1470\nSelene setapinnis 1471\nSelene vomer 1472\nSepia elliptica 1473\nSergestes sp 1474\nSetipinna taty 1475\nSiganus canaliculatus 1476\nSigmops gracile 1477\nSolenocera sp 1478\nSparidae 1479\nSpermatophytina 1480\nSphoeroides testudineus 1481\nSphyraena jello 1482\nSpyridia hypnoides 1483\nSqualiformes 1484\nSquillidae 1485\nStegophiura sladeni 1486\nStenella longirostris 1487\nStenobrachius leucopsarus 1488\nSternaspidae 1489\nStoechospermum polypodioides 1490\nStolephorus commersonnii 1491\nStromateus cinereus 1492\nStromateus niger 1493\nStromateus sinensis 1494\nSynidotea 1495\nTakifugu vermicularis 1496\nTelatrygon zugei 1497\nTerapon jarbua 1498\nTerebellidae 1499\nThryssa dussumieri 1500\nThunnini 1501\nTibia curta 1502\nTonna dolium 1503\nTrachinus draco 1504\nTrematomus bernacchii 1505\nTridacna 1506\nTrinectes paulistanus 1507\nTrochus radiatus 1508\nTurbinaria 1509\nTursiops truncatus 1510\nUcides 1511\nUlva compressa 1512\nUlva fasciata 1513\nUlva flexuosa 1514\nUlva rigida 1515\nUpeneus taeniopterus 1516\nUpogebiidae 1517\nUroteuthis Photololigo edulis 1518\nValoniopsis pachynema 1519\nVeneridae 1520\nVenus foveolata 1521\nVertebrata 1522\nVolutharpa ampullacea perryi 1523\nZannichellia palustris 1524\nZeus japonicus 1525\nFavites 1526\nGadiformes 1527\nGafrarium dispar 1528\nGalaxaura frutescens 1529\nGelidium crinale 1530\nGenidens genidens 1531\nGirella elevata 1532\nGirella tricuspidata 1533\nDentex hypselosomus 1534\nSaurida elongata 1535\nPseudolabrus eoethinus 1536\nAtrobucca nibe 1537\nDiagramma pictum 1538\nSepia lycidas 1539\nPlectorhinchus cinctus 1540\nMetapenaeopsis acclivis 1541\nMetapenaeopsis barbata 1542\nNibea albiflora 1543\nGirella leonina 1544\nSphyraenidae 1545\nParapercis pulchella 1546\nParapercis sexfasciata 1547\nThysanoteuthis rhombus 1548\nLepidotrigla kishinouyi 1549\nCystoseira 1550\nPadina 1551\nHalimeda 1552\nPacifastacus leniusculus 1553\nSalmo trutta 1554\nChondrus crispus 1555\nIctalurus punctatus 1556\nAcanthurus 1557\nScombridae 1558\nLeukoma staminea 1559\nTrochidae 1560\nProtonibea 1562\nAnchoa compressa 1563\nEnsis magnus 1564\nBolinus brandaris 1565\nLutjanus notatus 1566\nLethrinus olivaceus 1567\nCarassius auratus 1569\nMugil 1570\nGobius 1571\nLajonkairia lajonkairii 1572\nChrysophrys auratus 1573\nGaleorhinus australis 1574\nNototodarus sloanii gouldi 1575\nTylosurus crocodilus 1576\nAcanthogobius hasta 1577\nPenaeus chinensis 1578\nRuditapes variegatus 1579\nMarcia marmorata 1580\nRachycentron 1581\nScomber kanagurta 1582\nArius 1583\nPanulirus versicolor 1584\nTilapia zillii 1585\nSchizoporella errata 1586\nPhallusia nigra 1587\nPhyseter catodon 1588\nSalmo trutta trutta 1589\nTachysurus thalassinus 1590\nSillago domina 1591\nOtolithus argenteus 1592\nTrichiurus haumela 1593\nOtolithes maculata 1594\nHilsa kanagurta 1595\nOreochromis mossambicus 1596\nSiluriformes 1597\nTheodoxus euxinus 1598\nFormio niger 1599\nRastrelliger 1600\nNephasoma flagriferum 1601\nOphiomusium lymani 1602\nNematonurus armatus 1603\nThalamitoides spinigera 1604\nCapros aper 1605\nGadiculus argenteus thori 1606\nPhorcus lineatus 1607\nPenaeus vannamei 1608\nRaja montagui 1609\nScophthalmus rhombus 1610\nCrambe maritima 1611\nFucus ceranoides 1612\nMaja squinado 1613\nSalicornia europaea 1614\nAequipecten opercularis 1615\nGalathea squamifera 1616\nCynoglossus semilaevis 1617\nLoliolus beka 1619\nOctopus variabilis 1620\nAbudefduf sexfasciatus 1621\nAcanthurus blochii 1622\nAchillea millefolium 1623\nAlaria crassifolia 1624\nAlbulidae 1625\nAmmodytes 1626\nAnadara satowi 1627\nArgyrosomus japonicus 1628\nAscidiacea 1629\nAulopiformes 1630\nBabylonia japonica 1631\nBabylonia kirana 1632\nBathylagidae 1633\nBeryx decadactylus 1634\nBranchiostegus 1635\nBuccinum 1636\nCaesio lunaris 1637\nCallionymus curvicornis 1638\nCampylaephora hypnaeoides 1639\nCetoscarus ocellatus 1640\nCharonia tritonis 1641\nChelon haematocheilus 1642\nChlorurus sordidus 1643\nChoerodon azurio 1644\nChromis notata 1645\nCladosiphon okamuranus 1646\nCociella punctata 1647\nCoryphaena 1648\nCyclina sinensis 1649\nCymbacephalus beauforti 1650\nDendrobranchiata 1651\nDigenea simplex 1652\nDitrema viride 1653\nEnteromorpha prolifera 1654\nEpinephelus 1655\nEpinephelus akaara 1656\nEpinephelus awoara 1657\nEtelis carbunculus 1658\nFistularia commersonii 1659\nFulvia mutica 1660\nFusinus colus 1661\nGafrarium tumidum 1662\nGelidiaceae 1663\nGirella cyanea 1664\nGirella mezina 1665\nGoniistius zonatus 1666\nGracilaria 1667\nGymnocranius euanus 1668\nHeikeopsis japonica 1669\nHemitrygon 1670\nHippoglossoides pinetorum 1671\nHolothuria atra 1672\nHolothuria leucospilota 1673\nIdiosepiidae 1674\nInegocia japonica 1675\nInimicus didactylus 1676\nIshige 1677\nLagocephalus spadiceus 1678\nLambis truncata 1679\nLeiognathus equula 1680\nLethrinus xanthochilus 1681\nLutjanus erythropterus 1682\nLutjanus semicinctus 1683\nMonodonta labio 1684\nMonostroma kuroshiense 1685\nMulloidichthys flavolineatus 1686\nMulloidichthys vanicolensis 1687\nMuraenesocidae 1688\nMyagropsis myagroides 1689\nMytilisepta virgata 1690\nNaso brevirostris 1691\nNematalosa japonica 1692\nNemipterus virgatus 1693\nNipponacmea 1694\nNuchequula nuchalis 1695\nOctopus cyanea 1696\nPanopea generosa 1697\nParalichthys 1698\nParalithodes camtschaticus 1699\nParascolopsis inermis 1700\nPectinidae 1701\nPentapodus aureofasciatus 1702\nPinctada fucata 1703\nPitar citrinus 1704\nPlatycephalidae 1705\nPlecoglossus altivelis 1706\nPleuronectes herzensteini 1707\nPriacanthus macracanthus 1708\nPristipomoides 1709\nPsenopsis anomala 1710\nPseudobalistes fuscus 1711\nPseudocaranx dentex 1712\nPseudolabrus sieboldi 1713\nPseudorhombus arsius 1714\nPterocaesio chrysozona 1715\nRhynchopelates oxyrhynchus 1716\nRyukyupercis gushikeni 1717\nSaccostrea echinata 1718\nSargassum hemiphyllum 1719\nSargassum piluliferum 1720\nSaurida micropectoralis 1721\nSaurida undosquamis 1722\nSaurida wanieso 1723\nScarus forsteni 1724\nScarus ghobban 1725\nScarus ovifrons 1726\nScarus rubroviolaceus 1727\nScyphozoa 1728\nSebastes iracundus 1729\nSemicossyphus reticulatus 1730\nSepia latimanus 1731\nSiganus guttatus 1732\nSiganus luridus 1733\nSphaerotrichia divaricata 1734\nSphyrnidae 1735\nSpondylus regius 1736\nSpratelloides gracilis 1737\nSthenoteuthis oualaniensis 1738\nTetraodontidae 1739\nTrichiurus lepturus japonicus 1740\nTridacna crocea 1741\nTurbo argyrostomus 1742\nTylosurus pacificus 1743\nUlvophyceae 1744\nUpeneus japonicus 1745\nUpeneus moluccensis 1746\nUranoscopus japonicus 1747\nAnguilliformes 1748\nCrithmum maritimum 1749\nLittorina 1750\nNucella lapillus 1752\nScyliorhinus stellaris 1753\nAnnelida 1754\nAphrodita aculeata 1755\nCallionymus lyra 1756\nUrticina felina 1757\nGebiidea 1758\nBonellia viridis 1759\nAlcyonium glomeratum 1760",
    "crumbs": [
      "API",
      "Configs"
    ]
  },
  {
    "objectID": "handlers/helcom.html",
    "href": "handlers/helcom.html",
    "title": "HELCOM",
    "section": "",
    "text": "This data pipeline, known as a “handler” in Marisco terminology, is designed to clean, standardize, and encode HELCOM data into NetCDF format. The handler processes raw HELCOM data, applying various transformations and lookups to align it with MARIS data standards.\nKey functions of this handler:\nThis handler is a crucial component in the Marisco data processing workflow, ensuring HELCOM data is properly integrated into the MARIS database.\nThe present notebook pretends to be an instance of Literate Programming in the sense that it is a narrative that includes code snippets that are interspersed with explanations. When a function or a class needs to be exported in a dedicated python module (in our case marisco/handlers/helcom.py) the code snippet is added to the module using #| exports as provided by the wonderful nbdev library.",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#configuration-file-paths",
    "href": "handlers/helcom.html#configuration-file-paths",
    "title": "HELCOM",
    "section": "Configuration & file paths",
    "text": "Configuration & file paths\n\nsrc_dir: path to the maris-crawlers folder containing the HELCOM data in CSV format.\nfname_out: path and filename for the NetCDF output.The path can be defined as a relative path.\nZotero key: used to retrieve attributes related to the dataset from Zotero. The MARIS datasets include a library available on Zotero.\n\n\n\nExported source\nsrc_dir = 'https://raw.githubusercontent.com/franckalbinet/maris-crawlers/refs/heads/main/data/processed/HELCOM%20MORS'\nfname_out = '../../_data/output/100-HELCOM-MORS-2024.nc'\nzotero_key ='26VMZZ2Q' # HELCOM MORS zotero key",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#load-data",
    "href": "handlers/helcom.html#load-data",
    "title": "HELCOM",
    "section": "Load data",
    "text": "Load data\nHelcom MORS (Monitoring of Radioactive Substances in the Baltic Sea) data is provided as a zipped Microsoft Access database. We automatically fetch and convert this dataset with database tables exported as .csv files using a Github action here: maris-crawlers.\nThe dataset is then accessible in an amenable format for the marisco data pipeline.\n\nsource\n\nread_csv\n\n read_csv (file_name,\n           dir='https://raw.githubusercontent.com/franckalbinet/maris-\n           crawlers/refs/heads/main/data/processed/HELCOM%20MORS')\n\n\n\nExported source\ndefault_smp_types = {  \n    'BIO': 'BIOTA', \n    'SEA': 'SEAWATER', \n    'SED': 'SEDIMENT'\n}\n\n\n\n\nExported source\ndef read_csv(file_name, dir=src_dir):\n    file_path = f'{dir}/{file_name}'\n    return pd.read_csv(file_path)\n\n\n\nsource\n\n\nload_data\n\n load_data (src_url:str, smp_types:dict={'BIO': 'BIOTA', 'SEA':\n            'SEAWATER', 'SED': 'SEDIMENT'}, use_cache:bool=False,\n            save_to_cache:bool=False, verbose:bool=False)\n\nLoad HELCOM data and return the data in a dictionary of dataframes with the dictionary key as the sample type.\n\n\nExported source\ndef load_data(src_url: str, \n              smp_types: dict = default_smp_types, \n              use_cache: bool = False,\n              save_to_cache: bool = False,\n              verbose: bool = False) -&gt; Dict[str, pd.DataFrame]:\n    \"Load HELCOM data and return the data in a dictionary of dataframes with the dictionary key as the sample type.\"\n\n    \n    def load_and_merge(file_prefix: str) -&gt; pd.DataFrame:\n        \n        if use_cache:\n            dir=cache_path()\n        else:\n            dir = src_url\n            \n        file_smp_path = f'{dir}/{file_prefix}01.csv'\n        file_meas_path = f'{dir}/{file_prefix}02.csv'\n\n        if use_cache:\n            if not Path(file_smp_path).exists():\n                print(f'{file_smp_path} not found.')            \n            if not Path(file_meas_path).exists():\n                print(f'{file_meas_path} not found.')\n        \n        if verbose:\n            start_time = time.time()\n        df_meas = read_csv(f'{file_prefix}02.csv', dir)\n        df_smp = read_csv(f'{file_prefix}01.csv', dir)\n        \n        df_meas.columns = df_meas.columns.str.lower()\n        df_smp.columns = df_smp.columns.str.lower()\n        \n        merged_df = pd.merge(df_meas, df_smp, on='key', how='left')\n        \n        if verbose:\n            print(f\"Downloaded data for {file_prefix}01.csv and {file_prefix}02.csv in {time.time() - start_time:.2f} seconds.\")\n            \n        if save_to_cache:\n            dir = cache_path()\n            df_smp.to_csv(f'{dir}/{file_prefix}01.csv', index=False)\n            df_meas.to_csv(f'{dir}/{file_prefix}02.csv', index=False)\n            if verbose:\n                print(f\"Saved downloaded data to cache at {dir}/{file_prefix}01.csv and {dir}/{file_prefix}02.csv\")\n\n        return merged_df\n    return {smp_type: load_and_merge(file_prefix) for file_prefix, smp_type in smp_types.items()}\n\n\ndfs is a dictionary of dataframes created from the Helcom dataset located at the path src_dir. The data to be included in each dataframe is sorted by sample type. Each dictionary is defined with a key equal to the sample type.\n\ndfs = load_data(src_dir, verbose=True, save_to_cache=True)\nprint('keys/sample types: ', dfs.keys())\n\nDownloaded data for BIO01.csv and BIO02.csv in 0.30 seconds.\nSaved downloaded data to cache at /Users/franckalbinet/.marisco/cache/BIO01.csv and /Users/franckalbinet/.marisco/cache/BIO02.csv\nDownloaded data for SEA01.csv and SEA02.csv in 0.32 seconds.\nSaved downloaded data to cache at /Users/franckalbinet/.marisco/cache/SEA01.csv and /Users/franckalbinet/.marisco/cache/SEA02.csv\nDownloaded data for SED01.csv and SED02.csv in 0.55 seconds.\nSaved downloaded data to cache at /Users/franckalbinet/.marisco/cache/SED01.csv and /Users/franckalbinet/.marisco/cache/SED02.csv\nkeys/sample types:  dict_keys(['BIOTA', 'SEAWATER', 'SEDIMENT'])\n\n\nLets take a look at each DataFrame:\n\nfor key in dfs.keys():\n    display(Markdown(f\"&lt;b&gt; {key} DataFrame:&lt;/b&gt;\"))\n    with pd.option_context('display.max_columns', None):\n        display(dfs[key].head(2))\n\n BIOTA DataFrame:\n\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/kg\nvalue_bq/kg\nbasis\nerror%\nnumber\ndate_of_entry_x\ncountry\nlaboratory\nsequence\ndate\nyear\nmonth\nday\nstation\nlatitude ddmmmm\nlatitude dddddd\nlongitude ddmmmm\nlongitude dddddd\nsdepth\nrubin\nbiotatype\ntissue\nno\nlength\nweight\ndw%\nloi%\nmors_subbasin\nhelcom_subbasin\ndate_of_entry_y\n\n\n\n\n0\nBVTIG2012041\nCS134\nVTIG01\n&lt;\n0.01014\nW\nNaN\nNaN\n02/27/14 00:00:00\n6.0\nVTIG\n2012041\n09/23/12 00:00:00\n2012\n9.0\n23.0\nSD24\n54.17\n54.283333\n12.19\n12.316667\nNaN\nGADU MOR\nF\n5\n16.0\n45.7\n948.0\n18.453\n92.9\n2.0\n16\n02/27/14 00:00:00\n\n\n1\nBVTIG2012041\nK40\nVTIG01\n\n135.30000\nW\n3.57\nNaN\n02/27/14 00:00:00\n6.0\nVTIG\n2012041\n09/23/12 00:00:00\n2012\n9.0\n23.0\nSD24\n54.17\n54.283333\n12.19\n12.316667\nNaN\nGADU MOR\nF\n5\n16.0\n45.7\n948.0\n18.453\n92.9\n2.0\n16\n02/27/14 00:00:00\n\n\n\n\n\n\n\n SEAWATER DataFrame:\n\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/m³\nvalue_bq/m³\nerror%_m³\ndate_of_entry_x\ncountry\nlaboratory\nsequence\ndate\nyear\nmonth\nday\nstation\nlatitude (ddmmmm)\nlatitude (dddddd)\nlongitude (ddmmmm)\nlongitude (dddddd)\ntdepth\nsdepth\nsalin\nttemp\nfilt\nmors_subbasin\nhelcom_subbasin\ndate_of_entry_y\n\n\n\n\n0\nWKRIL2012003\nCS137\nNaN\nNaN\n5.3\n32.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012003.0\n05/23/12 00:00:00\n2012.0\n5.0\n23.0\nRU10\n60.05\n60.0833\n29.2\n29.3333\nNaN\n0.0\nNaN\nNaN\nNaN\n11.0\n11.0\n08/20/14 00:00:00\n\n\n1\nWKRIL2012004\nCS137\nNaN\nNaN\n19.9\n20.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012004.0\n05/23/12 00:00:00\n2012.0\n5.0\n23.0\nRU10\n60.05\n60.0833\n29.2\n29.3333\nNaN\n29.0\nNaN\nNaN\nNaN\n11.0\n11.0\n08/20/14 00:00:00\n\n\n\n\n\n\n\n SEDIMENT DataFrame:\n\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/kg\nvalue_bq/kg\nerror%_kg\n&lt; value_bq/m²\nvalue_bq/m²\nerror%_m²\ndate_of_entry_x\ncountry\nlaboratory\nsequence\ndate\nyear\nmonth\nday\nstation\nlatitude (ddmmmm)\nlatitude (dddddd)\nlongitude (ddmmmm)\nlongitude (dddddd)\ndevice\ntdepth\nuppsli\nlowsli\narea\nsedi\noxic\ndw%\nloi%\nmors_subbasin\nhelcom_subbasin\nsum_link\ndate_of_entry_y\n\n\n\n\n0\nSKRIL2012116\nCS137\nNaN\nNaN\n1200.0\n20.0\nNaN\nNaN\nNaN\n08/20/14 00:00:00\n90.0\nKRIL\n2012116.0\n05/25/12 00:00:00\n2012.0\n5.0\n25.0\nRU99\n60.28\n60,4667\n27.48\n27.8\nKRIL01\n25.0\n15.0\n20.0\n0.006\nNaN\nNaN\nNaN\nNaN\n11.0\n11.0\nNaN\n08/20/14 00:00:00\n\n\n1\nSKRIL2012117\nCS137\nNaN\nNaN\n250.0\n20.0\nNaN\nNaN\nNaN\n08/20/14 00:00:00\n90.0\nKRIL\n2012117.0\n05/25/12 00:00:00\n2012.0\n5.0\n25.0\nRU99\n60.28\n60,4667\n27.48\n27.8\nKRIL01\n25.0\n20.0\n25.0\n0.006\nNaN\nNaN\nNaN\nNaN\n11.0\n11.0\nNaN\n08/20/14 00:00:00",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#normalize-nuclide-names",
    "href": "handlers/helcom.html#normalize-nuclide-names",
    "title": "HELCOM",
    "section": "Normalize nuclide names",
    "text": "Normalize nuclide names\n\nLower & strip nuclide names\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nSome nuclide names contain one or multiple trailing spaces.\n\n\nThis is demonstrated below for the NUCLIDE column:\n\ndf = get_unique_across_dfs(load_data(src_dir, use_cache=True), 'nuclide', as_df=True, include_nchars=True)\ndf['stripped_chars'] = df['value'].str.strip().str.replace(' ', '').str.len()\nprint(df[df['n_chars'] != df['stripped_chars']])\n\n    index      value  n_chars  stripped_chars\n5       5    SR90           7               4\n6       6     SR90          6               4\n14     14   CS134           8               5\n16     16   SR90            8               4\n22     22     CS137         6               5\n36     36      SR90         5               4\n38     38   PU238           8               5\n49     49   K40             8               3\n61     61  CS137            9               5\n78     78   CS137           8               5\n80     80    TC99           7               4\n85     85   AM241           8               5\n86     86   CO60            8               4\n\n\nTo fix this issue, we use the LowerStripNameCB callback. For each dataframe in the dictionary of dataframes, it corrects the nuclide name by converting it lowercase, striping any leading or trailing whitespace(s).\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='nuclide', col_dst='NUCLIDE')])\ntfm()\nfor key, df in tfm.dfs.items():\n    print(f'{key} Nuclides: ')\n    print(df['NUCLIDE'].unique())\n    \nprint(tfm.logs)\n\nBIOTA Nuclides: \n['cs134' 'k40' 'co60' 'cs137' 'sr90' 'ag108m' 'mn54' 'co58' 'ag110m'\n 'zn65' 'sb125' 'pu239240' 'ru106' 'be7' 'ce144' 'pb210' 'po210' 'sb124'\n 'sr89' 'zr95' 'te129m' 'ru103' 'nb95' 'ce141' 'la140' 'i131' 'ba140'\n 'pu238' 'u235' 'bi214' 'pb214' 'pb212' 'tl208' 'ac228' 'ra223' 'eu155'\n 'ra226' 'gd153' 'sn113' 'fe59' 'tc99' 'co57' 'sn117m' 'eu152' 'sc46'\n 'rb86' 'ra224' 'th232' 'cs134137' 'am241' 'ra228' 'th228' 'k-40' 'cs138'\n 'cs139' 'cs140' 'cs141' 'cs142' 'cs143' 'cs144' 'cs145' 'cs146']\nSEAWATER Nuclides: \n['cs137' 'sr90' 'h3' 'cs134' 'pu238' 'pu239240' 'am241' 'cm242' 'cm244'\n 'tc99' 'k40' 'ru103' 'sr89' 'sb125' 'nb95' 'ru106' 'zr95' 'ag110m'\n 'cm243244' 'ba140' 'ce144' 'u234' 'u238' 'co60' 'pu239' 'pb210' 'po210'\n 'np237' 'pu240' 'mn54']\nSEDIMENT Nuclides: \n['cs137' 'ra226' 'ra228' 'k40' 'sr90' 'cs134137' 'cs134' 'pu239240'\n 'pu238' 'co60' 'ru103' 'ru106' 'sb125' 'ag110m' 'ce144' 'am241' 'be7'\n 'th228' 'pb210' 'co58' 'mn54' 'zr95' 'ba140' 'po210' 'ra224' 'nb95'\n 'pu238240' 'pu241' 'pu239' 'eu155' 'ir192' 'th232' 'cd109' 'sb124' 'zn65'\n 'th234' 'tl208' 'pb212' 'pb214' 'bi214' 'ac228' 'ra223' 'u235' 'bi212']\n[\"Convert 'nuclide' column values to lowercase, strip spaces, and store in 'NUCLIDE' column.\"]\n\n\n\n\nRemap nuclide names to MARIS data formats\nBelow, we map nuclide names used by HELCOM to the MARIS standard nuclide names.\nRemapping data provider nomenclatures to MARIS standards is a recurrent operation and is done in a semi-automated manner according to the following pattern:\n\nInspect data provider nomenclature:\nMatch automatically against MARIS nomenclature (using a fuzzy matching algorithm);\nFix potential mismatches;\nApply the lookup table to the dataframe.\n\nWe will refer to this process as IMFA (Inspect, Match, Fix, Apply).\nThe get_unique_across_dfs function is a utility in MARISCO that retrieves unique values from a specified column across all DataFrames. Note that there is one DataFrame for each sample type, such as biota, sediment, etc.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='nuclide', col_dst='NUCLIDE')])\n\ndfs_output = tfm()\n\n# Transpose to display the dataframe horizontally\nget_unique_across_dfs(dfs_output, col_name='NUCLIDE', as_df=True).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n\n\n\n\nindex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n\n\nvalue\nag108m\nbi214\npb214\nra226\nce144\nco57\ncs137\nu238\ntl208\nk-40\n...\nth234\ncs146\npo210\nth228\neu152\nsc46\npu238240\ncs134137\nth232\npu239240\n\n\n\n\n2 rows × 77 columns\n\n\n\nLet’s now create an instance of a fuzzy matching algorithm Remapper. This instance will match the nuclide names of the HELCOM dataset to the MARIS standard nuclide names.\n\nremapper = Remapper(provider_lut_df=get_unique_across_dfs(dfs_output, col_name='NUCLIDE', as_df=True),\n                    maris_lut_fn=nuc_lut_path,\n                    maris_col_id='nuclide_id',\n                    maris_col_name='nc_name',\n                    provider_col_to_match='value',\n                    provider_col_key='value',\n                    fname_cache='nuclides_helcom.pkl')\n\nLets try to match HELCOM nuclide names to MARIS standard nuclide names as automatically as possible. The match_score column allows to assess the results:\n\nremapper.generate_lookup_table(as_df=True)\nremapper.select_match(match_score_threshold=1, verbose=True)\n\nProcessing: 100%|██████████| 77/77 [00:01&lt;00:00, 54.80it/s]\n\n\n63 entries matched the criteria, while 14 entries had a match score of 1 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\ncm243244\ncm242\ncm243244\n3\n\n\npu238240\npu240\npu238240\n3\n\n\ncs134137\ncs137\ncs134137\n3\n\n\npu239240\npu239\npu239240\n3\n\n\ncs145\nce140\ncs145\n2\n\n\ncs142\nce140\ncs142\n2\n\n\ncs143\nce140\ncs143\n2\n\n\nk-40\nk40\nk-40\n1\n\n\ncs140\nce140\ncs140\n1\n\n\ncs139\nce139\ncs139\n1\n\n\ncs138\ncs134\ncs138\n1\n\n\ncs141\nce141\ncs141\n1\n\n\ncs144\ncs134\ncs144\n1\n\n\ncs146\ncs136\ncs146\n1\n\n\n\n\n\n\n\nWe can now manually inspect the unmatched nuclide names and create a table to correct them to the MARIS standard:\n\n\nExported source\nfixes_nuclide_names = {\n    'cs134137': 'cs134_137_tot',\n    'cm243244': 'cm243_244_tot',\n    'pu239240': 'pu239_240_tot',\n    'pu238240': 'pu238_240_tot',\n    'cs143': 'cs137',\n    'cs145': 'cs137',\n    'cs142': 'cs137',\n    'cs141': 'cs137',\n    'cs144': 'cs137',\n    'k-40': 'k40',\n    'cs140': 'cs137',\n    'cs146': 'cs137',\n    'cs139': 'cs137',\n    'cs138': 'cs137'\n    }\n\n\nWe now include the table fixes_nuclide_names, which applies manual corrections to the nuclide names before the remapping process. The generate_lookup_table function has an overwrite parameter (default is True), which, when set to True, creates a pickle file cache of the lookup table. We can now test the remapping process:\n\nremapper.generate_lookup_table(as_df=True, fixes=fixes_nuclide_names)\nfc.test_eq(len(remapper.select_match(match_score_threshold=1, verbose=True)), 0)\n\nProcessing: 100%|██████████| 77/77 [00:01&lt;00:00, 55.80it/s]\n\n\n77 entries matched the criteria, while 0 entries had a match score of 1 or higher.\n\n\n\n\n\nTest passes! We can now create a callback RemapNuclideNameCB to remap the nuclide names. Note that we pass overwrite=False to the Remapper constructor to now use the cached version.\n\n\nExported source\n# Create a lookup table for nuclide names\nlut_nuclides = lambda df: Remapper(provider_lut_df=df,\n                                   maris_lut_fn=nuc_lut_path,\n                                   maris_col_id='nuclide_id',\n                                   maris_col_name='nc_name',\n                                   provider_col_to_match='value',\n                                   provider_col_key='value',\n                                   fname_cache='nuclides_helcom.pkl').generate_lookup_table(fixes=fixes_nuclide_names, \n                                                                                            as_df=False, overwrite=False)\n\n\nThe callback RemapNuclideNameCB is now created to remap the nuclide names using the lut_nuclides lookup table.\n\nsource\n\n\nRemapNuclideNameCB\n\n RemapNuclideNameCB (fn_lut:Callable, col_name:str)\n\nRemap data provider nuclide names to standardized MARIS nuclide names.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfn_lut\nCallable\nFunction that returns the lookup table dictionary\n\n\ncol_name\nstr\nColumn name to remap\n\n\n\n\n\nExported source\nclass RemapNuclideNameCB(Callback):\n    \"Remap data provider nuclide names to standardized MARIS nuclide names.\"\n    def __init__(self, \n                 fn_lut: Callable, # Function that returns the lookup table dictionary\n                 col_name: str # Column name to remap\n                ):\n        fc.store_attr()\n\n    def __call__(self, tfm: Transformer):\n        df_uniques = get_unique_across_dfs(tfm.dfs, col_name=self.col_name, as_df=True)\n        #lut = {k: v.matched_maris_name for k, v in self.fn_lut(df_uniques).items()}    \n        lut = {k: v.matched_id for k, v in self.fn_lut(df_uniques).items()}    \n        for k in tfm.dfs.keys():\n            tfm.dfs[k]['NUCLIDE'] = tfm.dfs[k][self.col_name].replace(lut)\n\n\nLet’s see it in action, along with the LowerStripNameCB callback:\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[LowerStripNameCB(col_src='nuclide', col_dst='NUCLIDE'),\n                            RemapNuclideNameCB(lut_nuclides, col_name='NUCLIDE'),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\ndfs_out = tfm()\n\n# For instance\nfor key in dfs_out.keys():\n    print(f'{key} NUCLIDE unique: ', dfs_out[key]['NUCLIDE'].unique())\n    \nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n\nBIOTA NUCLIDE unique:  [31  4  9 33 12 21  6  8 22 10 24 77 17  2 37 41 47 23 11 13 25 16 14 36\n 35 29 34 67 63 46 43 42 94 55 50 40 53 87 92 86 15  7 93 85 91 90 51 59\n 76 72 54 57]\nSEAWATER NUCLIDE unique:  [33 12  1 31 67 77 72 73 75 15  4 16 11 24 14 17 13 22 80 34 37 62 64  9\n 68 41 47 65 69  6]\nSEDIMENT NUCLIDE unique:  [ 33  53  54   4  12  76  31  77  67   9  16  17  24  22  37  72   2  57\n  41   8   6  13  34  47  51  14  89  70  68  40  88  59  84  23  10  60\n  94  42  43  46  55  50  63 130]\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16124     21634     40744\nRows removed from original (tfm.dfs_removed)       0         0         0\nRows created in transformed (tfm.dfs_created)      0         0         0",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#standardize-time",
    "href": "handlers/helcom.html#standardize-time",
    "title": "HELCOM",
    "section": "Standardize Time",
    "text": "Standardize Time\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nTime/date is provide in the DATE, YEAR , MONTH, DAY columns. Note that the DATE contains missing values as indicated below. When missing, we fallback on the YEAR, MONTH, DAY columns. Note that sometimes DAY and MONTH contain 0. In this case we systematically set them to 1.\n\n\n\ndfs = load_data(src_dir, use_cache=True)\nfor key in dfs.keys():\n    print(f'{key} DATE null values: ', dfs[key]['date'].isna().sum())\n\nBIOTA DATE null values:  88\nSEAWATER DATE null values:  554\nSEDIMENT DATE null values:  830\n\n\n\nsource\n\nParseTimeCB\n\n ParseTimeCB ()\n\nStandardize time format across all dataframes.\n\n\nExported source\nclass ParseTimeCB(Callback):\n    \"Standardize time format across all dataframes.\"\n    def __call__(self, tfm: Transformer):\n        for df in tfm.dfs.values():\n            self._process_dates(df)\n\n    def _process_dates(self, df: pd.DataFrame) -&gt; None:\n        \"Process and correct date and time information in the DataFrame.\"\n        df['TIME'] = self._parse_date(df)\n        self._handle_missing_dates(df)\n        self._fill_missing_time(df)\n\n    def _parse_date(self, df: pd.DataFrame) -&gt; pd.Series:\n        \"Parse the DATE column if present.\"\n        return pd.to_datetime(df['date'], format='%m/%d/%y %H:%M:%S', errors='coerce')\n\n    def _handle_missing_dates(self, df: pd.DataFrame):\n        \"Handle cases where DAY or MONTH is 0 or missing.\"\n        df.loc[df[\"day\"] == 0, \"day\"] = 1\n        df.loc[df[\"month\"] == 0, \"month\"] = 1\n        \n        missing_day_month = (df[\"day\"].isna()) & (df[\"month\"].isna()) & (df[\"year\"].notna())\n        df.loc[missing_day_month, [\"day\", \"month\"]] = 1\n\n    def _fill_missing_time(self, df: pd.DataFrame) -&gt; None:\n        \"Fill missing time values using year, month, and day columns.\"\n        missing_time = df['TIME'].isna()\n        df.loc[missing_time, 'TIME'] = pd.to_datetime(\n            df.loc[missing_time, ['year', 'month', 'day']], \n            format='%Y%m%d', \n            errors='coerce'\n        )\n\n\nApply the transformer for callbacks ParseTimeCB. Then, print the TIME data for seawater. Passing the CompareDfsAndTfmCB callback allows us to compare the original dataframes with the transformed dataframes using the compare_stats attribute.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[ParseTimeCB(),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\nprint(tfm.dfs['SEAWATER'][['TIME']])\n\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16124     21634     40744\nRows removed from original (tfm.dfs_removed)       0         0         0\nRows created in transformed (tfm.dfs_created)      0         0         0 \n\n            TIME\n0     2012-05-23\n1     2012-05-23\n2     2012-06-17\n3     2012-05-24\n4     2012-05-24\n...          ...\n21629 2023-06-11\n21630 2023-06-11\n21631 2023-06-13\n21632 2023-06-13\n21633 2023-06-13\n\n[21634 rows x 1 columns]\n\n\nThe NetCDF time format requires that time be encoded as the number of milliseconds since a specified origin. In our case, the origin is 1970-01-01, as indicated in the cdl.toml file under the [vars.defaults.time.attrs] section.\nEncodeTimeCB converts the HELCOM time format to the MARIS NetCDF time format.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[ParseTimeCB(),\n                            EncodeTimeCB(),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n#print(tfm.logs)\n\nWarning: 8 missing time value(s) in SEAWATER\nWarning: 1 missing time value(s) in SEDIMENT\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16124     21626     40743\nRows removed from original (tfm.dfs_removed)       0         8         1\nRows created in transformed (tfm.dfs_created)      0         0         0",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#split-sediment-values",
    "href": "handlers/helcom.html#split-sediment-values",
    "title": "HELCOM",
    "section": "Split Sediment Values",
    "text": "Split Sediment Values\nHelcom reports two values for the SEDIMENT sample type: VALUE_Bq/kg and VALUE_Bq/m³. We need to split this and use a single column VALUE for the MARIS standard. We will use the UNIT column to identify the reported values.\nLets take a look at the unit lookup table for MARIS:\n\npd.read_excel(unit_lut_path())\n\n\n\n\n\n\n\n\nunit_id\nunit\nunit_sanitized\nordlist\nUnnamed: 4\nUnnamed: 5\nUnnamed: 6\n\n\n\n\n0\n-1\nNot applicable\nNot applicable\nNaN\nNaN\nNaN\nNaN\n\n\n1\n0\nNOT AVAILABLE\nNOT AVAILABLE\n0.0\nNaN\nNaN\nNaN\n\n\n2\n1\nBq/m3\nBq per m3\n1.0\nBq/m3\nNaN\nBq/m&lt;sup&gt;3&lt;/sup&gt;\n\n\n3\n2\nBq/m2\nBq per m2\n2.0\nNaN\nNaN\nNaN\n\n\n4\n3\nBq/kg\nBq per kg\n3.0\nNaN\nNaN\nNaN\n\n\n5\n4\nBq/kgd\nBq per kgd\n4.0\nNaN\nNaN\nNaN\n\n\n6\n5\nBq/kgw\nBq per kgw\n5.0\nNaN\nNaN\nNaN\n\n\n7\n6\nkg/kg\nkg per kg\n6.0\nNaN\nNaN\nNaN\n\n\n8\n7\nTU\nTU\n7.0\nNaN\nNaN\nNaN\n\n\n9\n8\nDELTA/mill\nDELTA per mill\n8.0\nNaN\nNaN\nNaN\n\n\n10\n9\natom/kg\natom per kg\n9.0\nNaN\nNaN\nNaN\n\n\n11\n10\natom/kgd\natom per kgd\n10.0\nNaN\nNaN\nNaN\n\n\n12\n11\natom/kgw\natom per kgw\n11.0\nNaN\nNaN\nNaN\n\n\n13\n12\natom/l\natom per l\n12.0\nNaN\nNaN\nNaN\n\n\n14\n13\nBq/kgC\nBq per kgC\n13.0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nWe will define the columns of interest for the SEDIMENT measurement types:\n\n\nExported source\ncoi_sediment = {\n    'kg_type': {\n        'VALUE': 'value_bq/kg',\n        'UNC': 'error%_kg',\n        'DL': '&lt; value_bq/kg',\n        'UNIT': 3,  # Unit ID for Bq/kg\n    },\n    'm2_type': {\n        'VALUE': 'value_bq/m²',\n        'UNC': 'error%_m²',\n        'DL': '&lt; value_bq/m²',\n        'UNIT': 2,  # Unit ID for Bq/m²\n    }\n}\n\n\nWe define the SplitSedimentValuesCB callback to split the sediment entries into separate rows for Bq/kg and Bq/m². We use underscore to denote the columns are temporary columns created during the splitting process.\n\nsource\n\nSplitSedimentValuesCB\n\n SplitSedimentValuesCB (coi:Dict[str,Dict[str,Any]])\n\nSeparate sediment entries into distinct rows for Bq/kg and Bq/m² measurements.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncoi\nDict\nColumns of interest with value, uncertainty, DL columns and units\n\n\n\n\n\nExported source\nclass SplitSedimentValuesCB(Callback):\n    \"Separate sediment entries into distinct rows for Bq/kg and Bq/m² measurements.\"\n    def __init__(self, \n                 coi: Dict[str, Dict[str, Any]] # Columns of interest with value, uncertainty, DL columns and units\n                ):\n        fc.store_attr()\n        \n    def __call__(self, tfm: Transformer):\n        if 'SEDIMENT' not in tfm.dfs:\n            return\n            \n        df = tfm.dfs['SEDIMENT']\n        dfs_to_concat = []\n        \n        # For each measurement type (kg and m2)\n        for measure_type, cols in self.coi.items():\n            # If any of value/uncertainty/DL exists, keep the row\n            has_data = (\n                df[cols['VALUE']].notna() | \n                df[cols['UNC']].notna() | \n                df[cols['DL']].notna()\n            )\n            \n            if has_data.any():\n                df_measure = df[has_data].copy()\n                \n                # Copy columns to standardized names\n                df_measure['_VALUE'] = df_measure[cols['VALUE']]\n                df_measure['_UNC'] = df_measure[cols['UNC']]\n                df_measure['_DL'] = df_measure[cols['DL']]\n                df_measure['_UNIT'] = cols['UNIT']\n                \n                dfs_to_concat.append(df_measure)\n        \n        # Combine all measurement type dataframes\n        if dfs_to_concat:\n            tfm.dfs['SEDIMENT'] = pd.concat(dfs_to_concat, ignore_index=True)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[SplitSedimentValuesCB(coi_sediment),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\n\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\nwith pd.option_context('display.max_columns', None):\n    display(tfm.dfs['SEDIMENT'].head())\n\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16124     21634     70697\nRows removed from original (tfm.dfs_removed)       0         0         0\nRows created in transformed (tfm.dfs_created)      0         0     29953 \n\n\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/kg\nvalue_bq/kg\nerror%_kg\n&lt; value_bq/m²\nvalue_bq/m²\nerror%_m²\ndate_of_entry_x\ncountry\nlaboratory\nsequence\ndate\nyear\nmonth\nday\nstation\nlatitude (ddmmmm)\nlatitude (dddddd)\nlongitude (ddmmmm)\nlongitude (dddddd)\ndevice\ntdepth\nuppsli\nlowsli\narea\nsedi\noxic\ndw%\nloi%\nmors_subbasin\nhelcom_subbasin\nsum_link\ndate_of_entry_y\n_VALUE\n_UNC\n_DL\n_UNIT\n\n\n\n\n0\nSKRIL2012116\nCS137\nNaN\nNaN\n1200.0\n20.0\nNaN\nNaN\nNaN\n08/20/14 00:00:00\n90.0\nKRIL\n2012116.0\n05/25/12 00:00:00\n2012.0\n5.0\n25.0\nRU99\n60.28\n60,4667\n27.48\n27.8\nKRIL01\n25.0\n15.0\n20.0\n0.006\nNaN\nNaN\nNaN\nNaN\n11.0\n11.0\nNaN\n08/20/14 00:00:00\n1200.0\n20.0\nNaN\n3\n\n\n1\nSKRIL2012117\nCS137\nNaN\nNaN\n250.0\n20.0\nNaN\nNaN\nNaN\n08/20/14 00:00:00\n90.0\nKRIL\n2012117.0\n05/25/12 00:00:00\n2012.0\n5.0\n25.0\nRU99\n60.28\n60,4667\n27.48\n27.8\nKRIL01\n25.0\n20.0\n25.0\n0.006\nNaN\nNaN\nNaN\nNaN\n11.0\n11.0\nNaN\n08/20/14 00:00:00\n250.0\n20.0\nNaN\n3\n\n\n2\nSKRIL2012118\nCS137\nNaN\nNaN\n140.0\n21.0\nNaN\nNaN\nNaN\n08/20/14 00:00:00\n90.0\nKRIL\n2012118.0\n05/25/12 00:00:00\n2012.0\n5.0\n25.0\nRU99\n60.28\n60,4667\n27.48\n27.8\nKRIL01\n25.0\n25.0\n30.0\n0.006\nNaN\nNaN\nNaN\nNaN\n11.0\n11.0\nNaN\n08/20/14 00:00:00\n140.0\n21.0\nNaN\n3\n\n\n3\nSKRIL2012119\nCS137\nNaN\nNaN\n79.0\n20.0\nNaN\nNaN\nNaN\n08/20/14 00:00:00\n90.0\nKRIL\n2012119.0\n05/25/12 00:00:00\n2012.0\n5.0\n25.0\nRU99\n60.28\n60,4667\n27.48\n27.8\nKRIL01\n25.0\n30.0\n35.0\n0.006\nNaN\nNaN\nNaN\nNaN\n11.0\n11.0\nNaN\n08/20/14 00:00:00\n79.0\n20.0\nNaN\n3\n\n\n4\nSKRIL2012120\nCS137\nNaN\nNaN\n29.0\n24.0\nNaN\nNaN\nNaN\n08/20/14 00:00:00\n90.0\nKRIL\n2012120.0\n05/25/12 00:00:00\n2012.0\n5.0\n25.0\nRU99\n60.28\n60,4667\n27.48\n27.8\nKRIL01\n25.0\n35.0\n40.0\n0.006\nNaN\nNaN\nNaN\nNaN\n11.0\n11.0\nNaN\n08/20/14 00:00:00\n29.0\n24.0\nNaN\n3",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#sanitize-value",
    "href": "handlers/helcom.html#sanitize-value",
    "title": "HELCOM",
    "section": "Sanitize value",
    "text": "Sanitize value\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nSome of the HELCOM datasets contain missing values in the VALUE column, see output after applying the SanitizeValueCB callback.\n\n\nWe allocate each column containing measurement values (named differently across sample types) into a single column VALUE and remove NA where needed.\n\nsource\n\nSanitizeValueCB\n\n SanitizeValueCB (coi:Dict[str,Dict[str,str]], verbose:bool=False)\n\nSanitize measurement values by removing blanks and standardizing to use the VALUE column.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncoi\nDict\n\nColumns of interest. Format: {group_name: {‘val’: ‘column_name’}}\n\n\nverbose\nbool\nFalse\n\n\n\n\n\n\nExported source\ncoi_val = {'SEAWATER' : {'VALUE': 'value_bq/m³'},\n           'BIOTA':  {'VALUE': 'value_bq/kg'},\n           'SEDIMENT': {'VALUE': '_VALUE'}}\n\n\n\n\nExported source\nclass SanitizeValueCB(Callback):\n    \"Sanitize measurement values by removing blanks and standardizing to use the `VALUE` column.\"\n    def __init__(self, \n                 coi: Dict[str, Dict[str, str]], # Columns of interest. Format: {group_name: {'val': 'column_name'}}\n                 verbose: bool=False\n                 ): \n        fc.store_attr()\n\n    def __call__(self, tfm: Transformer):\n        tfm.dfs_dropped = {}\n        for grp, df in tfm.dfs.items():\n            value_col = self.coi[grp]['VALUE']\n            # Count NaN values before dropping\n            initial_nan_count = df[value_col].isna().sum()\n                        \n            # define a dataframe with the rows that were dropped    \n            tfm.dfs_dropped[grp] = df[df[value_col].isna()]\n            \n            df.dropna(subset=[value_col], inplace=True)\n\n            # Count NaN values after dropping\n            final_nan_count = df[value_col].isna().sum()\n            dropped_nan_count = initial_nan_count - final_nan_count\n            \n            # Print the number of dropped NaN values\n            if dropped_nan_count &gt; 0 and self.verbose:\n                print(f\"Warning: {dropped_nan_count} missing value(s) in {value_col} for group {grp}.\")\n            \n            \n            df['VALUE'] = df[value_col]\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[SplitSedimentValuesCB(coi_sediment),\n                            SanitizeValueCB(coi_val, verbose=True),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\n\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n\nWarning: 30 missing value(s) in value_bq/kg for group BIOTA.\nWarning: 153 missing value(s) in value_bq/m³ for group SEAWATER.\nWarning: 246 missing value(s) in _VALUE for group SEDIMENT.\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16094     21481     70451\nRows removed from original (tfm.dfs_removed)      30       153       144\nRows created in transformed (tfm.dfs_created)      0         0     29851",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#normalize-uncertainty",
    "href": "handlers/helcom.html#normalize-uncertainty",
    "title": "HELCOM",
    "section": "Normalize uncertainty",
    "text": "Normalize uncertainty\nFunction unc_rel2stan converts uncertainty from relative uncertainty to standard uncertainty.\n\nsource\n\nunc_rel2stan\n\n unc_rel2stan (df:pandas.core.frame.DataFrame, meas_col:str, unc_col:str)\n\nConvert relative uncertainty to absolute uncertainty.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\nDataFrame\nDataFrame containing measurement and uncertainty columns\n\n\nmeas_col\nstr\nName of the column with measurement values\n\n\nunc_col\nstr\nName of the column with relative uncertainty values (percentages)\n\n\nReturns\nSeries\nSeries with calculated absolute uncertainties\n\n\n\n\n\nExported source\ndef unc_rel2stan(\n    df: pd.DataFrame, # DataFrame containing measurement and uncertainty columns\n    meas_col: str, # Name of the column with measurement values\n    unc_col: str # Name of the column with relative uncertainty values (percentages)\n) -&gt; pd.Series: # Series with calculated absolute uncertainties\n    \"Convert relative uncertainty to absolute uncertainty.\"\n    return df.apply(lambda row: row[unc_col] * row[meas_col] / 100, axis=1)\n\n\nFor each sample type in the Helcom dataset, the UNC is provided as a relative uncertainty. The column names for both the VALUE and the UNC vary by sample type. The coi_units_unc dictionary defines the column names for the VALUE and UNC for each sample type.\n\n\nExported source\n# Columns of interest\ncoi_units_unc = [('SEAWATER', 'value_bq/m³', 'error%_m³'),\n                 ('BIOTA', 'value_bq/kg', 'error%'),\n                 ('SEDIMENT', '_VALUE', '_UNC')]\n\n\nNormalizeUncCB callback normalizes the UNC by converting from relative uncertainty to standard uncertainty.\n\nsource\n\n\nNormalizeUncCB\n\n NormalizeUncCB (fn_convert_unc:Callable=&lt;function unc_rel2stan&gt;,\n                 coi:List[Tuple[str,str,str]]=[('SEAWATER', 'value_bq/m³',\n                 'error%_m³'), ('BIOTA', 'value_bq/kg', 'error%'),\n                 ('SEDIMENT', '_VALUE', '_UNC')])\n\nConvert from relative error to standard uncertainty.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfn_convert_unc\nCallable\nunc_rel2stan\nFunction converting relative uncertainty to absolute uncertainty\n\n\ncoi\nList\n[(‘SEAWATER’, ‘value_bq/m³’, ’error%_m³’), (‘BIOTA’, ‘value_bq/kg’, ‘error%’), (‘SEDIMENT’, ’_VALUE’, ’_UNC’)]\nList of columns of interest\n\n\n\n\n\nExported source\nclass NormalizeUncCB(Callback):\n    \"Convert from relative error to standard uncertainty.\"\n    def __init__(self, \n                 fn_convert_unc: Callable=unc_rel2stan, # Function converting relative uncertainty to absolute uncertainty\n                 coi: List[Tuple[str, str, str]]=coi_units_unc # List of columns of interest\n                ):\n        fc.store_attr()\n    \n    def __call__(self, tfm: Transformer):\n        for grp, val, unc in self.coi:\n            if grp in tfm.dfs:\n                df = tfm.dfs[grp]\n                df['UNC'] = self.fn_convert_unc(df, val, unc)\n\n\nApply the transformer for callback [NormalizeUncCB](https://franckalbinet.github.io/marisco/handlers/helcom.html#normalizeunccb). Then, print the value (i.e. activity per unit ) and standard uncertainty for each sample type.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[SplitSedimentValuesCB(coi_sediment),\n                            SanitizeValueCB(coi_val),\n                            NormalizeUncCB(),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\ntfm()\n\nprint(tfm.dfs['SEAWATER'][['VALUE', 'UNC']][:2])\nprint(tfm.dfs['BIOTA'][['VALUE', 'UNC']][:2])\nprint(tfm.dfs['SEDIMENT'][['VALUE', 'UNC']][:2])\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n\n   VALUE    UNC\n0    5.3  1.696\n1   19.9  3.980\n       VALUE      UNC\n0    0.01014      NaN\n1  135.30000  4.83021\n    VALUE    UNC\n0  1200.0  240.0\n1   250.0   50.0\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16094     21481     70451\nRows removed from original (tfm.dfs_removed)      30       153       144\nRows created in transformed (tfm.dfs_created)      0         0     29851",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#remap-units",
    "href": "handlers/helcom.html#remap-units",
    "title": "HELCOM",
    "section": "Remap units",
    "text": "Remap units\nHELCOM incorporates the unit directly into the column name. For the SEDIMENT sample type, the units are accounted for when Splitting the sediment values (i.e. SplitSedimentValuesCB). Let’s examine the units associated with the other sample types.\nFor the BIOTA sample type, the base unit is Bq/kg, as indicated in the value_bq/kg column. The distinction between wet (W) and dry weight (D) is specified in the basis column.\n\ndfs['BIOTA'][['value_bq/kg', 'basis']].head(1)\n\n\n\n\n\n\n\n\nvalue_bq/kg\nbasis\n\n\n\n\n0\n0.01014\nW\n\n\n\n\n\n\n\nFor the SEAWATER sample type, the unit is Bq/m³ as indicated in the value_bq/m³ column.\n\ndfs['SEAWATER'][['value_bq/m³']].head(1)\n\n\n\n\n\n\n\n\nvalue_bq/m³\n\n\n\n\n0\n5.3\n\n\n\n\n\n\n\nWe can now review the units that are available in MARIS:\n\npd.read_excel(unit_lut_path())[['unit_id', 'unit', 'unit_sanitized']]\n\n\n\n\n\n\n\n\nunit_id\nunit\nunit_sanitized\n\n\n\n\n0\n-1\nNot applicable\nNot applicable\n\n\n1\n0\nNOT AVAILABLE\nNOT AVAILABLE\n\n\n2\n1\nBq/m3\nBq per m3\n\n\n3\n2\nBq/m2\nBq per m2\n\n\n4\n3\nBq/kg\nBq per kg\n\n\n5\n4\nBq/kgd\nBq per kgd\n\n\n6\n5\nBq/kgw\nBq per kgw\n\n\n7\n6\nkg/kg\nkg per kg\n\n\n8\n7\nTU\nTU\n\n\n9\n8\nDELTA/mill\nDELTA per mill\n\n\n10\n9\natom/kg\natom per kg\n\n\n11\n10\natom/kgd\natom per kgd\n\n\n12\n11\natom/kgw\natom per kgw\n\n\n13\n12\natom/l\natom per l\n\n\n14\n13\nBq/kgC\nBq per kgC\n\n\n\n\n\n\n\nWe define unit renaming rules for HELCOM in an ad hoc way:\n\n\nExported source\nlut_units = {\n    'SEAWATER': 1,  # 'Bq/m3'\n    'SEDIMENT': '_UNIT', # account for in SplitSedimentValuesCB.\n    'BIOTA': {\n        'D': 4,  # 'Bq/kgd'\n        'W': 5,  # 'Bq/kgw'\n        'F': 5   # 'Bq/kgw' (assumed to be 'Fresh', so set to wet)\n    }\n}\n\n\nWe define the RemapUnitCB callback to set the UNIT column in the DataFrames based on the lookup table lut_units.\n\nsource\n\nRemapUnitCB\n\n RemapUnitCB (lut_units:dict={'SEAWATER': 1, 'SEDIMENT': '_UNIT', 'BIOTA':\n              {'D': 4, 'W': 5, 'F': 5}})\n\nSet the unit id column in the DataFrames based on a lookup table.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlut_units\ndict\n{‘SEAWATER’: 1, ‘SEDIMENT’: ’_UNIT’, ‘BIOTA’: {‘D’: 4, ‘W’: 5, ‘F’: 5}}\nDictionary containing renaming rules for different unit categories\n\n\n\n\n\nExported source\nclass RemapUnitCB(Callback):\n    \"Set the `unit` id column in the DataFrames based on a lookup table.\"\n    def __init__(self, \n                 lut_units: dict=lut_units # Dictionary containing renaming rules for different unit categories\n                ):\n        fc.store_attr()\n\n    def __call__(self, tfm: Transformer):\n        for grp in tfm.dfs.keys():\n            if grp == 'SEAWATER':\n                tfm.dfs[grp]['UNIT'] = self.lut_units[grp]\n            elif grp == 'BIOTA':\n                tfm.dfs[grp]['UNIT'] = tfm.dfs[grp]['basis'].apply(lambda x: lut_units[grp].get(x, 0))\n            elif grp == 'SEDIMENT':\n                tfm.dfs[grp]['UNIT'] = tfm.dfs[grp]['_UNIT']\n\n\nApply the transformer for callback RemapUnitCB(). Then, print the unique UNIT for the SEAWATER dataframe.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n                            SplitSedimentValuesCB(coi_sediment),\n                            SanitizeValueCB(coi_val),\n                            NormalizeUncCB(),\n                            RemapUnitCB(),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\n\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n\nfor grp in ['BIOTA', 'SEDIMENT', 'SEAWATER']:\n    print(f\"{grp}: {tfm()[grp]['UNIT'].unique()}\")\n\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16094     21481     70451\nRows removed from original (tfm.dfs_removed)      30       153       144\nRows created in transformed (tfm.dfs_created)      0         0     29851 \n\nBIOTA: [5 0 4]\nSEDIMENT: [3 2]\nSEAWATER: [1]",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#remap-detection-limit",
    "href": "handlers/helcom.html#remap-detection-limit",
    "title": "HELCOM",
    "section": "Remap detection limit",
    "text": "Remap detection limit\nDetection limits are encoded as follows in MARIS:\n\npd.read_excel(detection_limit_lut_path())\n\n\n\n\n\n\n\n\nid\nname\nname_sanitized\n\n\n\n\n0\n-1\nNot applicable\nNot applicable\n\n\n1\n0\nNot Available\nNot available\n\n\n2\n1\n=\nDetected value\n\n\n3\n2\n&lt;\nDetection limit\n\n\n4\n3\nND\nNot detected\n\n\n5\n4\nDE\nDerived\n\n\n\n\n\n\n\nWe simply set 'DL' to 2 if 'DL' is '&lt;' else 1 in original dataset (based on column containing the DL info for each sample type):\n\nsource\n\nRemapDetectionLimitCB\n\n RemapDetectionLimitCB (coi:dict)\n\nBase class for callbacks.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncoi\ndict\nDict of column hosting the detection limit info for each sample type\n\n\n\n\n\nExported source\ncoi_dl = {'SEAWATER' : {'DL' : '&lt; value_bq/m³'},\n          'BIOTA':  {'DL' : '&lt; value_bq/kg'},\n          'SEDIMENT': {'DL' : '_DL'}}\n\n\n\n\nExported source\nclass RemapDetectionLimitCB(Callback):\n    def __init__(self, \n                 coi: dict,  # Dict of column hosting the detection limit info for each sample type\n                ):\n        fc.store_attr()\n        \n    def __call__(self, tfm: Transformer):\n        for grp in tfm.dfs:\n            df = tfm.dfs[grp]\n            dl = self.coi[grp]['DL']\n            df['DL'] = df[dl].apply(lambda x: 2 if x == '&lt;' else 1)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n                            SplitSedimentValuesCB(coi_sediment),\n                            SanitizeValueCB(coi_val),\n                            NormalizeUncCB(),                  \n                            RemapUnitCB(),\n                            RemapDetectionLimitCB(coi_dl),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\n\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n\nfor grp in ['BIOTA', 'SEDIMENT', 'SEAWATER']:\n    print(f'Unique DL values for {grp}: {tfm.dfs[grp][\"DL\"].unique()}')\n\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16094     21481     70451\nRows removed from original (tfm.dfs_removed)      30       153       144\nRows created in transformed (tfm.dfs_created)      0         0     29851 \n\nUnique DL values for BIOTA: [2 1]\nUnique DL values for SEDIMENT: [1 2]\nUnique DL values for SEAWATER: [1 2]",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#remap-biota-species",
    "href": "handlers/helcom.html#remap-biota-species",
    "title": "HELCOM",
    "section": "Remap Biota species",
    "text": "Remap Biota species\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nDiscrepancies have been identified between some rubin codes in the HELCOM Biota dataset and the entries in the RUBIN_NAME lookup table. These discrepancies include typographical errors and trailing spaces, as illustrated below.\n\n\n\nset(dfs['BIOTA']['rubin']) - set(read_csv('RUBIN_NAME.csv')['RUBIN'])\n\n{'CHAR BALT', 'FUCU SPP', 'FUCU VES ', 'FURC LUMB', 'GADU MOR  ', 'STUC PECT'}\n\n\nLets review the data that includes inconsistent entries for the rubin column:\n\nrows_to_show = 5 \ndf = dfs['BIOTA'][dfs['BIOTA']['rubin'].isin(set(dfs['BIOTA']['rubin']) - set(read_csv('RUBIN_NAME.csv')['RUBIN']))]\nprint (f\"Number of inconsistent entries for the `rubin` column: {len(df)}\")\nwith pd.option_context('display.max_columns', None):\n    display(df.head(rows_to_show))\n\nNumber of inconsistent entries for the `rubin` column: 34\n\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/kg\nvalue_bq/kg\nbasis\nerror%\nnumber\ndate_of_entry_x\ncountry\nlaboratory\nsequence\ndate\nyear\nmonth\nday\nstation\nlatitude ddmmmm\nlatitude dddddd\nlongitude ddmmmm\nlongitude dddddd\nsdepth\nrubin\nbiotatype\ntissue\nno\nlength\nweight\ndw%\nloi%\nmors_subbasin\nhelcom_subbasin\ndate_of_entry_y\n\n\n\n\n13585\nBVTIG2012042\nK40\nVTIG01\nNaN\n144.00000\nW\n6.63\nNaN\n04/07/16 00:00:00\n6.0\nVTIG\n2012042\n12/15/12 00:00:00\n2012\n12.0\n15.0\nBARC11\n54.4717\n54.7862\n13.5096\n13.8493\n37.0\nGADU MOR\nF\n5\n14.0\n48.79\n1414.29\n19.2\n92.9\n2.0\n2\n04/07/16 00:00:00\n\n\n13586\nBVTIG2012042\nCS137\nVTIG01\nNaN\n6.17000\nW\n6.03\nNaN\n04/07/16 00:00:00\n6.0\nVTIG\n2012042\n12/15/12 00:00:00\n2012\n12.0\n15.0\nBARC11\n54.4717\n54.7862\n13.5096\n13.8493\n37.0\nGADU MOR\nF\n5\n14.0\n48.79\n1414.29\n19.2\n92.9\n2.0\n2\n04/07/16 00:00:00\n\n\n13587\nBVTIG2012042\nCS134\nVTIG01\n&lt;\n0.02366\nW\nNaN\nNaN\n04/07/16 00:00:00\n6.0\nVTIG\n2012042\n12/15/12 00:00:00\n2012\n12.0\n15.0\nBARC11\n54.4717\n54.7862\n13.5096\n13.8493\n37.0\nGADU MOR\nF\n5\n14.0\n48.79\n1414.29\n19.2\n92.9\n2.0\n2\n04/07/16 00:00:00\n\n\n13594\nBVTIG2012045\nK40\nVTIG01\nNaN\n131.00000\nW\n6.62\nNaN\n04/07/16 00:00:00\n6.0\nVTIG\n2012045\n12/16/12 00:00:00\n2012\n12.0\n16.0\nB12\n54.1385\n54.2308\n11.4691\n11.7818\n21.0\nGADU MOR\nF\n5\n15.0\n38.87\n1128.67\n18.7\n92.7\n5.0\n16\n04/07/16 00:00:00\n\n\n13595\nBVTIG2012045\nCS137\nVTIG01\nNaN\n5.77000\nW\n6.03\nNaN\n04/07/16 00:00:00\n6.0\nVTIG\n2012045\n12/16/12 00:00:00\n2012\n12.0\n16.0\nB12\n54.1385\n54.2308\n11.4691\n11.7818\n21.0\nGADU MOR\nF\n5\n15.0\n38.87\n1128.67\n18.7\n92.7\n5.0\n16\n04/07/16 00:00:00\n\n\n\n\n\n\n\nWe will remap the HELCOM RUBIN column to the MARIS SPECIES column using the IMFA (Inspect, Match, Fix, Apply) pattern. First lets inspect the RUBIN_NAME.csv file provided by HELCOM, which describes the nomenclature of BIOTA species.\n\nread_csv('RUBIN_NAME.csv').head()\n\n\n\n\n\n\n\n\nRUBIN_ID\nRUBIN\nSCIENTIFIC NAME\nENGLISH NAME\n\n\n\n\n0\n11\nABRA BRA\nABRAMIS BRAMA\nBREAM\n\n\n1\n12\nANGU ANG\nANGUILLA ANGUILLA\nEEL\n\n\n2\n13\nARCT ISL\nARCTICA ISLANDICA\nISLAND CYPRINE\n\n\n3\n14\nASTE RUB\nASTERIAS RUBENS\nCOMMON STARFISH\n\n\n4\n15\nCARD EDU\nCARDIUM EDULE\nCOCKLE\n\n\n\n\n\n\n\nNow we try to match the SCIENTIFIC NAME column of HELCOM BIOTA dataset to the species column of the MARIS species lookup table, again using a Remapper object:\n\nremapper = Remapper(provider_lut_df=read_csv('RUBIN_NAME.csv'),\n                    maris_lut_fn=species_lut_path,\n                    maris_col_id='species_id',\n                    maris_col_name='species',\n                    provider_col_to_match='SCIENTIFIC NAME',\n                    provider_col_key='RUBIN',\n                    fname_cache='species_helcom.pkl'\n                    )\n\nremapper.generate_lookup_table(as_df=True)\nremapper.select_match(match_score_threshold=1, verbose=True)\n\nProcessing: 100%|██████████| 46/46 [00:07&lt;00:00,  6.39it/s]\n\n\n38 entries matched the criteria, while 8 entries had a match score of 1 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\nSTIZ LUC\nSander lucioperca\nSTIZOSTEDION LUCIOPERCA\n10\n\n\nLAMI SAC\nLaminaria japonica\nLAMINARIA SACCHARINA\n7\n\n\nCARD EDU\nCardiidae\nCARDIUM EDULE\n6\n\n\nCH HI;BA\nMacoma balthica\nCHARA BALTICA\n6\n\n\nENCH CIM\nEchinodermata\nENCHINODERMATA CIM\n5\n\n\nPSET MAX\nPinctada maxima\nPSETTA MAXIMA\n5\n\n\nMACO BAL\nMacoma balthica\nMACOMA BALTICA\n1\n\n\nSTUC PEC\nStuckenia pectinata\nSTUCKENIA PECTINATE\n1\n\n\n\n\n\n\n\nBelow, we will correct the entries that were not properly matched by the Remapper object:\n\n\nExported source\nfixes_biota_species = {\n    'STIZOSTEDION LUCIOPERCA': 'Sander luciopercas',\n    'LAMINARIA SACCHARINA': 'Saccharina latissima',\n    'CARDIUM EDULE': 'Cerastoderma edule',\n    'CHARA BALTICA': NA,\n    'PSETTA MAXIMA': 'Scophthalmus maximus'\n    }\n\n\nAnd give the remapper another try:\n\nremapper.generate_lookup_table(fixes=fixes_biota_species)\nremapper.select_match(match_score_threshold=1, verbose=True)\n\nProcessing:   0%|          | 0/46 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 46/46 [00:07&lt;00:00,  6.24it/s]\n\n\n42 entries matched the criteria, while 4 entries had a match score of 1 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\nENCH CIM\nEchinodermata\nENCHINODERMATA CIM\n5\n\n\nMACO BAL\nMacoma balthica\nMACOMA BALTICA\n1\n\n\nSTIZ LUC\nSander lucioperca\nSTIZOSTEDION LUCIOPERCA\n1\n\n\nSTUC PEC\nStuckenia pectinata\nSTUCKENIA PECTINATE\n1\n\n\n\n\n\n\n\nVisual inspection of the remaining unperfectly matched entries seem acceptable to proceed.\nWe can now use the generic RemapCB callback to perform the remapping of the RUBIN column to the species column after having defined the lookup table lut_biota.\n\n\nExported source\nlut_biota = lambda: Remapper(provider_lut_df=read_csv('RUBIN_NAME.csv'),\n                             maris_lut_fn=species_lut_path,\n                             maris_col_id='species_id',\n                             maris_col_name='species',\n                             provider_col_to_match='SCIENTIFIC NAME',\n                             provider_col_key='RUBIN',\n                             fname_cache='species_helcom.pkl'\n                             ).generate_lookup_table(fixes=fixes_biota_species, as_df=False, overwrite=False)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='rubin', dest_grps='BIOTA')\n    ])\ntfm()\ntfm.dfs['BIOTA'].columns\ntfm.dfs['BIOTA']['SPECIES'].unique()\n\narray([  99,  243,   50,  139,  270,  192,  191,  284,   84,  269,  122,\n         96,  287,  279,  278,  288,  286,  244,  129,  275,  271,  285,\n        283,  247,  120,   59,  280,  274,  273,  290,  289,  272,  277,\n        276,   21,  282,  110,  281,  245,  704, 1524,  703,    0,  621,\n         60])",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#remap-body-part",
    "href": "handlers/helcom.html#remap-body-part",
    "title": "HELCOM",
    "section": "Remap Body Part",
    "text": "Remap Body Part\nLet’s inspect the TISSUE.csv file provided by HELCOM describing the tissue nomenclature. Biota tissue is known as body part in the MARIS data set.\n\nremapper = Remapper(provider_lut_df=read_csv('TISSUE.csv'),\n                    maris_lut_fn=bodyparts_lut_path,\n                    maris_col_id='bodypar_id',\n                    maris_col_name='bodypar',\n                    provider_col_to_match='TISSUE_DESCRIPTION',\n                    provider_col_key='TISSUE',\n                    fname_cache='tissues_helcom.pkl'\n                    )\n\nremapper.generate_lookup_table(as_df=True)\nremapper.select_match(match_score_threshold=1, verbose=True)\n\nProcessing: 100%|██████████| 29/29 [00:00&lt;00:00, 112.60it/s]\n\n\n21 entries matched the criteria, while 8 entries had a match score of 1 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\n3\nFlesh without bones\nWHOLE FISH WITHOUT HEAD AND ENTRAILS\n20\n\n\n2\nFlesh without bones\nWHOLE FISH WITHOUT ENTRAILS\n13\n\n\n8\nSoft parts\nSKIN/EPIDERMIS\n10\n\n\n5\nFlesh without bones\nFLESH WITHOUT BONES (FILETS)\n9\n\n\n1\nWhole animal\nWHOLE FISH\n5\n\n\n12\nBrain\nENTRAILS\n5\n\n\n15\nStomach and intestine\nSTOMACH + INTESTINE\n3\n\n\n41\nWhole animal\nWHOLE ANIMALS\n1\n\n\n\n\n\n\n\nWe address several entries that were not correctly matched by the Remapper object, as detailed below:\n\n\nExported source\nfixes_biota_tissues = {\n    'WHOLE FISH WITHOUT HEAD AND ENTRAILS': 'Whole animal eviscerated without head',\n    'WHOLE FISH WITHOUT ENTRAILS': 'Whole animal eviscerated',\n    'SKIN/EPIDERMIS': 'Skin',\n    'ENTRAILS': 'Viscera'\n    }\n\n\n\nremapper.generate_lookup_table(as_df=True, fixes=fixes_biota_tissues)\nremapper.select_match(match_score_threshold=1, verbose=True)\n\nProcessing:  31%|███       | 9/29 [00:00&lt;00:00, 87.12it/s]Processing: 100%|██████████| 29/29 [00:00&lt;00:00, 122.32it/s]\n\n\n25 entries matched the criteria, while 4 entries had a match score of 1 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\n5\nFlesh without bones\nFLESH WITHOUT BONES (FILETS)\n9\n\n\n1\nWhole animal\nWHOLE FISH\n5\n\n\n15\nStomach and intestine\nSTOMACH + INTESTINE\n3\n\n\n41\nWhole animal\nWHOLE ANIMALS\n1\n\n\n\n\n\n\n\nVisual inspection of the remaining unperfectly matched entries seem acceptable to proceed.\nWe can now use the generic RemapCB callback to perform the remapping of the TISSUE column to the BODY_PART column after having defined the lookup table lut_tissues.\n\n\nExported source\nlut_tissues = lambda: Remapper(provider_lut_df=read_csv('TISSUE.csv'),\n                               maris_lut_fn=bodyparts_lut_path,\n                               maris_col_id='bodypar_id',\n                               maris_col_name='bodypar',\n                               provider_col_to_match='TISSUE_DESCRIPTION',\n                               provider_col_key='TISSUE',\n                               fname_cache='tissues_helcom.pkl'\n                               ).generate_lookup_table(fixes=fixes_biota_tissues, as_df=False, overwrite=False)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='rubin', dest_grps='BIOTA'),\n    RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='tissue', dest_grps='BIOTA'),\n    ])\n\nprint(tfm()['BIOTA'][['tissue', 'BODY_PART']][:5])\n\n   tissue  BODY_PART\n0       5         52\n1       5         52\n2       5         52\n3       5         52\n4       5         52",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#remap-biological-group",
    "href": "handlers/helcom.html#remap-biological-group",
    "title": "HELCOM",
    "section": "Remap Biological Group",
    "text": "Remap Biological Group\nlut_biogroup_from_biota reads the file at species_lut_path() and from the contents of this file creates a dictionary linking species_id to biogroup_id.\n\n\nExported source\nlut_biogroup_from_biota = lambda: get_lut(src_dir=species_lut_path().parent, fname=species_lut_path().name, \n                               key='species_id', value='biogroup_id')\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='rubin', dest_grps='BIOTA'),\n    RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='tissue', dest_grps='BIOTA'),\n    RemapCB(fn_lut=lut_biogroup_from_biota, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA')\n    ])\n\nprint(tfm()['BIOTA']['BIO_GROUP'].unique())\n\n[ 4  2 14 11  8  3  0]",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#remap-sediment-types",
    "href": "handlers/helcom.html#remap-sediment-types",
    "title": "HELCOM",
    "section": "Remap Sediment Types",
    "text": "Remap Sediment Types\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nThe SEDI values 56 and 73 are not found in the SEDIMENT_TYPE.csv lookup table provided. Note there are many nan values. We reassign them to -99 for now but should be clarified/fixed. This is demonstrated below.\n\n\n\n# Load the sediment type lookup table\ndf_sed_lut = read_csv('SEDIMENT_TYPE.csv')\n\n# Load data with caching enabled\ndfs = load_data(src_dir, use_cache=True)\n\n# Extract unique sediment types from the dataset and lookup table\nsediment_sedi = set(dfs['SEDIMENT']['sedi'].unique())\nlookup_sedi = set(df_sed_lut['SEDI'])\n\n# Identify missing sediment types\nmissing = sediment_sedi - lookup_sedi\n\n# Output results\nprint(f\"Missing sediment type values in HELCOM lookup table: {missing if missing else 'None'}\")\nprint(f\"Number of `56.0` values: {(dfs['SEDIMENT']['sedi']== 56.0).sum()}\")\nprint(f\"Number of `73.0` values: {(dfs['SEDIMENT']['sedi']== 73.0).sum()}\")\nprint(f\"Number of `NA` values: {(dfs['SEDIMENT']['sedi'].isna()).sum()}\")\n\nMissing sediment type values in HELCOM lookup table: {np.float64(56.0), np.float64(73.0), np.float64(nan)}\nNumber of `56.0` values: 12\nNumber of `73.0` values: 3\nNumber of `NA` values: 1239\n\n\nOnce again, we employ the IMFA (Inspect, Match, Fix, Apply) pattern to remap the HELCOM sediment types. Let’s inspect the SEDIMENT_TYPE.csv file provided by HELCOM describing the sediment type nomenclature:\n\nwith pd.option_context('display.max_columns', None):\n    display(read_csv('SEDIMENT_TYPE.csv').T)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n\n\n\n\nSEDI\n-99\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n20\n21\n22\n23\n24\n25\n30\n31\n32\n33\n34\n35\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n54\n55\n57\n58\n59\n\n\nSEDIMENT TYPE\nNO DATA\nGRAVEL\nSAND\nFINE SAND\nSILT\nCLAY\nMUD\nGLACIAL\nSOFT\nSULPHIDIC\nFe-Mg CONCRETIONS\nSAND AND GRAVEL\nPURE SAND\nSAND AND FINE SAND\nSAND AND SILT\nSAND AND CLAY\nSAND AND MUD\nFINE SAND AND GRAVEL\nFINE SAND AND SAND\nPURE FINE SAND\nFINE SAND AND SILT\nFINE SAND AND CLAY\nFINE SAND AND MUD\nSILT AND GRAVEL\nSILT AND SAND\nSILT AND FINE SAND\nPURE SILT\nSILT AND CLAY\nSILT AND MUD\nCLAY AND GRAVEL\nCLAY AND SAND\nCLAY AND FINE SAND\nCLAY AND SILT\nPURE CLAY\nCLAY AND MUD\nCLACIAL CLAY\nSOFT CLAY\nSULPHIDIC CLAY\nCLAY AND Fe-Mg CONCRETIONS\nMUD AND GARVEL\nMUD AND SAND\nMUD AND FINE SAND\nMUD AND CLAY\nPURE MUD\nSOFT MUD\nSULPHIDIC MUD\nMUD AND Fe-Mg CONCRETIONS\n\n\nRECOMMENDED TO BE USED\nNaN\nYES\nYES\nNO\nYES\nYES\nYES\nNO\nNO\nNO (ONLY TO USE AS ADJECTIVE)\nNO (ONLY TO USE AS ADJECTIVE)\nYES\nNO\nNO\nYES\nYES\nYES\nNO\nNO\nNO\nNO\nNO\nNO\nYES\nYES\nNO\nNO\nYES\nYES\nYES\nYES\nNO\nYES\nNO\nYES\nNO\nNO\nYES\nYES\nYES\nYES\nNO\nYES\nNO\nNO\nYES\nYES\n\n\n\n\n\n\n\nLet’s try to match as many as possible of the HELCOM sediment types to the MARIS standard sediment types:\n\nremapper = Remapper(provider_lut_df=read_csv('SEDIMENT_TYPE.csv'),\n                    maris_lut_fn=sediments_lut_path,\n                    maris_col_id='sedtype_id',\n                    maris_col_name='sedtype',\n                    provider_col_to_match='SEDIMENT TYPE',\n                    provider_col_key='SEDI',\n                    fname_cache='sediments_helcom.pkl'\n                    )\n\nremapper.generate_lookup_table(as_df=True)\nremapper.select_match(match_score_threshold=1, verbose=True)\n\nProcessing:   0%|          | 0/47 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 47/47 [00:00&lt;00:00, 70.99it/s] \n\n\n44 entries matched the criteria, while 3 entries had a match score of 1 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\n-99\nSoft\nNO DATA\n5\n\n\n50\nMud and gravel\nMUD AND GARVEL\n2\n\n\n46\nGlacial clay\nCLACIAL CLAY\n1\n\n\n\n\n\n\n\nWe address the remaining unmatched values by adding fixes_sediments:\n\n\nExported source\nfixes_sediments = {\n    'NO DATA': NA\n}\n\n\n\nremapper.generate_lookup_table(as_df=True, fixes=fixes_sediments)\nremapper.select_match(match_score_threshold=1, verbose=True)\n\nProcessing:   0%|          | 0/47 [00:00&lt;?, ?it/s]Processing: 100%|██████████| 47/47 [00:00&lt;00:00, 71.18it/s]\n\n\n44 entries matched the criteria, while 3 entries had a match score of 1 or higher.\n\n\n\n\n\n\n\n\n\n\n\n\nmatched_maris_name\nsource_name\nmatch_score\n\n\nsource_key\n\n\n\n\n\n\n\n-99\n(Not available)\nNO DATA\n2\n\n\n50\nMud and gravel\nMUD AND GARVEL\n2\n\n\n46\nGlacial clay\nCLACIAL CLAY\n1\n\n\n\n\n\n\n\nUpon visual inspection, the remaining values are deemed acceptable for further processing. We will now implement a callback to remap the SEDI values to their corresponding MARIS standard sediment types, designated as SED_TYPE. The HELCOM SEDIMENT dataset contains SEDI values that are absent from the HELCOM lookup table. These values will be reassigned to -99, indicating ‘Not Available’ as per the HELCOM standards.\nReassign the SEDI values of 56, 73, and nan to -99 (Not available):\n\nsource\n\nRemapSedimentCB\n\n RemapSedimentCB (fn_lut:Callable, sed_grp_name:str='SEDIMENT',\n                  sed_col_name:str='sedi', replace_lut:dict=None)\n\nLookup sediment id using lookup table.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfn_lut\nCallable\n\nFunction that returns the lookup table dictionary\n\n\nsed_grp_name\nstr\nSEDIMENT\nThe name of the sediment group\n\n\nsed_col_name\nstr\nsedi\nThe name of the sediment column\n\n\nreplace_lut\ndict\nNone\nDictionary for replacing SEDI values\n\n\n\n\n\nExported source\nsed_replace_lut = {\n    56: -99,\n    73: -99,\n    NA: -99\n}\n\n\n\n\nExported source\nclass RemapSedimentCB(Callback):\n    \"Lookup sediment id using lookup table.\"\n    def __init__(self, \n                 fn_lut: Callable,  # Function that returns the lookup table dictionary\n                 sed_grp_name: str = 'SEDIMENT',  # The name of the sediment group\n                 sed_col_name: str = 'sedi',  # The name of the sediment column\n                 replace_lut: dict = None  # Dictionary for replacing SEDI values\n                 ):\n        fc.store_attr()\n\n    def __call__(self, tfm: Transformer):\n        \"Remap sediment types using lookup table.\"\n        df = tfm.dfs[self.sed_grp_name]\n        self._fix_inconsistent_values(df)\n        self._map_sediment_types(df)\n\n    def _fix_inconsistent_values(self, df: pd.DataFrame) -&gt; None:\n        \"Fix inconsistent values using the replace lookup table.\"\n        if self.replace_lut:\n            df[self.sed_col_name] = df[self.sed_col_name].replace(self.replace_lut)\n            if NA in self.replace_lut:\n                df[self.sed_col_name] = df[self.sed_col_name].fillna(self.replace_lut[NA])\n\n    def _map_sediment_types(self, df: pd.DataFrame) -&gt; None:\n        \"Map sediment types using the lookup table.\"\n        lut = self.fn_lut()\n        df['SED_TYPE'] = df[self.sed_col_name].map(\n            lambda x: lut.get(x, Match(0, None, None, None)).matched_id\n        )\n\n\n\n\nExported source\nlut_sediments = lambda: Remapper(provider_lut_df=read_csv('SEDIMENT_TYPE.csv'),\n                                 maris_lut_fn=sediments_lut_path,\n                                 maris_col_id='sedtype_id',\n                                 maris_col_name='sedtype',\n                                 provider_col_to_match='SEDIMENT TYPE',\n                                 provider_col_key='SEDI',\n                                 fname_cache='sediments_helcom.pkl'\n                                 ).generate_lookup_table(fixes=fixes_sediments, as_df=False, overwrite=False)\n\n\nUtilize the RemapSedimentCB callback to remap the SEDI values in the HELCOM dataset to the corresponding MARIS standard sediment type, referred to as SED_TYPE.\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n    RemapSedimentCB(fn_lut=lut_sediments, replace_lut=sed_replace_lut)\n    ])\n\ntfm()\ntfm.dfs['SEDIMENT']['SED_TYPE'].unique()\n\narray([ 0,  2, 58, 30, 59, 55, 56, 36, 29, 47,  4, 54, 33,  6, 44, 42, 48,\n       61, 57, 28, 49, 32, 45, 39, 46, 38, 31, 60, 62, 26, 53, 52,  1, 51,\n       37, 34, 50,  7, 10, 41, 43, 35])",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#remap-filtering-status",
    "href": "handlers/helcom.html#remap-filtering-status",
    "title": "HELCOM",
    "section": "Remap Filtering Status",
    "text": "Remap Filtering Status\nHELCOM filtered status is encoded as follows in the FILT column:\n\ndfs = load_data(src_dir, use_cache=True)\nget_unique_across_dfs(dfs, col_name='filt', as_df=True).head(5)\n\n\n\n\n\n\n\n\nindex\nvalue\n\n\n\n\n0\n0\nN\n\n\n1\n1\nNaN\n\n\n2\n2\nn\n\n\n3\n3\nF\n\n\n\n\n\n\n\nMARIS uses a different encoding for filtered status:\n\npd.read_excel(filtered_lut_path())\n\n\n\n\n\n\n\n\nid\nname\n\n\n\n\n0\n-1\nNot applicable\n\n\n1\n0\nNot available\n\n\n2\n1\nYes\n\n\n3\n2\nNo\n\n\n\n\n\n\n\nFor only four categories to remap, the Remapper is an overkill. We can use a simple dictionary to map the values:\n\n\nExported source\nlut_filtered = {\n    'N': 2, # No\n    'n': 2, # No\n    'F': 1 # Yes\n}\n\n\nRemapFiltCB converts the HELCOM filt data to the MARIS FILT format.\n\nsource\n\nRemapFiltCB\n\n RemapFiltCB (lut_filtered:dict={'N': 2, 'n': 2, 'F': 1})\n\nLookup filt value in dataframe using the lookup table.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlut_filtered\ndict\n{‘N’: 2, ‘n’: 2, ‘F’: 1}\nDictionary mapping filt codes to their corresponding names\n\n\n\n\n\nExported source\nclass RemapFiltCB(Callback):\n    \"Lookup filt value in dataframe using the lookup table.\"\n    def __init__(self,\n                 lut_filtered: dict=lut_filtered, # Dictionary mapping filt codes to their corresponding names\n                ):\n        fc.store_attr()\n\n    def __call__(self, tfm):\n        for df in tfm.dfs.values():\n            if 'filt' in df.columns:\n                df['FILT'] = df['filt'].map(lambda x: self.lut_filtered.get(x, 0))\n\n\nFor instance:\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[RemapFiltCB(lut_filtered)])\n\nprint(tfm()['SEAWATER']['FILT'].unique())\n\n[0 2 1]",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#add-sample-id",
    "href": "handlers/helcom.html#add-sample-id",
    "title": "HELCOM",
    "section": "Add Sample ID",
    "text": "Add Sample ID\n\nSMP_ID is a internal unique identifier for each sample\nSMP_ID_PROVIDER is data provided by the data provider\n\n\nsource\n\nAddSampleIDCB\n\n AddSampleIDCB ()\n\nGenerate a SMP_ID from the KEY values in the HELCOM dataset.\n\n\nExported source\nclass AddSampleIDCB(Callback):\n    \"Generate a SMP_ID from the KEY values in the HELCOM dataset.\"\n    def __call__(self, tfm: Transformer):\n        for _, df in tfm.dfs.items():\n            df['SMP_ID'] = range(1, len(df) + 1)\n            df['SMP_ID_PROVIDER'] = df['key'].astype(str)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n                        AddSampleIDCB(),\n                        CompareDfsAndTfmCB(dfs)\n                        ])\ntfm()\n\nprint(f'Number of unique sample ids in SEAWATER: {tfm.dfs[\"SEAWATER\"][\"SMP_ID\"].unique().size}')\nprint(f'Number of unique sample ids in BIOTA: {tfm.dfs[\"BIOTA\"][\"SMP_ID\"].unique().size}')\nprint(f'Number of unique sample ids in SEDIMENT: {tfm.dfs[\"SEDIMENT\"][\"SMP_ID\"].unique().size}')\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n\nNumber of unique sample ids in SEAWATER: 21634\nNumber of unique sample ids in BIOTA: 16124\nNumber of unique sample ids in SEDIMENT: 40744\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16124     21634     40744\nRows removed from original (tfm.dfs_removed)       0         0         0\nRows created in transformed (tfm.dfs_created)      0         0         0 \n\n\n\n\nwith pd.option_context('display.max_columns', None):\n    display(tfm.dfs['SEAWATER'].head())\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/m³\nvalue_bq/m³\nerror%_m³\ndate_of_entry_x\ncountry\nlaboratory\nsequence\ndate\nyear\nmonth\nday\nstation\nlatitude (ddmmmm)\nlatitude (dddddd)\nlongitude (ddmmmm)\nlongitude (dddddd)\ntdepth\nsdepth\nsalin\nttemp\nfilt\nmors_subbasin\nhelcom_subbasin\ndate_of_entry_y\nSMP_ID\nSMP_ID_PROVIDER\n\n\n\n\n0\nWKRIL2012003\nCS137\nNaN\nNaN\n5.3\n32.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012003.0\n05/23/12 00:00:00\n2012.0\n5.0\n23.0\nRU10\n60.05\n60.0833\n29.20\n29.3333\nNaN\n0.0\nNaN\nNaN\nNaN\n11.0\n11.0\n08/20/14 00:00:00\n1\nWKRIL2012003\n\n\n1\nWKRIL2012004\nCS137\nNaN\nNaN\n19.9\n20.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012004.0\n05/23/12 00:00:00\n2012.0\n5.0\n23.0\nRU10\n60.05\n60.0833\n29.20\n29.3333\nNaN\n29.0\nNaN\nNaN\nNaN\n11.0\n11.0\n08/20/14 00:00:00\n2\nWKRIL2012004\n\n\n2\nWKRIL2012005\nCS137\nNaN\nNaN\n25.5\n20.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012005.0\n06/17/12 00:00:00\n2012.0\n6.0\n17.0\nRU11\n59.26\n59.4333\n23.09\n23.1500\nNaN\n0.0\nNaN\nNaN\nNaN\n11.0\n3.0\n08/20/14 00:00:00\n3\nWKRIL2012005\n\n\n3\nWKRIL2012006\nCS137\nNaN\nNaN\n17.0\n29.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012006.0\n05/24/12 00:00:00\n2012.0\n5.0\n24.0\nRU19\n60.15\n60.2500\n27.59\n27.9833\nNaN\n0.0\nNaN\nNaN\nNaN\n11.0\n11.0\n08/20/14 00:00:00\n4\nWKRIL2012006\n\n\n4\nWKRIL2012007\nCS137\nNaN\nNaN\n22.2\n18.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012007.0\n05/24/12 00:00:00\n2012.0\n5.0\n24.0\nRU19\n60.15\n60.2500\n27.59\n27.9833\nNaN\n39.0\nNaN\nNaN\nNaN\n11.0\n11.0\n08/20/14 00:00:00\n5\nWKRIL2012007",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#add-depths",
    "href": "handlers/helcom.html#add-depths",
    "title": "HELCOM",
    "section": "Add depths",
    "text": "Add depths\nThe HELCOM dataset includes a column for the sampling depth (SDEPTH) for the SEAWATER and BIOTA datasets. Additionally, it contains a column for the total depth (TDEPTH) applicable to both the SEDIMENT and SEAWATER datasets. In this section, we will create a callback to incorporate both the sampling depth (smp_depth) and total depth (tot_depth) into the MARIS dataset.\n\nsource\n\nAddDepthCB\n\n AddDepthCB ()\n\nEnsure depth values are floats and add ‘SMP_DEPTH’ and ‘TOT_DEPTH’ columns.\n\n\nExported source\nclass AddDepthCB(Callback):\n    \"Ensure depth values are floats and add 'SMP_DEPTH' and 'TOT_DEPTH' columns.\"\n    def __call__(self, tfm: Transformer):\n        for df in tfm.dfs.values():\n            if 'sdepth' in df.columns:\n                df['SMP_DEPTH'] = df['sdepth'].astype(float)\n            if 'tdepth' in df.columns:\n                df['TOT_DEPTH'] = df['tdepth'].astype(float)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[AddDepthCB()])\ntfm()\nfor grp in tfm.dfs.keys():  \n    if 'SMP_DEPTH' in tfm.dfs[grp].columns and 'TOT_DEPTH' in tfm.dfs[grp].columns:\n        print(f'{grp}:', tfm.dfs[grp][['SMP_DEPTH','TOT_DEPTH']].drop_duplicates())\n    elif 'SMP_DEPTH' in tfm.dfs[grp].columns:\n        print(f'{grp}:', tfm.dfs[grp][['SMP_DEPTH']].drop_duplicates())\n    elif 'TOT_DEPTH' in tfm.dfs[grp].columns:\n        print(f'{grp}:', tfm.dfs[grp][['TOT_DEPTH']].drop_duplicates())\n\nBIOTA:        SMP_DEPTH\n0            NaN\n78         22.00\n88         39.00\n96         40.00\n183        65.00\n...          ...\n15874      43.10\n15921      30.43\n15984       7.60\n15985       5.50\n15988      11.20\n\n[301 rows x 1 columns]\nSEAWATER:        SMP_DEPTH  TOT_DEPTH\n0            0.0        NaN\n1           29.0        NaN\n4           39.0        NaN\n6           62.0        NaN\n10          71.0        NaN\n...          ...        ...\n21059       15.0       15.0\n21217        7.0       16.0\n21235       19.2       21.0\n21312        1.0        5.5\n21521        0.5        NaN\n\n[1686 rows x 2 columns]\nSEDIMENT:        TOT_DEPTH\n0           25.0\n6           61.0\n19          31.0\n33          39.0\n42          36.0\n...          ...\n35882        3.9\n36086      103.0\n36449      108.9\n36498        4.5\n36899      125.0\n\n[195 rows x 1 columns]",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#add-salinity",
    "href": "handlers/helcom.html#add-salinity",
    "title": "HELCOM",
    "section": "Add Salinity",
    "text": "Add Salinity\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nThe HELCOM dataset includes a column for the salinity of the water (SALIN). According to the HELCOM documentation, the SALIN column represents “Salinity of water in PSU units”.\nIn the SEAWATER dataset, three entries have salinity values greater than 50 PSU. While salinity values greater than 50 PSU are possible, these entries may require further verification. Notably, these three entries have a salinity value of 99.99 PSU, which suggests potential data entry errors.\n\n\n\ntfm.dfs['SEAWATER'][tfm.dfs['SEAWATER']['salin'] &gt; 50]\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/m³\nvalue_bq/m³\nerror%_m³\ndate_of_entry_x\ncountry\nlaboratory\nsequence\n...\ntdepth\nsdepth\nsalin\nttemp\nfilt\nmors_subbasin\nhelcom_subbasin\ndate_of_entry_y\nSMP_DEPTH\nTOT_DEPTH\n\n\n\n\n12288\nWDHIG1998072\nCS137\n3\nNaN\n40.1\n1.6\nNaN\n6.0\nDHIG\n1998072.0\n...\n25.0\n0.0\n99.99\n5.0\nF\n5.0\n15.0\nNaN\n0.0\n25.0\n\n\n12289\nWDHIG1998072\nCS134\n3\nNaN\n1.1\n23.6\nNaN\n6.0\nDHIG\n1998072.0\n...\n25.0\n0.0\n99.99\n5.0\nF\n5.0\n15.0\nNaN\n0.0\n25.0\n\n\n12290\nWDHIG1998072\nSR90\n2\nNaN\n8.5\n1.9\nNaN\n6.0\nDHIG\n1998072.0\n...\n25.0\n0.0\n99.99\n5.0\nF\n5.0\n15.0\nNaN\n0.0\n25.0\n\n\n\n\n3 rows × 29 columns\n\n\n\nLets add the salinity values to the SEAWATER DataFrame.\n\nsource\n\nAddSalinityCB\n\n AddSalinityCB (salinity_col:str='salin')\n\nBase class for callbacks.\n\n\nExported source\nclass AddSalinityCB(Callback):\n    def __init__(self, salinity_col: str = 'salin'):\n        self.salinity_col = salinity_col\n    \"Add salinity to the SEAWATER DataFrame.\"\n    def __call__(self, tfm: Transformer):\n        for df in tfm.dfs.values():\n            if self.salinity_col in df.columns:\n                df['SALINITY'] = df[self.salinity_col].astype(float)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[AddSalinityCB()])\ntfm()\nfor grp in tfm.dfs.keys():  \n    if 'SALINITY' in tfm.dfs[grp].columns:\n        print(f'{grp}:', tfm.dfs[grp][['SALINITY']].drop_duplicates())\n\nSEAWATER:        SALINITY\n0           NaN\n97        7.570\n98        7.210\n101       7.280\n104       7.470\n...         ...\n21449    11.244\n21450     7.426\n21451     9.895\n21452     2.805\n21453     7.341\n\n[2766 rows x 1 columns]",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#add-station",
    "href": "handlers/helcom.html#add-station",
    "title": "HELCOM",
    "section": "Add Station",
    "text": "Add Station\n\nsource\n\nAddStationCB\n\n AddStationCB ()\n\nAdd station to all DataFrames.\n\n\nExported source\nclass AddStationCB(Callback):\n    \"Add station to all DataFrames.\"\n    def __call__(self, tfm: Transformer):\n        for df in tfm.dfs.values():\n            df['STATION'] = df['station'].astype(str)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[AddStationCB()])\ntfm()['SEAWATER'].head()\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/m³\nvalue_bq/m³\nerror%_m³\ndate_of_entry_x\ncountry\nlaboratory\nsequence\n...\nlongitude (dddddd)\ntdepth\nsdepth\nsalin\nttemp\nfilt\nmors_subbasin\nhelcom_subbasin\ndate_of_entry_y\nSTATION\n\n\n\n\n0\nWKRIL2012003\nCS137\nNaN\nNaN\n5.3\n32.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012003.0\n...\n29.3333\nNaN\n0.0\nNaN\nNaN\nNaN\n11.0\n11.0\n08/20/14 00:00:00\nRU10\n\n\n1\nWKRIL2012004\nCS137\nNaN\nNaN\n19.9\n20.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012004.0\n...\n29.3333\nNaN\n29.0\nNaN\nNaN\nNaN\n11.0\n11.0\n08/20/14 00:00:00\nRU10\n\n\n2\nWKRIL2012005\nCS137\nNaN\nNaN\n25.5\n20.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012005.0\n...\n23.1500\nNaN\n0.0\nNaN\nNaN\nNaN\n11.0\n3.0\n08/20/14 00:00:00\nRU11\n\n\n3\nWKRIL2012006\nCS137\nNaN\nNaN\n17.0\n29.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012006.0\n...\n27.9833\nNaN\n0.0\nNaN\nNaN\nNaN\n11.0\n11.0\n08/20/14 00:00:00\nRU19\n\n\n4\nWKRIL2012007\nCS137\nNaN\nNaN\n22.2\n18.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012007.0\n...\n27.9833\nNaN\n39.0\nNaN\nNaN\nNaN\n11.0\n11.0\n08/20/14 00:00:00\nRU19\n\n\n\n\n5 rows × 28 columns",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#add-temperature",
    "href": "handlers/helcom.html#add-temperature",
    "title": "HELCOM",
    "section": "Add Temperature",
    "text": "Add Temperature\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nThe HELCOM dataset includes a column for the temperature of the water (TTEMP). According to the HELCOM documentation, the TTEMP column represents: &gt; ‘Water temperature in Celsius (ºC) degrees of sampled water’\nIn the SEAWATER dataset, several entries have temperature values greater than 50ºC. These entries may require further verification. Notably, these entries have a temperature value of 99.99ºC, which suggests potential data entry errors, see below.\n\n\n\nt_df= tfm.dfs['SEAWATER'][tfm.dfs['SEAWATER']['ttemp'] &gt; 50]\nprint('Number of entries with temperature greater than 50ºC: ', t_df.shape[0])\nt_df.head()\n\nNumber of entries with temperature greater than 50ºC:  92\n\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/m³\nvalue_bq/m³\nerror%_m³\ndate_of_entry_x\ncountry\nlaboratory\nsequence\n...\nlongitude (dddddd)\ntdepth\nsdepth\nsalin\nttemp\nfilt\nmors_subbasin\nhelcom_subbasin\ndate_of_entry_y\nSTATION\n\n\n\n\n5954\nWDHIG1995559\nCS134\n4\nNaN\n1.7\n15.0\nNaN\n6.0\nDHIG\n1995559.0\n...\n10.2033\n13.0\n11.0\n14.81\n99.9\nN\n5.0\n15.0\nNaN\nKFOTN6\n\n\n5955\nWDHIG1995559\nCS137\n4\nNaN\n58.7\n2.0\nNaN\n6.0\nDHIG\n1995559.0\n...\n10.2033\n13.0\n11.0\n14.81\n99.9\nN\n5.0\n15.0\nNaN\nKFOTN6\n\n\n5960\nWDHIG1995569\nCS134\n4\nNaN\n1.4\n12.0\nNaN\n6.0\nDHIG\n1995569.0\n...\n10.2777\n14.0\n12.0\n14.80\n99.9\nN\n5.0\n15.0\nNaN\nLTKIEL\n\n\n5961\nWDHIG1995569\nCS137\n4\nNaN\n62.8\n1.0\nNaN\n6.0\nDHIG\n1995569.0\n...\n10.2777\n14.0\n12.0\n14.80\n99.9\nN\n5.0\n15.0\nNaN\nLTKIEL\n\n\n5964\nWDHIG1995571\nCS134\n4\nNaN\n1.5\n17.0\nNaN\n6.0\nDHIG\n1995571.0\n...\n10.2000\n19.0\n17.0\n14.59\n99.9\nN\n5.0\n15.0\nNaN\nSTOLGR\n\n\n\n\n5 rows × 28 columns\n\n\n\nLets add the temperature values to the SEAWATER DataFrame.\n\nsource\n\nAddTemperatureCB\n\n AddTemperatureCB (temperature_col:str='ttemp')\n\nBase class for callbacks.\n\n\nExported source\nclass AddTemperatureCB(Callback):\n    def __init__(self, temperature_col: str = 'ttemp'):\n        self.temperature_col = temperature_col\n    \"Add temperature to the SEAWATER DataFrame.\"\n    def __call__(self, tfm: Transformer ):\n        for df in tfm.dfs.values():\n            if self.temperature_col in df.columns:\n                df['TEMPERATURE'] = df[self.temperature_col].astype(float)\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[AddTemperatureCB()])\ntfm()\nfor grp in tfm.dfs.keys():  \n    if 'TEMPERATURE' in tfm.dfs[grp].columns:\n        print(f'{grp}:', tfm.dfs[grp][['TEMPERATURE']].drop_duplicates())\n\nSEAWATER:        TEMPERATURE\n0              NaN\n987           7.80\n990           6.50\n993           4.10\n996           4.80\n...            ...\n21521         0.57\n21523        18.27\n21525        21.54\n21529         4.94\n21537         2.35\n\n[1086 rows x 1 columns]",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#add-slice-position-top-and-bottom",
    "href": "handlers/helcom.html#add-slice-position-top-and-bottom",
    "title": "HELCOM",
    "section": "Add slice position (TOP and BOTTOM)",
    "text": "Add slice position (TOP and BOTTOM)\n\nsource\n\nRemapSedSliceTopBottomCB\n\n RemapSedSliceTopBottomCB ()\n\nRemap Sediment slice top and bottom to MARIS format.\n\n\nExported source\nclass RemapSedSliceTopBottomCB(Callback):\n    \"Remap Sediment slice top and bottom to MARIS format.\"\n    def __call__(self, tfm: Transformer):\n        \"Iterate through all DataFrames in the transformer object and remap sediment slice top and bottom.\"\n        tfm.dfs['SEDIMENT']['TOP'] = tfm.dfs['SEDIMENT']['uppsli']\n        tfm.dfs['SEDIMENT']['BOTTOM'] = tfm.dfs['SEDIMENT']['lowsli']\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[RemapSedSliceTopBottomCB()])\ntfm()\nprint(tfm.dfs['SEDIMENT'][['TOP','BOTTOM']].head())\n\n    TOP  BOTTOM\n0  15.0    20.0\n1  20.0    25.0\n2  25.0    30.0\n3  30.0    35.0\n4  35.0    40.0",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#add-dry-weight-wet-weight-and-percentage-weight",
    "href": "handlers/helcom.html#add-dry-weight-wet-weight-and-percentage-weight",
    "title": "HELCOM",
    "section": "Add dry weight, wet weight and percentage weight",
    "text": "Add dry weight, wet weight and percentage weight\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nEntries for the BASIS value of the BIOTA dataset report a value of F which is not consistent with the HELCOM description provided in the metadata. The GUIDELINES FOR MONITORING OF RADIOACTIVE SUBSTANCES was obtained from here.\n\n\nLets take a look at the BIOTA BASIS values:\n\ndfs['BIOTA']['basis'].unique()\n\narray(['W', nan, 'D', 'F'], dtype=object)\n\n\nNumber of entries for each BASIS value:\n\ndfs['BIOTA']['basis'].value_counts()\n\nbasis\nW    12164\nD     3868\nF       25\nName: count, dtype: int64\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nSome entries for DW% (Dry weight as percentage (%) of fresh weight) are much higher than 100%. Additionally, DW% is repoted as 0% in some cases.\n\n\nFor BIOTA, the number of entries for DW% higher than 100%:\n\ndfs['BIOTA']['dw%'][dfs['BIOTA']['dw%'] &gt; 100].count()\n\nnp.int64(20)\n\n\nFor BIOTA, the number of entries for DW% equal to 0%:\n\ndfs['BIOTA']['dw%'][dfs['BIOTA']['dw%'] == 0].count()\n\nnp.int64(6)\n\n\nFor SEDIMENT, the number of entries for DW% higher than 100%:\n\ndfs['SEDIMENT']['dw%'][dfs['SEDIMENT']['dw%'] &gt; 100].count()\n\nnp.int64(625)\n\n\nFor SEDIMENT, the number of entries for DW% equal to 0%:\n\ndfs['SEDIMENT']['dw%'][dfs['SEDIMENT']['dw%'] == 0].count()\n\nnp.int64(302)\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nSeveral SEDIMENT entries have DW% (Dry weight as percentage of fresh weight) values less than 1%. While technically possible, this would indicate samples contained more than 99% water content.\n\n\nFor SEDIMENT, the number of entries for DW% less than 1% but greater than 0.001%:\n\npercent=1\ndfs['SEDIMENT']['dw%'][(dfs['SEDIMENT']['dw%'] &lt; percent) & (dfs['SEDIMENT']['dw%'] &gt; 0.001)].count()\n\nnp.int64(24)\n\n\nLets take a look at the MARIS description of the percentwt, drywt and wetwt variables:\n\npercentwt: Dry weight as ratio of fresh weight, expressed as a decimal .\ndrywt: Dry weight in grams.\nwetwt: Fresh weight in grams.\n\nLets take a look at the HELCOM dataset, the weight of the sample is not reported for SEDIMENT. However, the percentage dry weight is reported as DW%.\n\ndfs['SEDIMENT'].columns\n\nIndex(['key', 'nuclide', 'method', '&lt; value_bq/kg', 'value_bq/kg', 'error%_kg',\n       '&lt; value_bq/m²', 'value_bq/m²', 'error%_m²', 'date_of_entry_x',\n       'country', 'laboratory', 'sequence', 'date', 'year', 'month', 'day',\n       'station', 'latitude (ddmmmm)', 'latitude (dddddd)',\n       'longitude (ddmmmm)', 'longitude (dddddd)', 'device', 'tdepth',\n       'uppsli', 'lowsli', 'area', 'sedi', 'oxic', 'dw%', 'loi%',\n       'mors_subbasin', 'helcom_subbasin', 'sum_link', 'date_of_entry_y'],\n      dtype='object')\n\n\nThe BIOTA dataset reports the weight of the sample as WEIGHT and the percentage dry weight as DW%. The BASIS column describes the basis of the value reported. Lets create a callback to include the PERCENTWT, DRYWT and WETWT columns in the MARIS dataset.\n\nsource\n\nLookupDryWetPercentWeightCB\n\n LookupDryWetPercentWeightCB ()\n\nLookup dry-wet ratio and format for MARIS.\n\n\nExported source\nclass LookupDryWetPercentWeightCB(Callback):\n    \"Lookup dry-wet ratio and format for MARIS.\"\n    def __call__(self, tfm: Transformer):\n        \"Iterate through all DataFrames in the transformer object and apply the dry-wet ratio lookup.\"\n        for grp in tfm.dfs.keys():\n            if 'dw%' in tfm.dfs[grp].columns:\n                self._apply_dry_wet_ratio(tfm.dfs[grp])\n            if 'weight' in tfm.dfs[grp].columns and 'basis' in tfm.dfs[grp].columns:\n                self._correct_basis(tfm.dfs[grp])\n                self._apply_weight(tfm.dfs[grp])\n\n    def _apply_dry_wet_ratio(self, df: pd.DataFrame) -&gt; None:\n        \"Apply dry-wet ratio conversion and formatting to the given DataFrame.\"\n        df['PERCENTWT'] = df['dw%'] / 100  # Convert percentage to fraction\n        df.loc[df['PERCENTWT'] == 0, 'PERCENTWT'] = np.nan  # Convert 0% to nan\n\n    def _correct_basis(self, df: pd.DataFrame) -&gt; None:\n        \"Correct BASIS values. Assuming F = Fresh weight, so F = W\"\n        df.loc[df['basis'] == 'F', 'basis'] = 'W'\n\n    def _apply_weight(self, df: pd.DataFrame) -&gt; None:\n        \"Apply weight conversion and formatting to the given DataFrame.\"\n        dry_condition = df['basis'] == 'D'\n        wet_condition = df['basis'] == 'W'\n        \n        df.loc[dry_condition, 'DRYWT'] = df['weight']\n        df.loc[dry_condition & df['PERCENTWT'].notna(), 'WETWT'] = df['weight'] / df['PERCENTWT']\n        \n        df.loc[wet_condition, 'WETWT'] = df['weight']\n        df.loc[wet_condition & df['PERCENTWT'].notna(), 'DRYWT'] = df['weight'] * df['PERCENTWT']\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n                            LookupDryWetPercentWeightCB(),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\n\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\nprint('BIOTA:', tfm.dfs['BIOTA'][['PERCENTWT','DRYWT','WETWT']].head(), '\\n')\nprint('SEDIMENT:', tfm.dfs['SEDIMENT']['PERCENTWT'].unique())\n\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16124     21634     40744\nRows removed from original (tfm.dfs_removed)       0         0         0\nRows created in transformed (tfm.dfs_created)      0         0         0 \n\nBIOTA:    PERCENTWT      DRYWT  WETWT\n0    0.18453  174.93444  948.0\n1    0.18453  174.93444  948.0\n2    0.18453  174.93444  948.0\n3    0.18453  174.93444  948.0\n4    0.18458  177.93512  964.0 \n\nSEDIMENT: [       nan 0.1        0.13       ... 0.24418605 0.25764192 0.26396495]\n\n\nNote that the dry weight is greater than the wet weight for some entries in the BIOTA dataset due to the DW% being greater than 100%, see above. Lets take a look at the number of entries where this is the case:\n\ntfm.dfs['BIOTA'][['DRYWT','WETWT']][tfm.dfs['BIOTA']['DRYWT'] &gt; tfm.dfs['BIOTA']['WETWT']].count()\n\nDRYWT    20\nWETWT    20\ndtype: int64",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#standardize-coordinates",
    "href": "handlers/helcom.html#standardize-coordinates",
    "title": "HELCOM",
    "section": "Standardize Coordinates",
    "text": "Standardize Coordinates\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nColumn names for geographical coordinates are inconsistent across sample types (biota, sediment, seawater). Sometimes using parentheses, sometimes not.\n\n\n\ndfs = load_data(src_dir, use_cache=True)\nfor grp in dfs.keys():\n    print(f'{grp}: {[col for col in dfs[grp].columns if \"lon\" in col or \"lat\" in col]}')\n\nBIOTA: ['latitude ddmmmm', 'latitude dddddd', 'longitude ddmmmm', 'longitude dddddd']\nSEAWATER: ['latitude (ddmmmm)', 'latitude (dddddd)', 'longitude (ddmmmm)', 'longitude (dddddd)']\nSEDIMENT: ['latitude (ddmmmm)', 'latitude (dddddd)', 'longitude (ddmmmm)', 'longitude (dddddd)']\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nHELCOM SEAWATER data includes values of 0 or nan for both latitude and longitude.\n\n\nLets create a callback to parse the coordinates of the HELCOM dataset.\n\nsource\n\nParseCoordinates\n\n ParseCoordinates (fn_convert_cor:Callable)\n\nGet geographical coordinates from columns expressed in degrees decimal format or from columns in degrees/minutes decimal format where degrees decimal format is missing or zero.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfn_convert_cor\nCallable\nFunction that converts coordinates from degree-minute to decimal degree format\n\n\n\n\n\nExported source\nclass ParseCoordinates(Callback):\n    \"Get geographical coordinates from columns expressed in degrees decimal format or from columns in degrees/minutes decimal format where degrees decimal format is missing or zero.\"\n    def __init__(self, \n                 fn_convert_cor: Callable # Function that converts coordinates from degree-minute to decimal degree format\n                 ):\n        self.fn_convert_cor = fn_convert_cor\n\n    def __call__(self, tfm:Transformer):\n        for df in tfm.dfs.values():\n            self._format_coordinates(df)\n\n    def _format_coordinates(self, df:pd.DataFrame) -&gt; None:\n        coord_cols = self._get_coord_columns(df.columns)\n        \n        \n        for coord in ['lat', 'lon']:\n            decimal_col, minute_col = coord_cols[f'{coord}_d'], coord_cols[f'{coord}_m']\n            # Attempt to convert columns to numeric, coercing errors to NaN.\n            df[decimal_col] = pd.to_numeric(df[decimal_col], errors='coerce')\n            df[minute_col] = pd.to_numeric(df[minute_col], errors='coerce')\n            condition = df[decimal_col].isna() | (df[decimal_col] == 0)\n            df[coord.upper()] = np.where(condition,\n                                 df[minute_col].apply(self._safe_convert),\n                                 df[decimal_col])\n        \n        df.dropna(subset=['LAT', 'LON'], inplace=True)\n\n    def _get_coord_columns(self, columns) -&gt; dict:\n        return {\n            'lon_d': self._find_coord_column(columns, 'lon', 'dddddd'),\n            'lat_d': self._find_coord_column(columns, 'lat', 'dddddd'),\n            'lon_m': self._find_coord_column(columns, 'lon', 'ddmmmm'),\n            'lat_m': self._find_coord_column(columns, 'lat', 'ddmmmm')\n        }\n\n    def _find_coord_column(self, columns, coord_type, coord_format) -&gt; str:\n        pattern = re.compile(f'{coord_type}.*{coord_format}', re.IGNORECASE)\n        matching_columns = [col for col in columns if pattern.search(col)]\n        return matching_columns[0] if matching_columns else None\n\n    def _safe_convert(self, value) -&gt; str:\n        if pd.isna(value):\n            return value\n        try:\n            return self.fn_convert_cor(value)\n        except Exception as e:\n            print(f\"Error converting value {value}: {e}\")\n            return value\n\n\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[                    \n                            ParseCoordinates(ddmm_to_dd),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\nprint(tfm.dfs['BIOTA'][['LAT','LON']])\n\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16124     21626     40743\nRows removed from original (tfm.dfs_removed)       0         8         1\nRows created in transformed (tfm.dfs_created)      0         0         0 \n\n             LAT        LON\n0      54.283333  12.316667\n1      54.283333  12.316667\n2      54.283333  12.316667\n3      54.283333  12.316667\n4      54.283333  12.316667\n...          ...        ...\n16119  61.241500  21.395000\n16120  61.241500  21.395000\n16121  61.343333  21.385000\n16122  61.343333  21.385000\n16123  61.343333  21.385000\n\n[16124 rows x 2 columns]\n\n\nLets review the rows removed from SEAWATER dataset during the parsing of coordinates:\n\nwith pd.option_context('display.max_columns', None, 'display.max_colwidth', None):\n    display(tfm.dfs_removed['SEAWATER'])\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/m³\nvalue_bq/m³\nerror%_m³\ndate_of_entry_x\ncountry\nlaboratory\nsequence\ndate\nyear\nmonth\nday\nstation\nlatitude (ddmmmm)\nlatitude (dddddd)\nlongitude (ddmmmm)\nlongitude (dddddd)\ntdepth\nsdepth\nsalin\nttemp\nfilt\nmors_subbasin\nhelcom_subbasin\ndate_of_entry_y\n\n\n\n\n20556\nWSSSM2015009\nH3\nSTYR201\n&lt;\n2450.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n20557\nWSSSM2015010\nH3\nSTYR201\nNaN\n2510.0\n29.17\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n20558\nWSSSM2015011\nH3\nSTYR201\n&lt;\n2450.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n20559\nWSSSM2015012\nH3\nSTYR201\nNaN\n1740.0\n41.26\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n20560\nWSSSM2015013\nH3\nSTYR201\nNaN\n1650.0\n43.53\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n20561\nWSSSM2015014\nH3\nSTYR201\n&lt;\n2277.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n20562\nWSSSM2015015\nH3\nSTYR201\n&lt;\n2277.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n20563\nWSSSM2015016\nH3\nSTYR201\n&lt;\n2277.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nSanitize coordinates by dropping rows where both longitude and latitude are zero or contain unrealistic values. Convert the , separator in longitude and latitude to a . separator\n\ndfs = load_data(src_dir,  use_cache=True)\ntfm = Transformer(dfs, cbs=[\n                            ParseCoordinates(ddmm_to_dd),\n                            SanitizeLonLatCB(),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\n\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\nprint(tfm.dfs['BIOTA'][['LAT','LON']])\n\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16124     21626     40743\nRows removed from original (tfm.dfs_removed)       0         8         1\nRows created in transformed (tfm.dfs_created)      0         0         0 \n\n             LAT        LON\n0      54.283333  12.316667\n1      54.283333  12.316667\n2      54.283333  12.316667\n3      54.283333  12.316667\n4      54.283333  12.316667\n...          ...        ...\n16119  61.241500  21.395000\n16120  61.241500  21.395000\n16121  61.343333  21.385000\n16122  61.343333  21.385000\n16123  61.343333  21.385000\n\n[16124 rows x 2 columns]",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#review-all-callbacks",
    "href": "handlers/helcom.html#review-all-callbacks",
    "title": "HELCOM",
    "section": "Review all callbacks",
    "text": "Review all callbacks\n\ndfs = load_data(src_dir, use_cache=True)\ntfm = Transformer(dfs, cbs=[\n                            LowerStripNameCB(col_src='nuclide', col_dst='NUCLIDE'),\n                            RemapNuclideNameCB(lut_nuclides, col_name='NUCLIDE'),\n                            ParseTimeCB(),\n                            EncodeTimeCB(),\n                            SplitSedimentValuesCB(coi_sediment),\n                            SanitizeValueCB(coi_val),       \n                            NormalizeUncCB(),\n                            RemapUnitCB(),\n                            RemapDetectionLimitCB(coi_dl, lut_dl),                           \n                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='rubin', dest_grps='BIOTA'),\n                            RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='tissue', dest_grps='BIOTA'),\n                            RemapCB(fn_lut=lut_biogroup_from_biota, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA'),\n                            RemapSedimentCB(fn_lut=lut_sediments, replace_lut=sed_replace_lut),\n                            RemapFiltCB(lut_filtered),\n                            AddSampleIDCB(),\n                            AddDepthCB(),\n                            AddSalinityCB(),\n                            AddTemperatureCB(),\n                            RemapSedSliceTopBottomCB(),\n                            LookupDryWetPercentWeightCB(),\n                            ParseCoordinates(ddmm_to_dd),\n                            SanitizeLonLatCB(),\n                            AddStationCB(),\n                            CompareDfsAndTfmCB(dfs)\n                            ])\n\ntfm()\nprint(pd.DataFrame.from_dict(tfm.compare_stats) , '\\n')\n\nWarning: 8 missing time value(s) in SEAWATER\nWarning: 1 missing time value(s) in SEDIMENT\n                                               BIOTA  SEAWATER  SEDIMENT\nOriginal row count (dfs)                       16124     21634     40744\nTransformed row count (tfm.dfs)                16094     21473     70449\nRows removed from original (tfm.dfs_removed)      30       161       144\nRows created in transformed (tfm.dfs_created)      0         0     29849 \n\n\n\n\ntfm.dfs['SEAWATER'].head()\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/m³\nvalue_bq/m³\nerror%_m³\ndate_of_entry_x\ncountry\nlaboratory\nsequence\n...\nFILT\nSMP_ID\nSMP_ID_PROVIDER\nSMP_DEPTH\nTOT_DEPTH\nSALINITY\nTEMPERATURE\nLAT\nLON\nSTATION\n\n\n\n\n0\nWKRIL2012003\nCS137\nNaN\nNaN\n5.3\n32.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012003.0\n...\n0\n1\nWKRIL2012003\n0.0\nNaN\nNaN\nNaN\n60.0833\n29.3333\nRU10\n\n\n1\nWKRIL2012004\nCS137\nNaN\nNaN\n19.9\n20.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012004.0\n...\n0\n2\nWKRIL2012004\n29.0\nNaN\nNaN\nNaN\n60.0833\n29.3333\nRU10\n\n\n2\nWKRIL2012005\nCS137\nNaN\nNaN\n25.5\n20.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012005.0\n...\n0\n3\nWKRIL2012005\n0.0\nNaN\nNaN\nNaN\n59.4333\n23.1500\nRU11\n\n\n3\nWKRIL2012006\nCS137\nNaN\nNaN\n17.0\n29.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012006.0\n...\n0\n4\nWKRIL2012006\n0.0\nNaN\nNaN\nNaN\n60.2500\n27.9833\nRU19\n\n\n4\nWKRIL2012007\nCS137\nNaN\nNaN\n22.2\n18.0\n08/20/14 00:00:00\n90.0\nKRIL\n2012007.0\n...\n0\n5\nWKRIL2012007\n39.0\nNaN\nNaN\nNaN\n60.2500\n27.9833\nRU19\n\n\n\n\n5 rows × 43 columns\n\n\n\nLets inspect the rows that are removed for the SEAWATER data:\n\ngrp='SEAWATER' # 'SEAWATER', 'BIOTA' or 'SEDIMENT'\nprint(f'{grp}, number of dropped rows: {tfm.dfs_removed[grp].shape[0]}.')\nprint(f'Viewing dropped rows for {grp}:')\ntfm.dfs_removed[grp]\n\nSEAWATER, number of dropped rows: 161.\nViewing dropped rows for SEAWATER:\n\n\n\n\n\n\n\n\n\nkey\nnuclide\nmethod\n&lt; value_bq/m³\nvalue_bq/m³\nerror%_m³\ndate_of_entry_x\ncountry\nlaboratory\nsequence\n...\nlongitude (ddmmmm)\nlongitude (dddddd)\ntdepth\nsdepth\nsalin\nttemp\nfilt\nmors_subbasin\nhelcom_subbasin\ndate_of_entry_y\n\n\n\n\n13439\nWRISO2001025\nCS137\nRISO02\nNaN\nNaN\n10.0\nNaN\n26.0\nRISO\n2001025.0\n...\n10.500\n10.833333\n22.0\n20.0\n0.00\nNaN\nN\n5.0\n5.0\nNaN\n\n\n14017\nWLEPA2002001\nCS134\nLEPA02\n&lt;\nNaN\nNaN\nNaN\n93.0\nLEPA\n2002001.0\n...\n21.030\n21.050000\n16.0\n0.0\n3.77\n14.40\nN\n4.0\n9.0\nNaN\n\n\n14020\nWLEPA2002002\nCS134\nLEPA02\n&lt;\nNaN\nNaN\nNaN\n93.0\nLEPA\n2002004.0\n...\n20.574\n20.956667\n14.0\n0.0\n6.57\n11.95\nN\n4.0\n9.0\nNaN\n\n\n14023\nWLEPA2002003\nCS134\nLEPA02\n&lt;\nNaN\nNaN\nNaN\n93.0\nLEPA\n2002007.0\n...\n19.236\n19.393333\n73.0\n0.0\n7.00\n9.19\nN\n4.0\n9.0\nNaN\n\n\n14026\nWLEPA2002004\nCS134\nLEPA02\n&lt;\nNaN\nNaN\nNaN\n93.0\nLEPA\n2002010.0\n...\n20.205\n20.341700\n47.0\n0.0\n7.06\n8.65\nN\n4.0\n9.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n21542\nWLRPC2023011\nSR90\nLRPC02\nNaN\nNaN\nNaN\n05/03/24 00:00:00\n93.0\nLRPC\n2023011.0\n...\n20.480\n20.800000\n45.0\n1.0\n7.22\n19.80\nN\n4.0\n9.0\n05/03/24 00:00:00\n\n\n21543\nWLRPC2023012\nCS137\nLRPC01\nNaN\nNaN\nNaN\n05/03/24 00:00:00\n93.0\nLRPC\n2023012.0\n...\n20.480\n20.800000\n45.0\n1.0\n7.23\n8.80\nN\n4.0\n9.0\n05/03/24 00:00:00\n\n\n21544\nWLRPC2023012\nSR90\nLRPC02\nNaN\nNaN\nNaN\n05/03/24 00:00:00\n93.0\nLRPC\n2023012.0\n...\n20.480\n20.800000\n45.0\n1.0\n7.23\n8.80\nN\n4.0\n9.0\n05/03/24 00:00:00\n\n\n21545\nWLRPC2023013\nCS137\nLRPC01\nNaN\nNaN\nNaN\n05/03/24 00:00:00\n93.0\nLRPC\n2023013.0\n...\n20.427\n20.711700\n41.0\n1.0\n7.23\n19.30\nN\n4.0\n9.0\n05/03/24 00:00:00\n\n\n21546\nWLRPC2023013\nSR90\nLRPC02\nNaN\nNaN\nNaN\n05/03/24 00:00:00\n93.0\nLRPC\n2023013.0\n...\n20.427\n20.711700\n41.0\n1.0\n7.23\n19.30\nN\n4.0\n9.0\n05/03/24 00:00:00\n\n\n\n\n161 rows × 27 columns\n\n\n\n\nExample change logs\n\ndfs = load_data(src_dir, use_cache=True)\n\ntfm = Transformer(dfs, cbs=[\n                            LowerStripNameCB(col_src='nuclide', col_dst='NUCLIDE'),\n                            RemapNuclideNameCB(lut_nuclides, col_name='NUCLIDE'),\n                            ParseTimeCB(),\n                            EncodeTimeCB(),\n                            SplitSedimentValuesCB(coi_sediment),\n                            SanitizeValueCB(coi_val),       \n                            NormalizeUncCB(),\n                            RemapUnitCB(),\n                            RemapDetectionLimitCB(coi_dl, lut_dl),                           \n                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='rubin', dest_grps='BIOTA'),\n                            RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='tissue', dest_grps='BIOTA'),\n                            RemapCB(fn_lut=lut_biogroup_from_biota, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA'),\n                            RemapSedimentCB(fn_lut=lut_sediments, replace_lut=sed_replace_lut),\n                            RemapFiltCB(lut_filtered),\n                            AddSampleIDCB(),\n                            AddDepthCB(),\n                            AddSalinityCB(),\n                            AddTemperatureCB(),\n                            RemapSedSliceTopBottomCB(),\n                            LookupDryWetPercentWeightCB(),\n                            ParseCoordinates(ddmm_to_dd),\n                            SanitizeLonLatCB(),\n                            ])\n\ntfm()\ntfm.logs\n\nWarning: 8 missing time value(s) in SEAWATER\nWarning: 1 missing time value(s) in SEDIMENT\n\n\n[\"Convert 'nuclide' column values to lowercase, strip spaces, and store in 'NUCLIDE' column.\",\n 'Remap data provider nuclide names to standardized MARIS nuclide names.',\n 'Standardize time format across all dataframes.',\n 'Encode time as seconds since epoch.',\n 'Separate sediment entries into distinct rows for Bq/kg and Bq/m² measurements.',\n 'Sanitize measurement values by removing blanks and standardizing to use the `VALUE` column.',\n 'Convert from relative error to standard uncertainty.',\n 'Set the `unit` id column in the DataFrames based on a lookup table.',\n 'Remap value type to MARIS format.',\n \"Remap values from 'rubin' to 'SPECIES' for groups: BIOTA.\",\n \"Remap values from 'tissue' to 'BODY_PART' for groups: BIOTA.\",\n \"Remap values from 'SPECIES' to 'BIO_GROUP' for groups: BIOTA.\",\n 'Lookup sediment id using lookup table.',\n 'Lookup filt value in dataframe using the lookup table.',\n 'Generate a SMP_ID from the KEY values in the HELCOM dataset.',\n \"Ensure depth values are floats and add 'SMP_DEPTH' and 'TOT_DEPTH' columns.\",\n 'Remap Sediment slice top and bottom to MARIS format.',\n 'Lookup dry-wet ratio and format for MARIS.',\n 'Get geographical coordinates from columns expressed in degrees decimal format or from columns in degrees/minutes decimal format where degrees decimal format is missing or zero.',\n 'Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.']",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#feed-global-attributes",
    "href": "handlers/helcom.html#feed-global-attributes",
    "title": "HELCOM",
    "section": "Feed global attributes",
    "text": "Feed global attributes\n\nsource\n\nget_attrs\n\n get_attrs (tfm:marisco.callbacks.Transformer, zotero_key:str,\n            kw:list=['oceanography', 'Earth Science &gt; Oceans &gt; Ocean\n            Chemistry&gt; Radionuclides', 'Earth Science &gt; Human Dimensions &gt;\n            Environmental Impacts &gt; Nuclear Radiation Exposure', 'Earth\n            Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth\n            Science &gt; Oceans &gt; Marine Sediments', 'Earth Science &gt; Oceans\n            &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt;\n            Isotopes', 'Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean\n            Contaminants', 'Earth Science &gt; Biological Classification &gt;\n            Animals/Vertebrates &gt; Fish', 'Earth Science &gt; Biosphere &gt;\n            Ecosystems &gt; Marine Ecosystems', 'Earth Science &gt; Biological\n            Classification &gt; Animals/Invertebrates &gt; Mollusks', 'Earth\n            Science &gt; Biological Classification &gt; Animals/Invertebrates &gt;\n            Arthropods &gt; Crustaceans', 'Earth Science &gt; Biological\n            Classification &gt; Plants &gt; Macroalgae (Seaweeds)'])\n\nRetrieve all global attributes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntfm\nTransformer\n\nTransformer object\n\n\nzotero_key\nstr\n\nZotero dataset record key\n\n\nkw\nlist\n[‘oceanography’, ‘Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides’, ‘Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure’, ‘Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments’, ‘Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes’, ‘Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants’, ‘Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish’, ‘Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems’, ‘Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks’, ‘Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans’, ‘Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)’]\nList of keywords\n\n\nReturns\ndict\n\nGlobal attributes\n\n\n\n\n\nExported source\ndef get_attrs(\n    tfm: Transformer, # Transformer object\n    zotero_key: str, # Zotero dataset record key\n    kw: list = kw # List of keywords\n    ) -&gt; dict: # Global attributes\n    \"Retrieve all global attributes.\"\n    return GlobAttrsFeeder(tfm.dfs, cbs=[\n        BboxCB(),\n        DepthRangeCB(),\n        TimeRangeCB(),\n        ZoteroCB(zotero_key, cfg=cfg()),\n        KeyValuePairCB('keywords', ', '.join(kw)),\n        KeyValuePairCB('publisher_postprocess_logs', ', '.join(tfm.logs))\n        ])()\n\n\n\nget_attrs(tfm, zotero_key=zotero_key, kw=kw)\n\n{'geospatial_lat_min': '31.17',\n 'geospatial_lat_max': '65.75',\n 'geospatial_lon_min': '9.6333',\n 'geospatial_lon_max': '53.5',\n 'geospatial_bounds': 'POLYGON ((9.6333 53.5, 31.17 53.5, 31.17 65.75, 9.6333 65.75, 9.6333 53.5))',\n 'geospatial_vertical_max': '437.0',\n 'geospatial_vertical_min': '0.0',\n 'time_coverage_start': '1984-01-10T00:00:00',\n 'time_coverage_end': '2023-11-30T00:00:00',\n 'id': '26VMZZ2Q',\n 'title': 'Environmental database - Helsinki Commission Monitoring of Radioactive Substances',\n 'summary': 'MORS Environment database has been used to collate data resulting from monitoring of environmental radioactivity in the Baltic Sea based on HELCOM Recommendation 26/3.\\n\\nThe database is structured according to HELCOM Guidelines on Monitoring of Radioactive Substances (https://www.helcom.fi/wp-content/uploads/2019/08/Guidelines-for-Monitoring-of-Radioactive-Substances.pdf), which specifies reporting format, database structure, data types and obligatory parameters used for reporting data under Recommendation 26/3.\\n\\nThe database is updated and quality assured annually by HELCOM MORS EG.',\n 'creator_name': '[{\"creatorType\": \"author\", \"name\": \"HELCOM MORS\"}]',\n 'keywords': 'oceanography, Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides, Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure, Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments, Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes, Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants, Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish, Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans, Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)',\n 'publisher_postprocess_logs': \"Convert 'nuclide' column values to lowercase, strip spaces, and store in 'NUCLIDE' column., Remap data provider nuclide names to standardized MARIS nuclide names., Standardize time format across all dataframes., Encode time as seconds since epoch., Separate sediment entries into distinct rows for Bq/kg and Bq/m² measurements., Sanitize measurement values by removing blanks and standardizing to use the `VALUE` column., Convert from relative error to standard uncertainty., Set the `unit` id column in the DataFrames based on a lookup table., Remap value type to MARIS format., Remap values from 'rubin' to 'SPECIES' for groups: BIOTA., Remap values from 'tissue' to 'BODY_PART' for groups: BIOTA., Remap values from 'SPECIES' to 'BIO_GROUP' for groups: BIOTA., Lookup sediment id using lookup table., Lookup filt value in dataframe using the lookup table., Generate a SMP_ID from the KEY values in the HELCOM dataset., Ensure depth values are floats and add 'SMP_DEPTH' and 'TOT_DEPTH' columns., Remap Sediment slice top and bottom to MARIS format., Lookup dry-wet ratio and format for MARIS., Get geographical coordinates from columns expressed in degrees decimal format or from columns in degrees/minutes decimal format where degrees decimal format is missing or zero., Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.\"}",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#encoding-netcdf",
    "href": "handlers/helcom.html#encoding-netcdf",
    "title": "HELCOM",
    "section": "Encoding NetCDF",
    "text": "Encoding NetCDF\n\nsource\n\nencode\n\n encode (fname_out:str, **kwargs)\n\nEncode data to NetCDF.\n\n\n\n\nType\nDetails\n\n\n\n\nfname_out\nstr\nOutput file name\n\n\nkwargs\nVAR_KEYWORD\n\n\n\nReturns\nNone\nAdditional arguments\n\n\n\n\n\nExported source\ndef encode(\n    fname_out: str, # Output file name\n    **kwargs # Additional arguments\n    ) -&gt; None:\n    \"Encode data to NetCDF.\"\n    dfs = load_data(src_dir)\n    tfm = Transformer(dfs, cbs=[\n                            LowerStripNameCB(col_src='nuclide', col_dst='NUCLIDE'),\n                            RemapNuclideNameCB(lut_nuclides, col_name='NUCLIDE'),\n                            ParseTimeCB(),\n                            EncodeTimeCB(),\n                            SplitSedimentValuesCB(coi_sediment),\n                            SanitizeValueCB(coi_val),       \n                            NormalizeUncCB(),\n                            RemapUnitCB(),\n                            RemapDetectionLimitCB(coi_dl, lut_dl),                           \n                            RemapCB(fn_lut=lut_biota, col_remap='SPECIES', col_src='rubin', dest_grps='BIOTA'),\n                            RemapCB(fn_lut=lut_tissues, col_remap='BODY_PART', col_src='tissue', dest_grps='BIOTA'),\n                            RemapCB(fn_lut=lut_biogroup_from_biota, col_remap='BIO_GROUP', col_src='SPECIES', dest_grps='BIOTA'),\n                            RemapSedimentCB(fn_lut=lut_sediments, replace_lut=sed_replace_lut),\n                            RemapFiltCB(lut_filtered),\n                            AddSampleIDCB(),\n                            AddDepthCB(),\n                            AddSalinityCB(),\n                            AddTemperatureCB(),\n                            RemapSedSliceTopBottomCB(),\n                            LookupDryWetPercentWeightCB(),\n                            ParseCoordinates(ddmm_to_dd),\n                            SanitizeLonLatCB(),\n                            AddStationCB()\n                            ])\n    tfm()\n    encoder = NetCDFEncoder(tfm.dfs, \n                            dest_fname=fname_out, \n                            global_attrs=get_attrs(tfm, zotero_key=zotero_key, kw=kw),\n                            # custom_maps=tfm.custom_maps,\n                            verbose=kwargs.get('verbose', False),\n                           )\n    encoder.encode()\n\n\n\nencode(fname_out, verbose=False)\n\nWarning: 8 missing time value(s) in SEAWATER\nWarning: 1 missing time value(s) in SEDIMENT",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#netcdf-review",
    "href": "handlers/helcom.html#netcdf-review",
    "title": "HELCOM",
    "section": "NetCDF Review",
    "text": "NetCDF Review\nFirst lets review the global attributes of the NetCDF file:\n\ncontents = ExtractNetcdfContents(fname_out)\nprint(contents.global_attrs)\n\n{'id': '26VMZZ2Q', 'title': 'Environmental database - Helsinki Commission Monitoring of Radioactive Substances', 'summary': 'MORS Environment database has been used to collate data resulting from monitoring of environmental radioactivity in the Baltic Sea based on HELCOM Recommendation 26/3.\\n\\nThe database is structured according to HELCOM Guidelines on Monitoring of Radioactive Substances (https://www.helcom.fi/wp-content/uploads/2019/08/Guidelines-for-Monitoring-of-Radioactive-Substances.pdf), which specifies reporting format, database structure, data types and obligatory parameters used for reporting data under Recommendation 26/3.\\n\\nThe database is updated and quality assured annually by HELCOM MORS EG.', 'keywords': 'oceanography, Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides, Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure, Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments, Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes, Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants, Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish, Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans, Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)', 'history': 'TBD', 'keywords_vocabulary': 'GCMD Science Keywords', 'keywords_vocabulary_url': 'https://gcmd.earthdata.nasa.gov/static/kms/', 'record': 'TBD', 'featureType': 'TBD', 'cdm_data_type': 'TBD', 'Conventions': 'CF-1.10 ACDD-1.3', 'publisher_name': 'Paul MCGINNITY, Iolanda OSVATH, Florence DESCROIX-COMANDUCCI', 'publisher_email': 'p.mc-ginnity@iaea.org, i.osvath@iaea.org, F.Descroix-Comanducci@iaea.org', 'publisher_url': 'https://maris.iaea.org', 'publisher_institution': 'International Atomic Energy Agency - IAEA', 'creator_name': '[{\"creatorType\": \"author\", \"name\": \"HELCOM MORS\"}]', 'institution': 'TBD', 'metadata_link': 'TBD', 'creator_email': 'TBD', 'creator_url': 'TBD', 'references': 'TBD', 'license': 'Without prejudice to the applicable Terms and Conditions (https://nucleus.iaea.org/Pages/Others/Disclaimer.aspx), I hereby agree that any use of the data will contain appropriate acknowledgement of the data source(s) and the IAEA Marine Radioactivity Information System (MARIS).', 'comment': 'TBD', 'geospatial_lat_min': '31.17', 'geospatial_lon_min': '9.6333', 'geospatial_lat_max': '65.75', 'geospatial_lon_max': '53.5', 'geospatial_vertical_min': '0.0', 'geospatial_vertical_max': '437.0', 'geospatial_bounds': 'POLYGON ((9.6333 53.5, 31.17 53.5, 31.17 65.75, 9.6333 65.75, 9.6333 53.5))', 'geospatial_bounds_crs': 'EPSG:4326', 'time_coverage_start': '1984-01-10T00:00:00', 'time_coverage_end': '2023-11-30T00:00:00', 'local_time_zone': 'TBD', 'date_created': 'TBD', 'date_modified': 'TBD', 'publisher_postprocess_logs': \"Convert 'nuclide' column values to lowercase, strip spaces, and store in 'NUCLIDE' column., Remap data provider nuclide names to standardized MARIS nuclide names., Standardize time format across all dataframes., Encode time as seconds since epoch., Separate sediment entries into distinct rows for Bq/kg and Bq/m² measurements., Sanitize measurement values by removing blanks and standardizing to use the `VALUE` column., Convert from relative error to standard uncertainty., Set the `unit` id column in the DataFrames based on a lookup table., Remap value type to MARIS format., Remap values from 'rubin' to 'SPECIES' for groups: BIOTA., Remap values from 'tissue' to 'BODY_PART' for groups: BIOTA., Remap values from 'SPECIES' to 'BIO_GROUP' for groups: BIOTA., Lookup sediment id using lookup table., Lookup filt value in dataframe using the lookup table., Generate a SMP_ID from the KEY values in the HELCOM dataset., Ensure depth values are floats and add 'SMP_DEPTH' and 'TOT_DEPTH' columns., Remap Sediment slice top and bottom to MARIS format., Lookup dry-wet ratio and format for MARIS., Get geographical coordinates from columns expressed in degrees decimal format or from columns in degrees/minutes decimal format where degrees decimal format is missing or zero., Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator., Add station to all DataFrames.\"}\n\n\nReview the publisher_postprocess_logs.\n\nprint(contents.global_attrs['publisher_postprocess_logs'])\n\nConvert 'nuclide' column values to lowercase, strip spaces, and store in 'NUCLIDE' column., Remap data provider nuclide names to standardized MARIS nuclide names., Standardize time format across all dataframes., Encode time as seconds since epoch., Separate sediment entries into distinct rows for Bq/kg and Bq/m² measurements., Sanitize measurement values by removing blanks and standardizing to use the `VALUE` column., Convert from relative error to standard uncertainty., Set the `unit` id column in the DataFrames based on a lookup table., Remap value type to MARIS format., Remap values from 'rubin' to 'SPECIES' for groups: BIOTA., Remap values from 'tissue' to 'BODY_PART' for groups: BIOTA., Remap values from 'SPECIES' to 'BIO_GROUP' for groups: BIOTA., Lookup sediment id using lookup table., Lookup filt value in dataframe using the lookup table., Generate a SMP_ID from the KEY values in the HELCOM dataset., Ensure depth values are floats and add 'SMP_DEPTH' and 'TOT_DEPTH' columns., Remap Sediment slice top and bottom to MARIS format., Lookup dry-wet ratio and format for MARIS., Get geographical coordinates from columns expressed in degrees decimal format or from columns in degrees/minutes decimal format where degrees decimal format is missing or zero., Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator., Add station to all DataFrames.\n\n\nNow lets review the enums of the groups in the NetCDF file:\n\nprint('Example of enum_dicts:')\nprint(contents.enum_dicts['BIOTA']['bio_group'])\n\nExample of enum_dicts:\n{'Not applicable': '-1', 'Not available': '0', 'Birds': '1', 'Crustaceans': '2', 'Echinoderms': '3', 'Fish': '4', 'Mammals': '5', 'Molluscs': '6', 'Others': '7', 'Plankton': '8', 'Polychaete worms': '9', 'Reptile': '10', 'Seaweeds and plants': '11', 'Cephalopods': '12', 'Gastropods': '13', 'Bivalves': '14'}\n\n\nLets return the data contained in the NetCDF file:\n\ndfs = contents.dfs\n\nLets review the biota data:\n\nnc_dfs_biota=dfs['BIOTA']\nwith pd.option_context('display.max_columns', None):\n    display(nc_dfs_biota)\n\n\n\n\n\n\n\n\nSMP_ID_PROVIDER\nLON\nLAT\nSMP_DEPTH\nTIME\nSTATION\nSMP_ID\nNUCLIDE\nVALUE\nUNIT\nUNC\nDL\nBIO_GROUP\nSPECIES\nBODY_PART\nDRYWT\nWETWT\nPERCENTWT\n\n\n\n\n0\nBVTIG2012041\n12.316667\n54.283333\nNaN\n1348358400\nSD24\n1\n31\n0.010140\n5\nNaN\n2\n4\n99\n52\n174.934433\n948.0\n0.18453\n\n\n1\nBVTIG2012041\n12.316667\n54.283333\nNaN\n1348358400\nSD24\n2\n4\n135.300003\n5\n4.830210\n1\n4\n99\n52\n174.934433\n948.0\n0.18453\n\n\n2\nBVTIG2012041\n12.316667\n54.283333\nNaN\n1348358400\nSD24\n3\n9\n0.013980\n5\nNaN\n2\n4\n99\n52\n174.934433\n948.0\n0.18453\n\n\n3\nBVTIG2012041\n12.316667\n54.283333\nNaN\n1348358400\nSD24\n4\n33\n4.338000\n5\n0.150962\n1\n4\n99\n52\n174.934433\n948.0\n0.18453\n\n\n4\nBVTIG2012040\n12.316667\n54.283333\nNaN\n1348358400\nSD24\n5\n31\n0.009614\n5\nNaN\n2\n4\n99\n52\n177.935120\n964.0\n0.18458\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n16089\nBSTUK2022010\n21.395000\n61.241501\n2.0\n1652140800\nOLKB\n16090\n33\n13.700000\n4\n0.520600\n1\n11\n96\n55\nNaN\nNaN\nNaN\n\n\n16090\nBSTUK2022010\n21.395000\n61.241501\n2.0\n1652140800\nOLKB\n16091\n9\n0.500000\n4\n0.045500\n1\n11\n96\n55\nNaN\nNaN\nNaN\n\n\n16091\nBSTUK2022011\n21.385000\n61.343334\nNaN\n1663200000\nOLKC\n16092\n4\n50.700001\n4\n4.106700\n1\n14\n129\n1\nNaN\nNaN\nNaN\n\n\n16092\nBSTUK2022011\n21.385000\n61.343334\nNaN\n1663200000\nOLKC\n16093\n33\n0.880000\n4\n0.140800\n1\n14\n129\n1\nNaN\nNaN\nNaN\n\n\n16093\nBSTUK2022011\n21.385000\n61.343334\nNaN\n1663200000\nOLKC\n16094\n12\n6.600000\n4\n0.349800\n1\n14\n129\n1\nNaN\nNaN\nNaN\n\n\n\n\n16094 rows × 18 columns\n\n\n\nLets review the sediment data:\n\nnc_dfs_sediment = dfs['SEDIMENT']\nwith pd.option_context('display.max_columns', None):\n    display(nc_dfs_sediment)\n\n\n\n\n\n\n\n\nSMP_ID_PROVIDER\nLON\nLAT\nTOT_DEPTH\nTIME\nSTATION\nSMP_ID\nNUCLIDE\nVALUE\nUNIT\nUNC\nDL\nSED_TYPE\nTOP\nBOTTOM\nPERCENTWT\n\n\n\n\n0\nSKRIL2012116\n27.799999\n60.466667\n25.0\n1337904000\nRU99\n1\n33\n1200.000000\n3\n240.000000\n1\n0\n15.0\n20.0\nNaN\n\n\n1\nSKRIL2012117\n27.799999\n60.466667\n25.0\n1337904000\nRU99\n2\n33\n250.000000\n3\n50.000000\n1\n0\n20.0\n25.0\nNaN\n\n\n2\nSKRIL2012118\n27.799999\n60.466667\n25.0\n1337904000\nRU99\n3\n33\n140.000000\n3\n29.400000\n1\n0\n25.0\n30.0\nNaN\n\n\n3\nSKRIL2012119\n27.799999\n60.466667\n25.0\n1337904000\nRU99\n4\n33\n79.000000\n3\n15.800000\n1\n0\n30.0\n35.0\nNaN\n\n\n4\nSKRIL2012120\n27.799999\n60.466667\n25.0\n1337904000\nRU99\n5\n33\n29.000000\n3\n6.960000\n1\n0\n35.0\n40.0\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n70444\nSCLOR2022071\n15.537800\n54.617832\n62.0\n1654646400\nP39Z\n70445\n67\n0.044000\n2\n0.015312\n1\n10\n15.0\n17.0\n0.257642\n\n\n70445\nSCLOR2022071\n15.537800\n54.617832\n62.0\n1654646400\nP39Z\n70446\n77\n2.500000\n2\n0.185000\n1\n10\n15.0\n17.0\n0.257642\n\n\n70446\nSCLOR2022072\n15.537800\n54.617832\n62.0\n1654646400\nP39Z\n70447\n4\n5873.000000\n2\n164.444000\n1\n10\n17.0\n19.0\n0.263965\n\n\n70447\nSCLOR2022072\n15.537800\n54.617832\n62.0\n1654646400\nP39Z\n70448\n33\n21.200001\n2\n2.162400\n1\n10\n17.0\n19.0\n0.263965\n\n\n70448\nSCLOR2022072\n15.537800\n54.617832\n62.0\n1654646400\nP39Z\n70449\n77\n0.370000\n2\n0.048100\n1\n10\n17.0\n19.0\n0.263965\n\n\n\n\n70449 rows × 16 columns\n\n\n\nLets review the seawater data:\n\nnc_dfs_seawater = dfs['SEAWATER']\nwith pd.option_context('display.max_columns', None):\n    display(nc_dfs_seawater)\n\n\n\n\n\n\n\n\nSMP_ID_PROVIDER\nLON\nLAT\nSMP_DEPTH\nTOT_DEPTH\nTIME\nSTATION\nSMP_ID\nNUCLIDE\nVALUE\nUNIT\nUNC\nDL\nFILT\n\n\n\n\n0\nWKRIL2012003\n29.333300\n60.083302\n0.0\nNaN\n1337731200\nRU10\n1\n33\n5.300000\n1\n1.696000\n1\n0\n\n\n1\nWKRIL2012004\n29.333300\n60.083302\n29.0\nNaN\n1337731200\nRU10\n2\n33\n19.900000\n1\n3.980000\n1\n0\n\n\n2\nWKRIL2012005\n23.150000\n59.433300\n0.0\nNaN\n1339891200\nRU11\n3\n33\n25.500000\n1\n5.100000\n1\n0\n\n\n3\nWKRIL2012006\n27.983299\n60.250000\n0.0\nNaN\n1337817600\nRU19\n4\n33\n17.000000\n1\n4.930000\n1\n0\n\n\n4\nWKRIL2012007\n27.983299\n60.250000\n39.0\nNaN\n1337817600\nRU19\n5\n33\n22.200001\n1\n3.996000\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n21468\nWDHIG2023112\n13.499833\n54.600334\n0.0\n47.0\n1686441600\nWITTOW\n21469\n1\n702.838074\n1\n51.276207\n1\n0\n\n\n21469\nWDHIG2023113\n13.499833\n54.600334\n45.0\n47.0\n1686441600\nWITTOW\n21470\n1\n725.855713\n1\n52.686260\n1\n0\n\n\n21470\nWDHIG2023143\n14.200833\n54.600334\n0.0\n11.0\n1686614400\nODER\n21471\n1\n648.992920\n1\n48.154419\n1\n0\n\n\n21471\nWDHIG2023145\n14.665500\n54.600334\n0.0\n20.0\n1686614400\nOBANK\n21472\n1\n627.178406\n1\n46.245316\n1\n0\n\n\n21472\nWDHIG2023147\n14.330000\n54.600334\n0.0\n17.0\n1686614400\nADLERG\n21473\n1\n605.715088\n1\n45.691143\n1\n0\n\n\n\n\n21473 rows × 14 columns",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/helcom.html#data-format-conversion",
    "href": "handlers/helcom.html#data-format-conversion",
    "title": "HELCOM",
    "section": "Data Format Conversion",
    "text": "Data Format Conversion\nThe MARIS data processing workflow involves two key steps:\n\nNetCDF to Standardized CSV Compatible with OpenRefine Pipeline\n\nConvert standardized NetCDF files to CSV formats compatible with OpenRefine using the NetCDFDecoder.\nPreserve data integrity and variable relationships.\nMaintain standardized nomenclature and units.\n\nDatabase Integration\n\nProcess the converted CSV files using OpenRefine.\nApply data cleaning and standardization rules.\nExport validated data to the MARIS master database.\n\n\nThis section focuses on the first step: converting NetCDF files to a format suitable for OpenRefine processing using the NetCDFDecoder class.\n\ndecode(fname_in=fname_out, verbose=True)\n\nSaved BIOTA to ../../_data/output/100-HELCOM-MORS-2024_BIOTA.csv\nSaved SEAWATER to ../../_data/output/100-HELCOM-MORS-2024_SEAWATER.csv\nSaved SEDIMENT to ../../_data/output/100-HELCOM-MORS-2024_SEDIMENT.csv",
    "crumbs": [
      "Handlers",
      "HELCOM"
    ]
  },
  {
    "objectID": "handlers/maris_legacy.html",
    "href": "handlers/maris_legacy.html",
    "title": "MARIS Legacy",
    "section": "",
    "text": "This data pipeline, known as “handler” in Marisco terminology, contains a data pipeline (handler) that converts the master MARIS database dump into NetCDF format. It enables batch encoding of all legacy datasets into NetCDF.\nKey functions of this handler:\nThe result is a set of NetCDF files, one for each unique reference ID in the input data.\nThe present notebook pretends to be an instance of Literate Programming in the sense that it is a narrative that includes code snippets that are interspersed with explanations. When a function or a class needs to be exported in a dedicated python module (in our case marisco/handlers/helcom.py) the code snippet is added to the module using #| exports as provided by the wonderful nbdev library.",
    "crumbs": [
      "Handlers",
      "MARIS Legacy"
    ]
  },
  {
    "objectID": "handlers/maris_legacy.html#configuration-file-paths",
    "href": "handlers/maris_legacy.html#configuration-file-paths",
    "title": "MARIS Legacy",
    "section": "Configuration & file paths",
    "text": "Configuration & file paths\n\nfname_in: path to the folder containing the MARIS dump data in CSV format.\ndir_dest: path to the folder where the NetCDF output will be saved.\n\n\n\nExported source\n# fname_in = Path().home() / 'pro/data/maris/2024-11-20 MARIS_QA_shapetype_id=1.txt'\nfname_in = Path().home() / 'pro/data/maris/2025-06-03 MARIS_QA_shapetype_id = 1.txt'\n\ndir_dest = '../../_data/output/dump'\n\n\n\ndf = pd.read_csv(fname_in, sep='\\t', encoding='utf-8', low_memory=False)\n\n\ndf[df.ref_id == 160].iloc[0].zoterourl\n\n'https://www.zotero.org/groups/2432820/maris/items/2DG8PM5Q'\n\n\n\ndf[df.ref_id == 160].iloc[0]\n\nsample_id                                                        419906\narea_id                                                            4279\nareaname                              Mediterranean Sea - Western Basin\nsamptype_id                                                           1\nsamptype                                                       Seawater\nref_id                                                              160\ndisplaytext                                           Pham et al., 2010\nzoterourl             https://www.zotero.org/groups/2432820/maris/it...\nref_note                                                            NaN\ndatbase                                                             NaN\nlab_id                                                                0\nlab                                                       NOT AVAILABLE\nlatitude                                                      43.418611\nlongitude                                                      7.833889\nbegperiod                                       2001-02-18 00:00:00.000\nendperiod                                                           NaN\nsamplingyear                                                       2001\ntotdepth                                                            NaN\nsampdepth                                                           0.0\nstation                                                         DYFAMED\nsamplabcode                                                           1\nspecies_id                                                            0\ntaxonname                                               (Not available)\ntaxonrank                                                           NaN\nbiogroup                                                (Not available)\nbiogroup_id                                                           0\ntaxondb                                                             NaN\ntaxondbid                                                           NaN\ntaxondburl                                                          NaN\ntaxonrepname                                                        NaN\nbodypar_id                                                            0\nbodypar                                                 (Not available)\nsliceup                                                             NaN\nslicedown                                                           NaN\nsedtype_id                                                            0\nsedtype                                                 (Not available)\nsedrepname                                                          NaN\nnuclide_id                                                           28\nnusymbol                                                          I-129\nvolume                                                              NaN\nsalinity                                                            NaN\ntemperatur                                                          NaN\nfiltered                                                              Y\nfiltpore                                                            NaN\nsamparea                                                            NaN\ndrywt                                                               NaN\nwetwt                                                               NaN\npercentwt                                                           NaN\nsampmet_id                                                            1\nsampmet                                                 Bottle sampling\nprepmet_id                                                            0\nprepmet                                                   Not available\ndrymet_id                                                             0\ndrymet                                                    Not available\ncounmet_id                                                            7\ncounmet                                   Accelerator mass spectrometry\ndecayedto                                                           NaN\ndetection                                                             =\nactivity                                                       0.000319\nuncertaint                                                     0.000009\nunit_id                                                               1\nunit                                                              Bq/m3\nvartype                                                             NaN\nfreq                                                                NaN\nrangelow                                                            NaN\nrangeupp                                                            NaN\nprofile                                                             NaN\ntransect_id                                                         NaN\nmeasure_note                                                        AMS\nshapetype_id                                                          1\nprofile_id                                                          NaN\nsampnote              No sampling depth in the paper. According to t...\nref_fulltext          Pham, M.K., Betti, M., Povinec, P.P., Alfimov,...\nref_yearpub                                                        2010\nref_sampleTypes                                                       1\nLongLat                                                    7.834,43.419\nshiftedcoordinates       0xE6100000010C840D4FAF94B5454084D89942E7551F40\nshiftedlong                                                    7.833889\nshiftedlat                                                    43.418611\nid                                                               806966\nName: 806965, dtype: object\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nsample_id\narea_id\nareaname\nsamptype_id\nsamptype\nref_id\ndisplaytext\nzoterourl\nref_note\ndatbase\n...\nprofile_id\nsampnote\nref_fulltext\nref_yearpub\nref_sampleTypes\nLongLat\nshiftedcoordinates\nshiftedlong\nshiftedlat\nid\n\n\n\n\n0\n18810\n1904\nIndian Ocean\n1\nSeawater\n97\nASPAMARD, 2004\nhttps://www.zotero.org/groups/2432820/maris/it...\nP. Scotto(1975-2001);P. Morris (2003-2014)\nASPAMARD\n...\nNaN\nBecause the date does not have month & day, an...\nASPAMARD, 2004. Asia-Pacific Marine Radioactiv...\n2004\n1,3\n111.983,-25.05\n0xE6100000010CCDCCCCCCCC0C39C0F4FDD478E9FE5B40\n111.983333\n-25.050000\n1\n\n\n1\n63633\n1904\nIndian Ocean\n1\nSeawater\n99\nAoyama and Hirose, 2004\nhttps://www.zotero.org/groups/2432820/maris/it...\nNaN\nHAM 2008\n...\nNaN\nAuthor: Y.Bourlat, et.al. Unknown latitude and...\nAoyama, M., Hirose, K., 2004. HAM 2008 - Histo...\n2004\n1\n61.533,-52.017\n0xE6100000010CEEEBC03923024AC0787AA52C43C44E40\n61.533333\n-52.016667\n2\n\n\n2\n63633\n1904\nIndian Ocean\n1\nSeawater\n99\nAoyama and Hirose, 2004\nhttps://www.zotero.org/groups/2432820/maris/it...\nNaN\nHAM 2008\n...\nNaN\nAuthor: Y.Bourlat, et.al. Unknown latitude and...\nAoyama, M., Hirose, K., 2004. HAM 2008 - Histo...\n2004\n1\n61.533,-52.017\n0xE6100000010CEEEBC03923024AC0787AA52C43C44E40\n61.533333\n-52.016667\n3\n\n\n3\n63635\n1904\nIndian Ocean\n1\nSeawater\n99\nAoyama and Hirose, 2004\nhttps://www.zotero.org/groups/2432820/maris/it...\nNaN\nHAM 2008\n...\nNaN\nAuthor: Y.Bourlat, et.al. Unknown latitude and...\nAoyama, M., Hirose, K., 2004. HAM 2008 - Histo...\n2004\n1\n57.483,-44.483\n0xE6100000010C12143FC6DC3D46C012143FC6DCBD4C40\n57.483333\n-44.483333\n4\n\n\n4\n63635\n1904\nIndian Ocean\n1\nSeawater\n99\nAoyama and Hirose, 2004\nhttps://www.zotero.org/groups/2432820/maris/it...\nNaN\nHAM 2008\n...\nNaN\nAuthor: Y.Bourlat, et.al. Unknown latitude and...\nAoyama, M., Hirose, K., 2004. HAM 2008 - Histo...\n2004\n1\n57.483,-44.483\n0xE6100000010C12143FC6DC3D46C012143FC6DCBD4C40\n57.483333\n-44.483333\n5\n\n\n\n\n5 rows × 80 columns",
    "crumbs": [
      "Handlers",
      "MARIS Legacy"
    ]
  },
  {
    "objectID": "handlers/maris_legacy.html#utils",
    "href": "handlers/maris_legacy.html#utils",
    "title": "MARIS Legacy",
    "section": "Utils",
    "text": "Utils\nBelow a utility class to load a specific MARIS dump dataset optionally filtered through its ref_id.\n\nsource\n\nDataLoader\n\n DataLoader (fname:str, exclude_ref_id:Optional[List[int]]=[9999])\n\nLoad specific MARIS dataset through its ref_id.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfname\nstr\n\nPath to the MARIS global dump file\n\n\nexclude_ref_id\nOptional\n[9999]\nWhether to filter the dataframe by ref_id\n\n\n\n\n\nExported source\nclass DataLoader:\n    \"Load specific MARIS dataset through its ref_id.\"\n    LUT = {\n        'Biota': 'BIOTA', \n        'Seawater': 'SEAWATER', \n        'Sediment': 'SEDIMENT', \n        'Suspended matter': 'SUSPENDED_MATTER'\n    }\n\n    def __init__(self, \n                 fname: str, # Path to the MARIS global dump file\n                 exclude_ref_id: Optional[List[int]]=[9999] # Whether to filter the dataframe by ref_id\n                 ):\n        fc.store_attr()\n        self.df = self._load_data()\n\n    def _load_data(self):\n        df = pd.read_csv(self.fname, sep='\\t', encoding='utf-8', low_memory=False)\n        return df[~df.ref_id.isin(self.exclude_ref_id)] if self.exclude_ref_id else df\n\n    def __call__(self, \n                 ref_id: int # Reference ID of interest\n                 ) -&gt; dict: # Dictionary of dataframes\n        df = self.df[self.df.ref_id == ref_id].copy() if ref_id else self.df.copy()\n        return {self.LUT[name]: grp for name, grp in df.groupby('samptype') if name in self.LUT}\n\n\n\nsource\n\n\nget_zotero_key\n\n get_zotero_key (dfs)\n\nRetrieve Zotero key from MARIS dump.\n\n\nExported source\ndef get_zotero_key(dfs):\n    \"Retrieve Zotero key from MARIS dump.\"\n    return dfs[next(iter(dfs))][['zoterourl']].iloc[0].values[0].split('/')[-1]\n\n\n\nsource\n\n\nget_fname\n\n get_fname (dfs)\n\nGet NetCDF filename.\n\n\nExported source\ndef get_fname(dfs):\n    \"Get NetCDF filename.\"\n    return f\"{next(iter(dfs.values()))['ref_id'].iloc[0]}.nc\"",
    "crumbs": [
      "Handlers",
      "MARIS Legacy"
    ]
  },
  {
    "objectID": "handlers/maris_legacy.html#load-data",
    "href": "handlers/maris_legacy.html#load-data",
    "title": "MARIS Legacy",
    "section": "Load data",
    "text": "Load data\nHere below a quick overview of the MARIS dump data structure.\n\ndataloader = DataLoader(fname_in)\nref_id = 106 # Some other ref_id examples: OSPAR: 191, HELCOM: 100, 717 (only seawater)\n\ndfs = dataloader(ref_id=ref_id)\nprint(f'keys: {dfs.keys()}')\ndfs['SEAWATER'].head()\n\nkeys: dict_keys(['SEAWATER'])\n\n\n\n\n\n\n\n\n\nsample_id\narea_id\nareaname\nsamptype_id\nsamptype\nref_id\ndisplaytext\nzoterourl\nref_note\ndatbase\n...\nprofile_id\nsampnote\nref_fulltext\nref_yearpub\nref_sampleTypes\nLongLat\nshiftedcoordinates\nshiftedlong\nshiftedlat\nid\n\n\n\n\n5829\n73703\n1904\nIndian Ocean\n1\nSeawater\n106\nYamada et al., 2006\nhttps://www.zotero.org/groups/2432820/maris/it...\nNaN\nNaN\n...\nNaN\nNaN\nYamada, M., Zheng, J., Wang, Z.-L., 2006. 137C...\n2006\n1\n92.983,-0.008\n0xE6100000010CB289C4EB97DB7FBFA52C431CEB3E5740\n92.983056\n-0.007778\n5830\n\n\n5830\n73705\n1904\nIndian Ocean\n1\nSeawater\n106\nYamada et al., 2006\nhttps://www.zotero.org/groups/2432820/maris/it...\nNaN\nNaN\n...\nNaN\nNaN\nYamada, M., Zheng, J., Wang, Z.-L., 2006. 137C...\n2006\n1\n96.596,-4.029\n0xE6100000010CAF5A99F04B1D10C0D95F764F1E265840\n96.595556\n-4.028611\n5831\n\n\n5831\n73711\n1904\nIndian Ocean\n1\nSeawater\n106\nYamada et al., 2006\nhttps://www.zotero.org/groups/2432820/maris/it...\nNaN\nNaN\n...\nNaN\nNaN\nYamada, M., Zheng, J., Wang, Z.-L., 2006. 137C...\n2006\n1\n101.991,-9.997\n0xE6100000010C91D5AD9E93FE23C08195438B6C7F5940\n101.990556\n-9.997222\n5832\n\n\n5832\n73715\n1904\nIndian Ocean\n1\nSeawater\n106\nYamada et al., 2006\nhttps://www.zotero.org/groups/2432820/maris/it...\nNaN\nNaN\n...\nNaN\nNaN\nYamada, M., Zheng, J., Wang, Z.-L., 2006. 137C...\n2006\n1\n114.394,-18.496\n0xE6100000010C575BB1BFEC7E32C0F0A7C64B37995C40\n114.393889\n-18.495833\n5833\n\n\n5833\n73717\n1904\nIndian Ocean\n1\nSeawater\n106\nYamada et al., 2006\nhttps://www.zotero.org/groups/2432820/maris/it...\nNaN\nNaN\n...\nNaN\nNaN\nYamada, M., Zheng, J., Wang, Z.-L., 2006. 137C...\n2006\n1\n112.552,-23.066\n0xE6100000010C54742497FF1037C017D9CEF753235C40\n112.551667\n-23.066389\n5834\n\n\n\n\n5 rows × 80 columns",
    "crumbs": [
      "Handlers",
      "MARIS Legacy"
    ]
  },
  {
    "objectID": "handlers/maris_legacy.html#transform-data",
    "href": "handlers/maris_legacy.html#transform-data",
    "title": "MARIS Legacy",
    "section": "Transform data",
    "text": "Transform data\n\nSelect columns\n\n\nExported source\ncois_renaming_rules = {\n    'sample_id': 'SMP_ID',\n    'latitude': 'LAT',\n    'longitude': 'LON',\n    'begperiod': 'TIME',\n    'sampdepth': 'SMP_DEPTH',\n    'totdepth': 'TOT_DEPTH',\n    'station': 'STATION',\n    'uncertaint': 'UNC',\n    'unit_id': 'UNIT',\n    'detection': 'DL',\n    'area_id': 'AREA',\n    'species_id': 'SPECIES',\n    'biogroup_id': 'BIO_GROUP',\n    'bodypar_id': 'BODY_PART',\n    'sedtype_id': 'SED_TYPE',\n    'volume': 'VOL',\n    'salinity': 'SAL',\n    'temperatur': 'TEMP',\n    'sampmet_id': 'SAMP_MET',\n    'prepmet_id': 'PREP_MET',\n    'counmet_id': 'COUNT_MET',\n    'activity': 'VALUE',\n    'nuclide_id': 'NUCLIDE',\n    'sliceup': 'TOP',\n    'slicedown': 'BOTTOM'\n}\n\n\n\ndfs = dataloader(ref_id=ref_id)\ntfm = Transformer(dfs, cbs=[\n    SelectColumnsCB(cois_renaming_rules)\n    ])\n\nprint('Keys:', tfm().keys())\nprint('Columns:', tfm()['SEAWATER'].columns)\n\nKeys: dict_keys(['SEAWATER'])\nColumns: Index(['sample_id', 'latitude', 'longitude', 'begperiod', 'sampdepth',\n       'totdepth', 'station', 'uncertaint', 'unit_id', 'detection', 'area_id',\n       'species_id', 'biogroup_id', 'bodypar_id', 'sedtype_id', 'volume',\n       'salinity', 'temperatur', 'sampmet_id', 'prepmet_id', 'counmet_id',\n       'activity', 'nuclide_id', 'sliceup', 'slicedown'],\n      dtype='object')\n\n\n\n\nRename columns\n\ndfs = dataloader(ref_id=ref_id)\ntfm = Transformer(dfs, cbs=[\n    SelectColumnsCB(cois_renaming_rules),\n    RenameColumnsCB(cois_renaming_rules)\n    ])\n\ndfs_tfm = tfm()\nprint('Keys:', dfs_tfm.keys())\nprint('Columns:', dfs_tfm['SEAWATER'].columns)\n\nKeys: dict_keys(['SEAWATER'])\nColumns: Index(['SMP_ID', 'LAT', 'LON', 'TIME', 'SMP_DEPTH', 'TOT_DEPTH', 'STATION',\n       'UNC', 'UNIT', 'DL', 'AREA', 'SPECIES', 'BIO_GROUP', 'BODY_PART',\n       'SED_TYPE', 'VOL', 'SAL', 'TEMP', 'SAMP_MET', 'PREP_MET', 'COUNT_MET',\n       'VALUE', 'NUCLIDE', 'TOP', 'BOTTOM'],\n      dtype='object')\n\n\n\n\nCast STATION to str type\nThis is required for VLEN netcdf variables.\n\nsource\n\n\nCastStationToStringCB\n\n CastStationToStringCB ()\n\nConvert STATION column to string type, filling any missing values with empty string\n\n\nExported source\nclass CastStationToStringCB(Callback):\n    \"Convert STATION column to string type, filling any missing values with empty string\"\n    def __call__(self, tfm):\n        for k in tfm.dfs.keys():\n            if 'STATION' in tfm.dfs[k].columns:\n                tfm.dfs[k]['STATION'] = tfm.dfs[k]['STATION'].fillna('').astype('string')\n\n\n\ndfs = dataloader(ref_id=ref_id)\ntfm = Transformer(dfs, cbs=[\n    SelectColumnsCB(cois_renaming_rules),\n    RenameColumnsCB(cois_renaming_rules),\n    CastStationToStringCB()\n    ])\n\ndfs_tfm = tfm()\nprint('Keys:', dfs_tfm.keys())\nprint('Columns:', dfs_tfm['SEAWATER'].dtypes)\n\nKeys: dict_keys(['SEAWATER'])\nColumns: SMP_ID                int64\nLAT                 float64\nLON                 float64\nTIME                 object\nSMP_DEPTH           float64\nTOT_DEPTH           float64\nSTATION      string[python]\nUNC                 float64\nUNIT                  int64\nDL                   object\nAREA                  int64\nSPECIES               int64\nBIO_GROUP             int64\nBODY_PART             int64\nSED_TYPE              int64\nVOL                 float64\nSAL                 float64\nTEMP                float64\nSAMP_MET              int64\nPREP_MET              int64\nCOUNT_MET             int64\nVALUE               float64\nNUCLIDE               int64\nTOP                 float64\nBOTTOM              float64\ndtype: object\n\n\n\n\nDrop NaN only columns\nWe then remove columns containing only NaN values or ‘Not available’ (id=0 in MARIS lookup tables).\n\nsource\n\n\nDropNAColumnsCB\n\n DropNAColumnsCB (na_value=0)\n\nDrop variable containing only NaN or ‘Not available’ (id=0 in MARIS lookup tables).\n\n\nExported source\nclass DropNAColumnsCB(Callback):\n    \"Drop variable containing only NaN or 'Not available' (id=0 in MARIS lookup tables).\"\n    def __init__(self, na_value=0): fc.store_attr()\n    def isMarisNA(self, col): \n        return len(col.unique()) == 1 and col.iloc[0] == self.na_value\n    \n    def dropMarisNA(self, df):\n        na_cols = [col for col in df.columns if self.isMarisNA(df[col])]\n        return df.drop(labels=na_cols, axis=1)\n        \n    def __call__(self, tfm):\n        for k in tfm.dfs.keys():\n            tfm.dfs[k] = tfm.dfs[k].dropna(axis=1, how='all')\n            tfm.dfs[k] = self.dropMarisNA(tfm.dfs[k])\n\n\n\ndfs = dataloader(ref_id=ref_id)\ntfm = Transformer(dfs, cbs=[\n    SelectColumnsCB(cois_renaming_rules),\n    RenameColumnsCB(cois_renaming_rules),\n    CastStationToStringCB(),\n    DropNAColumnsCB()\n    ])\n\ndfs_tfm = tfm()\nprint('Keys:', dfs_tfm.keys())\nprint('Columns:', dfs_tfm['SEAWATER'].columns)\n\nKeys: dict_keys(['SEAWATER'])\nColumns: Index(['SMP_ID', 'LAT', 'LON', 'TIME', 'STATION', 'UNC', 'UNIT', 'DL', 'AREA',\n       'SAL', 'TEMP', 'VALUE', 'NUCLIDE'],\n      dtype='object')\n\n\n\n\nRemap detection limit values\nCategory-based NetCDF variables are encoded as integer values based on the MARIS lookup table dbo_detectlimit.xlsx. We recall that these lookup tables are included in the NetCDF file as custom enumeration types.\n\n\nExported source\ndl_name_to_id = lambda: get_lut(lut_path(), \n                                'dbo_detectlimit.xlsx', \n                                key='name', \n                                value='id')\n\n\n\ndl_name_to_id()\n\n{'Not applicable': -1, 'Not Available': 0, '=': 1, '&lt;': 2, 'ND': 3, 'DE': 4}\n\n\n\nsource\n\n\nSanitizeDetectionLimitCB\n\n SanitizeDetectionLimitCB (fn_lut=&lt;function &lt;lambda&gt;&gt;, dl_name='DL')\n\nAssign Detection Limit name to its id based on MARIS nomenclature.\n\n\nExported source\nclass SanitizeDetectionLimitCB(Callback):\n    \"Assign Detection Limit name to its id based on MARIS nomenclature.\"\n    def __init__(self,\n                 fn_lut=dl_name_to_id,\n                 dl_name='DL'):\n        fc.store_attr()\n\n    def __call__(self, tfm):\n        lut = self.fn_lut()\n        for k in tfm.dfs.keys():\n            tfm.dfs[k][self.dl_name] = tfm.dfs[k][self.dl_name].replace(lut)\n\n\n\ndfs = dataloader(ref_id=ref_id)\ntfm = Transformer(dfs, cbs=[\n    SelectColumnsCB(cois_renaming_rules),\n    RenameColumnsCB(cois_renaming_rules),\n    CastStationToStringCB(),\n    DropNAColumnsCB(),\n    SanitizeDetectionLimitCB()\n    ])\n\ndfs_tfm = tfm()\nprint('Keys:', dfs_tfm.keys())\nprint('Columns:', dfs_tfm['BIOTA'].columns)\nprint(f'{dfs_tfm[\"BIOTA\"][\"DL\"].unique()}')\nprint(f'{dfs_tfm[\"BIOTA\"].head()}')\n\nKeys: dict_keys(['BIOTA', 'SEAWATER', 'SEDIMENT'])\nColumns: Index(['SMP_ID', 'LAT', 'LON', 'TIME', 'SMP_DEPTH', 'STATION', 'UNC', 'UNIT',\n       'DL', 'AREA', 'SPECIES', 'BIO_GROUP', 'BODY_PART', 'PREP_MET',\n       'COUNT_MET', 'VALUE', 'NUCLIDE'],\n      dtype='object')\n[1 2]\n        SMP_ID    LAT        LON                     TIME  SMP_DEPTH STATION  \\\n603199  638133  57.25  12.083333  1986-01-31 00:00:00.000        0.0  RINGHA   \n603200  638133  57.25  12.083333  1986-01-31 00:00:00.000        0.0  RINGHA   \n603201  638134  57.25  12.083333  1986-02-28 00:00:00.000        0.0  RINGHA   \n603202  638134  57.25  12.083333  1986-02-28 00:00:00.000        0.0  RINGHA   \n603203  638134  57.25  12.083333  1986-02-28 00:00:00.000        0.0  RINGHA   \n\n         UNC  UNIT  DL  AREA  SPECIES  BIO_GROUP  BODY_PART  PREP_MET  \\\n603199  0.25     5   1  2374      129         14         19         7   \n603200  0.45     5   1  2374      129         14         19         7   \n603201  0.52     5   1  2374      129         14         19         7   \n603202  0.49     5   1  2374      129         14         19         7   \n603203  0.78     5   1  2374      129         14         19         7   \n\n        COUNT_MET  VALUE  NUCLIDE  \n603199         20    2.5       33  \n603200         20    4.5        9  \n603201         20    2.6       33  \n603202         20    4.9        9  \n603203         20    3.9       22  \n\n\n\n\nParse and encode time\nWe remind that in netCDF format time need to be encoded as integer representing the number of seconds since a time of reference. In our case we chose 1970-01-01 00:00:00.0 as defined in configs.ipynb.\n\nsource\n\n\nParseTimeCB\n\n ParseTimeCB (time_name='TIME')\n\nParse time column from MARIS dump.\n\n\nExported source\nclass ParseTimeCB(Callback):\n    \"Parse time column from MARIS dump.\"\n    def __init__(self,\n                 time_name='TIME'):\n        fc.store_attr()\n        \n    def __call__(self, tfm):\n        for k in tfm.dfs.keys():\n            tfm.dfs[k][self.time_name] = pd.to_datetime(tfm.dfs[k][self.time_name], format='ISO8601')\n\n\n\ndfs = dataloader(ref_id=ref_id)\ntfm = Transformer(dfs, cbs=[\n    SelectColumnsCB(cois_renaming_rules),\n    RenameColumnsCB(cois_renaming_rules),\n    CastStationToStringCB(),\n    DropNAColumnsCB(),\n    SanitizeDetectionLimitCB(),\n    ParseTimeCB(),\n    EncodeTimeCB()\n    ])\n\nprint(tfm()['BIOTA'])\n\n        SMP_ID        LAT        LON        TIME  SMP_DEPTH STATION      UNC  \\\n603199  638133  57.250000  12.083333   507513600        0.0  RINGHA  0.25000   \n603200  638133  57.250000  12.083333   507513600        0.0  RINGHA  0.45000   \n603201  638134  57.250000  12.083333   509932800        0.0  RINGHA  0.52000   \n603202  638134  57.250000  12.083333   509932800        0.0  RINGHA  0.49000   \n603203  638134  57.250000  12.083333   509932800        0.0  RINGHA  0.78000   \n...        ...        ...        ...         ...        ...     ...      ...   \n965909  639100  63.050000  21.616667   518572800        0.0   VAASA  0.01440   \n965910  639100  63.050000  21.616667   518572800        0.0   VAASA      NaN   \n965911  639137  63.066667  21.400000  1114732800        0.0   VAASA  1.46500   \n965912  639137  63.066667  21.400000  1114732800        0.0   VAASA  0.00204   \n965913  639137  63.066667  21.400000  1114732800        0.0   VAASA  5.00000   \n\n        UNIT  DL  AREA  SPECIES  BIO_GROUP  BODY_PART  PREP_MET  COUNT_MET  \\\n603199     5   1  2374      129         14         19         7         20   \n603200     5   1  2374      129         14         19         7         20   \n603201     5   1  2374      129         14         19         7         20   \n603202     5   1  2374      129         14         19         7         20   \n603203     5   1  2374      129         14         19         7         20   \n...      ...  ..   ...      ...        ...        ...       ...        ...   \n965909     5   1  9999      269          4         52        12          9   \n965910     5   1  9999      269          4         52        12          9   \n965911     5   1  9999      269          4         52         0         20   \n965912     5   1  9999      269          4         52         0          8   \n965913     5   1  9999      269          4         52         0         20   \n\n          VALUE  NUCLIDE  \n603199    2.500       33  \n603200    4.500        9  \n603201    2.600       33  \n603202    4.900        9  \n603203    3.900       22  \n...         ...      ...  \n965909    0.072       12  \n965910    0.015       11  \n965911   29.300       33  \n965912    0.017       12  \n965913  113.000        4  \n\n[14872 rows x 17 columns]\n\n\n\n\nSanitize coordinates\nWe ensure that coordinates are within the valid range.\n\ndfs = dataloader(ref_id=ref_id)\ntfm = Transformer(dfs, cbs=[\n    SelectColumnsCB(cois_renaming_rules),\n    RenameColumnsCB(cois_renaming_rules),\n    CastStationToStringCB(),\n    DropNAColumnsCB(),\n    SanitizeDetectionLimitCB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    SanitizeLonLatCB()\n    ])\n\ndfs_test = tfm()\ndfs_test['BIOTA']\n\n\n\n\n\n\n\n\nSMP_ID\nLAT\nLON\nTIME\nSMP_DEPTH\nSTATION\nUNC\nUNIT\nDL\nAREA\nSPECIES\nBIO_GROUP\nBODY_PART\nPREP_MET\nCOUNT_MET\nVALUE\nNUCLIDE\n\n\n\n\n603199\n638133\n57.250000\n12.083333\n507513600\n0.0\nRINGHA\n0.25000\n5\n1\n2374\n129\n14\n19\n7\n20\n2.500\n33\n\n\n603200\n638133\n57.250000\n12.083333\n507513600\n0.0\nRINGHA\n0.45000\n5\n1\n2374\n129\n14\n19\n7\n20\n4.500\n9\n\n\n603201\n638134\n57.250000\n12.083333\n509932800\n0.0\nRINGHA\n0.52000\n5\n1\n2374\n129\n14\n19\n7\n20\n2.600\n33\n\n\n603202\n638134\n57.250000\n12.083333\n509932800\n0.0\nRINGHA\n0.49000\n5\n1\n2374\n129\n14\n19\n7\n20\n4.900\n9\n\n\n603203\n638134\n57.250000\n12.083333\n509932800\n0.0\nRINGHA\n0.78000\n5\n1\n2374\n129\n14\n19\n7\n20\n3.900\n22\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n965909\n639100\n63.050000\n21.616667\n518572800\n0.0\nVAASA\n0.01440\n5\n1\n9999\n269\n4\n52\n12\n9\n0.072\n12\n\n\n965910\n639100\n63.050000\n21.616667\n518572800\n0.0\nVAASA\nNaN\n5\n1\n9999\n269\n4\n52\n12\n9\n0.015\n11\n\n\n965911\n639137\n63.066667\n21.400000\n1114732800\n0.0\nVAASA\n1.46500\n5\n1\n9999\n269\n4\n52\n0\n20\n29.300\n33\n\n\n965912\n639137\n63.066667\n21.400000\n1114732800\n0.0\nVAASA\n0.00204\n5\n1\n9999\n269\n4\n52\n0\n8\n0.017\n12\n\n\n965913\n639137\n63.066667\n21.400000\n1114732800\n0.0\nVAASA\n5.00000\n5\n1\n9999\n269\n4\n52\n0\n20\n113.000\n4\n\n\n\n\n14872 rows × 17 columns\n\n\n\n\n\nSet unique index\n\ndfs = dataloader(ref_id=ref_id)\ntfm = Transformer(dfs, cbs=[\n    SelectColumnsCB(cois_renaming_rules),\n    RenameColumnsCB(cois_renaming_rules),\n    CastStationToStringCB(),\n    DropNAColumnsCB(),\n    SanitizeDetectionLimitCB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    SanitizeLonLatCB(),\n    UniqueIndexCB()\n    ])\n\ndfs_test = tfm()    \ndfs_test['BIOTA']\n\n\n\n\n\n\n\n\nID\nSMP_ID\nLAT\nLON\nTIME\nSMP_DEPTH\nSTATION\nUNC\nUNIT\nDL\nAREA\nSPECIES\nBIO_GROUP\nBODY_PART\nPREP_MET\nCOUNT_MET\nVALUE\nNUCLIDE\n\n\n\n\n0\n0\n638133\n57.250000\n12.083333\n507513600\n0.0\nRINGHA\n0.25000\n5\n1\n2374\n129\n14\n19\n7\n20\n2.500\n33\n\n\n1\n1\n638133\n57.250000\n12.083333\n507513600\n0.0\nRINGHA\n0.45000\n5\n1\n2374\n129\n14\n19\n7\n20\n4.500\n9\n\n\n2\n2\n638134\n57.250000\n12.083333\n509932800\n0.0\nRINGHA\n0.52000\n5\n1\n2374\n129\n14\n19\n7\n20\n2.600\n33\n\n\n3\n3\n638134\n57.250000\n12.083333\n509932800\n0.0\nRINGHA\n0.49000\n5\n1\n2374\n129\n14\n19\n7\n20\n4.900\n9\n\n\n4\n4\n638134\n57.250000\n12.083333\n509932800\n0.0\nRINGHA\n0.78000\n5\n1\n2374\n129\n14\n19\n7\n20\n3.900\n22\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14867\n14867\n639100\n63.050000\n21.616667\n518572800\n0.0\nVAASA\n0.01440\n5\n1\n9999\n269\n4\n52\n12\n9\n0.072\n12\n\n\n14868\n14868\n639100\n63.050000\n21.616667\n518572800\n0.0\nVAASA\nNaN\n5\n1\n9999\n269\n4\n52\n12\n9\n0.015\n11\n\n\n14869\n14869\n639137\n63.066667\n21.400000\n1114732800\n0.0\nVAASA\n1.46500\n5\n1\n9999\n269\n4\n52\n0\n20\n29.300\n33\n\n\n14870\n14870\n639137\n63.066667\n21.400000\n1114732800\n0.0\nVAASA\n0.00204\n5\n1\n9999\n269\n4\n52\n0\n8\n0.017\n12\n\n\n14871\n14871\n639137\n63.066667\n21.400000\n1114732800\n0.0\nVAASA\n5.00000\n5\n1\n9999\n269\n4\n52\n0\n20\n113.000\n4\n\n\n\n\n14872 rows × 18 columns\n\n\n\n\ndfs = dataloader(ref_id=ref_id)\ntfm = Transformer(dfs, cbs=[\n    SelectColumnsCB(cois_renaming_rules),\n    RenameColumnsCB(cois_renaming_rules),\n    DropNAColumnsCB(),\n    SanitizeDetectionLimitCB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    SanitizeLonLatCB(),\n    UniqueIndexCB()\n    ])\n\ndfs_test = tfm()    \ndfs_test['BIOTA']\n\n\n\n\n\n\n\n\nID\nSMP_ID\nLAT\nLON\nTIME\nSMP_DEPTH\nSTATION\nUNC\nUNIT\nDL\nAREA\nSPECIES\nBIO_GROUP\nBODY_PART\nPREP_MET\nCOUNT_MET\nVALUE\nNUCLIDE\n\n\n\n\n0\n0\n638133\n57.250000\n12.083333\n507513600\n0.0\nRINGHA\n0.25000\n5\n1\n2374\n129\n14\n19\n7\n20\n2.500\n33\n\n\n1\n1\n638133\n57.250000\n12.083333\n507513600\n0.0\nRINGHA\n0.45000\n5\n1\n2374\n129\n14\n19\n7\n20\n4.500\n9\n\n\n2\n2\n638134\n57.250000\n12.083333\n509932800\n0.0\nRINGHA\n0.52000\n5\n1\n2374\n129\n14\n19\n7\n20\n2.600\n33\n\n\n3\n3\n638134\n57.250000\n12.083333\n509932800\n0.0\nRINGHA\n0.49000\n5\n1\n2374\n129\n14\n19\n7\n20\n4.900\n9\n\n\n4\n4\n638134\n57.250000\n12.083333\n509932800\n0.0\nRINGHA\n0.78000\n5\n1\n2374\n129\n14\n19\n7\n20\n3.900\n22\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n14867\n14867\n639100\n63.050000\n21.616667\n518572800\n0.0\nVAASA\n0.01440\n5\n1\n9999\n269\n4\n52\n12\n9\n0.072\n12\n\n\n14868\n14868\n639100\n63.050000\n21.616667\n518572800\n0.0\nVAASA\nNaN\n5\n1\n9999\n269\n4\n52\n12\n9\n0.015\n11\n\n\n14869\n14869\n639137\n63.066667\n21.400000\n1114732800\n0.0\nVAASA\n1.46500\n5\n1\n9999\n269\n4\n52\n0\n20\n29.300\n33\n\n\n14870\n14870\n639137\n63.066667\n21.400000\n1114732800\n0.0\nVAASA\n0.00204\n5\n1\n9999\n269\n4\n52\n0\n8\n0.017\n12\n\n\n14871\n14871\n639137\n63.066667\n21.400000\n1114732800\n0.0\nVAASA\n5.00000\n5\n1\n9999\n269\n4\n52\n0\n20\n113.000\n4\n\n\n\n\n14872 rows × 18 columns",
    "crumbs": [
      "Handlers",
      "MARIS Legacy"
    ]
  },
  {
    "objectID": "handlers/maris_legacy.html#encode-to-netcdf",
    "href": "handlers/maris_legacy.html#encode-to-netcdf",
    "title": "MARIS Legacy",
    "section": "Encode to NetCDF",
    "text": "Encode to NetCDF\n\ndfs = dataloader(ref_id=ref_id)\ntfm = Transformer(dfs, cbs=[\n    SelectColumnsCB(cois_renaming_rules),\n    RenameColumnsCB(cois_renaming_rules),\n    CastStationToStringCB(),\n    DropNAColumnsCB(),\n    SanitizeDetectionLimitCB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    SanitizeLonLatCB(),\n    UniqueIndexCB()\n    ])\n\ndfs_tfm = tfm()\ntfm.logs\n\n['Select columns of interest.',\n 'Renaming variables to MARIS standard names.',\n 'Convert STATION column to string type, filling any missing values with empty string',\n \"Drop variable containing only NaN or 'Not available' (id=0 in MARIS lookup tables).\",\n 'Assign Detection Limit name to its id based on MARIS nomenclature.',\n 'Parse time column from MARIS dump.',\n 'Encode time as seconds since epoch.',\n 'Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.',\n 'Set unique index for each group.']\n\n\n\nsource\n\nget_attrs\n\n get_attrs (tfm, zotero_key, kw=['oceanography', 'Earth Science &gt; Oceans &gt;\n            Ocean Chemistry&gt; Radionuclides', 'Earth Science &gt; Human\n            Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation\n            Exposure', 'Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean\n            Tracers, Earth Science &gt; Oceans &gt; Marine Sediments', 'Earth\n            Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt;\n            Sea Ice &gt; Isotopes', 'Earth Science &gt; Oceans &gt; Water Quality &gt;\n            Ocean Contaminants', 'Earth Science &gt; Biological\n            Classification &gt; Animals/Vertebrates &gt; Fish', 'Earth Science &gt;\n            Biosphere &gt; Ecosystems &gt; Marine Ecosystems', 'Earth Science &gt;\n            Biological Classification &gt; Animals/Invertebrates &gt; Mollusks',\n            'Earth Science &gt; Biological Classification &gt;\n            Animals/Invertebrates &gt; Arthropods &gt; Crustaceans', 'Earth\n            Science &gt; Biological Classification &gt; Plants &gt; Macroalgae\n            (Seaweeds)'])\n\nRetrieve global attributes from MARIS dump.\n\n\nExported source\nkw = ['oceanography', 'Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides',\n      'Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure',\n      'Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments',\n      'Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes',\n      'Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants',\n      'Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish',\n      'Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems',\n      'Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks',\n      'Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans',\n      'Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)']\n\n\n\n\nExported source\ndef get_attrs(tfm, zotero_key, kw=kw):\n    \"Retrieve global attributes from MARIS dump.\"\n    return GlobAttrsFeeder(tfm.dfs, cbs=[\n        BboxCB(),\n        DepthRangeCB(),\n        TimeRangeCB(),\n        ZoteroCB(zotero_key, cfg=cfg()),\n        KeyValuePairCB('keywords', ', '.join(kw)),\n        KeyValuePairCB('publisher_postprocess_logs', ', '.join(tfm.logs))\n        ])()\n\n\n\nget_attrs(tfm, zotero_key='3W354SQG', kw=kw)\n\n{'geospatial_lat_min': '137.8475',\n 'geospatial_lat_max': '31.2575',\n 'geospatial_lon_min': '88.9988888888889',\n 'geospatial_lon_max': '-42.3086111111111',\n 'geospatial_bounds': 'POLYGON ((88.9988888888889 -42.3086111111111, 137.8475 -42.3086111111111, 137.8475 31.2575, 88.9988888888889 31.2575, 88.9988888888889 -42.3086111111111))',\n 'time_coverage_start': '1996-12-20T00:00:00',\n 'time_coverage_end': '1997-02-11T00:00:00',\n 'id': '3W354SQG',\n 'title': 'Radioactivity Monitoring of the Irish Marine Environment 1991 and 1992',\n 'summary': '',\n 'creator_name': '[{\"creatorType\": \"author\", \"firstName\": \"A.\", \"lastName\": \"McGarry\"}, {\"creatorType\": \"author\", \"firstName\": \"S.\", \"lastName\": \"Lyons\"}, {\"creatorType\": \"author\", \"firstName\": \"C.\", \"lastName\": \"McEnri\"}, {\"creatorType\": \"author\", \"firstName\": \"T.\", \"lastName\": \"Ryan\"}, {\"creatorType\": \"author\", \"firstName\": \"M.\", \"lastName\": \"O\\'Colmain\"}, {\"creatorType\": \"author\", \"firstName\": \"J.D.\", \"lastName\": \"Cunningham\"}]',\n 'keywords': 'oceanography, Earth Science &gt; Oceans &gt; Ocean Chemistry&gt; Radionuclides, Earth Science &gt; Human Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation Exposure, Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean Tracers, Earth Science &gt; Oceans &gt; Marine Sediments, Earth Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt; Sea Ice &gt; Isotopes, Earth Science &gt; Oceans &gt; Water Quality &gt; Ocean Contaminants, Earth Science &gt; Biological Classification &gt; Animals/Vertebrates &gt; Fish, Earth Science &gt; Biosphere &gt; Ecosystems &gt; Marine Ecosystems, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Mollusks, Earth Science &gt; Biological Classification &gt; Animals/Invertebrates &gt; Arthropods &gt; Crustaceans, Earth Science &gt; Biological Classification &gt; Plants &gt; Macroalgae (Seaweeds)',\n 'publisher_postprocess_logs': \"Select columns of interest., Renaming variables to MARIS standard names., Convert STATION column to string type, filling any missing values with empty string, Drop variable containing only NaN or 'Not available' (id=0 in MARIS lookup tables)., Assign Detection Limit name to its id based on MARIS nomenclature., Parse time column from MARIS dump., Encode time as seconds since epoch., Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator., Set unique index for each group.\"}\n\n\n\nsource\n\n\nencode\n\n encode (fname_in:str, dir_dest:str, **kwargs)\n\nEncode MARIS dump to NetCDF.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfname_in\nstr\nPath to the MARIS dump data in CSV format\n\n\ndir_dest\nstr\nPath to the folder where the NetCDF output will be saved\n\n\nkwargs\nVAR_KEYWORD\n\n\n\n\n\n\nExported source\ndef encode(\n    fname_in: str, # Path to the MARIS dump data in CSV format\n    dir_dest: str, # Path to the folder where the NetCDF output will be saved\n    **kwargs # Additional keyword arguments\n    ):\n    \"Encode MARIS dump to NetCDF.\"\n    dataloader = DataLoader(fname_in)\n    ref_ids = kwargs.get('ref_ids')\n    if ref_ids is None:\n        ref_ids = dataloader.df.ref_id.unique()\n    print('Encoding ...')\n    for ref_id in tqdm(ref_ids, leave=False):\n        # if ref_id == 736: continue\n        dfs = dataloader(ref_id=ref_id)\n        print(get_fname(dfs))\n        tfm = Transformer(dfs, cbs=[\n            SelectColumnsCB(cois_renaming_rules),\n            RenameColumnsCB(cois_renaming_rules),\n            CastStationToStringCB(),\n            DropNAColumnsCB(),\n            SanitizeDetectionLimitCB(),\n            ParseTimeCB(),\n            EncodeTimeCB(),\n            SanitizeLonLatCB(),\n            UniqueIndexCB(),\n            ])\n        \n        tfm()\n        encoder = NetCDFEncoder(tfm.dfs, \n                                dest_fname=Path(dir_dest) / get_fname(dfs), \n                                global_attrs=get_attrs(tfm, zotero_key=get_zotero_key(dfs), kw=kw),\n                                verbose=kwargs.get('verbose', False)\n                                )\n        encoder.encode()\n\n\n\n\nSingle dataset\n\nref_id = 106\nencode(\n    fname_in,\n    dir_dest,\n    verbose=False, \n    ref_ids=[ref_id])\n\nEncoding ...\n\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n106.nc\n\n\n                                             \n\n\n\n\nAll datasets\n\nencode(\n    fname_in, \n    dir_dest, \n    ref_ids=None,\n    verbose=False)",
    "crumbs": [
      "Handlers",
      "MARIS Legacy"
    ]
  },
  {
    "objectID": "handlers/geotraces.html",
    "href": "handlers/geotraces.html",
    "title": "Geotraces",
    "section": "",
    "text": "This data pipeline, known as a “handler” in Marisco terminology, is designed to clean, standardize, and encode BODC Geotraces dataset into MARIS NetCDF format. The handler processes Geotraces data, applying various transformations and lookups to align it with MARIS data standards.\nKey functions of this handler:\nThis handler is a crucial component in the Marisco data processing workflow, ensuring Geotraces data is properly integrated into the MARIS database.\nThe present notebook pretends to be an instance of Literate Programming in the sense that it is a narrative that includes code snippets that are interspersed with explanations. When a function or a class needs to be exported in a dedicated python module (in our case marisco/handlers/geotraces.py) the code snippet is added to the module using #| exports as provided by the wonderful nbdev library.",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#configuration-file-paths",
    "href": "handlers/geotraces.html#configuration-file-paths",
    "title": "Geotraces",
    "section": "Configuration & file paths",
    "text": "Configuration & file paths\n\nfname_in: path to the folder containing the HELCOM data in CSV format. The path can be defined as a relative path.\nfname_out: path and filename for the NetCDF output.The path can be defined as a relative path.\nZotero key: used to retrieve attributes related to the dataset from Zotero. The MARIS datasets include a library available on Zotero.\n\n\n\nExported source\nfname_in = '../../_data/geotraces/GEOTRACES_IDP2021_v2/seawater/ascii/GEOTRACES_IDP2021_Seawater_Discrete_Sample_Data_v2.csv'\nfname_out = '../../_data/output/190-geotraces-2021.nc'\nzotero_key = '97UIMEXN'",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#load-data",
    "href": "handlers/geotraces.html#load-data",
    "title": "Geotraces",
    "section": "Load data",
    "text": "Load data\n\n\nExported source\nload_data = lambda fname: pd.read_csv(fname_in)\n\n\n\ndf = load_data(fname_in)\nprint(f'df shape: {df.shape}')\ndf.head()\n\ndf shape: (105417, 1188)\n\n\n\n\n\n\n\n\n\nCruise\nStation:METAVAR:INDEXED_TEXT\nType\nyyyy-mm-ddThh:mm:ss.sss\nLongitude [degrees_east]\nLatitude [degrees_north]\nBot. Depth [m]\nOperator's Cruise Name:METAVAR:INDEXED_TEXT\nShip Name:METAVAR:INDEXED_TEXT\nPeriod:METAVAR:INDEXED_TEXT\n...\nQV:SEADATANET.581\nCo_CELL_CONC_BOTTLE [amol/cell]\nQV:SEADATANET.582\nNi_CELL_CONC_BOTTLE [amol/cell]\nQV:SEADATANET.583\nCu_CELL_CONC_BOTTLE [amol/cell]\nQV:SEADATANET.584\nZn_CELL_CONC_BOTTLE [amol/cell]\nQV:SEADATANET.585\nQV:ODV:SAMPLE\n\n\n\n\n0\nGA01\n0\nB\n2014-05-17T22:29:00\n349.29999\n38.4329\n4854.0\nGEOVIDE\nPourquoi pas?\n15/05/2014 - 30/06/2014\n...\n9\nNaN\n9\nNaN\n9\nNaN\n9\nNaN\n9\n1\n\n\n1\nGA01\n0\nB\n2014-05-17T22:29:00\n349.29999\n38.4329\n4854.0\nGEOVIDE\nPourquoi pas?\n15/05/2014 - 30/06/2014\n...\n9\nNaN\n9\nNaN\n9\nNaN\n9\nNaN\n9\n1\n\n\n2\nGA01\n0\nB\n2014-05-17T22:29:00\n349.29999\n38.4329\n4854.0\nGEOVIDE\nPourquoi pas?\n15/05/2014 - 30/06/2014\n...\n9\nNaN\n9\nNaN\n9\nNaN\n9\nNaN\n9\n1\n\n\n3\nGA01\n0\nB\n2014-05-17T22:29:00\n349.29999\n38.4329\n4854.0\nGEOVIDE\nPourquoi pas?\n15/05/2014 - 30/06/2014\n...\n9\nNaN\n9\nNaN\n9\nNaN\n9\nNaN\n9\n1\n\n\n4\nGA01\n0\nB\n2014-05-17T22:29:00\n349.29999\n38.4329\n4854.0\nGEOVIDE\nPourquoi pas?\n15/05/2014 - 30/06/2014\n...\n9\nNaN\n9\nNaN\n9\nNaN\n9\nNaN\n9\n1\n\n\n\n\n5 rows × 1188 columns",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#select-columns-of-interest",
    "href": "handlers/geotraces.html#select-columns-of-interest",
    "title": "Geotraces",
    "section": "Select columns of interest",
    "text": "Select columns of interest\nWe select the columns of interest and in particular the elements of interest, in our case radionuclides.\n\nsource\n\nSelectColsOfInterestCB\n\n SelectColsOfInterestCB (common_coi, nuclides_pattern)\n\nSelect columns of interest.\n\n\nExported source\ncommon_coi = ['yyyy-mm-ddThh:mm:ss.sss', 'Longitude [degrees_east]',\n              'Latitude [degrees_north]', 'Bot. Depth [m]', 'DEPTH [m]', 'BODC Bottle Number:INTEGER']\n\nnuclides_pattern = ['^TRITI', '^Th_228', '^Th_23[024]', '^Pa_231', \n                    '^U_236_[DT]', '^Be_', '^Cs_137', '^Pb_210', '^Po_210',\n                    '^Ra_22[3468]', 'Np_237', '^Pu_239_[D]', '^Pu_240', '^Pu_239_Pu_240',\n                    '^I_129', '^Ac_227']  \n\nclass SelectColsOfInterestCB(Callback):\n    \"Select columns of interest.\"\n    def __init__(self, common_coi, nuclides_pattern): fc.store_attr()\n    def __call__(self, tfm):\n        nuc_of_interest = [c for c in tfm.df.columns if \n                           any(re.match(pattern, c) for pattern in self.nuclides_pattern)]\n\n        tfm.df = tfm.df[self.common_coi + nuc_of_interest]\n\n\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern)\n])\n\n\ndf_test = tfm()\nprint(f'df_test shape: {df_test.shape}')\ndf_test.head()\n\ndf_test shape: (105417, 86)\n\n\n\n\n\n\n\n\n\nyyyy-mm-ddThh:mm:ss.sss\nLongitude [degrees_east]\nLatitude [degrees_north]\nBot. Depth [m]\nDEPTH [m]\nBODC Bottle Number:INTEGER\nTRITIUM_D_CONC_BOTTLE [TU]\nCs_137_D_CONC_BOTTLE [uBq/kg]\nI_129_D_CONC_BOTTLE [atoms/kg]\nNp_237_D_CONC_BOTTLE [uBq/kg]\n...\nTh_230_TP_CONC_PUMP [uBq/kg]\nTh_230_SPT_CONC_PUMP [uBq/kg]\nTh_230_LPT_CONC_PUMP [uBq/kg]\nTh_232_TP_CONC_PUMP [pmol/kg]\nTh_232_SPT_CONC_PUMP [pmol/kg]\nTh_232_LPT_CONC_PUMP [pmol/kg]\nTh_234_SPT_CONC_PUMP [mBq/kg]\nTh_234_LPT_CONC_PUMP [mBq/kg]\nPo_210_TP_CONC_UWAY [mBq/kg]\nPb_210_TP_CONC_UWAY [mBq/kg]\n\n\n\n\n0\n2014-05-17T22:29:00\n349.29999\n38.4329\n4854.0\n2957.1\n1214048\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n2014-05-17T22:29:00\n349.29999\n38.4329\n4854.0\n2957.2\n1214039\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n2014-05-17T22:29:00\n349.29999\n38.4329\n4854.0\n2957.2\n1214027\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n2014-05-17T22:29:00\n349.29999\n38.4329\n4854.0\n2957.2\n1214018\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n2014-05-17T22:29:00\n349.29999\n38.4329\n4854.0\n2957.2\n1214036\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 86 columns\n\n\n\nBODC Bottle Number:INTEGER field allows to characterize uniquely a sample as shown below:\n\ncols_measurements = [col for col in df_test.columns if col not in common_coi]\n\nunique_key = ['BODC Bottle Number:INTEGER']\n\ndf_test.dropna(subset=cols_measurements, how='all', inplace=True);\nprint(f'df_test shape after dropping rows with no measurements: {df_test.shape}')\nprint(f'df_test duplicated keys: {df_test[unique_key].duplicated().sum()}')\n\ndf_test[df_test[unique_key].duplicated(keep=False)].sort_values(by=unique_key)\n\ndf_test shape after dropping rows with no measurements: (9389, 86)\ndf_test duplicated keys: 0\n\n\n\n\n\n\n\n\n\nyyyy-mm-ddThh:mm:ss.sss\nLongitude [degrees_east]\nLatitude [degrees_north]\nBot. Depth [m]\nDEPTH [m]\nBODC Bottle Number:INTEGER\nTRITIUM_D_CONC_BOTTLE [TU]\nCs_137_D_CONC_BOTTLE [uBq/kg]\nI_129_D_CONC_BOTTLE [atoms/kg]\nNp_237_D_CONC_BOTTLE [uBq/kg]\n...\nTh_230_TP_CONC_PUMP [uBq/kg]\nTh_230_SPT_CONC_PUMP [uBq/kg]\nTh_230_LPT_CONC_PUMP [uBq/kg]\nTh_232_TP_CONC_PUMP [pmol/kg]\nTh_232_SPT_CONC_PUMP [pmol/kg]\nTh_232_LPT_CONC_PUMP [pmol/kg]\nTh_234_SPT_CONC_PUMP [mBq/kg]\nTh_234_LPT_CONC_PUMP [mBq/kg]\nPo_210_TP_CONC_UWAY [mBq/kg]\nPb_210_TP_CONC_UWAY [mBq/kg]\n\n\n\n\n\n\n0 rows × 86 columns",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#reshape-wide-to-long",
    "href": "handlers/geotraces.html#reshape-wide-to-long",
    "title": "Geotraces",
    "section": "Reshape: wide to long",
    "text": "Reshape: wide to long\nSo that we can extract information such as sample methodology, filtering status, units included in Geotraces nuclides name.\n\nsource\n\nWideToLongCB\n\n WideToLongCB (common_coi, nuclides_pattern, var_name='NUCLIDE',\n               value_name='VALUE')\n\nGet Geotraces nuclide names as values not column names to extract contained information (unit, sampling method, …).\n\n\nExported source\nclass WideToLongCB(Callback):\n    \"\"\"\n    Get Geotraces nuclide names as values not column names \n    to extract contained information (unit, sampling method, ...).\n    \"\"\"\n    def __init__(self, common_coi, nuclides_pattern, \n                 var_name='NUCLIDE', value_name='VALUE'): \n        fc.store_attr()\n        \n    def __call__(self, tfm):\n        nuc_of_interest = [c for c in tfm.df.columns if \n                           any(re.match(pattern, c) for pattern in self.nuclides_pattern)]\n        tfm.df = pd.melt(tfm.df, id_vars=self.common_coi, value_vars=nuc_of_interest, \n                          var_name=self.var_name, value_name=self.value_name)\n        tfm.df.dropna(subset=self.value_name, inplace=True)\n\n\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern)\n])\ndf_test = tfm()\nprint(f'df_test shape: {df_test.shape}')\ndf_test.head()\n\ndf_test shape: (26745, 8)\n\n\n\n\n\n\n\n\n\nyyyy-mm-ddThh:mm:ss.sss\nLongitude [degrees_east]\nLatitude [degrees_north]\nBot. Depth [m]\nDEPTH [m]\nBODC Bottle Number:INTEGER\nNUCLIDE\nVALUE\n\n\n\n\n9223\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n17.8\n842525\nTRITIUM_D_CONC_BOTTLE [TU]\n0.733\n\n\n9231\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n34.7\n842528\nTRITIUM_D_CONC_BOTTLE [TU]\n0.696\n\n\n9237\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n67.5\n842531\nTRITIUM_D_CONC_BOTTLE [TU]\n0.718\n\n\n9244\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n91.9\n842534\nTRITIUM_D_CONC_BOTTLE [TU]\n0.709\n\n\n9256\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n136.6\n842540\nTRITIUM_D_CONC_BOTTLE [TU]\n0.692",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#extract",
    "href": "handlers/geotraces.html#extract",
    "title": "Geotraces",
    "section": "Extract",
    "text": "Extract\nUnit, Filtering status and Sampling method are extracted from column names as embedded in Geotraces data source.\n\nUnit\n\nsource\n\n\nExtractUnitCB\n\n ExtractUnitCB (var_name='NUCLIDE')\n\nExtract units from nuclide names.\n\n\nExported source\nclass ExtractUnitCB(Callback):\n    \"\"\"\n    Extract units from nuclide names.\n    \"\"\"\n    def __init__(self, var_name='NUCLIDE'): \n        fc.store_attr()\n        self.unit_col_name = 'UNIT'\n\n    def extract_unit(self, s):\n        match = re.search(r'\\[(.*?)\\]', s)\n        return match.group(1) if match else None\n        \n    def __call__(self, tfm):\n        tfm.df[self.unit_col_name] = tfm.df[self.var_name].apply(self.extract_unit)\n\n\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB()\n])\n\ndf_test = tfm()\ndf_test.head()\n\n\n\n\n\n\n\n\nyyyy-mm-ddThh:mm:ss.sss\nLongitude [degrees_east]\nLatitude [degrees_north]\nBot. Depth [m]\nDEPTH [m]\nBODC Bottle Number:INTEGER\nNUCLIDE\nVALUE\nUNIT\n\n\n\n\n9223\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n17.8\n842525\nTRITIUM_D_CONC_BOTTLE [TU]\n0.733\nTU\n\n\n9231\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n34.7\n842528\nTRITIUM_D_CONC_BOTTLE [TU]\n0.696\nTU\n\n\n9237\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n67.5\n842531\nTRITIUM_D_CONC_BOTTLE [TU]\n0.718\nTU\n\n\n9244\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n91.9\n842534\nTRITIUM_D_CONC_BOTTLE [TU]\n0.709\nTU\n\n\n9256\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n136.6\n842540\nTRITIUM_D_CONC_BOTTLE [TU]\n0.692\nTU\n\n\n\n\n\n\n\n\n\nFiltering status\n\nsource\n\n\nExtractFilteringStatusCB\n\n ExtractFilteringStatusCB (phase, var_name='NUCLIDE')\n\nExtract filtering status from nuclide names.\n\n\nExported source\nphase = {\n    'D': {'FILT': 1, 'group': 'SEAWATER'},\n    'T': {'FILT': 2, 'group': 'SEAWATER'},\n    'TP': {'FILT': 1, 'group': 'SUSPENDED_MATTER'}, \n    'LPT': {'FILT': 1, 'group': 'SUSPENDED_MATTER'},\n    'SPT': {'FILT': 1, 'group': 'SUSPENDED_MATTER'}}\n\n\n\n\nExported source\nclass ExtractFilteringStatusCB(Callback):\n    \"Extract filtering status from nuclide names.\"\n    def __init__(self, phase, var_name='NUCLIDE'): \n        fc.store_attr()\n        # self.filt_col_name = cdl_cfg()['vars']['suffixes']['filtered']['name']\n        self.filt_col_name = 'FILT'\n\n    def extract_filt_status(self, s):\n        matched_string = self.match(s)\n        return self.phase[matched_string.group(1)][self.filt_col_name] if matched_string else None\n\n    def match(self, s):\n        return re.search(r'_(' + '|'.join(self.phase.keys()) + ')_', s)\n        \n    def extract_group(self, s):\n        matched_string = self.match(s)\n        return self.phase[matched_string.group(1)]['group'] if matched_string else None\n        \n    def __call__(self, tfm):\n        tfm.df[self.filt_col_name] = tfm.df[self.var_name].apply(self.extract_filt_status)\n        tfm.df['GROUP'] = tfm.df[self.var_name].apply(self.extract_group)\n\n\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase)\n])\n\ndf_test = tfm()\ndf_test.head()\n\n\n\n\n\n\n\n\nyyyy-mm-ddThh:mm:ss.sss\nLongitude [degrees_east]\nLatitude [degrees_north]\nBot. Depth [m]\nDEPTH [m]\nBODC Bottle Number:INTEGER\nNUCLIDE\nVALUE\nUNIT\nFILT\nGROUP\n\n\n\n\n9223\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n17.8\n842525\nTRITIUM_D_CONC_BOTTLE [TU]\n0.733\nTU\n1\nSEAWATER\n\n\n9231\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n34.7\n842528\nTRITIUM_D_CONC_BOTTLE [TU]\n0.696\nTU\n1\nSEAWATER\n\n\n9237\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n67.5\n842531\nTRITIUM_D_CONC_BOTTLE [TU]\n0.718\nTU\n1\nSEAWATER\n\n\n9244\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n91.9\n842534\nTRITIUM_D_CONC_BOTTLE [TU]\n0.709\nTU\n1\nSEAWATER\n\n\n9256\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n136.6\n842540\nTRITIUM_D_CONC_BOTTLE [TU]\n0.692\nTU\n1\nSEAWATER\n\n\n\n\n\n\n\n\n\nSampling method\n\nsource\n\n\nExtractSamplingMethodCB\n\n ExtractSamplingMethodCB (smp_method:dict={'BOTTLE': 1, 'FISH': 18,\n                          'PUMP': 14, 'UWAY': 24}, var_name='NUCLIDE',\n                          smp_method_col_name='SAMP_MET')\n\nExtract sampling method from nuclide names.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsmp_method\ndict\n{‘BOTTLE’: 1, ‘FISH’: 18, ‘PUMP’: 14, ‘UWAY’: 24}\nSampling method lookup table\n\n\nvar_name\nstr\nNUCLIDE\nColumn name containing nuclide names\n\n\nsmp_method_col_name\nstr\nSAMP_MET\nColumn name for sampling method in output df\n\n\n\n\n\nExported source\n# To be validated\nsmp_method = {\n    'BOTTLE': 1,\n    'FISH': 18,\n    'PUMP': 14,\n    'UWAY': 24}\n\n\n\n\nExported source\nclass ExtractSamplingMethodCB(Callback):\n    \"Extract sampling method from nuclide names.\"\n    def __init__(self, \n                 smp_method:dict = smp_method, # Sampling method lookup table\n                 var_name='NUCLIDE', # Column name containing nuclide names\n                 smp_method_col_name = 'SAMP_MET' # Column name for sampling method in output df\n                 ): \n        fc.store_attr()\n\n    def extract_smp_method(self, s):\n        match = re.search(r'_(' + '|'.join(self.smp_method.keys()) + ') ', s)\n        return self.smp_method[match.group(1)] if match else None\n        \n    def __call__(self, tfm):\n        tfm.df[self.smp_method_col_name] = tfm.df[self.var_name].apply(self.extract_smp_method)\n\n\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method)\n])\n\ndf_test = tfm()\ndf_test.head()\n\n\n\n\n\n\n\n\nyyyy-mm-ddThh:mm:ss.sss\nLongitude [degrees_east]\nLatitude [degrees_north]\nBot. Depth [m]\nDEPTH [m]\nBODC Bottle Number:INTEGER\nNUCLIDE\nVALUE\nUNIT\nFILT\nGROUP\nSAMP_MET\n\n\n\n\n9223\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n17.8\n842525\nTRITIUM_D_CONC_BOTTLE [TU]\n0.733\nTU\n1\nSEAWATER\n1\n\n\n9231\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n34.7\n842528\nTRITIUM_D_CONC_BOTTLE [TU]\n0.696\nTU\n1\nSEAWATER\n1\n\n\n9237\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n67.5\n842531\nTRITIUM_D_CONC_BOTTLE [TU]\n0.718\nTU\n1\nSEAWATER\n1\n\n\n9244\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n91.9\n842534\nTRITIUM_D_CONC_BOTTLE [TU]\n0.709\nTU\n1\nSEAWATER\n1\n\n\n9256\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n136.6\n842540\nTRITIUM_D_CONC_BOTTLE [TU]\n0.692\nTU\n1\nSEAWATER\n1",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#remap-to-maris-nuclide-names",
    "href": "handlers/geotraces.html#remap-to-maris-nuclide-names",
    "title": "Geotraces",
    "section": "Remap to MARIS nuclide names",
    "text": "Remap to MARIS nuclide names\nWe normalize the nuclide names to MARIS standard for further lookup.\n\nsource\n\nRenameNuclideCB\n\n RenameNuclideCB (nuclides_name, var_name='NUCLIDE')\n\nRemap nuclides name to MARIS standard.\n\n\nExported source\nnuclides_name = {'TRITIUM': 'h3', 'Pu_239_Pu_240': 'pu239_240_tot'}\n\n\n\n\nExported source\nclass RenameNuclideCB(Callback):\n    \"Remap nuclides name to MARIS standard.\"\n    def __init__(self, nuclides_name, var_name='NUCLIDE'): \n        fc.store_attr()\n        self.patterns = ['_D', '_T', '_TP', '_LPT', '_SPT']\n\n    def extract_nuclide_name(self, s):\n        match = re.search(r'(.*?)(' + '|'.join(self.patterns) + ')', s)\n        return match.group(1) if match else None\n\n    def standardize_name(self, s):\n        s = self.extract_nuclide_name(s)\n        return self.nuclides_name[s] if s in self.nuclides_name else s.lower().replace('_', '')\n        \n    def __call__(self, tfm):\n        tfm.df[self.var_name] = tfm.df[self.var_name].apply(self.standardize_name)\n\n\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method),\n    RenameNuclideCB(nuclides_name)\n])\n\ndf_test = tfm()\ndf_test.head()\n\n\n\n\n\n\n\n\nyyyy-mm-ddThh:mm:ss.sss\nLongitude [degrees_east]\nLatitude [degrees_north]\nBot. Depth [m]\nDEPTH [m]\nBODC Bottle Number:INTEGER\nNUCLIDE\nVALUE\nUNIT\nFILT\nGROUP\nSAMP_MET\n\n\n\n\n9223\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n17.8\n842525\nh3\n0.733\nTU\n1\nSEAWATER\n1\n\n\n9231\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n34.7\n842528\nh3\n0.696\nTU\n1\nSEAWATER\n1\n\n\n9237\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n67.5\n842531\nh3\n0.718\nTU\n1\nSEAWATER\n1\n\n\n9244\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n91.9\n842534\nh3\n0.709\nTU\n1\nSEAWATER\n1\n\n\n9256\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n136.6\n842540\nh3\n0.692\nTU\n1\nSEAWATER\n1\n\n\n\n\n\n\n\n\ndf_test.NUCLIDE.unique()\n\narray(['h3', 'cs137', 'i129', 'np237', 'pu239', 'pu239_240_tot', 'pu240',\n       'u236', 'pa231', 'pb210', 'po210', 'ra224', 'ra226', 'ra228',\n       'th230', 'th232', 'th234', 'ac227', 'be7', 'ra223', 'th228'],\n      dtype=object)\n\n\n\n\n\n\n\n\nImportantFEEDBACK TO DATA PROVIDER\n\n\n\nNote that several measurements are negative as shown below. Further clarification is needed.\n\n\n\ndf_test[df_test.VALUE &lt; 0].groupby('NUCLIDE').size()\n\nNUCLIDE\nh3       71\npa231     3\nth228    22\nth230     1\nth232     6\ndtype: int64",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#standardize-unit",
    "href": "handlers/geotraces.html#standardize-unit",
    "title": "Geotraces",
    "section": "Standardize unit",
    "text": "Standardize unit\nHere below unit values used by Geotraces data source. We need to remap (sometimes convert) them to MARIS standard.\n\ndf_test['UNIT'].unique()\n\narray(['TU', 'uBq/kg', 'atoms/kg', 'mBq/kg', 'pmol/kg'], dtype=object)\n\n\n\nsource\n\nStandardizeUnitCB\n\n StandardizeUnitCB (units_lut, unit_col_name='UNIT', var_name='VALUE')\n\nRemap unit to MARIS standard ones and apply conversion where needed.\n\n\nExported source\nunits_lut = {\n    'TU': {'id': 7, 'factor': 1},\n    'uBq/kg': {'id': 3, 'factor': 1e-6},\n    'atoms/kg': {'id': 9, 'factor': 1},\n    'mBq/kg': {'id': 3, 'factor': 1e-3},\n    'pmol/kg': {'id': 9, 'factor': 1e-12 * AVOGADRO}\n    }\n\n\n\n\nExported source\nclass StandardizeUnitCB(Callback):\n    \"Remap unit to MARIS standard ones and apply conversion where needed.\"\n    def __init__(self, \n                 units_lut, \n                 unit_col_name='UNIT',\n                 var_name='VALUE'): \n        fc.store_attr()\n        # self.unit_col_name = cdl_cfg()['vars']['suffixes']['unit']['name']\n        \n    def __call__(self, tfm):\n        # Convert/rescale values\n        tfm.df[self.var_name] *= tfm.df[self.unit_col_name].map(\n            {k: v['factor'] for k, v in self.units_lut.items()})\n        \n        # Match MARIS unit id\n        tfm.df[self.unit_col_name] = tfm.df[self.unit_col_name].map(\n            {k: v['id'] for k, v in self.units_lut.items()})\n\n\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method),\n    RenameNuclideCB(nuclides_name),\n    StandardizeUnitCB(units_lut)\n])\n\ndf_test = tfm()\nprint(f'df_test.UNIT.unique(): {df_test.UNIT.unique()}')\n\ndf_test.UNIT.unique(): [7 3 9]",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#rename-common-columns",
    "href": "handlers/geotraces.html#rename-common-columns",
    "title": "Geotraces",
    "section": "Rename common columns",
    "text": "Rename common columns\nWe rename the common columns to MARIS standard names before NetCDF encoding.\n\nsource\n\nRenameColumnCB\n\n RenameColumnCB (lut={'yyyy-mm-ddThh:mm:ss.sss': 'TIME', 'Longitude\n                 [degrees_east]': 'LON', 'Latitude [degrees_north]':\n                 'LAT', 'DEPTH [m]': 'SMP_DEPTH', 'Bot. Depth [m]':\n                 'TOT_DEPTH', 'BODC Bottle Number:INTEGER': 'SMP_ID'})\n\nRenaming variables to MARIS standard names.\n\n\nExported source\nrenaming_rules = {\n    'yyyy-mm-ddThh:mm:ss.sss': 'TIME',\n    'Longitude [degrees_east]': 'LON',\n    'Latitude [degrees_north]': 'LAT',\n    'DEPTH [m]': 'SMP_DEPTH',\n    'Bot. Depth [m]': 'TOT_DEPTH',\n    'BODC Bottle Number:INTEGER': 'SMP_ID'\n}\n\n\n\n\nExported source\nclass RenameColumnCB(Callback):\n    \"Renaming variables to MARIS standard names.\"\n    def __init__(self, lut=renaming_rules): fc.store_attr()\n    def __call__(self, tfm):\n        # lut = self.renaming_rules()\n        new_col_names = [self.lut[name] if name in self.lut else name for name in tfm.df.columns]\n        tfm.df.columns = new_col_names\n\n\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method),\n    RenameNuclideCB(nuclides_name),\n    StandardizeUnitCB(units_lut),\n    RenameColumnCB(renaming_rules)\n])\n\ndf_test = tfm()\ndf_test.head()\n\n\n\n\n\n\n\n\nTIME\nLON\nLAT\nTOT_DEPTH\nSMP_DEPTH\nSMP_ID\nNUCLIDE\nVALUE\nUNIT\nFILT\nGROUP\nSAMP_MET\n\n\n\n\n9223\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n17.8\n842525\nh3\n0.733\n7\n1\nSEAWATER\n1\n\n\n9231\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n34.7\n842528\nh3\n0.696\n7\n1\nSEAWATER\n1\n\n\n9237\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n67.5\n842531\nh3\n0.718\n7\n1\nSEAWATER\n1\n\n\n9244\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n91.9\n842534\nh3\n0.709\n7\n1\nSEAWATER\n1\n\n\n9256\n2010-10-17T00:13:29\n350.33792\n38.3271\n2827.0\n136.6\n842540\nh3\n0.692\n7\n1\nSEAWATER\n1",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#unshift-longitudes",
    "href": "handlers/geotraces.html#unshift-longitudes",
    "title": "Geotraces",
    "section": "Unshift longitudes",
    "text": "Unshift longitudes\nIn Geotraces, longitudes are coded between 0 and 360 in Geotraces. We rescale it between -180 and 180 instead.\n\nsource\n\nUnshiftLongitudeCB\n\n UnshiftLongitudeCB (lon_col_name='LON')\n\nLongitudes are coded between 0 and 360 in Geotraces. We rescale it between -180 and 180 instead.\n\n\nExported source\nclass UnshiftLongitudeCB(Callback):\n    \"Longitudes are coded between 0 and 360 in Geotraces. We rescale it between -180 and 180 instead.\"\n    def __init__(self, lon_col_name='LON'): \n        fc.store_attr()\n    def __call__(self, tfm):\n        tfm.df[self.lon_col_name] = tfm.df[self.lon_col_name] - 180\n\n\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method),\n    RenameNuclideCB(nuclides_name),\n    StandardizeUnitCB(units_lut),\n    RenameColumnCB(renaming_rules),\n    UnshiftLongitudeCB()\n])\n\ndf_test = tfm()\ndf_test.head()\n\n\n\n\n\n\n\n\nTIME\nLON\nLAT\nTOT_DEPTH\nSMP_DEPTH\nSMP_ID\nNUCLIDE\nVALUE\nUNIT\nFILT\nGROUP\nSAMP_MET\n\n\n\n\n9223\n2010-10-17T00:13:29\n170.33792\n38.3271\n2827.0\n17.8\n842525\nh3\n0.733\n7\n1\nSEAWATER\n1\n\n\n9231\n2010-10-17T00:13:29\n170.33792\n38.3271\n2827.0\n34.7\n842528\nh3\n0.696\n7\n1\nSEAWATER\n1\n\n\n9237\n2010-10-17T00:13:29\n170.33792\n38.3271\n2827.0\n67.5\n842531\nh3\n0.718\n7\n1\nSEAWATER\n1\n\n\n9244\n2010-10-17T00:13:29\n170.33792\n38.3271\n2827.0\n91.9\n842534\nh3\n0.709\n7\n1\nSEAWATER\n1\n\n\n9256\n2010-10-17T00:13:29\n170.33792\n38.3271\n2827.0\n136.6\n842540\nh3\n0.692\n7\n1\nSEAWATER\n1\n\n\n\n\n\n\n\n\nnp.min(df_test.LON), np.max(df_test.LON)\n\n(-180.0, 179.9986)",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#dispatch-to-groups",
    "href": "handlers/geotraces.html#dispatch-to-groups",
    "title": "Geotraces",
    "section": "Dispatch to groups",
    "text": "Dispatch to groups\nWe encode each sample type (seawater, suspended matter, …) into a dedicated dataframe as each sample type is further encoded as NetCDF group.\n\nsource\n\nDispatchToGroupCB\n\n DispatchToGroupCB (group_name='GROUP')\n\nConvert to a dictionary of dataframe with sample type (seawater,…) as keys.\n\n\nExported source\nclass DispatchToGroupCB(Callback):\n    \"Convert to a dictionary of dataframe with sample type (seawater,...) as keys.\"\n    def __init__(self, group_name='GROUP'): \n        fc.store_attr()\n        \n    def __call__(self, tfm):\n        tfm.dfs = dict(tuple(tfm.df.groupby(self.group_name)))\n        for key in tfm.dfs:\n            tfm.dfs[key] = tfm.dfs[key].drop(self.group_name, axis=1)\n\n\n\ndf = pd.read_csv(fname_in)\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method),\n    RenameNuclideCB(nuclides_name),\n    StandardizeUnitCB(units_lut),\n    RenameColumnCB(renaming_rules),\n    UnshiftLongitudeCB(),\n    DispatchToGroupCB()\n])\n\ndfs_test = tfm()\nprint(f'dfs_test keys: {dfs_test.keys()}')\nprint(dfs_test['SEAWATER'].head())\n\ndfs_test keys: dict_keys(['SEAWATER', 'SUSPENDED_MATTER'])\n                     TIME        LON      LAT  TOT_DEPTH  SMP_DEPTH  SMP_ID  \\\n9223  2010-10-17T00:13:29  170.33792  38.3271     2827.0       17.8  842525   \n9231  2010-10-17T00:13:29  170.33792  38.3271     2827.0       34.7  842528   \n9237  2010-10-17T00:13:29  170.33792  38.3271     2827.0       67.5  842531   \n9244  2010-10-17T00:13:29  170.33792  38.3271     2827.0       91.9  842534   \n9256  2010-10-17T00:13:29  170.33792  38.3271     2827.0      136.6  842540   \n\n     NUCLIDE  VALUE  UNIT  FILT  SAMP_MET  \n9223      h3  0.733     7     1         1  \n9231      h3  0.696     7     1         1  \n9237      h3  0.718     7     1         1  \n9244      h3  0.709     7     1         1  \n9256      h3  0.692     7     1         1",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#parse-time",
    "href": "handlers/geotraces.html#parse-time",
    "title": "Geotraces",
    "section": "Parse time",
    "text": "Parse time\nWe parse the time column to datetime format.\n\nsource\n\nParseTimeCB\n\n ParseTimeCB ()\n\nBase class for callbacks.\n\ndf = pd.read_csv(fname_in)\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method),\n    RenameNuclideCB(nuclides_name),\n    StandardizeUnitCB(units_lut),\n    RenameColumnCB(renaming_rules),\n    UnshiftLongitudeCB(),\n    DispatchToGroupCB(),\n    ParseTimeCB()\n])\n\ndfs_test = tfm()\nprint('time data type: ', dfs_test['SEAWATER'].TIME.dtype)\nprint(dfs_test['SEAWATER'].head())\n\ntime data type:  datetime64[ns]\n                    TIME        LON      LAT  TOT_DEPTH  SMP_DEPTH  SMP_ID  \\\n9223 2010-10-17 00:13:29  170.33792  38.3271     2827.0       17.8  842525   \n9231 2010-10-17 00:13:29  170.33792  38.3271     2827.0       34.7  842528   \n9237 2010-10-17 00:13:29  170.33792  38.3271     2827.0       67.5  842531   \n9244 2010-10-17 00:13:29  170.33792  38.3271     2827.0       91.9  842534   \n9256 2010-10-17 00:13:29  170.33792  38.3271     2827.0      136.6  842540   \n\n     NUCLIDE  VALUE  UNIT  FILT  SAMP_MET  \n9223      h3  0.733     7     1         1  \n9231      h3  0.696     7     1         1  \n9237      h3  0.718     7     1         1  \n9244      h3  0.709     7     1         1  \n9256      h3  0.692     7     1         1",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#encode-time-seconds-since",
    "href": "handlers/geotraces.html#encode-time-seconds-since",
    "title": "Geotraces",
    "section": "Encode time (seconds since …)",
    "text": "Encode time (seconds since …)\nThen encode it to seconds since 1970-01-01 as specified in MARIS NetCDF CDL and template.\n\ndf = pd.read_csv(fname_in)\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method),\n    RenameNuclideCB(nuclides_name),\n    StandardizeUnitCB(units_lut),\n    RenameColumnCB(renaming_rules),\n    UnshiftLongitudeCB(),\n    DispatchToGroupCB(),\n    ParseTimeCB(),\n    EncodeTimeCB()\n])\n\ndfs_test = tfm()['SEAWATER']\ndfs_test.head()\n\n\n\n\n\n\n\n\nTIME\nLON\nLAT\nTOT_DEPTH\nSMP_DEPTH\nSMP_ID\nNUCLIDE\nVALUE\nUNIT\nFILT\nSAMP_MET\n\n\n\n\n9223\n1287274409\n170.33792\n38.3271\n2827.0\n17.8\n842525\nh3\n0.733\n7\n1\n1\n\n\n9231\n1287274409\n170.33792\n38.3271\n2827.0\n34.7\n842528\nh3\n0.696\n7\n1\n1\n\n\n9237\n1287274409\n170.33792\n38.3271\n2827.0\n67.5\n842531\nh3\n0.718\n7\n1\n1\n\n\n9244\n1287274409\n170.33792\n38.3271\n2827.0\n91.9\n842534\nh3\n0.709\n7\n1\n1\n\n\n9256\n1287274409\n170.33792\n38.3271\n2827.0\n136.6\n842540\nh3\n0.692\n7\n1\n1",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#sanitize-coordinates",
    "href": "handlers/geotraces.html#sanitize-coordinates",
    "title": "Geotraces",
    "section": "Sanitize coordinates",
    "text": "Sanitize coordinates\n\ndf = pd.read_csv(fname_in)\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method),\n    RenameNuclideCB(nuclides_name),\n    StandardizeUnitCB(units_lut),\n    RenameColumnCB(renaming_rules),\n    UnshiftLongitudeCB(),\n    DispatchToGroupCB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    SanitizeLonLatCB()\n])\ndfs_test = tfm()\ndfs_test['SEAWATER'].head()\n\n\n\n\n\n\n\n\nTIME\nLON\nLAT\nTOT_DEPTH\nSMP_DEPTH\nSMP_ID\nNUCLIDE\nVALUE\nUNIT\nFILT\nSAMP_MET\n\n\n\n\n9223\n1287274409\n170.33792\n38.3271\n2827.0\n17.8\n842525\nh3\n0.733\n7\n1\n1\n\n\n9231\n1287274409\n170.33792\n38.3271\n2827.0\n34.7\n842528\nh3\n0.696\n7\n1\n1\n\n\n9237\n1287274409\n170.33792\n38.3271\n2827.0\n67.5\n842531\nh3\n0.718\n7\n1\n1\n\n\n9244\n1287274409\n170.33792\n38.3271\n2827.0\n91.9\n842534\nh3\n0.709\n7\n1\n1\n\n\n9256\n1287274409\n170.33792\n38.3271\n2827.0\n136.6\n842540\nh3\n0.692\n7\n1\n1",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#remap-nuclides-name-to-id",
    "href": "handlers/geotraces.html#remap-nuclides-name-to-id",
    "title": "Geotraces",
    "section": "Remap nuclides name to id",
    "text": "Remap nuclides name to id\nAll MARIS lookup tables are embeded in the NetCDF file as enumeration types. Data itself is encoded as integer for the sake of space efficiency. We need to remap it to the corresponding MARIS nuclide id.\n\n\nExported source\nlut_nuclides = lambda: get_lut(lut_path(), 'dbo_nuclide.xlsx', \n                               key='nc_name', value='nuclide_id', reverse=False)\n\n\n\nlut_nuclides()\n\n{'NOT APPLICABLE': -1,\n 'NOT AVAILABLE': 0,\n 'h3': 1,\n 'be7': 2,\n 'c14': 3,\n 'k40': 4,\n 'cr51': 5,\n 'mn54': 6,\n 'co57': 7,\n 'co58': 8,\n 'co60': 9,\n 'zn65': 10,\n 'sr89': 11,\n 'sr90': 12,\n 'zr95': 13,\n 'nb95': 14,\n 'tc99': 15,\n 'ru103': 16,\n 'ru106': 17,\n 'rh106': 18,\n 'ag106m': 19,\n 'ag108': 20,\n 'ag108m': 21,\n 'ag110m': 22,\n 'sb124': 23,\n 'sb125': 24,\n 'te129m': 25,\n 'i129': 28,\n 'i131': 29,\n 'cs127': 30,\n 'cs134': 31,\n 'cs137': 33,\n 'ba140': 34,\n 'la140': 35,\n 'ce141': 36,\n 'ce144': 37,\n 'pm147': 38,\n 'eu154': 39,\n 'eu155': 40,\n 'pb210': 41,\n 'pb212': 42,\n 'pb214': 43,\n 'bi207': 44,\n 'bi211': 45,\n 'bi214': 46,\n 'po210': 47,\n 'rn220': 48,\n 'rn222': 49,\n 'ra223': 50,\n 'ra224': 51,\n 'ra225': 52,\n 'ra226': 53,\n 'ra228': 54,\n 'ac228': 55,\n 'th227': 56,\n 'th228': 57,\n 'th232': 59,\n 'th234': 60,\n 'pa234': 61,\n 'u234': 62,\n 'u235': 63,\n 'u238': 64,\n 'np237': 65,\n 'np239': 66,\n 'pu238': 67,\n 'pu239': 68,\n 'pu240': 69,\n 'pu241': 70,\n 'am240': 71,\n 'am241': 72,\n 'cm242': 73,\n 'cm243': 74,\n 'cm244': 75,\n 'cs134_137_tot': 76,\n 'pu239_240_tot': 77,\n 'pu239_240_iii_iv_tot': 78,\n 'pu239_240_v_vi_tot': 79,\n 'cm243_244_tot': 80,\n 'pu238_pu239_240_tot_ratio': 81,\n 'am241_pu239_240_tot_ratio': 82,\n 'cs137_134_ratio': 83,\n 'cd109': 84,\n 'eu152': 85,\n 'fe59': 86,\n 'gd153': 87,\n 'ir192': 88,\n 'pu238_240_tot': 89,\n 'rb86': 90,\n 'sc46': 91,\n 'sn113': 92,\n 'sn117m': 93,\n 'tl208': 94,\n 'mo99': 95,\n 'tc99m': 96,\n 'ru105': 97,\n 'te129': 98,\n 'te132': 99,\n 'i132': 100,\n 'i135': 101,\n 'cs136': 102,\n 'tbeta': 103,\n 'talpha': 104,\n 'i133': 105,\n 'th230': 106,\n 'pa231': 107,\n 'u236': 108,\n 'ag111': 109,\n 'in116m': 110,\n 'te123m': 111,\n 'sb127': 112,\n 'ba133': 113,\n 'ce139': 114,\n 'tl201': 116,\n 'hg203': 117,\n 'na22': 122,\n 'pa234m': 123,\n 'am243': 124,\n 'se75': 126,\n 'sr85': 127,\n 'y88': 128,\n 'ce140': 129,\n 'bi212': 130,\n 'u236_238_ratio': 131,\n 'i125': 132,\n 'ba137m': 133,\n 'u232': 134,\n 'pa233': 135,\n 'ru106_rh106_tot': 136,\n 'tu': 137,\n 'tbeta40k': 138,\n 'fe55': 139,\n 'ce144_pr144_tot': 140,\n 'pu240_pu239_ratio': 141,\n 'u233': 142,\n 'pu239_242_tot': 143,\n 'ac227': 144}\n\n\n\ndf = pd.read_csv(fname_in)\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method),\n    RenameNuclideCB(nuclides_name),\n    StandardizeUnitCB(units_lut),\n    RenameColumnCB(renaming_rules),\n    UnshiftLongitudeCB(),\n    DispatchToGroupCB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    SanitizeLonLatCB(),\n    RemapCB(fn_lut=lut_nuclides, col_remap='NUCLIDE', col_src='NUCLIDE')\n])\n\ndfs_test = tfm()\ndfs_test['SEAWATER'].NUCLIDE.unique()\n\nGroup BIOTA not found in the dataframes.\nGroup SEDIMENT not found in the dataframes.\n\n\narray([  1,  33,  28,  65,  68,  77,  69, 108, 107,  41,  47,  51,  53,\n        54, 106,  59,  60, 144,   2,  50,  57])",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "handlers/geotraces.html#netcdf-encoder",
    "href": "handlers/geotraces.html#netcdf-encoder",
    "title": "Geotraces",
    "section": "NetCDF encoder",
    "text": "NetCDF encoder\n\nExample change logs\n\ndf = pd.read_csv(fname_in)\n\ntfm = Transformer(df, cbs=[\n    SelectColsOfInterestCB(common_coi, nuclides_pattern),\n    WideToLongCB(common_coi, nuclides_pattern),\n    ExtractUnitCB(),\n    ExtractFilteringStatusCB(phase),\n    ExtractSamplingMethodCB(smp_method),\n    RenameNuclideCB(nuclides_name),\n    StandardizeUnitCB(units_lut),\n    RenameColumnCB(renaming_rules),\n    UnshiftLongitudeCB(),\n    DispatchToGroupCB(),\n    ParseTimeCB(),\n    EncodeTimeCB(),\n    SanitizeLonLatCB(),\n    RemapCB(fn_lut=lut_nuclides, col_remap='NUCLIDE', col_src='NUCLIDE')\n])\n\ntfm();\n\nGroup BIOTA not found in the dataframes.\nGroup SEDIMENT not found in the dataframes.\n\n\n\ntfm.logs\n\n['Select columns of interest.',\n '\\n    Get Geotraces nuclide names as values not column names \\n    to extract contained information (unit, sampling method, ...).\\n    ',\n '\\n    Extract units from nuclide names.\\n    ',\n 'Extract filtering status from nuclide names.',\n 'Extract sampling method from nuclide names.',\n 'Remap nuclides name to MARIS standard.',\n 'Remap unit to MARIS standard ones and apply conversion where needed.',\n 'Renaming variables to MARIS standard names.',\n 'Longitudes are coded between 0 and 360 in Geotraces. We rescale it between -180 and 180 instead.',\n 'Convert to a dictionary of dataframe with sample type (seawater,...) as keys.',\n 'Encode time as seconds since epoch.',\n 'Drop rows with invalid longitude & latitude values. Convert `,` separator to `.` separator.',\n \"Remap values from 'NUCLIDE' to 'NUCLIDE' for groups: dict_keys(['BIOTA', 'SEAWATER', 'SEDIMENT', 'SUSPENDED_MATTER']).\"]\n\n\n\n\nFeed global attributes\n\nsource\n\n\nget_attrs\n\n get_attrs (tfm, zotero_key, kw=['oceanography', 'Earth Science &gt; Oceans &gt;\n            Ocean Chemistry&gt; Radionuclides', 'Earth Science &gt; Human\n            Dimensions &gt; Environmental Impacts &gt; Nuclear Radiation\n            Exposure', 'Earth Science &gt; Oceans &gt; Ocean Chemistry &gt; Ocean\n            Tracers, Earth Science &gt; Oceans &gt; Marine Sediments', 'Earth\n            Science &gt; Oceans &gt; Ocean Chemistry, Earth Science &gt; Oceans &gt;\n            Sea Ice &gt; Isotopes', 'Earth Science &gt; Oceans &gt; Water Quality &gt;\n            Ocean Contaminants', 'Earth Science &gt; Biological\n            Classification &gt; Animals/Vertebrates &gt; Fish', 'Earth Science &gt;\n            Biosphere &gt; Ecosystems &gt; Marine Ecosystems', 'Earth Science &gt;\n            Biological Classification &gt; Animals/Invertebrates &gt; Mollusks',\n            'Earth Science &gt; Biological Classification &gt;\n            Animals/Invertebrates &gt; Arthropods &gt; Crustaceans', 'Earth\n            Science &gt; Biological Classification &gt; Plants &gt; Macroalgae\n            (Seaweeds)'])\n\nRetrieve global attributes from Geotraces dataset.\n\n\nExported source\ndef get_attrs(tfm, zotero_key, kw=kw):\n    \"Retrieve global attributes from Geotraces dataset.\"\n    return GlobAttrsFeeder(tfm.dfs, cbs=[\n        BboxCB(),\n        DepthRangeCB(),\n        TimeRangeCB(),\n        ZoteroCB(zotero_key, cfg=cfg()),\n        KeyValuePairCB('keywords', ', '.join(kw)),\n        KeyValuePairCB('publisher_postprocess_logs', ', '.join(tfm.logs))\n        ])()\n\n\n\nzotero_metadata = get_attrs(tfm, zotero_key=zotero_key, kw=kw)\nprint('Keys: ', zotero_metadata.keys())\nprint('Title: ', zotero_metadata['title'])\n\nKeys:  dict_keys(['geospatial_lat_min', 'geospatial_lat_max', 'geospatial_lon_min', 'geospatial_lon_max', 'geospatial_bounds', 'geospatial_vertical_max', 'geospatial_vertical_min', 'time_coverage_start', 'time_coverage_end', 'id', 'title', 'summary', 'creator_name', 'keywords', 'publisher_postprocess_logs'])\nTitle:  The GEOTRACES Intermediate Data Product 2017\n\n\n\n\nEncoding\n\nsource\n\n\nencode\n\n encode (fname_in, fname_out, **kwargs)\n\n\nencode(fname_in, fname_out, verbose=False)\n\nGroup BIOTA not found in the dataframes.\nGroup SEDIMENT not found in the dataframes.\n\n\nTODO:\n\nAdd salinity, temperature, oxygen variables\n\n\ndecode(fname_in=fname_out, verbose=True)\n\nSaved SEAWATER to ../../_data/output/190-geotraces-2021_SEAWATER.csv\nSaved SUSPENDED_MATTER to ../../_data/output/190-geotraces-2021_SUSPENDED_MATTER.csv",
    "crumbs": [
      "Handlers",
      "Geotraces"
    ]
  },
  {
    "objectID": "cli/init.html",
    "href": "cli/init.html",
    "title": "marisco",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nsource\n\nmain\n\n main ()\n\nCreate configuration files & download lookup tables"
  },
  {
    "objectID": "cli/db_to_nc.html",
    "href": "cli/db_to_nc.html",
    "title": "marisco",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nsource\n\nimport_handler\n\n import_handler (handler_name, fn_name='encode')\n\n\nsource\n\n\nmain\n\n main (src:str, dest:str, ref_ids:str='')\n\n*Convert MARIS legacy database to NetCDF4 format.\nIf ref_ids is provided as comma-separated values, only encodes those subsets.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsrc\nstr\n\nPath to MARIS database dump as .txt file\n\n\ndest\nstr\n\nOutput path for NetCDF file(s)\n\n\nref_ids\nstr\n\nOptional comma-separated reference IDs (e.g., “123,456,789”)\n\n\nReturns\nNone"
  },
  {
    "objectID": "metadata/data-curation-rules.html",
    "href": "metadata/data-curation-rules.html",
    "title": "Data curation rules",
    "section": "",
    "text": "What are the data curation rules applied to MARIS data compilation?\nThis document outlines the rules and guidelines followed when curating data in the MARIS database, including: - Data validation criteria - Quality control measures - Standardization procedures - Handling of duplicate measurements - Treatment of missing values",
    "crumbs": [
      "Metadata",
      "Data curation rules"
    ]
  },
  {
    "objectID": "metadata/data-curation-rules.html#sample-preparation-methods",
    "href": "metadata/data-curation-rules.html#sample-preparation-methods",
    "title": "Data curation rules",
    "section": "Sample preparation methods",
    "text": "Sample preparation methods\nAt some occasions, biota samples measurements are reported both in dry and wet weight for a given sample and nuclide.\nWhen for a given sample and nuclide, sample preparation methods are reported, MARIS will follow the following rules:\n\nif sample preparation method is reported as wet only, then report measurement on a wet basis (in Bq/kg wet)\nif sample preparation method is reported as dry only, then report measurement on a dry basis (in Bq/kg dry )\nif sample preparation method is reported as both wet and dry, then report measurement on a wet basis (in Bq/kg wet) but also calculate and report percent wet weight.\n\nWIP …",
    "crumbs": [
      "Metadata",
      "Data curation rules"
    ]
  },
  {
    "objectID": "metadata/enum_rules.html",
    "href": "metadata/enum_rules.html",
    "title": "Enum rules",
    "section": "",
    "text": "Enums are created from a lookup table (LUT) to map the values in the source data to the values in the NetCDF file. An Enum can be created using the nc.createEnumType method. The createEnumType(self, datatype, datatype_name, enum_dict) method requires three arguments:\n\ndatatype: The data type of the enum, e.g. np.int64.\ndatatype_name: The name of the enum, e.g. 'sed_type_t'.\nenum_dict: A dictionary that maps the values in the source data to the values in the NetCDF file, e.g. {'Not applicable': -1, 'Not available': 0, 'Clay': 1, 'Gravel': 2, ...}.\n\nThe key of the enum_dict dictionary cannot contain illegal characters.\nIllegal Characters and Constraints:\nSpecial Characters:\n Names cannot include the characters:\n      / (forward slash)\n     \\ (backslash)\n    - . (dot) at the beginning of a name\n    - @ (at symbol)\n    - : (colon)\n    - Control characters (ASCII codes 0–31 and 127)\nReserved Characters:\nNames starting with _ are reserved for system use in certain NetCDF conventions.\nLets write a function that checks if the key of the enum_dict dictionary contains illegal characters or reserved characters.\n\nimport pandas as pd\nfrom marisco.configs import NC_DTYPES, lut_path\n\n\ndef check_lut_characters():\n    illegal_chars = ['\\\\', '@', ':', '•', '\"'] + [chr(i) for i in range(32)] + [chr(127)]\n    reserved_start_char = '_'\n    print('illegal_chars', illegal_chars)\n    for lut_name, lut_details in NC_DTYPES.items():\n        print(lut_name)\n        # Construct the file path\n        file_path = lut_path() / lut_details['fname']\n        \n        # Read the Excel file into a DataFrame\n        df = pd.read_excel(file_path)\n        \n        # Construct enum_dict using the specified columns for keys and values\n        enum_dict = {df[lut_details['key']][i]: df[lut_details['value']][i] for i in df.index}\n        # Check each key in the enum_dict\n        for key in enum_dict.keys():\n            \n            key_str = str(key)  # Ensure the key is a string for checks\n            if any(char in key_str for char in illegal_chars):\n                print(f\"Key '{key_str}' in LUT '{lut_name}' contains an illegal character.\")\n            if key_str.startswith(reserved_start_char):\n                print(f\"Key '{key_str}' in LUT '{lut_name}' starts with a reserved character.\")\n\n\ncheck_lut_characters()\n\nillegal_chars ['\\\\', '@', ':', '•', '\"', '\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06', '\\x07', '\\x08', '\\t', '\\n', '\\x0b', '\\x0c', '\\r', '\\x0e', '\\x0f', '\\x10', '\\x11', '\\x12', '\\x13', '\\x14', '\\x15', '\\x16', '\\x17', '\\x18', '\\x19', '\\x1a', '\\x1b', '\\x1c', '\\x1d', '\\x1e', '\\x1f', '\\x7f']\nAREA\nKey 'Mediterranean Sea - Western Basin_x000D_\n' in LUT 'AREA' contains an illegal character.\nBIO_GROUP\nBODY_PART\nCOUNT_MET\nDL\nFILT\nNUCLIDE\nPREP_MET\nSAMP_MET\nSED_TYPE\nSPECIES\nUNIT\nLAB"
  }
]